{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "angry-brooklyn",
   "metadata": {},
   "source": [
    "### Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-beaver",
   "metadata": {},
   "source": [
    "Numpy is the core library for scientific calculation/computing in Python. It provides a high performance multidimensional array object, and tools for working with these arrays. MATLAB is very similar to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "peripheral-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-technical",
   "metadata": {},
   "source": [
    "### LinAlg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-johnson",
   "metadata": {},
   "source": [
    "It includes very useful algebra tools: norm, inv, solve, det, eig, eigvalues, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "illegal-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "realistic-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "la?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "found-secret",
   "metadata": {},
   "source": [
    "### Scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-gravity",
   "metadata": {},
   "source": [
    "Library used for scientific and technical computing. It contains modules for optimization, linear algebra, integration, interpolation, special functions, FFT, signal and image processing, ODE solvers, and other tasks common in science and enginerring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "suburban-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as scio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "optimum-advocacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "scio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aafd0eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cf685d3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function pdist in module scipy.spatial.distance:\n",
      "\n",
      "pdist(X, metric='euclidean', *args, **kwargs)\n",
      "    Pairwise distances between observations in n-dimensional space.\n",
      "    \n",
      "    See Notes for common calling conventions.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : ndarray\n",
      "        An m by n array of m original observations in an\n",
      "        n-dimensional space.\n",
      "    metric : str or function, optional\n",
      "        The distance metric to use. The distance function can\n",
      "        be 'braycurtis', 'canberra', 'chebyshev', 'cityblock',\n",
      "        'correlation', 'cosine', 'dice', 'euclidean', 'hamming',\n",
      "        'jaccard', 'jensenshannon', 'kulsinski', 'mahalanobis', 'matching',\n",
      "        'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',\n",
      "        'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule'.\n",
      "    *args : tuple. Deprecated.\n",
      "        Additional arguments should be passed as keyword arguments\n",
      "    **kwargs : dict, optional\n",
      "        Extra arguments to `metric`: refer to each metric documentation for a\n",
      "        list of all possible arguments.\n",
      "    \n",
      "        Some possible arguments:\n",
      "    \n",
      "        p : scalar\n",
      "        The p-norm to apply for Minkowski, weighted and unweighted.\n",
      "        Default: 2.\n",
      "    \n",
      "        w : ndarray\n",
      "        The weight vector for metrics that support weights (e.g., Minkowski).\n",
      "    \n",
      "        V : ndarray\n",
      "        The variance vector for standardized Euclidean.\n",
      "        Default: var(X, axis=0, ddof=1)\n",
      "    \n",
      "        VI : ndarray\n",
      "        The inverse of the covariance matrix for Mahalanobis.\n",
      "        Default: inv(cov(X.T)).T\n",
      "    \n",
      "        out : ndarray.\n",
      "        The output array\n",
      "        If not None, condensed distance matrix Y is stored in this array.\n",
      "        Note: metric independent, it will become a regular keyword arg in a\n",
      "        future scipy version\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Y : ndarray\n",
      "        Returns a condensed distance matrix Y.  For\n",
      "        each :math:`i` and :math:`j` (where :math:`i<j<m`),where m is the number\n",
      "        of original observations. The metric ``dist(u=X[i], v=X[j])``\n",
      "        is computed and stored in entry \n",
      "        ``m * i + j - ((i + 2) * (i + 1)) // 2``.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    squareform : converts between condensed distance matrices and\n",
      "                 square distance matrices.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    See ``squareform`` for information on how to calculate the index of\n",
      "    this entry or to convert the condensed distance matrix to a\n",
      "    redundant square matrix.\n",
      "    \n",
      "    The following are common calling conventions.\n",
      "    \n",
      "    1. ``Y = pdist(X, 'euclidean')``\n",
      "    \n",
      "       Computes the distance between m points using Euclidean distance\n",
      "       (2-norm) as the distance metric between the points. The points\n",
      "       are arranged as m n-dimensional row vectors in the matrix X.\n",
      "    \n",
      "    2. ``Y = pdist(X, 'minkowski', p=2.)``\n",
      "    \n",
      "       Computes the distances using the Minkowski distance\n",
      "       :math:`||u-v||_p` (p-norm) where :math:`p \\geq 1`.\n",
      "    \n",
      "    3. ``Y = pdist(X, 'cityblock')``\n",
      "    \n",
      "       Computes the city block or Manhattan distance between the\n",
      "       points.\n",
      "    \n",
      "    4. ``Y = pdist(X, 'seuclidean', V=None)``\n",
      "    \n",
      "       Computes the standardized Euclidean distance. The standardized\n",
      "       Euclidean distance between two n-vectors ``u`` and ``v`` is\n",
      "    \n",
      "       .. math::\n",
      "    \n",
      "          \\sqrt{\\sum {(u_i-v_i)^2 / V[x_i]}}\n",
      "    \n",
      "    \n",
      "       V is the variance vector; V[i] is the variance computed over all\n",
      "       the i'th components of the points.  If not passed, it is\n",
      "       automatically computed.\n",
      "    \n",
      "    5. ``Y = pdist(X, 'sqeuclidean')``\n",
      "    \n",
      "       Computes the squared Euclidean distance :math:`||u-v||_2^2` between\n",
      "       the vectors.\n",
      "    \n",
      "    6. ``Y = pdist(X, 'cosine')``\n",
      "    \n",
      "       Computes the cosine distance between vectors u and v,\n",
      "    \n",
      "       .. math::\n",
      "    \n",
      "          1 - \\frac{u \\cdot v}\n",
      "                   {{||u||}_2 {||v||}_2}\n",
      "    \n",
      "       where :math:`||*||_2` is the 2-norm of its argument ``*``, and\n",
      "       :math:`u \\cdot v` is the dot product of ``u`` and ``v``.\n",
      "    \n",
      "    7. ``Y = pdist(X, 'correlation')``\n",
      "    \n",
      "       Computes the correlation distance between vectors u and v. This is\n",
      "    \n",
      "       .. math::\n",
      "    \n",
      "          1 - \\frac{(u - \\bar{u}) \\cdot (v - \\bar{v})}\n",
      "                   {{||(u - \\bar{u})||}_2 {||(v - \\bar{v})||}_2}\n",
      "    \n",
      "       where :math:`\\bar{v}` is the mean of the elements of vector v,\n",
      "       and :math:`x \\cdot y` is the dot product of :math:`x` and :math:`y`.\n",
      "    \n",
      "    8. ``Y = pdist(X, 'hamming')``\n",
      "    \n",
      "       Computes the normalized Hamming distance, or the proportion of\n",
      "       those vector elements between two n-vectors ``u`` and ``v``\n",
      "       which disagree. To save memory, the matrix ``X`` can be of type\n",
      "       boolean.\n",
      "    \n",
      "    9. ``Y = pdist(X, 'jaccard')``\n",
      "    \n",
      "       Computes the Jaccard distance between the points. Given two\n",
      "       vectors, ``u`` and ``v``, the Jaccard distance is the\n",
      "       proportion of those elements ``u[i]`` and ``v[i]`` that\n",
      "       disagree.\n",
      "    \n",
      "    10. ``Y = pdist(X, 'chebyshev')``\n",
      "    \n",
      "       Computes the Chebyshev distance between the points. The\n",
      "       Chebyshev distance between two n-vectors ``u`` and ``v`` is the\n",
      "       maximum norm-1 distance between their respective elements. More\n",
      "       precisely, the distance is given by\n",
      "    \n",
      "       .. math::\n",
      "    \n",
      "          d(u,v) = \\max_i {|u_i-v_i|}\n",
      "    \n",
      "    11. ``Y = pdist(X, 'canberra')``\n",
      "    \n",
      "       Computes the Canberra distance between the points. The\n",
      "       Canberra distance between two points ``u`` and ``v`` is\n",
      "    \n",
      "       .. math::\n",
      "    \n",
      "         d(u,v) = \\sum_i \\frac{|u_i-v_i|}\n",
      "                              {|u_i|+|v_i|}\n",
      "    \n",
      "    \n",
      "    12. ``Y = pdist(X, 'braycurtis')``\n",
      "    \n",
      "       Computes the Bray-Curtis distance between the points. The\n",
      "       Bray-Curtis distance between two points ``u`` and ``v`` is\n",
      "    \n",
      "    \n",
      "       .. math::\n",
      "    \n",
      "            d(u,v) = \\frac{\\sum_i {|u_i-v_i|}}\n",
      "                           {\\sum_i {|u_i+v_i|}}\n",
      "    \n",
      "    13. ``Y = pdist(X, 'mahalanobis', VI=None)``\n",
      "    \n",
      "       Computes the Mahalanobis distance between the points. The\n",
      "       Mahalanobis distance between two points ``u`` and ``v`` is\n",
      "       :math:`\\sqrt{(u-v)(1/V)(u-v)^T}` where :math:`(1/V)` (the ``VI``\n",
      "       variable) is the inverse covariance. If ``VI`` is not None,\n",
      "       ``VI`` will be used as the inverse covariance matrix.\n",
      "    \n",
      "    14. ``Y = pdist(X, 'yule')``\n",
      "    \n",
      "       Computes the Yule distance between each pair of boolean\n",
      "       vectors. (see yule function documentation)\n",
      "    \n",
      "    15. ``Y = pdist(X, 'matching')``\n",
      "    \n",
      "       Synonym for 'hamming'.\n",
      "    \n",
      "    16. ``Y = pdist(X, 'dice')``\n",
      "    \n",
      "       Computes the Dice distance between each pair of boolean\n",
      "       vectors. (see dice function documentation)\n",
      "    \n",
      "    17. ``Y = pdist(X, 'kulsinski')``\n",
      "    \n",
      "       Computes the Kulsinski distance between each pair of\n",
      "       boolean vectors. (see kulsinski function documentation)\n",
      "    \n",
      "    18. ``Y = pdist(X, 'rogerstanimoto')``\n",
      "    \n",
      "       Computes the Rogers-Tanimoto distance between each pair of\n",
      "       boolean vectors. (see rogerstanimoto function documentation)\n",
      "    \n",
      "    19. ``Y = pdist(X, 'russellrao')``\n",
      "    \n",
      "       Computes the Russell-Rao distance between each pair of\n",
      "       boolean vectors. (see russellrao function documentation)\n",
      "    \n",
      "    20. ``Y = pdist(X, 'sokalmichener')``\n",
      "    \n",
      "       Computes the Sokal-Michener distance between each pair of\n",
      "       boolean vectors. (see sokalmichener function documentation)\n",
      "    \n",
      "    21. ``Y = pdist(X, 'sokalsneath')``\n",
      "    \n",
      "       Computes the Sokal-Sneath distance between each pair of\n",
      "       boolean vectors. (see sokalsneath function documentation)\n",
      "    \n",
      "    22. ``Y = pdist(X, 'wminkowski', p=2, w=w)``\n",
      "    \n",
      "       Computes the weighted Minkowski distance between each pair of\n",
      "       vectors. (see wminkowski function documentation)\n",
      "    \n",
      "       'wminkowski' is deprecated and will be removed in SciPy 1.8.0.\n",
      "       Use 'minkowski' instead.\n",
      "    \n",
      "    23. ``Y = pdist(X, f)``\n",
      "    \n",
      "       Computes the distance between all pairs of vectors in X\n",
      "       using the user supplied 2-arity function f. For example,\n",
      "       Euclidean distance between the vectors could be computed\n",
      "       as follows::\n",
      "    \n",
      "         dm = pdist(X, lambda u, v: np.sqrt(((u-v)**2).sum()))\n",
      "    \n",
      "       Note that you should avoid passing a reference to one of\n",
      "       the distance functions defined in this library. For example,::\n",
      "    \n",
      "         dm = pdist(X, sokalsneath)\n",
      "    \n",
      "       would calculate the pair-wise distances between the vectors in\n",
      "       X using the Python function sokalsneath. This would result in\n",
      "       sokalsneath being called :math:`{n \\choose 2}` times, which\n",
      "       is inefficient. Instead, the optimized C version is more\n",
      "       efficient, and we call it using the following syntax.::\n",
      "    \n",
      "         dm = pdist(X, 'sokalsneath')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pdist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "566ef151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76fd7846",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function dendrogram in module scipy.cluster.hierarchy:\n",
      "\n",
      "dendrogram(Z, p=30, truncate_mode=None, color_threshold=None, get_leaves=True, orientation='top', labels=None, count_sort=False, distance_sort=False, show_leaf_counts=True, no_plot=False, no_labels=False, leaf_font_size=None, leaf_rotation=None, leaf_label_func=None, show_contracted=False, link_color_func=None, ax=None, above_threshold_color='C0')\n",
      "    Plot the hierarchical clustering as a dendrogram.\n",
      "    \n",
      "    The dendrogram illustrates how each cluster is\n",
      "    composed by drawing a U-shaped link between a non-singleton\n",
      "    cluster and its children. The top of the U-link indicates a\n",
      "    cluster merge. The two legs of the U-link indicate which clusters\n",
      "    were merged. The length of the two legs of the U-link represents\n",
      "    the distance between the child clusters. It is also the\n",
      "    cophenetic distance between original observations in the two\n",
      "    children clusters.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    Z : ndarray\n",
      "        The linkage matrix encoding the hierarchical clustering to\n",
      "        render as a dendrogram. See the ``linkage`` function for more\n",
      "        information on the format of ``Z``.\n",
      "    p : int, optional\n",
      "        The ``p`` parameter for ``truncate_mode``.\n",
      "    truncate_mode : str, optional\n",
      "        The dendrogram can be hard to read when the original\n",
      "        observation matrix from which the linkage is derived is\n",
      "        large. Truncation is used to condense the dendrogram. There\n",
      "        are several modes:\n",
      "    \n",
      "        ``None``\n",
      "          No truncation is performed (default).\n",
      "          Note: ``'none'`` is an alias for ``None`` that's kept for\n",
      "          backward compatibility.\n",
      "    \n",
      "        ``'lastp'``\n",
      "          The last ``p`` non-singleton clusters formed in the linkage are the\n",
      "          only non-leaf nodes in the linkage; they correspond to rows\n",
      "          ``Z[n-p-2:end]`` in ``Z``. All other non-singleton clusters are\n",
      "          contracted into leaf nodes.\n",
      "    \n",
      "        ``'level'``\n",
      "          No more than ``p`` levels of the dendrogram tree are displayed.\n",
      "          A \"level\" includes all nodes with ``p`` merges from the last merge.\n",
      "    \n",
      "          Note: ``'mtica'`` is an alias for ``'level'`` that's kept for\n",
      "          backward compatibility.\n",
      "    \n",
      "    color_threshold : double, optional\n",
      "        For brevity, let :math:`t` be the ``color_threshold``.\n",
      "        Colors all the descendent links below a cluster node\n",
      "        :math:`k` the same color if :math:`k` is the first node below\n",
      "        the cut threshold :math:`t`. All links connecting nodes with\n",
      "        distances greater than or equal to the threshold are colored\n",
      "        with de default matplotlib color ``'C0'``. If :math:`t` is less\n",
      "        than or equal to zero, all nodes are colored ``'C0'``.\n",
      "        If ``color_threshold`` is None or 'default',\n",
      "        corresponding with MATLAB(TM) behavior, the threshold is set to\n",
      "        ``0.7*max(Z[:,2])``.\n",
      "    \n",
      "    get_leaves : bool, optional\n",
      "        Includes a list ``R['leaves']=H`` in the result\n",
      "        dictionary. For each :math:`i`, ``H[i] == j``, cluster node\n",
      "        ``j`` appears in position ``i`` in the left-to-right traversal\n",
      "        of the leaves, where :math:`j < 2n-1` and :math:`i < n`.\n",
      "    orientation : str, optional\n",
      "        The direction to plot the dendrogram, which can be any\n",
      "        of the following strings:\n",
      "    \n",
      "        ``'top'``\n",
      "          Plots the root at the top, and plot descendent links going downwards.\n",
      "          (default).\n",
      "    \n",
      "        ``'bottom'``\n",
      "          Plots the root at the bottom, and plot descendent links going\n",
      "          upwards.\n",
      "    \n",
      "        ``'left'``\n",
      "          Plots the root at the left, and plot descendent links going right.\n",
      "    \n",
      "        ``'right'``\n",
      "          Plots the root at the right, and plot descendent links going left.\n",
      "    \n",
      "    labels : ndarray, optional\n",
      "        By default, ``labels`` is None so the index of the original observation\n",
      "        is used to label the leaf nodes.  Otherwise, this is an :math:`n`-sized\n",
      "        sequence, with ``n == Z.shape[0] + 1``. The ``labels[i]`` value is the\n",
      "        text to put under the :math:`i` th leaf node only if it corresponds to\n",
      "        an original observation and not a non-singleton cluster.\n",
      "    count_sort : str or bool, optional\n",
      "        For each node n, the order (visually, from left-to-right) n's\n",
      "        two descendent links are plotted is determined by this\n",
      "        parameter, which can be any of the following values:\n",
      "    \n",
      "        ``False``\n",
      "          Nothing is done.\n",
      "    \n",
      "        ``'ascending'`` or ``True``\n",
      "          The child with the minimum number of original objects in its cluster\n",
      "          is plotted first.\n",
      "    \n",
      "        ``'descending'``\n",
      "          The child with the maximum number of original objects in its cluster\n",
      "          is plotted first.\n",
      "    \n",
      "        Note, ``distance_sort`` and ``count_sort`` cannot both be True.\n",
      "    distance_sort : str or bool, optional\n",
      "        For each node n, the order (visually, from left-to-right) n's\n",
      "        two descendent links are plotted is determined by this\n",
      "        parameter, which can be any of the following values:\n",
      "    \n",
      "        ``False``\n",
      "          Nothing is done.\n",
      "    \n",
      "        ``'ascending'`` or ``True``\n",
      "          The child with the minimum distance between its direct descendents is\n",
      "          plotted first.\n",
      "    \n",
      "        ``'descending'``\n",
      "          The child with the maximum distance between its direct descendents is\n",
      "          plotted first.\n",
      "    \n",
      "        Note ``distance_sort`` and ``count_sort`` cannot both be True.\n",
      "    show_leaf_counts : bool, optional\n",
      "         When True, leaf nodes representing :math:`k>1` original\n",
      "         observation are labeled with the number of observations they\n",
      "         contain in parentheses.\n",
      "    no_plot : bool, optional\n",
      "        When True, the final rendering is not performed. This is\n",
      "        useful if only the data structures computed for the rendering\n",
      "        are needed or if matplotlib is not available.\n",
      "    no_labels : bool, optional\n",
      "        When True, no labels appear next to the leaf nodes in the\n",
      "        rendering of the dendrogram.\n",
      "    leaf_rotation : double, optional\n",
      "        Specifies the angle (in degrees) to rotate the leaf\n",
      "        labels. When unspecified, the rotation is based on the number of\n",
      "        nodes in the dendrogram (default is 0).\n",
      "    leaf_font_size : int, optional\n",
      "        Specifies the font size (in points) of the leaf labels. When\n",
      "        unspecified, the size based on the number of nodes in the\n",
      "        dendrogram.\n",
      "    leaf_label_func : lambda or function, optional\n",
      "        When leaf_label_func is a callable function, for each\n",
      "        leaf with cluster index :math:`k < 2n-1`. The function\n",
      "        is expected to return a string with the label for the\n",
      "        leaf.\n",
      "    \n",
      "        Indices :math:`k < n` correspond to original observations\n",
      "        while indices :math:`k \\geq n` correspond to non-singleton\n",
      "        clusters.\n",
      "    \n",
      "        For example, to label singletons with their node id and\n",
      "        non-singletons with their id, count, and inconsistency\n",
      "        coefficient, simply do::\n",
      "    \n",
      "            # First define the leaf label function.\n",
      "            def llf(id):\n",
      "                if id < n:\n",
      "                    return str(id)\n",
      "                else:\n",
      "                    return '[%d %d %1.2f]' % (id, count, R[n-id,3])\n",
      "            # The text for the leaf nodes is going to be big so force\n",
      "            # a rotation of 90 degrees.\n",
      "            dendrogram(Z, leaf_label_func=llf, leaf_rotation=90)\n",
      "    \n",
      "    show_contracted : bool, optional\n",
      "        When True the heights of non-singleton nodes contracted\n",
      "        into a leaf node are plotted as crosses along the link\n",
      "        connecting that leaf node.  This really is only useful when\n",
      "        truncation is used (see ``truncate_mode`` parameter).\n",
      "    link_color_func : callable, optional\n",
      "        If given, `link_color_function` is called with each non-singleton id\n",
      "        corresponding to each U-shaped link it will paint. The function is\n",
      "        expected to return the color to paint the link, encoded as a matplotlib\n",
      "        color string code. For example::\n",
      "    \n",
      "            dendrogram(Z, link_color_func=lambda k: colors[k])\n",
      "    \n",
      "        colors the direct links below each untruncated non-singleton node\n",
      "        ``k`` using ``colors[k]``.\n",
      "    ax : matplotlib Axes instance, optional\n",
      "        If None and `no_plot` is not True, the dendrogram will be plotted\n",
      "        on the current axes.  Otherwise if `no_plot` is not True the\n",
      "        dendrogram will be plotted on the given ``Axes`` instance. This can be\n",
      "        useful if the dendrogram is part of a more complex figure.\n",
      "    above_threshold_color : str, optional\n",
      "        This matplotlib color string sets the color of the links above the\n",
      "        color_threshold. The default is ``'C0'``.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    R : dict\n",
      "        A dictionary of data structures computed to render the\n",
      "        dendrogram. Its has the following keys:\n",
      "    \n",
      "        ``'color_list'``\n",
      "          A list of color names. The k'th element represents the color of the\n",
      "          k'th link.\n",
      "    \n",
      "        ``'icoord'`` and ``'dcoord'``\n",
      "          Each of them is a list of lists. Let ``icoord = [I1, I2, ..., Ip]``\n",
      "          where ``Ik = [xk1, xk2, xk3, xk4]`` and ``dcoord = [D1, D2, ..., Dp]``\n",
      "          where ``Dk = [yk1, yk2, yk3, yk4]``, then the k'th link painted is\n",
      "          ``(xk1, yk1)`` - ``(xk2, yk2)`` - ``(xk3, yk3)`` - ``(xk4, yk4)``.\n",
      "    \n",
      "        ``'ivl'``\n",
      "          A list of labels corresponding to the leaf nodes.\n",
      "    \n",
      "        ``'leaves'``\n",
      "          For each i, ``H[i] == j``, cluster node ``j`` appears in position\n",
      "          ``i`` in the left-to-right traversal of the leaves, where\n",
      "          :math:`j < 2n-1` and :math:`i < n`. If ``j`` is less than ``n``, the\n",
      "          ``i``-th leaf node corresponds to an original observation.\n",
      "          Otherwise, it corresponds to a non-singleton cluster.\n",
      "    \n",
      "        ``'leaves_color_list'``\n",
      "          A list of color names. The k'th element represents the color of the\n",
      "          k'th leaf.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    linkage, set_link_color_palette\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    It is expected that the distances in ``Z[:,2]`` be monotonic, otherwise\n",
      "    crossings appear in the dendrogram.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from scipy.cluster import hierarchy\n",
      "    >>> import matplotlib.pyplot as plt\n",
      "    \n",
      "    A very basic example:\n",
      "    \n",
      "    >>> ytdist = np.array([662., 877., 255., 412., 996., 295., 468., 268.,\n",
      "    ...                    400., 754., 564., 138., 219., 869., 669.])\n",
      "    >>> Z = hierarchy.linkage(ytdist, 'single')\n",
      "    >>> plt.figure()\n",
      "    >>> dn = hierarchy.dendrogram(Z)\n",
      "    \n",
      "    Now, plot in given axes, improve the color scheme and use both vertical and\n",
      "    horizontal orientations:\n",
      "    \n",
      "    >>> hierarchy.set_link_color_palette(['m', 'c', 'y', 'k'])\n",
      "    >>> fig, axes = plt.subplots(1, 2, figsize=(8, 3))\n",
      "    >>> dn1 = hierarchy.dendrogram(Z, ax=axes[0], above_threshold_color='y',\n",
      "    ...                            orientation='top')\n",
      "    >>> dn2 = hierarchy.dendrogram(Z, ax=axes[1],\n",
      "    ...                            above_threshold_color='#bcbddc',\n",
      "    ...                            orientation='right')\n",
      "    >>> hierarchy.set_link_color_palette(None)  # reset to default after use\n",
      "    >>> plt.show()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dendrogram?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bba635ed",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function linkage in module scipy.cluster.hierarchy:\n",
      "\n",
      "linkage(y, method='single', metric='euclidean', optimal_ordering=False)\n",
      "    Perform hierarchical/agglomerative clustering.\n",
      "    \n",
      "    The input y may be either a 1-D condensed distance matrix\n",
      "    or a 2-D array of observation vectors.\n",
      "    \n",
      "    If y is a 1-D condensed distance matrix,\n",
      "    then y must be a :math:`\\binom{n}{2}` sized\n",
      "    vector, where n is the number of original observations paired\n",
      "    in the distance matrix. The behavior of this function is very\n",
      "    similar to the MATLAB linkage function.\n",
      "    \n",
      "    A :math:`(n-1)` by 4 matrix ``Z`` is returned. At the\n",
      "    :math:`i`-th iteration, clusters with indices ``Z[i, 0]`` and\n",
      "    ``Z[i, 1]`` are combined to form cluster :math:`n + i`. A\n",
      "    cluster with an index less than :math:`n` corresponds to one of\n",
      "    the :math:`n` original observations. The distance between\n",
      "    clusters ``Z[i, 0]`` and ``Z[i, 1]`` is given by ``Z[i, 2]``. The\n",
      "    fourth value ``Z[i, 3]`` represents the number of original\n",
      "    observations in the newly formed cluster.\n",
      "    \n",
      "    The following linkage methods are used to compute the distance\n",
      "    :math:`d(s, t)` between two clusters :math:`s` and\n",
      "    :math:`t`. The algorithm begins with a forest of clusters that\n",
      "    have yet to be used in the hierarchy being formed. When two\n",
      "    clusters :math:`s` and :math:`t` from this forest are combined\n",
      "    into a single cluster :math:`u`, :math:`s` and :math:`t` are\n",
      "    removed from the forest, and :math:`u` is added to the\n",
      "    forest. When only one cluster remains in the forest, the algorithm\n",
      "    stops, and this cluster becomes the root.\n",
      "    \n",
      "    A distance matrix is maintained at each iteration. The ``d[i,j]``\n",
      "    entry corresponds to the distance between cluster :math:`i` and\n",
      "    :math:`j` in the original forest.\n",
      "    \n",
      "    At each iteration, the algorithm must update the distance matrix\n",
      "    to reflect the distance of the newly formed cluster u with the\n",
      "    remaining clusters in the forest.\n",
      "    \n",
      "    Suppose there are :math:`|u|` original observations\n",
      "    :math:`u[0], \\ldots, u[|u|-1]` in cluster :math:`u` and\n",
      "    :math:`|v|` original objects :math:`v[0], \\ldots, v[|v|-1]` in\n",
      "    cluster :math:`v`. Recall, :math:`s` and :math:`t` are\n",
      "    combined to form cluster :math:`u`. Let :math:`v` be any\n",
      "    remaining cluster in the forest that is not :math:`u`.\n",
      "    \n",
      "    The following are methods for calculating the distance between the\n",
      "    newly formed cluster :math:`u` and each :math:`v`.\n",
      "    \n",
      "      * method='single' assigns\n",
      "    \n",
      "        .. math::\n",
      "           d(u,v) = \\min(dist(u[i],v[j]))\n",
      "    \n",
      "        for all points :math:`i` in cluster :math:`u` and\n",
      "        :math:`j` in cluster :math:`v`. This is also known as the\n",
      "        Nearest Point Algorithm.\n",
      "    \n",
      "      * method='complete' assigns\n",
      "    \n",
      "        .. math::\n",
      "           d(u, v) = \\max(dist(u[i],v[j]))\n",
      "    \n",
      "        for all points :math:`i` in cluster u and :math:`j` in\n",
      "        cluster :math:`v`. This is also known by the Farthest Point\n",
      "        Algorithm or Voor Hees Algorithm.\n",
      "    \n",
      "      * method='average' assigns\n",
      "    \n",
      "        .. math::\n",
      "           d(u,v) = \\sum_{ij} \\frac{d(u[i], v[j])}\n",
      "                                   {(|u|*|v|)}\n",
      "    \n",
      "        for all points :math:`i` and :math:`j` where :math:`|u|`\n",
      "        and :math:`|v|` are the cardinalities of clusters :math:`u`\n",
      "        and :math:`v`, respectively. This is also called the UPGMA\n",
      "        algorithm.\n",
      "    \n",
      "      * method='weighted' assigns\n",
      "    \n",
      "        .. math::\n",
      "           d(u,v) = (dist(s,v) + dist(t,v))/2\n",
      "    \n",
      "        where cluster u was formed with cluster s and t and v\n",
      "        is a remaining cluster in the forest (also called WPGMA).\n",
      "    \n",
      "      * method='centroid' assigns\n",
      "    \n",
      "        .. math::\n",
      "           dist(s,t) = ||c_s-c_t||_2\n",
      "    \n",
      "        where :math:`c_s` and :math:`c_t` are the centroids of\n",
      "        clusters :math:`s` and :math:`t`, respectively. When two\n",
      "        clusters :math:`s` and :math:`t` are combined into a new\n",
      "        cluster :math:`u`, the new centroid is computed over all the\n",
      "        original objects in clusters :math:`s` and :math:`t`. The\n",
      "        distance then becomes the Euclidean distance between the\n",
      "        centroid of :math:`u` and the centroid of a remaining cluster\n",
      "        :math:`v` in the forest. This is also known as the UPGMC\n",
      "        algorithm.\n",
      "    \n",
      "      * method='median' assigns :math:`d(s,t)` like the ``centroid``\n",
      "        method. When two clusters :math:`s` and :math:`t` are combined\n",
      "        into a new cluster :math:`u`, the average of centroids s and t\n",
      "        give the new centroid :math:`u`. This is also known as the\n",
      "        WPGMC algorithm.\n",
      "    \n",
      "      * method='ward' uses the Ward variance minimization algorithm.\n",
      "        The new entry :math:`d(u,v)` is computed as follows,\n",
      "    \n",
      "        .. math::\n",
      "    \n",
      "           d(u,v) = \\sqrt{\\frac{|v|+|s|}\n",
      "                               {T}d(v,s)^2\n",
      "                        + \\frac{|v|+|t|}\n",
      "                               {T}d(v,t)^2\n",
      "                        - \\frac{|v|}\n",
      "                               {T}d(s,t)^2}\n",
      "    \n",
      "        where :math:`u` is the newly joined cluster consisting of\n",
      "        clusters :math:`s` and :math:`t`, :math:`v` is an unused\n",
      "        cluster in the forest, :math:`T=|v|+|s|+|t|`, and\n",
      "        :math:`|*|` is the cardinality of its argument. This is also\n",
      "        known as the incremental algorithm.\n",
      "    \n",
      "    Warning: When the minimum distance pair in the forest is chosen, there\n",
      "    may be two or more pairs with the same minimum distance. This\n",
      "    implementation may choose a different minimum than the MATLAB\n",
      "    version.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    y : ndarray\n",
      "        A condensed distance matrix. A condensed distance matrix\n",
      "        is a flat array containing the upper triangular of the distance matrix.\n",
      "        This is the form that ``pdist`` returns. Alternatively, a collection of\n",
      "        :math:`m` observation vectors in :math:`n` dimensions may be passed as\n",
      "        an :math:`m` by :math:`n` array. All elements of the condensed distance\n",
      "        matrix must be finite, i.e., no NaNs or infs.\n",
      "    method : str, optional\n",
      "        The linkage algorithm to use. See the ``Linkage Methods`` section below\n",
      "        for full descriptions.\n",
      "    metric : str or function, optional\n",
      "        The distance metric to use in the case that y is a collection of\n",
      "        observation vectors; ignored otherwise. See the ``pdist``\n",
      "        function for a list of valid distance metrics. A custom distance\n",
      "        function can also be used.\n",
      "    optimal_ordering : bool, optional\n",
      "        If True, the linkage matrix will be reordered so that the distance\n",
      "        between successive leaves is minimal. This results in a more intuitive\n",
      "        tree structure when the data are visualized. defaults to False, because\n",
      "        this algorithm can be slow, particularly on large datasets [2]_. See\n",
      "        also the `optimal_leaf_ordering` function.\n",
      "    \n",
      "        .. versionadded:: 1.0.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Z : ndarray\n",
      "        The hierarchical clustering encoded as a linkage matrix.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    1. For method 'single', an optimized algorithm based on minimum spanning\n",
      "       tree is implemented. It has time complexity :math:`O(n^2)`.\n",
      "       For methods 'complete', 'average', 'weighted' and 'ward', an algorithm\n",
      "       called nearest-neighbors chain is implemented. It also has time\n",
      "       complexity :math:`O(n^2)`.\n",
      "       For other methods, a naive algorithm is implemented with :math:`O(n^3)`\n",
      "       time complexity.\n",
      "       All algorithms use :math:`O(n^2)` memory.\n",
      "       Refer to [1]_ for details about the algorithms.\n",
      "    2. Methods 'centroid', 'median', and 'ward' are correctly defined only if\n",
      "       Euclidean pairwise metric is used. If `y` is passed as precomputed\n",
      "       pairwise distances, then it is the user's responsibility to assure that\n",
      "       these distances are in fact Euclidean, otherwise the produced result\n",
      "       will be incorrect.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    scipy.spatial.distance.pdist : pairwise distance metrics\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Daniel Mullner, \"Modern hierarchical, agglomerative clustering\n",
      "           algorithms\", :arXiv:`1109.2378v1`.\n",
      "    .. [2] Ziv Bar-Joseph, David K. Gifford, Tommi S. Jaakkola, \"Fast optimal\n",
      "           leaf ordering for hierarchical clustering\", 2001. Bioinformatics\n",
      "           :doi:`10.1093/bioinformatics/17.suppl_1.S22`\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from scipy.cluster.hierarchy import dendrogram, linkage\n",
      "    >>> from matplotlib import pyplot as plt\n",
      "    >>> X = [[i] for i in [2, 8, 0, 4, 1, 9, 9, 0]]\n",
      "    \n",
      "    >>> Z = linkage(X, 'ward')\n",
      "    >>> fig = plt.figure(figsize=(25, 10))\n",
      "    >>> dn = dendrogram(Z)\n",
      "    \n",
      "    >>> Z = linkage(X, 'single')\n",
      "    >>> fig = plt.figure(figsize=(25, 10))\n",
      "    >>> dn = dendrogram(Z)\n",
      "    >>> plt.show()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linkage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18c7c67c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fcluster in module scipy.cluster.hierarchy:\n",
      "\n",
      "fcluster(Z, t, criterion='inconsistent', depth=2, R=None, monocrit=None)\n",
      "    Form flat clusters from the hierarchical clustering defined by\n",
      "    the given linkage matrix.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    Z : ndarray\n",
      "        The hierarchical clustering encoded with the matrix returned\n",
      "        by the `linkage` function.\n",
      "    t : scalar\n",
      "        For criteria 'inconsistent', 'distance' or 'monocrit',\n",
      "         this is the threshold to apply when forming flat clusters.\n",
      "        For 'maxclust' or 'maxclust_monocrit' criteria,\n",
      "         this would be max number of clusters requested.\n",
      "    criterion : str, optional\n",
      "        The criterion to use in forming flat clusters. This can\n",
      "        be any of the following values:\n",
      "    \n",
      "          ``inconsistent`` :\n",
      "              If a cluster node and all its\n",
      "              descendants have an inconsistent value less than or equal\n",
      "              to `t`, then all its leaf descendants belong to the\n",
      "              same flat cluster. When no non-singleton cluster meets\n",
      "              this criterion, every node is assigned to its own\n",
      "              cluster. (Default)\n",
      "    \n",
      "          ``distance`` :\n",
      "              Forms flat clusters so that the original\n",
      "              observations in each flat cluster have no greater a\n",
      "              cophenetic distance than `t`.\n",
      "    \n",
      "          ``maxclust`` :\n",
      "              Finds a minimum threshold ``r`` so that\n",
      "              the cophenetic distance between any two original\n",
      "              observations in the same flat cluster is no more than\n",
      "              ``r`` and no more than `t` flat clusters are formed.\n",
      "    \n",
      "          ``monocrit`` :\n",
      "              Forms a flat cluster from a cluster node c\n",
      "              with index i when ``monocrit[j] <= t``.\n",
      "    \n",
      "              For example, to threshold on the maximum mean distance\n",
      "              as computed in the inconsistency matrix R with a\n",
      "              threshold of 0.8 do::\n",
      "    \n",
      "                  MR = maxRstat(Z, R, 3)\n",
      "                  fcluster(Z, t=0.8, criterion='monocrit', monocrit=MR)\n",
      "    \n",
      "          ``maxclust_monocrit`` :\n",
      "              Forms a flat cluster from a\n",
      "              non-singleton cluster node ``c`` when ``monocrit[i] <=\n",
      "              r`` for all cluster indices ``i`` below and including\n",
      "              ``c``. ``r`` is minimized such that no more than ``t``\n",
      "              flat clusters are formed. monocrit must be\n",
      "              monotonic. For example, to minimize the threshold t on\n",
      "              maximum inconsistency values so that no more than 3 flat\n",
      "              clusters are formed, do::\n",
      "    \n",
      "                  MI = maxinconsts(Z, R)\n",
      "                  fcluster(Z, t=3, criterion='maxclust_monocrit', monocrit=MI)\n",
      "    \n",
      "    depth : int, optional\n",
      "        The maximum depth to perform the inconsistency calculation.\n",
      "        It has no meaning for the other criteria. Default is 2.\n",
      "    R : ndarray, optional\n",
      "        The inconsistency matrix to use for the 'inconsistent'\n",
      "        criterion. This matrix is computed if not provided.\n",
      "    monocrit : ndarray, optional\n",
      "        An array of length n-1. `monocrit[i]` is the\n",
      "        statistics upon which non-singleton i is thresholded. The\n",
      "        monocrit vector must be monotonic, i.e., given a node c with\n",
      "        index i, for all node indices j corresponding to nodes\n",
      "        below c, ``monocrit[i] >= monocrit[j]``.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    fcluster : ndarray\n",
      "        An array of length ``n``. ``T[i]`` is the flat cluster number to\n",
      "        which original observation ``i`` belongs.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    linkage : for information about hierarchical clustering methods work.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from scipy.cluster.hierarchy import ward, fcluster\n",
      "    >>> from scipy.spatial.distance import pdist\n",
      "    \n",
      "    All cluster linkage methods - e.g., `scipy.cluster.hierarchy.ward`\n",
      "    generate a linkage matrix ``Z`` as their output:\n",
      "    \n",
      "    >>> X = [[0, 0], [0, 1], [1, 0],\n",
      "    ...      [0, 4], [0, 3], [1, 4],\n",
      "    ...      [4, 0], [3, 0], [4, 1],\n",
      "    ...      [4, 4], [3, 4], [4, 3]]\n",
      "    \n",
      "    >>> Z = ward(pdist(X))\n",
      "    \n",
      "    >>> Z\n",
      "    array([[ 0.        ,  1.        ,  1.        ,  2.        ],\n",
      "           [ 3.        ,  4.        ,  1.        ,  2.        ],\n",
      "           [ 6.        ,  7.        ,  1.        ,  2.        ],\n",
      "           [ 9.        , 10.        ,  1.        ,  2.        ],\n",
      "           [ 2.        , 12.        ,  1.29099445,  3.        ],\n",
      "           [ 5.        , 13.        ,  1.29099445,  3.        ],\n",
      "           [ 8.        , 14.        ,  1.29099445,  3.        ],\n",
      "           [11.        , 15.        ,  1.29099445,  3.        ],\n",
      "           [16.        , 17.        ,  5.77350269,  6.        ],\n",
      "           [18.        , 19.        ,  5.77350269,  6.        ],\n",
      "           [20.        , 21.        ,  8.16496581, 12.        ]])\n",
      "    \n",
      "    This matrix represents a dendrogram, where the first and second elements\n",
      "    are the two clusters merged at each step, the third element is the\n",
      "    distance between these clusters, and the fourth element is the size of\n",
      "    the new cluster - the number of original data points included.\n",
      "    \n",
      "    `scipy.cluster.hierarchy.fcluster` can be used to flatten the\n",
      "    dendrogram, obtaining as a result an assignation of the original data\n",
      "    points to single clusters.\n",
      "    \n",
      "    This assignation mostly depends on a distance threshold ``t`` - the maximum\n",
      "    inter-cluster distance allowed:\n",
      "    \n",
      "    >>> fcluster(Z, t=0.9, criterion='distance')\n",
      "    array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)\n",
      "    \n",
      "    >>> fcluster(Z, t=1.1, criterion='distance')\n",
      "    array([1, 1, 2, 3, 3, 4, 5, 5, 6, 7, 7, 8], dtype=int32)\n",
      "    \n",
      "    >>> fcluster(Z, t=3, criterion='distance')\n",
      "    array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4], dtype=int32)\n",
      "    \n",
      "    >>> fcluster(Z, t=9, criterion='distance')\n",
      "    array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)\n",
      "    \n",
      "    In the first case, the threshold ``t`` is too small to allow any two\n",
      "    samples in the data to form a cluster, so 12 different clusters are\n",
      "    returned.\n",
      "    \n",
      "    In the second case, the threshold is large enough to allow the first\n",
      "    4 points to be merged with their nearest neighbors. So, here, only 8\n",
      "    clusters are returned.\n",
      "    \n",
      "    The third case, with a much higher threshold, allows for up to 8 data\n",
      "    points to be connected - so 4 clusters are returned here.\n",
      "    \n",
      "    Lastly, the threshold of the fourth case is large enough to allow for\n",
      "    all data points to be merged together - so a single cluster is returned.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fcluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5ef2fb",
   "metadata": {},
   "source": [
    "### Sklearn (Scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6523a6",
   "metadata": {},
   "source": [
    "Scikit-learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities.\n",
    "\n",
    "scikit-learn comes with a few standard datasets, for instance the iris and digits datasets for classification and the diabetes dataset for regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd0c6ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "903ddcc9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.datasets in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.datasets\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.datasets` module includes utilities to load datasets,\n",
      "    including methods to load and fetch popular reference datasets. It also\n",
      "    features some artificial data generators.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _base\n",
      "    _california_housing\n",
      "    _covtype\n",
      "    _kddcup99\n",
      "    _lfw\n",
      "    _olivetti_faces\n",
      "    _openml\n",
      "    _rcv1\n",
      "    _samples_generator\n",
      "    _species_distributions\n",
      "    _svmlight_format_fast\n",
      "    _svmlight_format_io\n",
      "    _twenty_newsgroups\n",
      "    setup\n",
      "    tests (package)\n",
      "\n",
      "FUNCTIONS\n",
      "    clear_data_home(data_home=None)\n",
      "        Delete all the content of the data home cache.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data_home : str, default=None\n",
      "            The path to scikit-learn data directory. If `None`, the default path\n",
      "            is `~/sklearn_learn_data`.\n",
      "    \n",
      "    dump_svmlight_file(X, y, f, *, zero_based=True, comment=None, query_id=None, multilabel=False)\n",
      "        Dump the dataset in svmlight / libsvm file format.\n",
      "        \n",
      "        This format is a text-based format, with one sample per line. It does\n",
      "        not store zero valued features hence is suitable for sparse dataset.\n",
      "        \n",
      "        The first element of each line can be used to store a target variable\n",
      "        to predict.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Training vectors, where n_samples is the number of samples and\n",
      "            n_features is the number of features.\n",
      "        \n",
      "        y : {array-like, sparse matrix}, shape = [n_samples (, n_labels)]\n",
      "            Target values. Class labels must be an\n",
      "            integer or float, or array-like objects of integer or float for\n",
      "            multilabel classifications.\n",
      "        \n",
      "        f : string or file-like in binary mode\n",
      "            If string, specifies the path that will contain the data.\n",
      "            If file-like, data will be written to f. f should be opened in binary\n",
      "            mode.\n",
      "        \n",
      "        zero_based : boolean, default=True\n",
      "            Whether column indices should be written zero-based (True) or one-based\n",
      "            (False).\n",
      "        \n",
      "        comment : string, default=None\n",
      "            Comment to insert at the top of the file. This should be either a\n",
      "            Unicode string, which will be encoded as UTF-8, or an ASCII byte\n",
      "            string.\n",
      "            If a comment is given, then it will be preceded by one that identifies\n",
      "            the file as having been dumped by scikit-learn. Note that not all\n",
      "            tools grok comments in SVMlight files.\n",
      "        \n",
      "        query_id : array-like of shape (n_samples,), default=None\n",
      "            Array containing pairwise preference constraints (qid in svmlight\n",
      "            format).\n",
      "        \n",
      "        multilabel : boolean, default=False\n",
      "            Samples may have several labels each (see\n",
      "            https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html)\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "               parameter *multilabel* to support multilabel datasets.\n",
      "    \n",
      "    fetch_20newsgroups(*, data_home=None, subset='train', categories=None, shuffle=True, random_state=42, remove=(), download_if_missing=True, return_X_y=False)\n",
      "        Load the filenames and data from the 20 newsgroups dataset (classification).\n",
      "        \n",
      "        Download it if necessary.\n",
      "        \n",
      "        =================   ==========\n",
      "        Classes                     20\n",
      "        Samples total            18846\n",
      "        Dimensionality               1\n",
      "        Features                  text\n",
      "        =================   ==========\n",
      "        \n",
      "        Read more in the :ref:`User Guide <20newsgroups_dataset>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data_home : str, default=None\n",
      "            Specify a download and cache folder for the datasets. If None,\n",
      "            all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "        \n",
      "        subset : {'train', 'test', 'all'}, default='train'\n",
      "            Select the dataset to load: 'train' for the training set, 'test'\n",
      "            for the test set, 'all' for both, with shuffled ordering.\n",
      "        \n",
      "        categories : array-like, dtype=str or unicode, default=None\n",
      "            If None (default), load all the categories.\n",
      "            If not None, list of category names to load (other categories\n",
      "            ignored).\n",
      "        \n",
      "        shuffle : bool, default=True\n",
      "            Whether or not to shuffle the data: might be important for models that\n",
      "            make the assumption that the samples are independent and identically\n",
      "            distributed (i.i.d.), such as stochastic gradient descent.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for dataset shuffling. Pass an int\n",
      "            for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        remove : tuple, default=()\n",
      "            May contain any subset of ('headers', 'footers', 'quotes'). Each of\n",
      "            these are kinds of text that will be detected and removed from the\n",
      "            newsgroup posts, preventing classifiers from overfitting on\n",
      "            metadata.\n",
      "        \n",
      "            'headers' removes newsgroup headers, 'footers' removes blocks at the\n",
      "            ends of posts that look like signatures, and 'quotes' removes lines\n",
      "            that appear to be quoting another post.\n",
      "        \n",
      "            'headers' follows an exact standard; the other filters are not always\n",
      "            correct.\n",
      "        \n",
      "        download_if_missing : bool, default=True\n",
      "            If False, raise an IOError if the data is not locally available\n",
      "            instead of trying to download the data from the source site.\n",
      "        \n",
      "        return_X_y : bool, default=False\n",
      "            If True, returns `(data.data, data.target)` instead of a Bunch\n",
      "            object.\n",
      "        \n",
      "            .. versionadded:: 0.22\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        bunch : :class:`~sklearn.utils.Bunch`\n",
      "            Dictionary-like object, with the following attributes.\n",
      "        \n",
      "            data : list of shape (n_samples,)\n",
      "                The data list to learn.\n",
      "            target: ndarray of shape (n_samples,)\n",
      "                The target labels.\n",
      "            filenames: list of shape (n_samples,)\n",
      "                The path to the location of the data.\n",
      "            DESCR: str\n",
      "                The full description of the dataset.\n",
      "            target_names: list of shape (n_classes,)\n",
      "                The names of target classes.\n",
      "        \n",
      "        (data, target) : tuple if `return_X_y=True`\n",
      "            .. versionadded:: 0.22\n",
      "    \n",
      "    fetch_20newsgroups_vectorized(*, subset='train', remove=(), data_home=None, download_if_missing=True, return_X_y=False, normalize=True, as_frame=False)\n",
      "        Load and vectorize the 20 newsgroups dataset (classification).\n",
      "        \n",
      "        Download it if necessary.\n",
      "        \n",
      "        This is a convenience function; the transformation is done using the\n",
      "        default settings for\n",
      "        :class:`~sklearn.feature_extraction.text.CountVectorizer`. For more\n",
      "        advanced usage (stopword filtering, n-gram extraction, etc.), combine\n",
      "        fetch_20newsgroups with a custom\n",
      "        :class:`~sklearn.feature_extraction.text.CountVectorizer`,\n",
      "        :class:`~sklearn.feature_extraction.text.HashingVectorizer`,\n",
      "        :class:`~sklearn.feature_extraction.text.TfidfTransformer` or\n",
      "        :class:`~sklearn.feature_extraction.text.TfidfVectorizer`.\n",
      "        \n",
      "        The resulting counts are normalized using\n",
      "        :func:`sklearn.preprocessing.normalize` unless normalize is set to False.\n",
      "        \n",
      "        =================   ==========\n",
      "        Classes                     20\n",
      "        Samples total            18846\n",
      "        Dimensionality          130107\n",
      "        Features                  real\n",
      "        =================   ==========\n",
      "        \n",
      "        Read more in the :ref:`User Guide <20newsgroups_dataset>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        subset : {'train', 'test', 'all'}, default='train'\n",
      "            Select the dataset to load: 'train' for the training set, 'test'\n",
      "            for the test set, 'all' for both, with shuffled ordering.\n",
      "        \n",
      "        remove : tuple, default=()\n",
      "            May contain any subset of ('headers', 'footers', 'quotes'). Each of\n",
      "            these are kinds of text that will be detected and removed from the\n",
      "            newsgroup posts, preventing classifiers from overfitting on\n",
      "            metadata.\n",
      "        \n",
      "            'headers' removes newsgroup headers, 'footers' removes blocks at the\n",
      "            ends of posts that look like signatures, and 'quotes' removes lines\n",
      "            that appear to be quoting another post.\n",
      "        \n",
      "        data_home : str, default=None\n",
      "            Specify an download and cache folder for the datasets. If None,\n",
      "            all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "        \n",
      "        download_if_missing : bool, default=True\n",
      "            If False, raise an IOError if the data is not locally available\n",
      "            instead of trying to download the data from the source site.\n",
      "        \n",
      "        return_X_y : bool, default=False\n",
      "            If True, returns ``(data.data, data.target)`` instead of a Bunch\n",
      "            object.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If True, normalizes each document's feature vector to unit norm using\n",
      "            :func:`sklearn.preprocessing.normalize`.\n",
      "        \n",
      "            .. versionadded:: 0.22\n",
      "        \n",
      "        as_frame : bool, default=False\n",
      "            If True, the data is a pandas DataFrame including columns with\n",
      "            appropriate dtypes (numeric, string, or categorical). The target is\n",
      "            a pandas DataFrame or Series depending on the number of\n",
      "            `target_columns`.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        bunch : :class:`~sklearn.utils.Bunch`\n",
      "            Dictionary-like object, with the following attributes.\n",
      "        \n",
      "            data: {sparse matrix, dataframe} of shape (n_samples, n_features)\n",
      "                The input data matrix. If ``as_frame`` is `True`, ``data`` is\n",
      "                a pandas DataFrame with sparse columns.\n",
      "            target: {ndarray, series} of shape (n_samples,)\n",
      "                The target labels. If ``as_frame`` is `True`, ``target`` is a\n",
      "                pandas Series.\n",
      "            target_names: list of shape (n_classes,)\n",
      "                The names of target classes.\n",
      "            DESCR: str\n",
      "                The full description of the dataset.\n",
      "            frame: dataframe of shape (n_samples, n_features + 1)\n",
      "                Only present when `as_frame=True`. Pandas DataFrame with ``data``\n",
      "                and ``target``.\n",
      "        \n",
      "                .. versionadded:: 0.24\n",
      "        \n",
      "        (data, target) : tuple if ``return_X_y`` is True\n",
      "            `data` and `target` would be of the format defined in the `Bunch`\n",
      "            description above.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "    \n",
      "    fetch_california_housing(*, data_home=None, download_if_missing=True, return_X_y=False, as_frame=False)\n",
      "        Load the California housing dataset (regression).\n",
      "        \n",
      "        ==============   ==============\n",
      "        Samples total             20640\n",
      "        Dimensionality                8\n",
      "        Features                   real\n",
      "        Target           real 0.15 - 5.\n",
      "        ==============   ==============\n",
      "        \n",
      "        Read more in the :ref:`User Guide <california_housing_dataset>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data_home : str, default=None\n",
      "            Specify another download and cache folder for the datasets. By default\n",
      "            all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "        \n",
      "        download_if_missing : bool, default=True\n",
      "            If False, raise a IOError if the data is not locally available\n",
      "            instead of trying to download the data from the source site.\n",
      "        \n",
      "        \n",
      "        return_X_y : bool, default=False.\n",
      "            If True, returns ``(data.data, data.target)`` instead of a Bunch\n",
      "            object.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        as_frame : bool, default=False\n",
      "            If True, the data is a pandas DataFrame including columns with\n",
      "            appropriate dtypes (numeric, string or categorical). The target is\n",
      "            a pandas DataFrame or Series depending on the number of target_columns.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dataset : :class:`~sklearn.utils.Bunch`\n",
      "            Dictionary-like object, with the following attributes.\n",
      "        \n",
      "            data : ndarray, shape (20640, 8)\n",
      "                Each row corresponding to the 8 feature values in order.\n",
      "                If ``as_frame`` is True, ``data`` is a pandas object.\n",
      "            target : numpy array of shape (20640,)\n",
      "                Each value corresponds to the average\n",
      "                house value in units of 100,000.\n",
      "                If ``as_frame`` is True, ``target`` is a pandas object.\n",
      "            feature_names : list of length 8\n",
      "                Array of ordered feature names used in the dataset.\n",
      "            DESCR : string\n",
      "                Description of the California housing dataset.\n",
      "            frame : pandas DataFrame\n",
      "                Only present when `as_frame=True`. DataFrame with ``data`` and\n",
      "                ``target``.\n",
      "        \n",
      "                .. versionadded:: 0.23\n",
      "        \n",
      "        (data, target) : tuple if ``return_X_y`` is True\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        This dataset consists of 20,640 samples and 9 features.\n",
      "    \n",
      "    fetch_covtype(*, data_home=None, download_if_missing=True, random_state=None, shuffle=False, return_X_y=False, as_frame=False)\n",
      "        Load the covertype dataset (classification).\n",
      "        \n",
      "        Download it if necessary.\n",
      "        \n",
      "        =================   ============\n",
      "        Classes                        7\n",
      "        Samples total             581012\n",
      "        Dimensionality                54\n",
      "        Features                     int\n",
      "        =================   ============\n",
      "        \n",
      "        Read more in the :ref:`User Guide <covtype_dataset>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data_home : str, default=None\n",
      "            Specify another download and cache folder for the datasets. By default\n",
      "            all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "        \n",
      "        download_if_missing : bool, default=True\n",
      "            If False, raise a IOError if the data is not locally available\n",
      "            instead of trying to download the data from the source site.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for dataset shuffling. Pass an int\n",
      "            for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        shuffle : bool, default=False\n",
      "            Whether to shuffle dataset.\n",
      "        \n",
      "        return_X_y : bool, default=False\n",
      "            If True, returns ``(data.data, data.target)`` instead of a Bunch\n",
      "            object.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        as_frame : bool, default=False\n",
      "            If True, the data is a pandas DataFrame including columns with\n",
      "            appropriate dtypes (numeric). The target is a pandas DataFrame or\n",
      "            Series depending on the number of target columns. If `return_X_y` is\n",
      "            True, then (`data`, `target`) will be pandas DataFrames or Series as\n",
      "            described below.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dataset : :class:`~sklearn.utils.Bunch`\n",
      "            Dictionary-like object, with the following attributes.\n",
      "        \n",
      "            data : ndarray of shape (581012, 54)\n",
      "                Each row corresponds to the 54 features in the dataset.\n",
      "            target : ndarray of shape (581012,)\n",
      "                Each value corresponds to one of\n",
      "                the 7 forest covertypes with values\n",
      "                ranging between 1 to 7.\n",
      "            frame : dataframe of shape (581012, 55)\n",
      "                Only present when `as_frame=True`. Contains `data` and `target`.\n",
      "            DESCR : str\n",
      "                Description of the forest covertype dataset.\n",
      "            feature_names : list\n",
      "                The names of the dataset columns.\n",
      "            target_names: list\n",
      "                The names of the target columns.\n",
      "        \n",
      "        (data, target) : tuple if ``return_X_y`` is True\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "    \n",
      "    fetch_kddcup99(*, subset=None, data_home=None, shuffle=False, random_state=None, percent10=True, download_if_missing=True, return_X_y=False, as_frame=False)\n",
      "        Load the kddcup99 dataset (classification).\n",
      "        \n",
      "        Download it if necessary.\n",
      "        \n",
      "        =================   ====================================\n",
      "        Classes                                               23\n",
      "        Samples total                                    4898431\n",
      "        Dimensionality                                        41\n",
      "        Features            discrete (int) or continuous (float)\n",
      "        =================   ====================================\n",
      "        \n",
      "        Read more in the :ref:`User Guide <kddcup99_dataset>`.\n",
      "        \n",
      "        .. versionadded:: 0.18\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        subset : {'SA', 'SF', 'http', 'smtp'}, default=None\n",
      "            To return the corresponding classical subsets of kddcup 99.\n",
      "            If None, return the entire kddcup 99 dataset.\n",
      "        \n",
      "        data_home : str, default=None\n",
      "            Specify another download and cache folder for the datasets. By default\n",
      "            all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "            .. versionadded:: 0.19\n",
      "        \n",
      "        shuffle : bool, default=False\n",
      "            Whether to shuffle dataset.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for dataset shuffling and for\n",
      "            selection of abnormal samples if `subset='SA'`. Pass an int for\n",
      "            reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        percent10 : bool, default=True\n",
      "            Whether to load only 10 percent of the data.\n",
      "        \n",
      "        download_if_missing : bool, default=True\n",
      "            If False, raise a IOError if the data is not locally available\n",
      "            instead of trying to download the data from the source site.\n",
      "        \n",
      "        return_X_y : bool, default=False\n",
      "            If True, returns ``(data, target)`` instead of a Bunch object. See\n",
      "            below for more information about the `data` and `target` object.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        as_frame : bool, default=False\n",
      "            If `True`, returns a pandas Dataframe for the ``data`` and ``target``\n",
      "            objects in the `Bunch` returned object; `Bunch` return object will also\n",
      "            have a ``frame`` member.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : :class:`~sklearn.utils.Bunch`\n",
      "            Dictionary-like object, with the following attributes.\n",
      "        \n",
      "            data : {ndarray, dataframe} of shape (494021, 41)\n",
      "                The data matrix to learn. If `as_frame=True`, `data` will be a\n",
      "                pandas DataFrame.\n",
      "            target : {ndarray, series} of shape (494021,)\n",
      "                The regression target for each sample. If `as_frame=True`, `target`\n",
      "                will be a pandas Series.\n",
      "            frame : dataframe of shape (494021, 42)\n",
      "                Only present when `as_frame=True`. Contains `data` and `target`.\n",
      "            DESCR : str\n",
      "                The full description of the dataset.\n",
      "            feature_names : list\n",
      "                The names of the dataset columns\n",
      "            target_names: list\n",
      "                The names of the target columns\n",
      "        \n",
      "        (data, target) : tuple if ``return_X_y`` is True\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "    \n",
      "    fetch_lfw_pairs(*, subset='train', data_home=None, funneled=True, resize=0.5, color=False, slice_=(slice(70, 195, None), slice(78, 172, None)), download_if_missing=True)\n",
      "        Load the Labeled Faces in the Wild (LFW) pairs dataset (classification).\n",
      "        \n",
      "        Download it if necessary.\n",
      "        \n",
      "        =================   =======================\n",
      "        Classes                                   2\n",
      "        Samples total                         13233\n",
      "        Dimensionality                         5828\n",
      "        Features            real, between 0 and 255\n",
      "        =================   =======================\n",
      "        \n",
      "        In the official `README.txt`_ this task is described as the\n",
      "        \"Restricted\" task.  As I am not sure as to implement the\n",
      "        \"Unrestricted\" variant correctly, I left it as unsupported for now.\n",
      "        \n",
      "          .. _`README.txt`: http://vis-www.cs.umass.edu/lfw/README.txt\n",
      "        \n",
      "        The original images are 250 x 250 pixels, but the default slice and resize\n",
      "        arguments reduce them to 62 x 47.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <labeled_faces_in_the_wild_dataset>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        subset : {'train', 'test', '10_folds'}, default='train'\n",
      "            Select the dataset to load: 'train' for the development training\n",
      "            set, 'test' for the development test set, and '10_folds' for the\n",
      "            official evaluation set that is meant to be used with a 10-folds\n",
      "            cross validation.\n",
      "        \n",
      "        data_home : str, default=None\n",
      "            Specify another download and cache folder for the datasets. By\n",
      "            default all scikit-learn data is stored in '~/scikit_learn_data'\n",
      "            subfolders.\n",
      "        \n",
      "        funneled : bool, default=True\n",
      "            Download and use the funneled variant of the dataset.\n",
      "        \n",
      "        resize : float, default=0.5\n",
      "            Ratio used to resize the each face picture.\n",
      "        \n",
      "        color : bool, default=False\n",
      "            Keep the 3 RGB channels instead of averaging them to a single\n",
      "            gray level channel. If color is True the shape of the data has\n",
      "            one more dimension than the shape with color = False.\n",
      "        \n",
      "        slice_ : tuple of slice, default=(slice(70, 195), slice(78, 172))\n",
      "            Provide a custom 2D slice (height, width) to extract the\n",
      "            'interesting' part of the jpeg files and avoid use statistical\n",
      "            correlation from the background\n",
      "        \n",
      "        download_if_missing : bool, default=True\n",
      "            If False, raise a IOError if the data is not locally available\n",
      "            instead of trying to download the data from the source site.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : :class:`~sklearn.utils.Bunch`\n",
      "            Dictionary-like object, with the following attributes.\n",
      "        \n",
      "            data : ndarray of shape (2200, 5828). Shape depends on ``subset``.\n",
      "                Each row corresponds to 2 ravel'd face images\n",
      "                of original size 62 x 47 pixels.\n",
      "                Changing the ``slice_``, ``resize`` or ``subset`` parameters\n",
      "                will change the shape of the output.\n",
      "            pairs : ndarray of shape (2200, 2, 62, 47). Shape depends on ``subset``\n",
      "                Each row has 2 face images corresponding\n",
      "                to same or different person from the dataset\n",
      "                containing 5749 people. Changing the ``slice_``,\n",
      "                ``resize`` or ``subset`` parameters will change the shape of the\n",
      "                output.\n",
      "            target : numpy array of shape (2200,). Shape depends on ``subset``.\n",
      "                Labels associated to each pair of images.\n",
      "                The two label values being different persons or the same person.\n",
      "            DESCR : string\n",
      "                Description of the Labeled Faces in the Wild (LFW) dataset.\n",
      "    \n",
      "    fetch_lfw_people(*, data_home=None, funneled=True, resize=0.5, min_faces_per_person=0, color=False, slice_=(slice(70, 195, None), slice(78, 172, None)), download_if_missing=True, return_X_y=False)\n",
      "        Load the Labeled Faces in the Wild (LFW) people dataset (classification).\n",
      "        \n",
      "        Download it if necessary.\n",
      "        \n",
      "        =================   =======================\n",
      "        Classes                                5749\n",
      "        Samples total                         13233\n",
      "        Dimensionality                         5828\n",
      "        Features            real, between 0 and 255\n",
      "        =================   =======================\n",
      "        \n",
      "        Read more in the :ref:`User Guide <labeled_faces_in_the_wild_dataset>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data_home : str, default=None\n",
      "            Specify another download and cache folder for the datasets. By default\n",
      "            all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "        \n",
      "        funneled : bool, default=True\n",
      "            Download and use the funneled variant of the dataset.\n",
      "        \n",
      "        resize : float, default=0.5\n",
      "            Ratio used to resize the each face picture.\n",
      "        \n",
      "        min_faces_per_person : int, default=None\n",
      "            The extracted dataset will only retain pictures of people that have at\n",
      "            least `min_faces_per_person` different pictures.\n",
      "        \n",
      "        color : bool, default=False\n",
      "            Keep the 3 RGB channels instead of averaging them to a single\n",
      "            gray level channel. If color is True the shape of the data has\n",
      "            one more dimension than the shape with color = False.\n",
      "        \n",
      "        slice_ : tuple of slice, default=(slice(70, 195), slice(78, 172))\n",
      "            Provide a custom 2D slice (height, width) to extract the\n",
      "            'interesting' part of the jpeg files and avoid use statistical\n",
      "            correlation from the background\n",
      "        \n",
      "        download_if_missing : bool, default=True\n",
      "            If False, raise a IOError if the data is not locally available\n",
      "            instead of trying to download the data from the source site.\n",
      "        \n",
      "        return_X_y : bool, default=False\n",
      "            If True, returns ``(dataset.data, dataset.target)`` instead of a Bunch\n",
      "            object. See below for more information about the `dataset.data` and\n",
      "            `dataset.target` object.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dataset : :class:`~sklearn.utils.Bunch`\n",
      "            Dictionary-like object, with the following attributes.\n",
      "        \n",
      "            data : numpy array of shape (13233, 2914)\n",
      "                Each row corresponds to a ravelled face image\n",
      "                of original size 62 x 47 pixels.\n",
      "                Changing the ``slice_`` or resize parameters will change the\n",
      "                shape of the output.\n",
      "            images : numpy array of shape (13233, 62, 47)\n",
      "                Each row is a face image corresponding to one of the 5749 people in\n",
      "                the dataset. Changing the ``slice_``\n",
      "                or resize parameters will change the shape of the output.\n",
      "            target : numpy array of shape (13233,)\n",
      "                Labels associated to each face image.\n",
      "                Those labels range from 0-5748 and correspond to the person IDs.\n",
      "            DESCR : string\n",
      "                Description of the Labeled Faces in the Wild (LFW) dataset.\n",
      "        \n",
      "        (data, target) : tuple if ``return_X_y`` is True\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "    \n",
      "    fetch_olivetti_faces(*, data_home=None, shuffle=False, random_state=0, download_if_missing=True, return_X_y=False)\n",
      "        Load the Olivetti faces data-set from AT&T (classification).\n",
      "        \n",
      "        Download it if necessary.\n",
      "        \n",
      "        =================   =====================\n",
      "        Classes                                40\n",
      "        Samples total                         400\n",
      "        Dimensionality                       4096\n",
      "        Features            real, between 0 and 1\n",
      "        =================   =====================\n",
      "        \n",
      "        Read more in the :ref:`User Guide <olivetti_faces_dataset>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data_home : str, default=None\n",
      "            Specify another download and cache folder for the datasets. By default\n",
      "            all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "        \n",
      "        shuffle : bool, default=False\n",
      "            If True the order of the dataset is shuffled to avoid having\n",
      "            images of the same person grouped.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=0\n",
      "            Determines random number generation for dataset shuffling. Pass an int\n",
      "            for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        download_if_missing : bool, default=True\n",
      "            If False, raise a IOError if the data is not locally available\n",
      "            instead of trying to download the data from the source site.\n",
      "        \n",
      "        return_X_y : bool, default=False\n",
      "            If True, returns `(data, target)` instead of a `Bunch` object. See\n",
      "            below for more information about the `data` and `target` object.\n",
      "        \n",
      "            .. versionadded:: 0.22\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : :class:`~sklearn.utils.Bunch`\n",
      "            Dictionary-like object, with the following attributes.\n",
      "        \n",
      "            data: ndarray, shape (400, 4096)\n",
      "                Each row corresponds to a ravelled\n",
      "                face image of original size 64 x 64 pixels.\n",
      "            images : ndarray, shape (400, 64, 64)\n",
      "                Each row is a face image\n",
      "                corresponding to one of the 40 subjects of the dataset.\n",
      "            target : ndarray, shape (400,)\n",
      "                Labels associated to each face image.\n",
      "                Those labels are ranging from 0-39 and correspond to the\n",
      "                Subject IDs.\n",
      "            DESCR : str\n",
      "                Description of the modified Olivetti Faces Dataset.\n",
      "        \n",
      "        (data, target) : tuple if `return_X_y=True`\n",
      "            .. versionadded:: 0.22\n",
      "    \n",
      "    fetch_openml(name: Optional[str] = None, *, version: Union[str, int] = 'active', data_id: Optional[int] = None, data_home: Optional[str] = None, target_column: Union[str, List, NoneType] = 'default-target', cache: bool = True, return_X_y: bool = False, as_frame: Union[str, bool] = 'auto')\n",
      "        Fetch dataset from openml by name or dataset id.\n",
      "        \n",
      "        Datasets are uniquely identified by either an integer ID or by a\n",
      "        combination of name and version (i.e. there might be multiple\n",
      "        versions of the 'iris' dataset). Please give either name or data_id\n",
      "        (not both). In case a name is given, a version can also be\n",
      "        provided.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <openml>`.\n",
      "        \n",
      "        .. versionadded:: 0.20\n",
      "        \n",
      "        .. note:: EXPERIMENTAL\n",
      "        \n",
      "            The API is experimental (particularly the return value structure),\n",
      "            and might have small backward-incompatible changes without notice\n",
      "            or warning in future releases.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        name : str, default=None\n",
      "            String identifier of the dataset. Note that OpenML can have multiple\n",
      "            datasets with the same name.\n",
      "        \n",
      "        version : int or 'active', default='active'\n",
      "            Version of the dataset. Can only be provided if also ``name`` is given.\n",
      "            If 'active' the oldest version that's still active is used. Since\n",
      "            there may be more than one active version of a dataset, and those\n",
      "            versions may fundamentally be different from one another, setting an\n",
      "            exact version is highly recommended.\n",
      "        \n",
      "        data_id : int, default=None\n",
      "            OpenML ID of the dataset. The most specific way of retrieving a\n",
      "            dataset. If data_id is not given, name (and potential version) are\n",
      "            used to obtain a dataset.\n",
      "        \n",
      "        data_home : str, default=None\n",
      "            Specify another download and cache folder for the data sets. By default\n",
      "            all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "        \n",
      "        target_column : str, list or None, default='default-target'\n",
      "            Specify the column name in the data to use as target. If\n",
      "            'default-target', the standard target column a stored on the server\n",
      "            is used. If ``None``, all columns are returned as data and the\n",
      "            target is ``None``. If list (of strings), all columns with these names\n",
      "            are returned as multi-target (Note: not all scikit-learn classifiers\n",
      "            can handle all types of multi-output combinations)\n",
      "        \n",
      "        cache : bool, default=True\n",
      "            Whether to cache downloaded datasets using joblib.\n",
      "        \n",
      "        return_X_y : bool, default=False\n",
      "            If True, returns ``(data, target)`` instead of a Bunch object. See\n",
      "            below for more information about the `data` and `target` objects.\n",
      "        \n",
      "        as_frame : bool or 'auto', default='auto'\n",
      "            If True, the data is a pandas DataFrame including columns with\n",
      "            appropriate dtypes (numeric, string or categorical). The target is\n",
      "            a pandas DataFrame or Series depending on the number of target_columns.\n",
      "            The Bunch will contain a ``frame`` attribute with the target and the\n",
      "            data. If ``return_X_y`` is True, then ``(data, target)`` will be pandas\n",
      "            DataFrames or Series as describe above.\n",
      "        \n",
      "            If as_frame is 'auto', the data and target will be converted to\n",
      "            DataFrame or Series as if as_frame is set to True, unless the dataset\n",
      "            is stored in sparse format.\n",
      "        \n",
      "            .. versionchanged:: 0.24\n",
      "               The default value of `as_frame` changed from `False` to `'auto'`\n",
      "               in 0.24.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        \n",
      "        data : :class:`~sklearn.utils.Bunch`\n",
      "            Dictionary-like object, with the following attributes.\n",
      "        \n",
      "            data : np.array, scipy.sparse.csr_matrix of floats, or pandas DataFrame\n",
      "                The feature matrix. Categorical features are encoded as ordinals.\n",
      "            target : np.array, pandas Series or DataFrame\n",
      "                The regression target or classification labels, if applicable.\n",
      "                Dtype is float if numeric, and object if categorical. If\n",
      "                ``as_frame`` is True, ``target`` is a pandas object.\n",
      "            DESCR : str\n",
      "                The full description of the dataset\n",
      "            feature_names : list\n",
      "                The names of the dataset columns\n",
      "            target_names: list\n",
      "                The names of the target columns\n",
      "        \n",
      "            .. versionadded:: 0.22\n",
      "        \n",
      "            categories : dict or None\n",
      "                Maps each categorical feature name to a list of values, such\n",
      "                that the value encoded as i is ith in the list. If ``as_frame``\n",
      "                is True, this is None.\n",
      "            details : dict\n",
      "                More metadata from OpenML\n",
      "            frame : pandas DataFrame\n",
      "                Only present when `as_frame=True`. DataFrame with ``data`` and\n",
      "                ``target``.\n",
      "        \n",
      "        (data, target) : tuple if ``return_X_y`` is True\n",
      "        \n",
      "            .. note:: EXPERIMENTAL\n",
      "        \n",
      "                This interface is **experimental** and subsequent releases may\n",
      "                change attributes without notice (although there should only be\n",
      "                minor changes to ``data`` and ``target``).\n",
      "        \n",
      "            Missing values in the 'data' are represented as NaN's. Missing values\n",
      "            in 'target' are represented as NaN's (numerical target) or None\n",
      "            (categorical target)\n",
      "    \n",
      "    fetch_rcv1(*, data_home=None, subset='all', download_if_missing=True, random_state=None, shuffle=False, return_X_y=False)\n",
      "        Load the RCV1 multilabel dataset (classification).\n",
      "        \n",
      "        Download it if necessary.\n",
      "        \n",
      "        Version: RCV1-v2, vectors, full sets, topics multilabels.\n",
      "        \n",
      "        =================   =====================\n",
      "        Classes                               103\n",
      "        Samples total                      804414\n",
      "        Dimensionality                      47236\n",
      "        Features            real, between 0 and 1\n",
      "        =================   =====================\n",
      "        \n",
      "        Read more in the :ref:`User Guide <rcv1_dataset>`.\n",
      "        \n",
      "        .. versionadded:: 0.17\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data_home : str, default=None\n",
      "            Specify another download and cache folder for the datasets. By default\n",
      "            all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "        \n",
      "        subset : {'train', 'test', 'all'}, default='all'\n",
      "            Select the dataset to load: 'train' for the training set\n",
      "            (23149 samples), 'test' for the test set (781265 samples),\n",
      "            'all' for both, with the training samples first if shuffle is False.\n",
      "            This follows the official LYRL2004 chronological split.\n",
      "        \n",
      "        download_if_missing : bool, default=True\n",
      "            If False, raise a IOError if the data is not locally available\n",
      "            instead of trying to download the data from the source site.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for dataset shuffling. Pass an int\n",
      "            for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        shuffle : bool, default=False\n",
      "            Whether to shuffle dataset.\n",
      "        \n",
      "        return_X_y : bool, default=False\n",
      "            If True, returns ``(dataset.data, dataset.target)`` instead of a Bunch\n",
      "            object. See below for more information about the `dataset.data` and\n",
      "            `dataset.target` object.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dataset : :class:`~sklearn.utils.Bunch`\n",
      "            Dictionary-like object, with the following attributes.\n",
      "        \n",
      "            data : sparse matrix of shape (804414, 47236), dtype=np.float64\n",
      "                The array has 0.16% of non zero values. Will be of CSR format.\n",
      "            target : sparse matrix of shape (804414, 103), dtype=np.uint8\n",
      "                Each sample has a value of 1 in its categories, and 0 in others.\n",
      "                The array has 3.15% of non zero values. Will be of CSR format.\n",
      "            sample_id : ndarray of shape (804414,), dtype=np.uint32,\n",
      "                Identification number of each sample, as ordered in dataset.data.\n",
      "            target_names : ndarray of shape (103,), dtype=object\n",
      "                Names of each target (RCV1 topics), as ordered in dataset.target.\n",
      "            DESCR : str\n",
      "                Description of the RCV1 dataset.\n",
      "        \n",
      "        (data, target) : tuple if ``return_X_y`` is True\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "    \n",
      "    fetch_species_distributions(*, data_home=None, download_if_missing=True)\n",
      "        Loader for species distribution dataset from Phillips et. al. (2006)\n",
      "        \n",
      "        Read more in the :ref:`User Guide <datasets>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data_home : str, default=None\n",
      "            Specify another download and cache folder for the datasets. By default\n",
      "            all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "        \n",
      "        download_if_missing : bool, default=True\n",
      "            If False, raise a IOError if the data is not locally available\n",
      "            instead of trying to download the data from the source site.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : :class:`~sklearn.utils.Bunch`\n",
      "            Dictionary-like object, with the following attributes.\n",
      "        \n",
      "            coverages : array, shape = [14, 1592, 1212]\n",
      "                These represent the 14 features measured\n",
      "                at each point of the map grid.\n",
      "                The latitude/longitude values for the grid are discussed below.\n",
      "                Missing data is represented by the value -9999.\n",
      "            train : record array, shape = (1624,)\n",
      "                The training points for the data.  Each point has three fields:\n",
      "        \n",
      "                - train['species'] is the species name\n",
      "                - train['dd long'] is the longitude, in degrees\n",
      "                - train['dd lat'] is the latitude, in degrees\n",
      "            test : record array, shape = (620,)\n",
      "                The test points for the data.  Same format as the training data.\n",
      "            Nx, Ny : integers\n",
      "                The number of longitudes (x) and latitudes (y) in the grid\n",
      "            x_left_lower_corner, y_left_lower_corner : floats\n",
      "                The (x,y) position of the lower-left corner, in degrees\n",
      "            grid_size : float\n",
      "                The spacing between points of the grid, in degrees\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        * `\"Maximum entropy modeling of species geographic distributions\"\n",
      "          <http://rob.schapire.net/papers/ecolmod.pdf>`_\n",
      "          S. J. Phillips, R. P. Anderson, R. E. Schapire - Ecological Modelling,\n",
      "          190:231-259, 2006.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        This dataset represents the geographic distribution of species.\n",
      "        The dataset is provided by Phillips et. al. (2006).\n",
      "        \n",
      "        The two species are:\n",
      "        \n",
      "        - `\"Bradypus variegatus\"\n",
      "          <http://www.iucnredlist.org/details/3038/0>`_ ,\n",
      "          the Brown-throated Sloth.\n",
      "        \n",
      "        - `\"Microryzomys minutus\"\n",
      "          <http://www.iucnredlist.org/details/13408/0>`_ ,\n",
      "          also known as the Forest Small Rice Rat, a rodent that lives in Peru,\n",
      "          Colombia, Ecuador, Peru, and Venezuela.\n",
      "        \n",
      "        - For an example of using this dataset with scikit-learn, see\n",
      "          :ref:`examples/applications/plot_species_distribution_modeling.py\n",
      "          <sphx_glr_auto_examples_applications_plot_species_distribution_modeling.py>`.\n",
      "    \n",
      "    get_data_home(data_home=None) -> str\n",
      "        Return the path of the scikit-learn data dir.\n",
      "        \n",
      "        This folder is used by some large dataset loaders to avoid downloading the\n",
      "        data several times.\n",
      "        \n",
      "        By default the data dir is set to a folder named 'scikit_learn_data' in the\n",
      "        user home folder.\n",
      "        \n",
      "        Alternatively, it can be set by the 'SCIKIT_LEARN_DATA' environment\n",
      "        variable or programmatically by giving an explicit folder path. The '~'\n",
      "        symbol is expanded to the user home folder.\n",
      "        \n",
      "        If the folder does not already exist, it is automatically created.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        data_home : str, default=None\n",
      "            The path to scikit-learn data directory. If `None`, the default path\n",
      "            is `~/sklearn_learn_data`.\n",
      "    \n",
      "    load_boston(*, return_X_y=False)\n",
      "        Load and return the boston house-prices dataset (regression).\n",
      "        \n",
      "        ==============   ==============\n",
      "        Samples total               506\n",
      "        Dimensionality               13\n",
      "        Features         real, positive\n",
      "        Targets           real 5. - 50.\n",
      "        ==============   ==============\n",
      "        \n",
      "        Read more in the :ref:`User Guide <boston_dataset>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        return_X_y : bool, default=False\n",
      "            If True, returns ``(data, target)`` instead of a Bunch object.\n",
      "            See below for more information about the `data` and `target` object.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : :class:`~sklearn.utils.Bunch`\n",
      "            Dictionary-like object, with the following attributes.\n",
      "        \n",
      "            data : ndarray of shape (506, 13)\n",
      "                The data matrix.\n",
      "            target : ndarray of shape (506, )\n",
      "                The regression target.\n",
      "            filename : str\n",
      "                The physical location of boston csv dataset.\n",
      "        \n",
      "                .. versionadded:: 0.20\n",
      "        \n",
      "            DESCR : str\n",
      "                The full description of the dataset.\n",
      "            feature_names : ndarray\n",
      "                The names of features\n",
      "        \n",
      "        (data, target) : tuple if ``return_X_y`` is True\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "            .. versionchanged:: 0.20\n",
      "                Fixed a wrong data point at [445, 0].\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.datasets import load_boston\n",
      "        >>> X, y = load_boston(return_X_y=True)\n",
      "        >>> print(X.shape)\n",
      "        (506, 13)\n",
      "    \n",
      "    load_breast_cancer(*, return_X_y=False, as_frame=False)\n",
      "        Load and return the breast cancer wisconsin dataset (classification).\n",
      "        \n",
      "        The breast cancer dataset is a classic and very easy binary classification\n",
      "        dataset.\n",
      "        \n",
      "        =================   ==============\n",
      "        Classes                          2\n",
      "        Samples per class    212(M),357(B)\n",
      "        Samples total                  569\n",
      "        Dimensionality                  30\n",
      "        Features            real, positive\n",
      "        =================   ==============\n",
      "        \n",
      "        Read more in the :ref:`User Guide <breast_cancer_dataset>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        return_X_y : bool, default=False\n",
      "            If True, returns ``(data, target)`` instead of a Bunch object.\n",
      "            See below for more information about the `data` and `target` object.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        as_frame : bool, default=False\n",
      "            If True, the data is a pandas DataFrame including columns with\n",
      "            appropriate dtypes (numeric). The target is\n",
      "            a pandas DataFrame or Series depending on the number of target columns.\n",
      "            If `return_X_y` is True, then (`data`, `target`) will be pandas\n",
      "            DataFrames or Series as described below.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : :class:`~sklearn.utils.Bunch`\n",
      "            Dictionary-like object, with the following attributes.\n",
      "        \n",
      "            data : {ndarray, dataframe} of shape (569, 30)\n",
      "                The data matrix. If `as_frame=True`, `data` will be a pandas\n",
      "                DataFrame.\n",
      "            target: {ndarray, Series} of shape (569,)\n",
      "                The classification target. If `as_frame=True`, `target` will be\n",
      "                a pandas Series.\n",
      "            feature_names: list\n",
      "                The names of the dataset columns.\n",
      "            target_names: list\n",
      "                The names of target classes.\n",
      "            frame: DataFrame of shape (569, 31)\n",
      "                Only present when `as_frame=True`. DataFrame with `data` and\n",
      "                `target`.\n",
      "        \n",
      "                .. versionadded:: 0.23\n",
      "            DESCR: str\n",
      "                The full description of the dataset.\n",
      "            filename: str\n",
      "                The path to the location of the data.\n",
      "        \n",
      "                .. versionadded:: 0.20\n",
      "        \n",
      "        (data, target) : tuple if ``return_X_y`` is True\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        The copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is\n",
      "        downloaded from:\n",
      "        https://goo.gl/U2Uwz2\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Let's say you are interested in the samples 10, 50, and 85, and want to\n",
      "        know their class name.\n",
      "        \n",
      "        >>> from sklearn.datasets import load_breast_cancer\n",
      "        >>> data = load_breast_cancer()\n",
      "        >>> data.target[[10, 50, 85]]\n",
      "        array([0, 1, 0])\n",
      "        >>> list(data.target_names)\n",
      "        ['malignant', 'benign']\n",
      "    \n",
      "    load_diabetes(*, return_X_y=False, as_frame=False)\n",
      "        Load and return the diabetes dataset (regression).\n",
      "        \n",
      "        ==============   ==================\n",
      "        Samples total    442\n",
      "        Dimensionality   10\n",
      "        Features         real, -.2 < x < .2\n",
      "        Targets          integer 25 - 346\n",
      "        ==============   ==================\n",
      "        \n",
      "        .. note::\n",
      "           The meaning of each feature (i.e. `feature_names`) might be unclear\n",
      "           (especially for `ltg`) as the documentation of the original dataset is\n",
      "           not explicit. We provide information that seems correct in regard with\n",
      "           the scientific literature in this field of research.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <diabetes_dataset>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        return_X_y : bool, default=False.\n",
      "            If True, returns ``(data, target)`` instead of a Bunch object.\n",
      "            See below for more information about the `data` and `target` object.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        as_frame : bool, default=False\n",
      "            If True, the data is a pandas DataFrame including columns with\n",
      "            appropriate dtypes (numeric). The target is\n",
      "            a pandas DataFrame or Series depending on the number of target columns.\n",
      "            If `return_X_y` is True, then (`data`, `target`) will be pandas\n",
      "            DataFrames or Series as described below.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : :class:`~sklearn.utils.Bunch`\n",
      "            Dictionary-like object, with the following attributes.\n",
      "        \n",
      "            data : {ndarray, dataframe} of shape (442, 10)\n",
      "                The data matrix. If `as_frame=True`, `data` will be a pandas\n",
      "                DataFrame.\n",
      "            target: {ndarray, Series} of shape (442,)\n",
      "                The regression target. If `as_frame=True`, `target` will be\n",
      "                a pandas Series.\n",
      "            feature_names: list\n",
      "                The names of the dataset columns.\n",
      "            frame: DataFrame of shape (442, 11)\n",
      "                Only present when `as_frame=True`. DataFrame with `data` and\n",
      "                `target`.\n",
      "        \n",
      "                .. versionadded:: 0.23\n",
      "            DESCR: str\n",
      "                The full description of the dataset.\n",
      "            data_filename: str\n",
      "                The path to the location of the data.\n",
      "            target_filename: str\n",
      "                The path to the location of the target.\n",
      "        \n",
      "        (data, target) : tuple if ``return_X_y`` is True\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "    \n",
      "    load_digits(*, n_class=10, return_X_y=False, as_frame=False)\n",
      "        Load and return the digits dataset (classification).\n",
      "        \n",
      "        Each datapoint is a 8x8 image of a digit.\n",
      "        \n",
      "        =================   ==============\n",
      "        Classes                         10\n",
      "        Samples per class             ~180\n",
      "        Samples total                 1797\n",
      "        Dimensionality                  64\n",
      "        Features             integers 0-16\n",
      "        =================   ==============\n",
      "        \n",
      "        Read more in the :ref:`User Guide <digits_dataset>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_class : int, default=10\n",
      "            The number of classes to return. Between 0 and 10.\n",
      "        \n",
      "        return_X_y : bool, default=False\n",
      "            If True, returns ``(data, target)`` instead of a Bunch object.\n",
      "            See below for more information about the `data` and `target` object.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        as_frame : bool, default=False\n",
      "            If True, the data is a pandas DataFrame including columns with\n",
      "            appropriate dtypes (numeric). The target is\n",
      "            a pandas DataFrame or Series depending on the number of target columns.\n",
      "            If `return_X_y` is True, then (`data`, `target`) will be pandas\n",
      "            DataFrames or Series as described below.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : :class:`~sklearn.utils.Bunch`\n",
      "            Dictionary-like object, with the following attributes.\n",
      "        \n",
      "            data : {ndarray, dataframe} of shape (1797, 64)\n",
      "                The flattened data matrix. If `as_frame=True`, `data` will be\n",
      "                a pandas DataFrame.\n",
      "            target: {ndarray, Series} of shape (1797,)\n",
      "                The classification target. If `as_frame=True`, `target` will be\n",
      "                a pandas Series.\n",
      "            feature_names: list\n",
      "                The names of the dataset columns.\n",
      "            target_names: list\n",
      "                The names of target classes.\n",
      "        \n",
      "                .. versionadded:: 0.20\n",
      "        \n",
      "            frame: DataFrame of shape (1797, 65)\n",
      "                Only present when `as_frame=True`. DataFrame with `data` and\n",
      "                `target`.\n",
      "        \n",
      "                .. versionadded:: 0.23\n",
      "            images: {ndarray} of shape (1797, 8, 8)\n",
      "                The raw image data.\n",
      "            DESCR: str\n",
      "                The full description of the dataset.\n",
      "        \n",
      "        (data, target) : tuple if ``return_X_y`` is True\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "        https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        To load the data and visualize the images::\n",
      "        \n",
      "            >>> from sklearn.datasets import load_digits\n",
      "            >>> digits = load_digits()\n",
      "            >>> print(digits.data.shape)\n",
      "            (1797, 64)\n",
      "            >>> import matplotlib.pyplot as plt #doctest: +SKIP\n",
      "            >>> plt.gray() #doctest: +SKIP\n",
      "            >>> plt.matshow(digits.images[0]) #doctest: +SKIP\n",
      "            >>> plt.show() #doctest: +SKIP\n",
      "    \n",
      "    load_files(container_path, *, description=None, categories=None, load_content=True, shuffle=True, encoding=None, decode_error='strict', random_state=0)\n",
      "        Load text files with categories as subfolder names.\n",
      "        \n",
      "        Individual samples are assumed to be files stored a two levels folder\n",
      "        structure such as the following:\n",
      "        \n",
      "            container_folder/\n",
      "                category_1_folder/\n",
      "                    file_1.txt\n",
      "                    file_2.txt\n",
      "                    ...\n",
      "                    file_42.txt\n",
      "                category_2_folder/\n",
      "                    file_43.txt\n",
      "                    file_44.txt\n",
      "                    ...\n",
      "        \n",
      "        The folder names are used as supervised signal label names. The individual\n",
      "        file names are not important.\n",
      "        \n",
      "        This function does not try to extract features into a numpy array or scipy\n",
      "        sparse matrix. In addition, if load_content is false it does not try to\n",
      "        load the files in memory.\n",
      "        \n",
      "        To use text files in a scikit-learn classification or clustering algorithm,\n",
      "        you will need to use the :mod`~sklearn.feature_extraction.text` module to\n",
      "        build a feature extraction transformer that suits your problem.\n",
      "        \n",
      "        If you set load_content=True, you should also specify the encoding of the\n",
      "        text using the 'encoding' parameter. For many modern text files, 'utf-8'\n",
      "        will be the correct encoding. If you leave encoding equal to None, then the\n",
      "        content will be made of bytes instead of Unicode, and you will not be able\n",
      "        to use most functions in :mod:`~sklearn.feature_extraction.text`.\n",
      "        \n",
      "        Similar feature extractors should be built for other kind of unstructured\n",
      "        data input such as images, audio, video, ...\n",
      "        \n",
      "        Read more in the :ref:`User Guide <datasets>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        container_path : str or unicode\n",
      "            Path to the main folder holding one subfolder per category\n",
      "        \n",
      "        description : str or unicode, default=None\n",
      "            A paragraph describing the characteristic of the dataset: its source,\n",
      "            reference, etc.\n",
      "        \n",
      "        categories : list of str, default=None\n",
      "            If None (default), load all the categories. If not None, list of\n",
      "            category names to load (other categories ignored).\n",
      "        \n",
      "        load_content : bool, default=True\n",
      "            Whether to load or not the content of the different files. If true a\n",
      "            'data' attribute containing the text information is present in the data\n",
      "            structure returned. If not, a filenames attribute gives the path to the\n",
      "            files.\n",
      "        \n",
      "        shuffle : bool, default=True\n",
      "            Whether or not to shuffle the data: might be important for models that\n",
      "            make the assumption that the samples are independent and identically\n",
      "            distributed (i.i.d.), such as stochastic gradient descent.\n",
      "        \n",
      "        encoding : str, default=None\n",
      "            If None, do not try to decode the content of the files (e.g. for images\n",
      "            or other non-text content). If not None, encoding to use to decode text\n",
      "            files to Unicode if load_content is True.\n",
      "        \n",
      "        decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
      "            Instruction on what to do if a byte sequence is given to analyze that\n",
      "            contains characters not of the given `encoding`. Passed as keyword\n",
      "            argument 'errors' to bytes.decode.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=0\n",
      "            Determines random number generation for dataset shuffling. Pass an int\n",
      "            for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : :class:`~sklearn.utils.Bunch`\n",
      "            Dictionary-like object, with the following attributes.\n",
      "        \n",
      "            data : list of str\n",
      "                Only present when `load_content=True`.\n",
      "                The raw text data to learn.\n",
      "            target : ndarray\n",
      "                The target labels (integer index).\n",
      "            target_names : list\n",
      "                The names of target classes.\n",
      "            DESCR : str\n",
      "                The full description of the dataset.\n",
      "            filenames: ndarray\n",
      "                The filenames holding the dataset.\n",
      "    \n",
      "    load_iris(*, return_X_y=False, as_frame=False)\n",
      "        Load and return the iris dataset (classification).\n",
      "        \n",
      "        The iris dataset is a classic and very easy multi-class classification\n",
      "        dataset.\n",
      "        \n",
      "        =================   ==============\n",
      "        Classes                          3\n",
      "        Samples per class               50\n",
      "        Samples total                  150\n",
      "        Dimensionality                   4\n",
      "        Features            real, positive\n",
      "        =================   ==============\n",
      "        \n",
      "        Read more in the :ref:`User Guide <iris_dataset>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        return_X_y : bool, default=False\n",
      "            If True, returns ``(data, target)`` instead of a Bunch object. See\n",
      "            below for more information about the `data` and `target` object.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        as_frame : bool, default=False\n",
      "            If True, the data is a pandas DataFrame including columns with\n",
      "            appropriate dtypes (numeric). The target is\n",
      "            a pandas DataFrame or Series depending on the number of target columns.\n",
      "            If `return_X_y` is True, then (`data`, `target`) will be pandas\n",
      "            DataFrames or Series as described below.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : :class:`~sklearn.utils.Bunch`\n",
      "            Dictionary-like object, with the following attributes.\n",
      "        \n",
      "            data : {ndarray, dataframe} of shape (150, 4)\n",
      "                The data matrix. If `as_frame=True`, `data` will be a pandas\n",
      "                DataFrame.\n",
      "            target: {ndarray, Series} of shape (150,)\n",
      "                The classification target. If `as_frame=True`, `target` will be\n",
      "                a pandas Series.\n",
      "            feature_names: list\n",
      "                The names of the dataset columns.\n",
      "            target_names: list\n",
      "                The names of target classes.\n",
      "            frame: DataFrame of shape (150, 5)\n",
      "                Only present when `as_frame=True`. DataFrame with `data` and\n",
      "                `target`.\n",
      "        \n",
      "                .. versionadded:: 0.23\n",
      "            DESCR: str\n",
      "                The full description of the dataset.\n",
      "            filename: str\n",
      "                The path to the location of the data.\n",
      "        \n",
      "                .. versionadded:: 0.20\n",
      "        \n",
      "        (data, target) : tuple if ``return_X_y`` is True\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "            .. versionchanged:: 0.20\n",
      "                Fixed two wrong data points according to Fisher's paper.\n",
      "                The new version is the same as in R, but not as in the UCI\n",
      "                Machine Learning Repository.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Let's say you are interested in the samples 10, 25, and 50, and want to\n",
      "        know their class name.\n",
      "        \n",
      "        >>> from sklearn.datasets import load_iris\n",
      "        >>> data = load_iris()\n",
      "        >>> data.target[[10, 25, 50]]\n",
      "        array([0, 0, 1])\n",
      "        >>> list(data.target_names)\n",
      "        ['setosa', 'versicolor', 'virginica']\n",
      "    \n",
      "    load_linnerud(*, return_X_y=False, as_frame=False)\n",
      "        Load and return the physical excercise linnerud dataset.\n",
      "        \n",
      "        This dataset is suitable for multi-ouput regression tasks.\n",
      "        \n",
      "        ==============   ============================\n",
      "        Samples total    20\n",
      "        Dimensionality   3 (for both data and target)\n",
      "        Features         integer\n",
      "        Targets          integer\n",
      "        ==============   ============================\n",
      "        \n",
      "        Read more in the :ref:`User Guide <linnerrud_dataset>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        return_X_y : bool, default=False\n",
      "            If True, returns ``(data, target)`` instead of a Bunch object.\n",
      "            See below for more information about the `data` and `target` object.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        as_frame : bool, default=False\n",
      "            If True, the data is a pandas DataFrame including columns with\n",
      "            appropriate dtypes (numeric, string or categorical). The target is\n",
      "            a pandas DataFrame or Series depending on the number of target columns.\n",
      "            If `return_X_y` is True, then (`data`, `target`) will be pandas\n",
      "            DataFrames or Series as described below.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : :class:`~sklearn.utils.Bunch`\n",
      "            Dictionary-like object, with the following attributes.\n",
      "        \n",
      "            data : {ndarray, dataframe} of shape (20, 3)\n",
      "                The data matrix. If `as_frame=True`, `data` will be a pandas\n",
      "                DataFrame.\n",
      "            target: {ndarray, dataframe} of shape (20, 3)\n",
      "                The regression targets. If `as_frame=True`, `target` will be\n",
      "                a pandas DataFrame.\n",
      "            feature_names: list\n",
      "                The names of the dataset columns.\n",
      "            target_names: list\n",
      "                The names of the target columns.\n",
      "            frame: DataFrame of shape (20, 6)\n",
      "                Only present when `as_frame=True`. DataFrame with `data` and\n",
      "                `target`.\n",
      "        \n",
      "                .. versionadded:: 0.23\n",
      "            DESCR: str\n",
      "                The full description of the dataset.\n",
      "            data_filename: str\n",
      "                The path to the location of the data.\n",
      "            target_filename: str\n",
      "                The path to the location of the target.\n",
      "        \n",
      "                .. versionadded:: 0.20\n",
      "        \n",
      "        (data, target) : tuple if ``return_X_y`` is True\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "    \n",
      "    load_sample_image(image_name)\n",
      "        Load the numpy array of a single sample image\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_images>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        image_name : {`china.jpg`, `flower.jpg`}\n",
      "            The name of the sample image loaded\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        img : 3D array\n",
      "            The image as a numpy array: height x width x color\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        >>> from sklearn.datasets import load_sample_image\n",
      "        >>> china = load_sample_image('china.jpg')   # doctest: +SKIP\n",
      "        >>> china.dtype                              # doctest: +SKIP\n",
      "        dtype('uint8')\n",
      "        >>> china.shape                              # doctest: +SKIP\n",
      "        (427, 640, 3)\n",
      "        >>> flower = load_sample_image('flower.jpg') # doctest: +SKIP\n",
      "        >>> flower.dtype                             # doctest: +SKIP\n",
      "        dtype('uint8')\n",
      "        >>> flower.shape                             # doctest: +SKIP\n",
      "        (427, 640, 3)\n",
      "    \n",
      "    load_sample_images()\n",
      "        Load sample images for image manipulation.\n",
      "        \n",
      "        Loads both, ``china`` and ``flower``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_images>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : :class:`~sklearn.utils.Bunch`\n",
      "            Dictionary-like object, with the following attributes.\n",
      "        \n",
      "            images : list of ndarray of shape (427, 640, 3)\n",
      "                The two sample image.\n",
      "            filenames : list\n",
      "                The filenames for the images.\n",
      "            DESCR : str\n",
      "                The full description of the dataset.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        To load the data and visualize the images:\n",
      "        \n",
      "        >>> from sklearn.datasets import load_sample_images\n",
      "        >>> dataset = load_sample_images()     #doctest: +SKIP\n",
      "        >>> len(dataset.images)                #doctest: +SKIP\n",
      "        2\n",
      "        >>> first_img_data = dataset.images[0] #doctest: +SKIP\n",
      "        >>> first_img_data.shape               #doctest: +SKIP\n",
      "        (427, 640, 3)\n",
      "        >>> first_img_data.dtype               #doctest: +SKIP\n",
      "        dtype('uint8')\n",
      "    \n",
      "    load_svmlight_file(f, *, n_features=None, dtype=<class 'numpy.float64'>, multilabel=False, zero_based='auto', query_id=False, offset=0, length=-1)\n",
      "        Load datasets in the svmlight / libsvm format into sparse CSR matrix\n",
      "        \n",
      "        This format is a text-based format, with one sample per line. It does\n",
      "        not store zero valued features hence is suitable for sparse dataset.\n",
      "        \n",
      "        The first element of each line can be used to store a target variable\n",
      "        to predict.\n",
      "        \n",
      "        This format is used as the default format for both svmlight and the\n",
      "        libsvm command line programs.\n",
      "        \n",
      "        Parsing a text based source can be expensive. When working on\n",
      "        repeatedly on the same dataset, it is recommended to wrap this\n",
      "        loader with joblib.Memory.cache to store a memmapped backup of the\n",
      "        CSR results of the first call and benefit from the near instantaneous\n",
      "        loading of memmapped structures for the subsequent calls.\n",
      "        \n",
      "        In case the file contains a pairwise preference constraint (known\n",
      "        as \"qid\" in the svmlight format) these are ignored unless the\n",
      "        query_id parameter is set to True. These pairwise preference\n",
      "        constraints can be used to constraint the combination of samples\n",
      "        when using pairwise loss functions (as is the case in some\n",
      "        learning to rank problems) so that only pairs with the same\n",
      "        query_id value are considered.\n",
      "        \n",
      "        This implementation is written in Cython and is reasonably fast.\n",
      "        However, a faster API-compatible loader is also available at:\n",
      "        \n",
      "          https://github.com/mblondel/svmlight-loader\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : str, file-like or int\n",
      "            (Path to) a file to load. If a path ends in \".gz\" or \".bz2\", it will\n",
      "            be uncompressed on the fly. If an integer is passed, it is assumed to\n",
      "            be a file descriptor. A file-like or file descriptor will not be closed\n",
      "            by this function. A file-like object must be opened in binary mode.\n",
      "        \n",
      "        n_features : int, default=None\n",
      "            The number of features to use. If None, it will be inferred. This\n",
      "            argument is useful to load several files that are subsets of a\n",
      "            bigger sliced dataset: each subset might not have examples of\n",
      "            every feature, hence the inferred shape might vary from one\n",
      "            slice to another.\n",
      "            n_features is only required if ``offset`` or ``length`` are passed a\n",
      "            non-default value.\n",
      "        \n",
      "        dtype : numpy data type, default=np.float64\n",
      "            Data type of dataset to be loaded. This will be the data type of the\n",
      "            output numpy arrays ``X`` and ``y``.\n",
      "        \n",
      "        multilabel : bool, default=False\n",
      "            Samples may have several labels each (see\n",
      "            https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html)\n",
      "        \n",
      "        zero_based : bool or \"auto\", default=\"auto\"\n",
      "            Whether column indices in f are zero-based (True) or one-based\n",
      "            (False). If column indices are one-based, they are transformed to\n",
      "            zero-based to match Python/NumPy conventions.\n",
      "            If set to \"auto\", a heuristic check is applied to determine this from\n",
      "            the file contents. Both kinds of files occur \"in the wild\", but they\n",
      "            are unfortunately not self-identifying. Using \"auto\" or True should\n",
      "            always be safe when no ``offset`` or ``length`` is passed.\n",
      "            If ``offset`` or ``length`` are passed, the \"auto\" mode falls back\n",
      "            to ``zero_based=True`` to avoid having the heuristic check yield\n",
      "            inconsistent results on different segments of the file.\n",
      "        \n",
      "        query_id : bool, default=False\n",
      "            If True, will return the query_id array for each file.\n",
      "        \n",
      "        offset : int, default=0\n",
      "            Ignore the offset first bytes by seeking forward, then\n",
      "            discarding the following bytes up until the next new line\n",
      "            character.\n",
      "        \n",
      "        length : int, default=-1\n",
      "            If strictly positive, stop reading any new line of data once the\n",
      "            position in the file has reached the (offset + length) bytes threshold.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : scipy.sparse matrix of shape (n_samples, n_features)\n",
      "        \n",
      "        y : ndarray of shape (n_samples,), or, in the multilabel a list of\n",
      "            tuples of length n_samples.\n",
      "        \n",
      "        query_id : array of shape (n_samples,)\n",
      "           query_id for each sample. Only returned when query_id is set to\n",
      "           True.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        load_svmlight_files : Similar function for loading multiple files in this\n",
      "            format, enforcing the same number of features/columns on all of them.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        To use joblib.Memory to cache the svmlight file::\n",
      "        \n",
      "            from joblib import Memory\n",
      "            from .datasets import load_svmlight_file\n",
      "            mem = Memory(\"./mycache\")\n",
      "        \n",
      "            @mem.cache\n",
      "            def get_data():\n",
      "                data = load_svmlight_file(\"mysvmlightfile\")\n",
      "                return data[0], data[1]\n",
      "        \n",
      "            X, y = get_data()\n",
      "    \n",
      "    load_svmlight_files(files, *, n_features=None, dtype=<class 'numpy.float64'>, multilabel=False, zero_based='auto', query_id=False, offset=0, length=-1)\n",
      "        Load dataset from multiple files in SVMlight format\n",
      "        \n",
      "        This function is equivalent to mapping load_svmlight_file over a list of\n",
      "        files, except that the results are concatenated into a single, flat list\n",
      "        and the samples vectors are constrained to all have the same number of\n",
      "        features.\n",
      "        \n",
      "        In case the file contains a pairwise preference constraint (known\n",
      "        as \"qid\" in the svmlight format) these are ignored unless the\n",
      "        query_id parameter is set to True. These pairwise preference\n",
      "        constraints can be used to constraint the combination of samples\n",
      "        when using pairwise loss functions (as is the case in some\n",
      "        learning to rank problems) so that only pairs with the same\n",
      "        query_id value are considered.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        files : array-like, dtype=str, file-like or int\n",
      "            (Paths of) files to load. If a path ends in \".gz\" or \".bz2\", it will\n",
      "            be uncompressed on the fly. If an integer is passed, it is assumed to\n",
      "            be a file descriptor. File-likes and file descriptors will not be\n",
      "            closed by this function. File-like objects must be opened in binary\n",
      "            mode.\n",
      "        \n",
      "        n_features : int, default=None\n",
      "            The number of features to use. If None, it will be inferred from the\n",
      "            maximum column index occurring in any of the files.\n",
      "        \n",
      "            This can be set to a higher value than the actual number of features\n",
      "            in any of the input files, but setting it to a lower value will cause\n",
      "            an exception to be raised.\n",
      "        \n",
      "        dtype : numpy data type, default=np.float64\n",
      "            Data type of dataset to be loaded. This will be the data type of the\n",
      "            output numpy arrays ``X`` and ``y``.\n",
      "        \n",
      "        multilabel : bool, default=False\n",
      "            Samples may have several labels each (see\n",
      "            https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html)\n",
      "        \n",
      "        zero_based : bool or \"auto\", default=\"auto\"\n",
      "            Whether column indices in f are zero-based (True) or one-based\n",
      "            (False). If column indices are one-based, they are transformed to\n",
      "            zero-based to match Python/NumPy conventions.\n",
      "            If set to \"auto\", a heuristic check is applied to determine this from\n",
      "            the file contents. Both kinds of files occur \"in the wild\", but they\n",
      "            are unfortunately not self-identifying. Using \"auto\" or True should\n",
      "            always be safe when no offset or length is passed.\n",
      "            If offset or length are passed, the \"auto\" mode falls back\n",
      "            to zero_based=True to avoid having the heuristic check yield\n",
      "            inconsistent results on different segments of the file.\n",
      "        \n",
      "        query_id : bool, default=False\n",
      "            If True, will return the query_id array for each file.\n",
      "        \n",
      "        offset : int, default=0\n",
      "            Ignore the offset first bytes by seeking forward, then\n",
      "            discarding the following bytes up until the next new line\n",
      "            character.\n",
      "        \n",
      "        length : int, default=-1\n",
      "            If strictly positive, stop reading any new line of data once the\n",
      "            position in the file has reached the (offset + length) bytes threshold.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        [X1, y1, ..., Xn, yn]\n",
      "        where each (Xi, yi) pair is the result from load_svmlight_file(files[i]).\n",
      "        \n",
      "        If query_id is set to True, this will return instead [X1, y1, q1,\n",
      "        ..., Xn, yn, qn] where (Xi, yi, qi) is the result from\n",
      "        load_svmlight_file(files[i])\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When fitting a model to a matrix X_train and evaluating it against a\n",
      "        matrix X_test, it is essential that X_train and X_test have the same\n",
      "        number of features (X_train.shape[1] == X_test.shape[1]). This may not\n",
      "        be the case if you load the files individually with load_svmlight_file.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        load_svmlight_file\n",
      "    \n",
      "    load_wine(*, return_X_y=False, as_frame=False)\n",
      "        Load and return the wine dataset (classification).\n",
      "        \n",
      "        .. versionadded:: 0.18\n",
      "        \n",
      "        The wine dataset is a classic and very easy multi-class classification\n",
      "        dataset.\n",
      "        \n",
      "        =================   ==============\n",
      "        Classes                          3\n",
      "        Samples per class        [59,71,48]\n",
      "        Samples total                  178\n",
      "        Dimensionality                  13\n",
      "        Features            real, positive\n",
      "        =================   ==============\n",
      "        \n",
      "        Read more in the :ref:`User Guide <wine_dataset>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        return_X_y : bool, default=False\n",
      "            If True, returns ``(data, target)`` instead of a Bunch object.\n",
      "            See below for more information about the `data` and `target` object.\n",
      "        \n",
      "        as_frame : bool, default=False\n",
      "            If True, the data is a pandas DataFrame including columns with\n",
      "            appropriate dtypes (numeric). The target is\n",
      "            a pandas DataFrame or Series depending on the number of target columns.\n",
      "            If `return_X_y` is True, then (`data`, `target`) will be pandas\n",
      "            DataFrames or Series as described below.\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : :class:`~sklearn.utils.Bunch`\n",
      "            Dictionary-like object, with the following attributes.\n",
      "        \n",
      "            data : {ndarray, dataframe} of shape (178, 13)\n",
      "                The data matrix. If `as_frame=True`, `data` will be a pandas\n",
      "                DataFrame.\n",
      "            target: {ndarray, Series} of shape (178,)\n",
      "                The classification target. If `as_frame=True`, `target` will be\n",
      "                a pandas Series.\n",
      "            feature_names: list\n",
      "                The names of the dataset columns.\n",
      "            target_names: list\n",
      "                The names of target classes.\n",
      "            frame: DataFrame of shape (178, 14)\n",
      "                Only present when `as_frame=True`. DataFrame with `data` and\n",
      "                `target`.\n",
      "        \n",
      "                .. versionadded:: 0.23\n",
      "            DESCR: str\n",
      "                The full description of the dataset.\n",
      "        \n",
      "        (data, target) : tuple if ``return_X_y`` is True\n",
      "        \n",
      "        The copy of UCI ML Wine Data Set dataset is downloaded and modified to fit\n",
      "        standard format from:\n",
      "        https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Let's say you are interested in the samples 10, 80, and 140, and want to\n",
      "        know their class name.\n",
      "        \n",
      "        >>> from sklearn.datasets import load_wine\n",
      "        >>> data = load_wine()\n",
      "        >>> data.target[[10, 80, 140]]\n",
      "        array([0, 1, 2])\n",
      "        >>> list(data.target_names)\n",
      "        ['class_0', 'class_1', 'class_2']\n",
      "    \n",
      "    make_biclusters(shape, n_clusters, *, noise=0.0, minval=10, maxval=100, shuffle=True, random_state=None)\n",
      "        Generate an array with constant block diagonal structure for\n",
      "        biclustering.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        shape : iterable of shape (n_rows, n_cols)\n",
      "            The shape of the result.\n",
      "        \n",
      "        n_clusters : int\n",
      "            The number of biclusters.\n",
      "        \n",
      "        noise : float, default=0.0\n",
      "            The standard deviation of the gaussian noise.\n",
      "        \n",
      "        minval : int, default=10\n",
      "            Minimum value of a bicluster.\n",
      "        \n",
      "        maxval : int, default=100\n",
      "            Maximum value of a bicluster.\n",
      "        \n",
      "        shuffle : bool, default=True\n",
      "            Shuffle the samples.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for dataset creation. Pass an int\n",
      "            for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : ndarray of shape `shape`\n",
      "            The generated array.\n",
      "        \n",
      "        rows : ndarray of shape (n_clusters, X.shape[0])\n",
      "            The indicators for cluster membership of each row.\n",
      "        \n",
      "        cols : ndarray of shape (n_clusters, X.shape[1])\n",
      "            The indicators for cluster membership of each column.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] Dhillon, I. S. (2001, August). Co-clustering documents and\n",
      "            words using bipartite spectral graph partitioning. In Proceedings\n",
      "            of the seventh ACM SIGKDD international conference on Knowledge\n",
      "            discovery and data mining (pp. 269-274). ACM.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        make_checkerboard\n",
      "    \n",
      "    make_blobs(n_samples=100, n_features=2, *, centers=None, cluster_std=1.0, center_box=(-10.0, 10.0), shuffle=True, random_state=None, return_centers=False)\n",
      "        Generate isotropic Gaussian blobs for clustering.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int or array-like, default=100\n",
      "            If int, it is the total number of points equally divided among\n",
      "            clusters.\n",
      "            If array-like, each element of the sequence indicates\n",
      "            the number of samples per cluster.\n",
      "        \n",
      "            .. versionchanged:: v0.20\n",
      "                one can now pass an array-like to the ``n_samples`` parameter\n",
      "        \n",
      "        n_features : int, default=2\n",
      "            The number of features for each sample.\n",
      "        \n",
      "        centers : int or ndarray of shape (n_centers, n_features), default=None\n",
      "            The number of centers to generate, or the fixed center locations.\n",
      "            If n_samples is an int and centers is None, 3 centers are generated.\n",
      "            If n_samples is array-like, centers must be\n",
      "            either None or an array of length equal to the length of n_samples.\n",
      "        \n",
      "        cluster_std : float or array-like of float, default=1.0\n",
      "            The standard deviation of the clusters.\n",
      "        \n",
      "        center_box : tuple of float (min, max), default=(-10.0, 10.0)\n",
      "            The bounding box for each cluster center when centers are\n",
      "            generated at random.\n",
      "        \n",
      "        shuffle : bool, default=True\n",
      "            Shuffle the samples.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for dataset creation. Pass an int\n",
      "            for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        return_centers : bool, default=False\n",
      "            If True, then return the centers of each cluster\n",
      "        \n",
      "            .. versionadded:: 0.23\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : ndarray of shape (n_samples, n_features)\n",
      "            The generated samples.\n",
      "        \n",
      "        y : ndarray of shape (n_samples,)\n",
      "            The integer labels for cluster membership of each sample.\n",
      "        \n",
      "        centers : ndarray of shape (n_centers, n_features)\n",
      "            The centers of each cluster. Only returned if\n",
      "            ``return_centers=True``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.datasets import make_blobs\n",
      "        >>> X, y = make_blobs(n_samples=10, centers=3, n_features=2,\n",
      "        ...                   random_state=0)\n",
      "        >>> print(X.shape)\n",
      "        (10, 2)\n",
      "        >>> y\n",
      "        array([0, 0, 1, 0, 2, 2, 2, 1, 1, 0])\n",
      "        >>> X, y = make_blobs(n_samples=[3, 3, 4], centers=None, n_features=2,\n",
      "        ...                   random_state=0)\n",
      "        >>> print(X.shape)\n",
      "        (10, 2)\n",
      "        >>> y\n",
      "        array([0, 1, 2, 0, 2, 2, 2, 1, 1, 0])\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        make_classification : A more intricate variant.\n",
      "    \n",
      "    make_checkerboard(shape, n_clusters, *, noise=0.0, minval=10, maxval=100, shuffle=True, random_state=None)\n",
      "        Generate an array with block checkerboard structure for\n",
      "        biclustering.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        shape : tuple of shape (n_rows, n_cols)\n",
      "            The shape of the result.\n",
      "        \n",
      "        n_clusters : int or array-like or shape (n_row_clusters, n_column_clusters)\n",
      "            The number of row and column clusters.\n",
      "        \n",
      "        noise : float, default=0.0\n",
      "            The standard deviation of the gaussian noise.\n",
      "        \n",
      "        minval : int, default=10\n",
      "            Minimum value of a bicluster.\n",
      "        \n",
      "        maxval : int, default=100\n",
      "            Maximum value of a bicluster.\n",
      "        \n",
      "        shuffle : bool, default=True\n",
      "            Shuffle the samples.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for dataset creation. Pass an int\n",
      "            for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : ndarray of shape `shape`\n",
      "            The generated array.\n",
      "        \n",
      "        rows : ndarray of shape (n_clusters, X.shape[0])\n",
      "            The indicators for cluster membership of each row.\n",
      "        \n",
      "        cols : ndarray of shape (n_clusters, X.shape[1])\n",
      "            The indicators for cluster membership of each column.\n",
      "        \n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] Kluger, Y., Basri, R., Chang, J. T., & Gerstein, M. (2003).\n",
      "            Spectral biclustering of microarray data: coclustering genes\n",
      "            and conditions. Genome research, 13(4), 703-716.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        make_biclusters\n",
      "    \n",
      "    make_circles(n_samples=100, *, shuffle=True, noise=None, random_state=None, factor=0.8)\n",
      "        Make a large circle containing a smaller circle in 2d.\n",
      "        \n",
      "        A simple toy dataset to visualize clustering and classification\n",
      "        algorithms.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int or tuple of shape (2,), dtype=int, default=100\n",
      "            If int, it is the total number of points generated.\n",
      "            For odd numbers, the inner circle will have one point more than the\n",
      "            outer circle.\n",
      "            If two-element tuple, number of points in outer circle and inner\n",
      "            circle.\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "               Added two-element tuple.\n",
      "        \n",
      "        shuffle : bool, default=True\n",
      "            Whether to shuffle the samples.\n",
      "        \n",
      "        noise : float, default=None\n",
      "            Standard deviation of Gaussian noise added to the data.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for dataset shuffling and noise.\n",
      "            Pass an int for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        factor : float, default=.8\n",
      "            Scale factor between inner and outer circle in the range `(0, 1)`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : ndarray of shape (n_samples, 2)\n",
      "            The generated samples.\n",
      "        \n",
      "        y : ndarray of shape (n_samples,)\n",
      "            The integer labels (0 or 1) for class membership of each sample.\n",
      "    \n",
      "    make_classification(n_samples=100, n_features=20, *, n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)\n",
      "        Generate a random n-class classification problem.\n",
      "        \n",
      "        This initially creates clusters of points normally distributed (std=1)\n",
      "        about vertices of an ``n_informative``-dimensional hypercube with sides of\n",
      "        length ``2*class_sep`` and assigns an equal number of clusters to each\n",
      "        class. It introduces interdependence between these features and adds\n",
      "        various types of further noise to the data.\n",
      "        \n",
      "        Without shuffling, ``X`` horizontally stacks features in the following\n",
      "        order: the primary ``n_informative`` features, followed by ``n_redundant``\n",
      "        linear combinations of the informative features, followed by ``n_repeated``\n",
      "        duplicates, drawn randomly with replacement from the informative and\n",
      "        redundant features. The remaining features are filled with random noise.\n",
      "        Thus, without shuffling, all useful features are contained in the columns\n",
      "        ``X[:, :n_informative + n_redundant + n_repeated]``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, default=100\n",
      "            The number of samples.\n",
      "        \n",
      "        n_features : int, default=20\n",
      "            The total number of features. These comprise ``n_informative``\n",
      "            informative features, ``n_redundant`` redundant features,\n",
      "            ``n_repeated`` duplicated features and\n",
      "            ``n_features-n_informative-n_redundant-n_repeated`` useless features\n",
      "            drawn at random.\n",
      "        \n",
      "        n_informative : int, default=2\n",
      "            The number of informative features. Each class is composed of a number\n",
      "            of gaussian clusters each located around the vertices of a hypercube\n",
      "            in a subspace of dimension ``n_informative``. For each cluster,\n",
      "            informative features are drawn independently from  N(0, 1) and then\n",
      "            randomly linearly combined within each cluster in order to add\n",
      "            covariance. The clusters are then placed on the vertices of the\n",
      "            hypercube.\n",
      "        \n",
      "        n_redundant : int, default=2\n",
      "            The number of redundant features. These features are generated as\n",
      "            random linear combinations of the informative features.\n",
      "        \n",
      "        n_repeated : int, default=0\n",
      "            The number of duplicated features, drawn randomly from the informative\n",
      "            and the redundant features.\n",
      "        \n",
      "        n_classes : int, default=2\n",
      "            The number of classes (or labels) of the classification problem.\n",
      "        \n",
      "        n_clusters_per_class : int, default=2\n",
      "            The number of clusters per class.\n",
      "        \n",
      "        weights : array-like of shape (n_classes,) or (n_classes - 1,),              default=None\n",
      "            The proportions of samples assigned to each class. If None, then\n",
      "            classes are balanced. Note that if ``len(weights) == n_classes - 1``,\n",
      "            then the last class weight is automatically inferred.\n",
      "            More than ``n_samples`` samples may be returned if the sum of\n",
      "            ``weights`` exceeds 1. Note that the actual class proportions will\n",
      "            not exactly match ``weights`` when ``flip_y`` isn't 0.\n",
      "        \n",
      "        flip_y : float, default=0.01\n",
      "            The fraction of samples whose class is assigned randomly. Larger\n",
      "            values introduce noise in the labels and make the classification\n",
      "            task harder. Note that the default setting flip_y > 0 might lead\n",
      "            to less than ``n_classes`` in y in some cases.\n",
      "        \n",
      "        class_sep : float, default=1.0\n",
      "            The factor multiplying the hypercube size.  Larger values spread\n",
      "            out the clusters/classes and make the classification task easier.\n",
      "        \n",
      "        hypercube : bool, default=True\n",
      "            If True, the clusters are put on the vertices of a hypercube. If\n",
      "            False, the clusters are put on the vertices of a random polytope.\n",
      "        \n",
      "        shift : float, ndarray of shape (n_features,) or None, default=0.0\n",
      "            Shift features by the specified value. If None, then features\n",
      "            are shifted by a random value drawn in [-class_sep, class_sep].\n",
      "        \n",
      "        scale : float, ndarray of shape (n_features,) or None, default=1.0\n",
      "            Multiply features by the specified value. If None, then features\n",
      "            are scaled by a random value drawn in [1, 100]. Note that scaling\n",
      "            happens after shifting.\n",
      "        \n",
      "        shuffle : bool, default=True\n",
      "            Shuffle the samples and the features.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for dataset creation. Pass an int\n",
      "            for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : ndarray of shape (n_samples, n_features)\n",
      "            The generated samples.\n",
      "        \n",
      "        y : ndarray of shape (n_samples,)\n",
      "            The integer labels for class membership of each sample.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The algorithm is adapted from Guyon [1] and was designed to generate\n",
      "        the \"Madelon\" dataset.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] I. Guyon, \"Design of experiments for the NIPS 2003 variable\n",
      "               selection benchmark\", 2003.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        make_blobs : Simplified variant.\n",
      "        make_multilabel_classification : Unrelated generator for multilabel tasks.\n",
      "    \n",
      "    make_friedman1(n_samples=100, n_features=10, *, noise=0.0, random_state=None)\n",
      "        Generate the \"Friedman #1\" regression problem.\n",
      "        \n",
      "        This dataset is described in Friedman [1] and Breiman [2].\n",
      "        \n",
      "        Inputs `X` are independent features uniformly distributed on the interval\n",
      "        [0, 1]. The output `y` is created according to the formula::\n",
      "        \n",
      "            y(X) = 10 * sin(pi * X[:, 0] * X[:, 1]) + 20 * (X[:, 2] - 0.5) ** 2 + 10 * X[:, 3] + 5 * X[:, 4] + noise * N(0, 1).\n",
      "        \n",
      "        Out of the `n_features` features, only 5 are actually used to compute\n",
      "        `y`. The remaining features are independent of `y`.\n",
      "        \n",
      "        The number of features has to be >= 5.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, default=100\n",
      "            The number of samples.\n",
      "        \n",
      "        n_features : int, default=10\n",
      "            The number of features. Should be at least 5.\n",
      "        \n",
      "        noise : float, default=0.0\n",
      "            The standard deviation of the gaussian noise applied to the output.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for dataset noise. Pass an int\n",
      "            for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : ndarray of shape (n_samples, n_features)\n",
      "            The input samples.\n",
      "        \n",
      "        y : ndarray of shape (n_samples,)\n",
      "            The output values.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] J. Friedman, \"Multivariate adaptive regression splines\", The Annals\n",
      "               of Statistics 19 (1), pages 1-67, 1991.\n",
      "        \n",
      "        .. [2] L. Breiman, \"Bagging predictors\", Machine Learning 24,\n",
      "               pages 123-140, 1996.\n",
      "    \n",
      "    make_friedman2(n_samples=100, *, noise=0.0, random_state=None)\n",
      "        Generate the \"Friedman #2\" regression problem.\n",
      "        \n",
      "        This dataset is described in Friedman [1] and Breiman [2].\n",
      "        \n",
      "        Inputs `X` are 4 independent features uniformly distributed on the\n",
      "        intervals::\n",
      "        \n",
      "            0 <= X[:, 0] <= 100,\n",
      "            40 * pi <= X[:, 1] <= 560 * pi,\n",
      "            0 <= X[:, 2] <= 1,\n",
      "            1 <= X[:, 3] <= 11.\n",
      "        \n",
      "        The output `y` is created according to the formula::\n",
      "        \n",
      "            y(X) = (X[:, 0] ** 2 + (X[:, 1] * X[:, 2]  - 1 / (X[:, 1] * X[:, 3])) ** 2) ** 0.5 + noise * N(0, 1).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, default=100\n",
      "            The number of samples.\n",
      "        \n",
      "        noise : float, default=0.0\n",
      "            The standard deviation of the gaussian noise applied to the output.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for dataset noise. Pass an int\n",
      "            for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : ndarray of shape (n_samples, 4)\n",
      "            The input samples.\n",
      "        \n",
      "        y : ndarray of shape (n_samples,)\n",
      "            The output values.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] J. Friedman, \"Multivariate adaptive regression splines\", The Annals\n",
      "               of Statistics 19 (1), pages 1-67, 1991.\n",
      "        \n",
      "        .. [2] L. Breiman, \"Bagging predictors\", Machine Learning 24,\n",
      "               pages 123-140, 1996.\n",
      "    \n",
      "    make_friedman3(n_samples=100, *, noise=0.0, random_state=None)\n",
      "        Generate the \"Friedman #3\" regression problem.\n",
      "        \n",
      "        This dataset is described in Friedman [1] and Breiman [2].\n",
      "        \n",
      "        Inputs `X` are 4 independent features uniformly distributed on the\n",
      "        intervals::\n",
      "        \n",
      "            0 <= X[:, 0] <= 100,\n",
      "            40 * pi <= X[:, 1] <= 560 * pi,\n",
      "            0 <= X[:, 2] <= 1,\n",
      "            1 <= X[:, 3] <= 11.\n",
      "        \n",
      "        The output `y` is created according to the formula::\n",
      "        \n",
      "            y(X) = arctan((X[:, 1] * X[:, 2] - 1 / (X[:, 1] * X[:, 3])) / X[:, 0]) + noise * N(0, 1).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, default=100\n",
      "            The number of samples.\n",
      "        \n",
      "        noise : float, default=0.0\n",
      "            The standard deviation of the gaussian noise applied to the output.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for dataset noise. Pass an int\n",
      "            for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : ndarray of shape (n_samples, 4)\n",
      "            The input samples.\n",
      "        \n",
      "        y : ndarray of shape (n_samples,)\n",
      "            The output values.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] J. Friedman, \"Multivariate adaptive regression splines\", The Annals\n",
      "               of Statistics 19 (1), pages 1-67, 1991.\n",
      "        \n",
      "        .. [2] L. Breiman, \"Bagging predictors\", Machine Learning 24,\n",
      "               pages 123-140, 1996.\n",
      "    \n",
      "    make_gaussian_quantiles(*, mean=None, cov=1.0, n_samples=100, n_features=2, n_classes=3, shuffle=True, random_state=None)\n",
      "        Generate isotropic Gaussian and label samples by quantile.\n",
      "        \n",
      "        This classification dataset is constructed by taking a multi-dimensional\n",
      "        standard normal distribution and defining classes separated by nested\n",
      "        concentric multi-dimensional spheres such that roughly equal numbers of\n",
      "        samples are in each class (quantiles of the :math:`\\chi^2` distribution).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        mean : ndarray of shape (n_features,), default=None\n",
      "            The mean of the multi-dimensional normal distribution.\n",
      "            If None then use the origin (0, 0, ...).\n",
      "        \n",
      "        cov : float, default=1.0\n",
      "            The covariance matrix will be this value times the unit matrix. This\n",
      "            dataset only produces symmetric normal distributions.\n",
      "        \n",
      "        n_samples : int, default=100\n",
      "            The total number of points equally divided among classes.\n",
      "        \n",
      "        n_features : int, default=2\n",
      "            The number of features for each sample.\n",
      "        \n",
      "        n_classes : int, default=3\n",
      "            The number of classes\n",
      "        \n",
      "        shuffle : bool, default=True\n",
      "            Shuffle the samples.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for dataset creation. Pass an int\n",
      "            for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : ndarray of shape (n_samples, n_features)\n",
      "            The generated samples.\n",
      "        \n",
      "        y : ndarray of shape (n_samples,)\n",
      "            The integer labels for quantile membership of each sample.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The dataset is from Zhu et al [1].\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n",
      "    \n",
      "    make_hastie_10_2(n_samples=12000, *, random_state=None)\n",
      "        Generates data for binary classification used in\n",
      "        Hastie et al. 2009, Example 10.2.\n",
      "        \n",
      "        The ten features are standard independent Gaussian and\n",
      "        the target ``y`` is defined by::\n",
      "        \n",
      "          y[i] = 1 if np.sum(X[i] ** 2) > 9.34 else -1\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, default=12000\n",
      "            The number of samples.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for dataset creation. Pass an int\n",
      "            for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : ndarray of shape (n_samples, 10)\n",
      "            The input samples.\n",
      "        \n",
      "        y : ndarray of shape (n_samples,)\n",
      "            The output values.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] T. Hastie, R. Tibshirani and J. Friedman, \"Elements of Statistical\n",
      "               Learning Ed. 2\", Springer, 2009.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        make_gaussian_quantiles : A generalization of this dataset approach.\n",
      "    \n",
      "    make_low_rank_matrix(n_samples=100, n_features=100, *, effective_rank=10, tail_strength=0.5, random_state=None)\n",
      "        Generate a mostly low rank matrix with bell-shaped singular values.\n",
      "        \n",
      "        Most of the variance can be explained by a bell-shaped curve of width\n",
      "        effective_rank: the low rank part of the singular values profile is::\n",
      "        \n",
      "            (1 - tail_strength) * exp(-1.0 * (i / effective_rank) ** 2)\n",
      "        \n",
      "        The remaining singular values' tail is fat, decreasing as::\n",
      "        \n",
      "            tail_strength * exp(-0.1 * i / effective_rank).\n",
      "        \n",
      "        The low rank part of the profile can be considered the structured\n",
      "        signal part of the data while the tail can be considered the noisy\n",
      "        part of the data that cannot be summarized by a low number of linear\n",
      "        components (singular vectors).\n",
      "        \n",
      "        This kind of singular profiles is often seen in practice, for instance:\n",
      "         - gray level pictures of faces\n",
      "         - TF-IDF vectors of text documents crawled from the web\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, default=100\n",
      "            The number of samples.\n",
      "        \n",
      "        n_features : int, default=100\n",
      "            The number of features.\n",
      "        \n",
      "        effective_rank : int, default=10\n",
      "            The approximate number of singular vectors required to explain most of\n",
      "            the data by linear combinations.\n",
      "        \n",
      "        tail_strength : float, default=0.5\n",
      "            The relative importance of the fat noisy tail of the singular values\n",
      "            profile. The value should be between 0 and 1.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for dataset creation. Pass an int\n",
      "            for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : ndarray of shape (n_samples, n_features)\n",
      "            The matrix.\n",
      "    \n",
      "    make_moons(n_samples=100, *, shuffle=True, noise=None, random_state=None)\n",
      "        Make two interleaving half circles.\n",
      "        \n",
      "        A simple toy dataset to visualize clustering and classification\n",
      "        algorithms. Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int or tuple of shape (2,), dtype=int, default=100\n",
      "            If int, the total number of points generated.\n",
      "            If two-element tuple, number of points in each of two moons.\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "               Added two-element tuple.\n",
      "        \n",
      "        shuffle : bool, default=True\n",
      "            Whether to shuffle the samples.\n",
      "        \n",
      "        noise : float, default=None\n",
      "            Standard deviation of Gaussian noise added to the data.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for dataset shuffling and noise.\n",
      "            Pass an int for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : ndarray of shape (n_samples, 2)\n",
      "            The generated samples.\n",
      "        \n",
      "        y : ndarray of shape (n_samples,)\n",
      "            The integer labels (0 or 1) for class membership of each sample.\n",
      "    \n",
      "    make_multilabel_classification(n_samples=100, n_features=20, *, n_classes=5, n_labels=2, length=50, allow_unlabeled=True, sparse=False, return_indicator='dense', return_distributions=False, random_state=None)\n",
      "        Generate a random multilabel classification problem.\n",
      "        \n",
      "        For each sample, the generative process is:\n",
      "            - pick the number of labels: n ~ Poisson(n_labels)\n",
      "            - n times, choose a class c: c ~ Multinomial(theta)\n",
      "            - pick the document length: k ~ Poisson(length)\n",
      "            - k times, choose a word: w ~ Multinomial(theta_c)\n",
      "        \n",
      "        In the above process, rejection sampling is used to make sure that\n",
      "        n is never zero or more than `n_classes`, and that the document length\n",
      "        is never zero. Likewise, we reject classes which have already been chosen.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, default=100\n",
      "            The number of samples.\n",
      "        \n",
      "        n_features : int, default=20\n",
      "            The total number of features.\n",
      "        \n",
      "        n_classes : int, default=5\n",
      "            The number of classes of the classification problem.\n",
      "        \n",
      "        n_labels : int, default=2\n",
      "            The average number of labels per instance. More precisely, the number\n",
      "            of labels per sample is drawn from a Poisson distribution with\n",
      "            ``n_labels`` as its expected value, but samples are bounded (using\n",
      "            rejection sampling) by ``n_classes``, and must be nonzero if\n",
      "            ``allow_unlabeled`` is False.\n",
      "        \n",
      "        length : int, default=50\n",
      "            The sum of the features (number of words if documents) is drawn from\n",
      "            a Poisson distribution with this expected value.\n",
      "        \n",
      "        allow_unlabeled : bool, default=True\n",
      "            If ``True``, some instances might not belong to any class.\n",
      "        \n",
      "        sparse : bool, default=False\n",
      "            If ``True``, return a sparse feature matrix\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "               parameter to allow *sparse* output.\n",
      "        \n",
      "        return_indicator : {'dense', 'sparse'} or False, default='dense'\n",
      "            If ``'dense'`` return ``Y`` in the dense binary indicator format. If\n",
      "            ``'sparse'`` return ``Y`` in the sparse binary indicator format.\n",
      "            ``False`` returns a list of lists of labels.\n",
      "        \n",
      "        return_distributions : bool, default=False\n",
      "            If ``True``, return the prior class probability and conditional\n",
      "            probabilities of features given classes, from which the data was\n",
      "            drawn.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for dataset creation. Pass an int\n",
      "            for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : ndarray of shape (n_samples, n_features)\n",
      "            The generated samples.\n",
      "        \n",
      "        Y : {ndarray, sparse matrix} of shape (n_samples, n_classes)\n",
      "            The label sets. Sparse matrix should be of CSR format.\n",
      "        \n",
      "        p_c : ndarray of shape (n_classes,)\n",
      "            The probability of each class being drawn. Only returned if\n",
      "            ``return_distributions=True``.\n",
      "        \n",
      "        p_w_c : ndarray of shape (n_features, n_classes)\n",
      "            The probability of each feature being drawn given each class.\n",
      "            Only returned if ``return_distributions=True``.\n",
      "    \n",
      "    make_regression(n_samples=100, n_features=100, *, n_informative=10, n_targets=1, bias=0.0, effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=None)\n",
      "        Generate a random regression problem.\n",
      "        \n",
      "        The input set can either be well conditioned (by default) or have a low\n",
      "        rank-fat tail singular profile. See :func:`make_low_rank_matrix` for\n",
      "        more details.\n",
      "        \n",
      "        The output is generated by applying a (potentially biased) random linear\n",
      "        regression model with `n_informative` nonzero regressors to the previously\n",
      "        generated input and some gaussian centered noise with some adjustable\n",
      "        scale.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, default=100\n",
      "            The number of samples.\n",
      "        \n",
      "        n_features : int, default=100\n",
      "            The number of features.\n",
      "        \n",
      "        n_informative : int, default=10\n",
      "            The number of informative features, i.e., the number of features used\n",
      "            to build the linear model used to generate the output.\n",
      "        \n",
      "        n_targets : int, default=1\n",
      "            The number of regression targets, i.e., the dimension of the y output\n",
      "            vector associated with a sample. By default, the output is a scalar.\n",
      "        \n",
      "        bias : float, default=0.0\n",
      "            The bias term in the underlying linear model.\n",
      "        \n",
      "        effective_rank : int, default=None\n",
      "            if not None:\n",
      "                The approximate number of singular vectors required to explain most\n",
      "                of the input data by linear combinations. Using this kind of\n",
      "                singular spectrum in the input allows the generator to reproduce\n",
      "                the correlations often observed in practice.\n",
      "            if None:\n",
      "                The input set is well conditioned, centered and gaussian with\n",
      "                unit variance.\n",
      "        \n",
      "        tail_strength : float, default=0.5\n",
      "            The relative importance of the fat noisy tail of the singular values\n",
      "            profile if `effective_rank` is not None. When a float, it should be\n",
      "            between 0 and 1.\n",
      "        \n",
      "        noise : float, default=0.0\n",
      "            The standard deviation of the gaussian noise applied to the output.\n",
      "        \n",
      "        shuffle : bool, default=True\n",
      "            Shuffle the samples and the features.\n",
      "        \n",
      "        coef : bool, default=False\n",
      "            If True, the coefficients of the underlying linear model are returned.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for dataset creation. Pass an int\n",
      "            for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : ndarray of shape (n_samples, n_features)\n",
      "            The input samples.\n",
      "        \n",
      "        y : ndarray of shape (n_samples,) or (n_samples, n_targets)\n",
      "            The output values.\n",
      "        \n",
      "        coef : ndarray of shape (n_features,) or (n_features, n_targets)\n",
      "            The coefficient of the underlying linear model. It is returned only if\n",
      "            coef is True.\n",
      "    \n",
      "    make_s_curve(n_samples=100, *, noise=0.0, random_state=None)\n",
      "        Generate an S curve dataset.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, default=100\n",
      "            The number of sample points on the S curve.\n",
      "        \n",
      "        noise : float, default=0.0\n",
      "            The standard deviation of the gaussian noise.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for dataset creation. Pass an int\n",
      "            for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : ndarray of shape (n_samples, 3)\n",
      "            The points.\n",
      "        \n",
      "        t : ndarray of shape (n_samples,)\n",
      "            The univariate position of the sample according to the main dimension\n",
      "            of the points in the manifold.\n",
      "    \n",
      "    make_sparse_coded_signal(n_samples, *, n_components, n_features, n_nonzero_coefs, random_state=None)\n",
      "        Generate a signal as a sparse combination of dictionary elements.\n",
      "        \n",
      "        Returns a matrix Y = DX, such as D is (n_features, n_components),\n",
      "        X is (n_components, n_samples) and each column of X has exactly\n",
      "        n_nonzero_coefs non-zero elements.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int\n",
      "            Number of samples to generate\n",
      "        \n",
      "        n_components : int\n",
      "            Number of components in the dictionary\n",
      "        \n",
      "        n_features : int\n",
      "            Number of features of the dataset to generate\n",
      "        \n",
      "        n_nonzero_coefs : int\n",
      "            Number of active (non-zero) coefficients in each sample\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for dataset creation. Pass an int\n",
      "            for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        data : ndarray of shape (n_features, n_samples)\n",
      "            The encoded signal (Y).\n",
      "        \n",
      "        dictionary : ndarray of shape (n_features, n_components)\n",
      "            The dictionary with normalized components (D).\n",
      "        \n",
      "        code : ndarray of shape (n_components, n_samples)\n",
      "            The sparse code such that each column of this matrix has exactly\n",
      "            n_nonzero_coefs non-zero items (X).\n",
      "    \n",
      "    make_sparse_spd_matrix(dim=1, *, alpha=0.95, norm_diag=False, smallest_coef=0.1, largest_coef=0.9, random_state=None)\n",
      "        Generate a sparse symmetric definite positive matrix.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        dim : int, default=1\n",
      "            The size of the random matrix to generate.\n",
      "        \n",
      "        alpha : float, default=0.95\n",
      "            The probability that a coefficient is zero (see notes). Larger values\n",
      "            enforce more sparsity. The value should be in the range 0 and 1.\n",
      "        \n",
      "        norm_diag : bool, default=False\n",
      "            Whether to normalize the output matrix to make the leading diagonal\n",
      "            elements all 1\n",
      "        \n",
      "        smallest_coef : float, default=0.1\n",
      "            The value of the smallest coefficient between 0 and 1.\n",
      "        \n",
      "        largest_coef : float, default=0.9\n",
      "            The value of the largest coefficient between 0 and 1.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for dataset creation. Pass an int\n",
      "            for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        prec : sparse matrix of shape (dim, dim)\n",
      "            The generated matrix.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The sparsity is actually imposed on the cholesky factor of the matrix.\n",
      "        Thus alpha does not translate directly into the filling fraction of\n",
      "        the matrix itself.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        make_spd_matrix\n",
      "    \n",
      "    make_sparse_uncorrelated(n_samples=100, n_features=10, *, random_state=None)\n",
      "        Generate a random regression problem with sparse uncorrelated design.\n",
      "        \n",
      "        This dataset is described in Celeux et al [1]. as::\n",
      "        \n",
      "            X ~ N(0, 1)\n",
      "            y(X) = X[:, 0] + 2 * X[:, 1] - 2 * X[:, 2] - 1.5 * X[:, 3]\n",
      "        \n",
      "        Only the first 4 features are informative. The remaining features are\n",
      "        useless.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, default=100\n",
      "            The number of samples.\n",
      "        \n",
      "        n_features : int, default=10\n",
      "            The number of features.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for dataset creation. Pass an int\n",
      "            for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : ndarray of shape (n_samples, n_features)\n",
      "            The input samples.\n",
      "        \n",
      "        y : ndarray of shape (n_samples,)\n",
      "            The output values.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] G. Celeux, M. El Anbari, J.-M. Marin, C. P. Robert,\n",
      "               \"Regularization in regression: comparing Bayesian and frequentist\n",
      "               methods in a poorly informative situation\", 2009.\n",
      "    \n",
      "    make_spd_matrix(n_dim, *, random_state=None)\n",
      "        Generate a random symmetric, positive-definite matrix.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_dim : int\n",
      "            The matrix dimension.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for dataset creation. Pass an int\n",
      "            for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : ndarray of shape (n_dim, n_dim)\n",
      "            The random symmetric, positive-definite matrix.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        make_sparse_spd_matrix\n",
      "    \n",
      "    make_swiss_roll(n_samples=100, *, noise=0.0, random_state=None)\n",
      "        Generate a swiss roll dataset.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <sample_generators>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n_samples : int, default=100\n",
      "            The number of sample points on the S curve.\n",
      "        \n",
      "        noise : float, default=0.0\n",
      "            The standard deviation of the gaussian noise.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for dataset creation. Pass an int\n",
      "            for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        X : ndarray of shape (n_samples, 3)\n",
      "            The points.\n",
      "        \n",
      "        t : ndarray of shape (n_samples,)\n",
      "            The univariate position of the sample according to the main dimension\n",
      "            of the points in the manifold.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The algorithm is from Marsland [1].\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] S. Marsland, \"Machine Learning: An Algorithmic Perspective\",\n",
      "               Chapter 10, 2009.\n",
      "               http://seat.massey.ac.nz/personal/s.r.marsland/Code/10/lle.py\n",
      "\n",
      "DATA\n",
      "    __all__ = ['clear_data_home', 'dump_svmlight_file', 'fetch_20newsgroup...\n",
      "\n",
      "FILE\n",
      "    /opt/anaconda3/envs/ksunsupervised/lib/python3.9/site-packages/sklearn/datasets/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0d1792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eed661c6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.model_selection in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.model_selection\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _search\n",
      "    _search_successive_halving\n",
      "    _split\n",
      "    _validation\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        sklearn.model_selection._search.ParameterGrid\n",
      "        sklearn.model_selection._search.ParameterSampler\n",
      "        sklearn.model_selection._split.BaseCrossValidator\n",
      "            sklearn.model_selection._split.LeaveOneGroupOut\n",
      "            sklearn.model_selection._split.LeaveOneOut\n",
      "            sklearn.model_selection._split.LeavePGroupsOut\n",
      "            sklearn.model_selection._split.LeavePOut\n",
      "            sklearn.model_selection._split.PredefinedSplit\n",
      "    sklearn.model_selection._search.BaseSearchCV(sklearn.base.MetaEstimatorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.model_selection._search.GridSearchCV\n",
      "        sklearn.model_selection._search.RandomizedSearchCV\n",
      "    sklearn.model_selection._split.BaseShuffleSplit(builtins.object)\n",
      "        sklearn.model_selection._split.ShuffleSplit\n",
      "            sklearn.model_selection._split.GroupShuffleSplit\n",
      "        sklearn.model_selection._split.StratifiedShuffleSplit\n",
      "    sklearn.model_selection._split._BaseKFold(sklearn.model_selection._split.BaseCrossValidator)\n",
      "        sklearn.model_selection._split.GroupKFold\n",
      "        sklearn.model_selection._split.KFold\n",
      "        sklearn.model_selection._split.StratifiedKFold\n",
      "        sklearn.model_selection._split.TimeSeriesSplit\n",
      "    sklearn.model_selection._split._RepeatedSplits(builtins.object)\n",
      "        sklearn.model_selection._split.RepeatedKFold\n",
      "        sklearn.model_selection._split.RepeatedStratifiedKFold\n",
      "    \n",
      "    class BaseCrossValidator(builtins.object)\n",
      "     |  Base class for all cross-validators\n",
      "     |  \n",
      "     |  Implementations must define `_iter_test_masks` or `_iter_test_indices`.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'get_n_splits'})\n",
      "    \n",
      "    class GridSearchCV(BaseSearchCV)\n",
      "     |  GridSearchCV(estimator, param_grid, *, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)\n",
      "     |  \n",
      "     |  Exhaustive search over specified parameter values for an estimator.\n",
      "     |  \n",
      "     |  Important members are fit, predict.\n",
      "     |  \n",
      "     |  GridSearchCV implements a \"fit\" and a \"score\" method.\n",
      "     |  It also implements \"score_samples\", \"predict\", \"predict_proba\",\n",
      "     |  \"decision_function\", \"transform\" and \"inverse_transform\" if they are\n",
      "     |  implemented in the estimator used.\n",
      "     |  \n",
      "     |  The parameters of the estimator used to apply these methods are optimized\n",
      "     |  by cross-validated grid-search over a parameter grid.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <grid_search>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimator : estimator object.\n",
      "     |      This is assumed to implement the scikit-learn estimator interface.\n",
      "     |      Either estimator needs to provide a ``score`` function,\n",
      "     |      or ``scoring`` must be passed.\n",
      "     |  \n",
      "     |  param_grid : dict or list of dictionaries\n",
      "     |      Dictionary with parameters names (`str`) as keys and lists of\n",
      "     |      parameter settings to try as values, or a list of such\n",
      "     |      dictionaries, in which case the grids spanned by each dictionary\n",
      "     |      in the list are explored. This enables searching over any sequence\n",
      "     |      of parameter settings.\n",
      "     |  \n",
      "     |  scoring : str, callable, list, tuple or dict, default=None\n",
      "     |      Strategy to evaluate the performance of the cross-validated model on\n",
      "     |      the test set.\n",
      "     |  \n",
      "     |      If `scoring` represents a single score, one can use:\n",
      "     |  \n",
      "     |      - a single string (see :ref:`scoring_parameter`);\n",
      "     |      - a callable (see :ref:`scoring`) that returns a single value.\n",
      "     |  \n",
      "     |      If `scoring` represents multiple scores, one can use:\n",
      "     |  \n",
      "     |      - a list or tuple of unique strings;\n",
      "     |      - a callable returning a dictionary where the keys are the metric\n",
      "     |        names and the values are the metric scores;\n",
      "     |      - a dictionary with metric names as keys and callables a values.\n",
      "     |  \n",
      "     |      See :ref:`multimetric_grid_search` for an example.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of jobs to run in parallel.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |      .. versionchanged:: v0.20\n",
      "     |         `n_jobs` default changed from 1 to None\n",
      "     |  \n",
      "     |  refit : bool, str, or callable, default=True\n",
      "     |      Refit an estimator using the best found parameters on the whole\n",
      "     |      dataset.\n",
      "     |  \n",
      "     |      For multiple metric evaluation, this needs to be a `str` denoting the\n",
      "     |      scorer that would be used to find the best parameters for refitting\n",
      "     |      the estimator at the end.\n",
      "     |  \n",
      "     |      Where there are considerations other than maximum score in\n",
      "     |      choosing a best estimator, ``refit`` can be set to a function which\n",
      "     |      returns the selected ``best_index_`` given ``cv_results_``. In that\n",
      "     |      case, the ``best_estimator_`` and ``best_params_`` will be set\n",
      "     |      according to the returned ``best_index_`` while the ``best_score_``\n",
      "     |      attribute will not be available.\n",
      "     |  \n",
      "     |      The refitted estimator is made available at the ``best_estimator_``\n",
      "     |      attribute and permits using ``predict`` directly on this\n",
      "     |      ``GridSearchCV`` instance.\n",
      "     |  \n",
      "     |      Also for multiple metric evaluation, the attributes ``best_index_``,\n",
      "     |      ``best_score_`` and ``best_params_`` will only be available if\n",
      "     |      ``refit`` is set and all of them will be determined w.r.t this specific\n",
      "     |      scorer.\n",
      "     |  \n",
      "     |      See ``scoring`` parameter to know more about multiple metric\n",
      "     |      evaluation.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.20\n",
      "     |          Support for callable added.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross validation,\n",
      "     |      - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      "     |      either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "     |      other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "     |      with `shuffle=False` so the splits will be the same across calls.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  verbose : int\n",
      "     |      Controls the verbosity: the higher, the more messages.\n",
      "     |  \n",
      "     |      - >1 : the computation time for each fold and parameter candidate is\n",
      "     |        displayed;\n",
      "     |      - >2 : the score is also displayed;\n",
      "     |      - >3 : the fold and candidate parameter indexes are also displayed\n",
      "     |        together with the starting time of the computation.\n",
      "     |  \n",
      "     |  pre_dispatch : int, or str, default=n_jobs\n",
      "     |      Controls the number of jobs that get dispatched during parallel\n",
      "     |      execution. Reducing this number can be useful to avoid an\n",
      "     |      explosion of memory consumption when more jobs get dispatched\n",
      "     |      than CPUs can process. This parameter can be:\n",
      "     |  \n",
      "     |          - None, in which case all the jobs are immediately\n",
      "     |            created and spawned. Use this for lightweight and\n",
      "     |            fast-running jobs, to avoid delays due to on-demand\n",
      "     |            spawning of the jobs\n",
      "     |  \n",
      "     |          - An int, giving the exact number of total jobs that are\n",
      "     |            spawned\n",
      "     |  \n",
      "     |          - A str, giving an expression as a function of n_jobs,\n",
      "     |            as in '2*n_jobs'\n",
      "     |  \n",
      "     |  error_score : 'raise' or numeric, default=np.nan\n",
      "     |      Value to assign to the score if an error occurs in estimator fitting.\n",
      "     |      If set to 'raise', the error is raised. If a numeric value is given,\n",
      "     |      FitFailedWarning is raised. This parameter does not affect the refit\n",
      "     |      step, which will always raise the error.\n",
      "     |  \n",
      "     |  return_train_score : bool, default=False\n",
      "     |      If ``False``, the ``cv_results_`` attribute will not include training\n",
      "     |      scores.\n",
      "     |      Computing training scores is used to get insights on how different\n",
      "     |      parameter settings impact the overfitting/underfitting trade-off.\n",
      "     |      However computing the scores on the training set can be computationally\n",
      "     |      expensive and is not strictly required to select the parameters that\n",
      "     |      yield the best generalization performance.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.21\n",
      "     |          Default value was changed from ``True`` to ``False``\n",
      "     |  \n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn import svm, datasets\n",
      "     |  >>> from sklearn.model_selection import GridSearchCV\n",
      "     |  >>> iris = datasets.load_iris()\n",
      "     |  >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
      "     |  >>> svc = svm.SVC()\n",
      "     |  >>> clf = GridSearchCV(svc, parameters)\n",
      "     |  >>> clf.fit(iris.data, iris.target)\n",
      "     |  GridSearchCV(estimator=SVC(),\n",
      "     |               param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n",
      "     |  >>> sorted(clf.cv_results_.keys())\n",
      "     |  ['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n",
      "     |   'param_C', 'param_kernel', 'params',...\n",
      "     |   'rank_test_score', 'split0_test_score',...\n",
      "     |   'split2_test_score', ...\n",
      "     |   'std_fit_time', 'std_score_time', 'std_test_score']\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  cv_results_ : dict of numpy (masked) ndarrays\n",
      "     |      A dict with keys as column headers and values as columns, that can be\n",
      "     |      imported into a pandas ``DataFrame``.\n",
      "     |  \n",
      "     |      For instance the below given table\n",
      "     |  \n",
      "     |      +------------+-----------+------------+-----------------+---+---------+\n",
      "     |      |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n",
      "     |      +============+===========+============+=================+===+=========+\n",
      "     |      |  'poly'    |     --    |      2     |       0.80      |...|    2    |\n",
      "     |      +------------+-----------+------------+-----------------+---+---------+\n",
      "     |      |  'poly'    |     --    |      3     |       0.70      |...|    4    |\n",
      "     |      +------------+-----------+------------+-----------------+---+---------+\n",
      "     |      |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |\n",
      "     |      +------------+-----------+------------+-----------------+---+---------+\n",
      "     |      |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |\n",
      "     |      +------------+-----------+------------+-----------------+---+---------+\n",
      "     |  \n",
      "     |      will be represented by a ``cv_results_`` dict of::\n",
      "     |  \n",
      "     |          {\n",
      "     |          'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n",
      "     |                                       mask = [False False False False]...)\n",
      "     |          'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n",
      "     |                                      mask = [ True  True False False]...),\n",
      "     |          'param_degree': masked_array(data = [2.0 3.0 -- --],\n",
      "     |                                       mask = [False False  True  True]...),\n",
      "     |          'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n",
      "     |          'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n",
      "     |          'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n",
      "     |          'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n",
      "     |          'rank_test_score'    : [2, 4, 3, 1],\n",
      "     |          'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n",
      "     |          'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n",
      "     |          'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n",
      "     |          'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n",
      "     |          'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n",
      "     |          'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n",
      "     |          'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\n",
      "     |          'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\n",
      "     |          'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n",
      "     |          }\n",
      "     |  \n",
      "     |      NOTE\n",
      "     |  \n",
      "     |      The key ``'params'`` is used to store a list of parameter\n",
      "     |      settings dicts for all the parameter candidates.\n",
      "     |  \n",
      "     |      The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n",
      "     |      ``std_score_time`` are all in seconds.\n",
      "     |  \n",
      "     |      For multi-metric evaluation, the scores for all the scorers are\n",
      "     |      available in the ``cv_results_`` dict at the keys ending with that\n",
      "     |      scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n",
      "     |      above. ('split0_test_precision', 'mean_train_precision' etc.)\n",
      "     |  \n",
      "     |  best_estimator_ : estimator\n",
      "     |      Estimator that was chosen by the search, i.e. estimator\n",
      "     |      which gave highest score (or smallest loss if specified)\n",
      "     |      on the left out data. Not available if ``refit=False``.\n",
      "     |  \n",
      "     |      See ``refit`` parameter for more information on allowed values.\n",
      "     |  \n",
      "     |  best_score_ : float\n",
      "     |      Mean cross-validated score of the best_estimator\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      "     |      specified.\n",
      "     |  \n",
      "     |      This attribute is not available if ``refit`` is a function.\n",
      "     |  \n",
      "     |  best_params_ : dict\n",
      "     |      Parameter setting that gave the best results on the hold out data.\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      "     |      specified.\n",
      "     |  \n",
      "     |  best_index_ : int\n",
      "     |      The index (of the ``cv_results_`` arrays) which corresponds to the best\n",
      "     |      candidate parameter setting.\n",
      "     |  \n",
      "     |      The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n",
      "     |      the parameter setting for the best model, that gives the highest\n",
      "     |      mean score (``search.best_score_``).\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this is present only if ``refit`` is\n",
      "     |      specified.\n",
      "     |  \n",
      "     |  scorer_ : function or a dict\n",
      "     |      Scorer function used on the held out data to choose the best\n",
      "     |      parameters for the model.\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this attribute holds the validated\n",
      "     |      ``scoring`` dict which maps the scorer key to the scorer callable.\n",
      "     |  \n",
      "     |  n_splits_ : int\n",
      "     |      The number of cross-validation splits (folds/iterations).\n",
      "     |  \n",
      "     |  refit_time_ : float\n",
      "     |      Seconds used for refitting the best model on the whole dataset.\n",
      "     |  \n",
      "     |      This is present only if ``refit`` is not False.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  multimetric_ : bool\n",
      "     |      Whether or not the scorers compute several metrics.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The parameters selected are those that maximize the score of the left out\n",
      "     |  data, unless an explicit score is passed in which case it is used instead.\n",
      "     |  \n",
      "     |  If `n_jobs` was set to a value higher than one, the data is copied for each\n",
      "     |  point in the grid (and not `n_jobs` times). This is done for efficiency\n",
      "     |  reasons if individual jobs take very little time, but may raise errors if\n",
      "     |  the dataset is large and not enough memory is available.  A workaround in\n",
      "     |  this case is to set `pre_dispatch`. Then, the memory is copied only\n",
      "     |  `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n",
      "     |  n_jobs`.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  ---------\n",
      "     |  ParameterGrid : Generates all the combinations of a hyperparameter grid.\n",
      "     |  train_test_split : Utility function to split the data into a development\n",
      "     |      set usable for fitting a GridSearchCV instance and an evaluation set\n",
      "     |      for its final evaluation.\n",
      "     |  sklearn.metrics.make_scorer : Make a scorer from a performance metric or\n",
      "     |      loss function.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GridSearchCV\n",
      "     |      BaseSearchCV\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimator, param_grid, *, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', error_score=nan, return_train_score=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSearchCV:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Call decision_function on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``decision_function``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, *, groups=None, **fit_params)\n",
      "     |      Run fit with all sets of parameters.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      \n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training vector, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples, n_output)             or (n_samples,), default=None\n",
      "     |          Target relative to X for classification or regression;\n",
      "     |          None for unsupervised learning.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "     |          instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).\n",
      "     |      \n",
      "     |      **fit_params : dict of str -> object\n",
      "     |          Parameters passed to the ``fit`` method of the estimator\n",
      "     |  \n",
      "     |  inverse_transform(self, Xt)\n",
      "     |      Call inverse_transform on the estimator with the best found params.\n",
      "     |      \n",
      "     |      Only available if the underlying estimator implements\n",
      "     |      ``inverse_transform`` and ``refit=True``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      Xt : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Call predict on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``predict``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Call predict_log_proba on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``predict_log_proba``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Call predict_proba on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``predict_proba``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |  \n",
      "     |  score(self, X, y=None)\n",
      "     |      Returns the score on the given data, if the estimator has been refit.\n",
      "     |      \n",
      "     |      This uses the score defined by ``scoring`` where provided, and the\n",
      "     |      ``best_estimator_.score`` method otherwise.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input data, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples, n_output)             or (n_samples,), default=None\n",
      "     |          Target relative to X for classification or regression;\n",
      "     |          None for unsupervised learning.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |  \n",
      "     |  score_samples(self, X)\n",
      "     |      Call score_samples on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``score_samples``.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.24\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : iterable\n",
      "     |          Data to predict on. Must fulfill input requirements\n",
      "     |          of the underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_score : ndarray of shape (n_samples,)\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Call transform on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if the underlying estimator supports ``transform`` and\n",
      "     |      ``refit=True``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseSearchCV:\n",
      "     |  \n",
      "     |  classes_\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MetaEstimatorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class GroupKFold(_BaseKFold)\n",
      "     |  GroupKFold(n_splits=5)\n",
      "     |  \n",
      "     |  K-fold iterator variant with non-overlapping groups.\n",
      "     |  \n",
      "     |  The same group will not appear in two different folds (the number of\n",
      "     |  distinct groups has to be at least equal to the number of folds).\n",
      "     |  \n",
      "     |  The folds are approximately balanced in the sense that the number of\n",
      "     |  distinct groups is approximately the same in each fold.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <group_k_fold>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=5\n",
      "     |      Number of folds. Must be at least 2.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``n_splits`` default value changed from 3 to 5.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import GroupKFold\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
      "     |  >>> y = np.array([1, 2, 3, 4])\n",
      "     |  >>> groups = np.array([0, 0, 2, 2])\n",
      "     |  >>> group_kfold = GroupKFold(n_splits=2)\n",
      "     |  >>> group_kfold.get_n_splits(X, y, groups)\n",
      "     |  2\n",
      "     |  >>> print(group_kfold)\n",
      "     |  GroupKFold(n_splits=2)\n",
      "     |  >>> for train_index, test_index in group_kfold.split(X, y, groups):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  ...     print(X_train, X_test, y_train, y_test)\n",
      "     |  ...\n",
      "     |  TRAIN: [0 1] TEST: [2 3]\n",
      "     |  [[1 2]\n",
      "     |   [3 4]] [[5 6]\n",
      "     |   [7 8]] [1 2] [3 4]\n",
      "     |  TRAIN: [2 3] TEST: [0 1]\n",
      "     |  [[5 6]\n",
      "     |   [7 8]] [[1 2]\n",
      "     |   [3 4]] [3 4] [1 2]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  LeaveOneGroupOut : For splitting the data according to explicit\n",
      "     |      domain-specific stratification of the dataset.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GroupKFold\n",
      "     |      _BaseKFold\n",
      "     |      BaseCrossValidator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=5)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,), default=None\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,)\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseKFold:\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class GroupShuffleSplit(ShuffleSplit)\n",
      "     |  GroupShuffleSplit(n_splits=5, *, test_size=None, train_size=None, random_state=None)\n",
      "     |  \n",
      "     |  Shuffle-Group(s)-Out cross-validation iterator\n",
      "     |  \n",
      "     |  Provides randomized train/test indices to split data according to a\n",
      "     |  third-party provided group. This group information can be used to encode\n",
      "     |  arbitrary domain specific stratifications of the samples as integers.\n",
      "     |  \n",
      "     |  For instance the groups could be the year of collection of the samples\n",
      "     |  and thus allow for cross-validation against time-based splits.\n",
      "     |  \n",
      "     |  The difference between LeavePGroupsOut and GroupShuffleSplit is that\n",
      "     |  the former generates splits using all subsets of size ``p`` unique groups,\n",
      "     |  whereas GroupShuffleSplit generates a user-determined number of random\n",
      "     |  test splits, each with a user-determined fraction of unique groups.\n",
      "     |  \n",
      "     |  For example, a less computationally intensive alternative to\n",
      "     |  ``LeavePGroupsOut(p=10)`` would be\n",
      "     |  ``GroupShuffleSplit(test_size=10, n_splits=100)``.\n",
      "     |  \n",
      "     |  Note: The parameters ``test_size`` and ``train_size`` refer to groups, and\n",
      "     |  not to samples, as in ShuffleSplit.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <group_shuffle_split>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=5\n",
      "     |      Number of re-shuffling & splitting iterations.\n",
      "     |  \n",
      "     |  test_size : float, int, default=0.2\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "     |      of groups to include in the test split (rounded up). If int,\n",
      "     |      represents the absolute number of test groups. If None, the value is\n",
      "     |      set to the complement of the train size.\n",
      "     |      The default will change in version 0.21. It will remain 0.2 only\n",
      "     |      if ``train_size`` is unspecified, otherwise it will complement\n",
      "     |      the specified ``train_size``.\n",
      "     |  \n",
      "     |  train_size : float or int, default=None\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the\n",
      "     |      proportion of the groups to include in the train split. If\n",
      "     |      int, represents the absolute number of train groups. If None,\n",
      "     |      the value is automatically set to the complement of the test size.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the randomness of the training and testing indices produced.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import GroupShuffleSplit\n",
      "     |  >>> X = np.ones(shape=(8, 2))\n",
      "     |  >>> y = np.ones(shape=(8, 1))\n",
      "     |  >>> groups = np.array([1, 1, 2, 2, 2, 3, 3, 3])\n",
      "     |  >>> print(groups.shape)\n",
      "     |  (8,)\n",
      "     |  >>> gss = GroupShuffleSplit(n_splits=2, train_size=.7, random_state=42)\n",
      "     |  >>> gss.get_n_splits()\n",
      "     |  2\n",
      "     |  >>> for train_idx, test_idx in gss.split(X, y, groups):\n",
      "     |  ...     print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n",
      "     |  TRAIN: [2 3 4 5 6 7] TEST: [0 1]\n",
      "     |  TRAIN: [0 1 5 6 7] TEST: [2 3 4]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GroupShuffleSplit\n",
      "     |      ShuffleSplit\n",
      "     |      BaseShuffleSplit\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=5, *, test_size=None, train_size=None, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,), default=None\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,)\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Randomized CV splitters may return different results for each call of\n",
      "     |      split. You can make the results identical by setting `random_state`\n",
      "     |      to an integer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseShuffleSplit:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseShuffleSplit:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class KFold(_BaseKFold)\n",
      "     |  KFold(n_splits=5, *, shuffle=False, random_state=None)\n",
      "     |  \n",
      "     |  K-Folds cross-validator\n",
      "     |  \n",
      "     |  Provides train/test indices to split data in train/test sets. Split\n",
      "     |  dataset into k consecutive folds (without shuffling by default).\n",
      "     |  \n",
      "     |  Each fold is then used once as a validation while the k - 1 remaining\n",
      "     |  folds form the training set.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <k_fold>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=5\n",
      "     |      Number of folds. Must be at least 2.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``n_splits`` default value changed from 3 to 5.\n",
      "     |  \n",
      "     |  shuffle : bool, default=False\n",
      "     |      Whether to shuffle the data before splitting into batches.\n",
      "     |      Note that the samples within each split will not be shuffled.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      When `shuffle` is True, `random_state` affects the ordering of the\n",
      "     |      indices, which controls the randomness of each fold. Otherwise, this\n",
      "     |      parameter has no effect.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import KFold\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([1, 2, 3, 4])\n",
      "     |  >>> kf = KFold(n_splits=2)\n",
      "     |  >>> kf.get_n_splits(X)\n",
      "     |  2\n",
      "     |  >>> print(kf)\n",
      "     |  KFold(n_splits=2, random_state=None, shuffle=False)\n",
      "     |  >>> for train_index, test_index in kf.split(X):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  TRAIN: [2 3] TEST: [0 1]\n",
      "     |  TRAIN: [0 1] TEST: [2 3]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The first ``n_samples % n_splits`` folds have size\n",
      "     |  ``n_samples // n_splits + 1``, other folds have size\n",
      "     |  ``n_samples // n_splits``, where ``n_samples`` is the number of samples.\n",
      "     |  \n",
      "     |  Randomized CV splitters may return different results for each call of\n",
      "     |  split. You can make the results identical by setting `random_state`\n",
      "     |  to an integer.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  StratifiedKFold : Takes group information into account to avoid building\n",
      "     |      folds with imbalanced class distributions (for binary or multiclass\n",
      "     |      classification tasks).\n",
      "     |  \n",
      "     |  GroupKFold : K-fold iterator variant with non-overlapping groups.\n",
      "     |  \n",
      "     |  RepeatedKFold : Repeats K-Fold n times.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KFold\n",
      "     |      _BaseKFold\n",
      "     |      BaseCrossValidator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=5, *, shuffle=False, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseKFold:\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,), default=None\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LeaveOneGroupOut(BaseCrossValidator)\n",
      "     |  Leave One Group Out cross-validator\n",
      "     |  \n",
      "     |  Provides train/test indices to split data according to a third-party\n",
      "     |  provided group. This group information can be used to encode arbitrary\n",
      "     |  domain specific stratifications of the samples as integers.\n",
      "     |  \n",
      "     |  For instance the groups could be the year of collection of the samples\n",
      "     |  and thus allow for cross-validation against time-based splits.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <leave_one_group_out>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import LeaveOneGroupOut\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
      "     |  >>> y = np.array([1, 2, 1, 2])\n",
      "     |  >>> groups = np.array([1, 1, 2, 2])\n",
      "     |  >>> logo = LeaveOneGroupOut()\n",
      "     |  >>> logo.get_n_splits(X, y, groups)\n",
      "     |  2\n",
      "     |  >>> logo.get_n_splits(groups=groups)  # 'groups' is always required\n",
      "     |  2\n",
      "     |  >>> print(logo)\n",
      "     |  LeaveOneGroupOut()\n",
      "     |  >>> for train_index, test_index in logo.split(X, y, groups):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  ...     print(X_train, X_test, y_train, y_test)\n",
      "     |  TRAIN: [2 3] TEST: [0 1]\n",
      "     |  [[5 6]\n",
      "     |   [7 8]] [[1 2]\n",
      "     |   [3 4]] [1 2] [1 2]\n",
      "     |  TRAIN: [0 1] TEST: [2 3]\n",
      "     |  [[1 2]\n",
      "     |   [3 4]] [[5 6]\n",
      "     |   [7 8]] [1 2] [1 2]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LeaveOneGroupOut\n",
      "     |      BaseCrossValidator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,)\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set. This 'groups' parameter must always be specified to\n",
      "     |          calculate the number of splits, though the other parameters can be\n",
      "     |          omitted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,), default=None\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,)\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LeaveOneOut(BaseCrossValidator)\n",
      "     |  Leave-One-Out cross-validator\n",
      "     |  \n",
      "     |  Provides train/test indices to split data in train/test sets. Each\n",
      "     |  sample is used once as a test set (singleton) while the remaining\n",
      "     |  samples form the training set.\n",
      "     |  \n",
      "     |  Note: ``LeaveOneOut()`` is equivalent to ``KFold(n_splits=n)`` and\n",
      "     |  ``LeavePOut(p=1)`` where ``n`` is the number of samples.\n",
      "     |  \n",
      "     |  Due to the high number of test sets (which is the same as the\n",
      "     |  number of samples) this cross-validation method can be very costly.\n",
      "     |  For large datasets one should favor :class:`KFold`, :class:`ShuffleSplit`\n",
      "     |  or :class:`StratifiedKFold`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <leave_one_out>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import LeaveOneOut\n",
      "     |  >>> X = np.array([[1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([1, 2])\n",
      "     |  >>> loo = LeaveOneOut()\n",
      "     |  >>> loo.get_n_splits(X)\n",
      "     |  2\n",
      "     |  >>> print(loo)\n",
      "     |  LeaveOneOut()\n",
      "     |  >>> for train_index, test_index in loo.split(X):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  ...     print(X_train, X_test, y_train, y_test)\n",
      "     |  TRAIN: [1] TEST: [0]\n",
      "     |  [[3 4]] [[1 2]] [2] [1]\n",
      "     |  TRAIN: [0] TEST: [1]\n",
      "     |  [[1 2]] [[3 4]] [1] [2]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  LeaveOneGroupOut : For splitting the data according to explicit,\n",
      "     |      domain-specific stratification of the dataset.\n",
      "     |  GroupKFold : K-fold iterator variant with non-overlapping groups.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LeaveOneOut\n",
      "     |      BaseCrossValidator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  get_n_splits(self, X, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LeavePGroupsOut(BaseCrossValidator)\n",
      "     |  LeavePGroupsOut(n_groups)\n",
      "     |  \n",
      "     |  Leave P Group(s) Out cross-validator\n",
      "     |  \n",
      "     |  Provides train/test indices to split data according to a third-party\n",
      "     |  provided group. This group information can be used to encode arbitrary\n",
      "     |  domain specific stratifications of the samples as integers.\n",
      "     |  \n",
      "     |  For instance the groups could be the year of collection of the samples\n",
      "     |  and thus allow for cross-validation against time-based splits.\n",
      "     |  \n",
      "     |  The difference between LeavePGroupsOut and LeaveOneGroupOut is that\n",
      "     |  the former builds the test sets with all the samples assigned to\n",
      "     |  ``p`` different values of the groups while the latter uses samples\n",
      "     |  all assigned the same groups.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <leave_p_groups_out>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_groups : int\n",
      "     |      Number of groups (``p``) to leave out in the test split.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import LeavePGroupsOut\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [5, 6]])\n",
      "     |  >>> y = np.array([1, 2, 1])\n",
      "     |  >>> groups = np.array([1, 2, 3])\n",
      "     |  >>> lpgo = LeavePGroupsOut(n_groups=2)\n",
      "     |  >>> lpgo.get_n_splits(X, y, groups)\n",
      "     |  3\n",
      "     |  >>> lpgo.get_n_splits(groups=groups)  # 'groups' is always required\n",
      "     |  3\n",
      "     |  >>> print(lpgo)\n",
      "     |  LeavePGroupsOut(n_groups=2)\n",
      "     |  >>> for train_index, test_index in lpgo.split(X, y, groups):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  ...     print(X_train, X_test, y_train, y_test)\n",
      "     |  TRAIN: [2] TEST: [0 1]\n",
      "     |  [[5 6]] [[1 2]\n",
      "     |   [3 4]] [1] [1 2]\n",
      "     |  TRAIN: [1] TEST: [0 2]\n",
      "     |  [[3 4]] [[1 2]\n",
      "     |   [5 6]] [2] [1 1]\n",
      "     |  TRAIN: [0] TEST: [1 2]\n",
      "     |  [[1 2]] [[3 4]\n",
      "     |   [5 6]] [1] [2 1]\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  GroupKFold : K-fold iterator variant with non-overlapping groups.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LeavePGroupsOut\n",
      "     |      BaseCrossValidator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_groups)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,)\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set. This 'groups' parameter must always be specified to\n",
      "     |          calculate the number of splits, though the other parameters can be\n",
      "     |          omitted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,), default=None\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,)\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LeavePOut(BaseCrossValidator)\n",
      "     |  LeavePOut(p)\n",
      "     |  \n",
      "     |  Leave-P-Out cross-validator\n",
      "     |  \n",
      "     |  Provides train/test indices to split data in train/test sets. This results\n",
      "     |  in testing on all distinct samples of size p, while the remaining n - p\n",
      "     |  samples form the training set in each iteration.\n",
      "     |  \n",
      "     |  Note: ``LeavePOut(p)`` is NOT equivalent to\n",
      "     |  ``KFold(n_splits=n_samples // p)`` which creates non-overlapping test sets.\n",
      "     |  \n",
      "     |  Due to the high number of iterations which grows combinatorically with the\n",
      "     |  number of samples this cross-validation method can be very costly. For\n",
      "     |  large datasets one should favor :class:`KFold`, :class:`StratifiedKFold`\n",
      "     |  or :class:`ShuffleSplit`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <leave_p_out>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  p : int\n",
      "     |      Size of the test sets. Must be strictly less than the number of\n",
      "     |      samples.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import LeavePOut\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
      "     |  >>> y = np.array([1, 2, 3, 4])\n",
      "     |  >>> lpo = LeavePOut(2)\n",
      "     |  >>> lpo.get_n_splits(X)\n",
      "     |  6\n",
      "     |  >>> print(lpo)\n",
      "     |  LeavePOut(p=2)\n",
      "     |  >>> for train_index, test_index in lpo.split(X):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  TRAIN: [2 3] TEST: [0 1]\n",
      "     |  TRAIN: [1 3] TEST: [0 2]\n",
      "     |  TRAIN: [1 2] TEST: [0 3]\n",
      "     |  TRAIN: [0 3] TEST: [1 2]\n",
      "     |  TRAIN: [0 2] TEST: [1 3]\n",
      "     |  TRAIN: [0 1] TEST: [2 3]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LeavePOut\n",
      "     |      BaseCrossValidator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, p)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_n_splits(self, X, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ParameterGrid(builtins.object)\n",
      "     |  ParameterGrid(param_grid)\n",
      "     |  \n",
      "     |  Grid of parameters with a discrete number of values for each.\n",
      "     |  \n",
      "     |  Can be used to iterate over parameter value combinations with the\n",
      "     |  Python built-in function iter.\n",
      "     |  The order of the generated parameter combinations is deterministic.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <grid_search>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  param_grid : dict of str to sequence, or sequence of such\n",
      "     |      The parameter grid to explore, as a dictionary mapping estimator\n",
      "     |      parameters to sequences of allowed values.\n",
      "     |  \n",
      "     |      An empty dict signifies default parameters.\n",
      "     |  \n",
      "     |      A sequence of dicts signifies a sequence of grids to search, and is\n",
      "     |      useful to avoid exploring parameter combinations that make no sense\n",
      "     |      or have no effect. See the examples below.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.model_selection import ParameterGrid\n",
      "     |  >>> param_grid = {'a': [1, 2], 'b': [True, False]}\n",
      "     |  >>> list(ParameterGrid(param_grid)) == (\n",
      "     |  ...    [{'a': 1, 'b': True}, {'a': 1, 'b': False},\n",
      "     |  ...     {'a': 2, 'b': True}, {'a': 2, 'b': False}])\n",
      "     |  True\n",
      "     |  \n",
      "     |  >>> grid = [{'kernel': ['linear']}, {'kernel': ['rbf'], 'gamma': [1, 10]}]\n",
      "     |  >>> list(ParameterGrid(grid)) == [{'kernel': 'linear'},\n",
      "     |  ...                               {'kernel': 'rbf', 'gamma': 1},\n",
      "     |  ...                               {'kernel': 'rbf', 'gamma': 10}]\n",
      "     |  True\n",
      "     |  >>> ParameterGrid(grid)[1] == {'kernel': 'rbf', 'gamma': 1}\n",
      "     |  True\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  GridSearchCV : Uses :class:`ParameterGrid` to perform a full parallelized\n",
      "     |      parameter search.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, ind)\n",
      "     |      Get the parameters that would be ``ind``th in iteration\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ind : int\n",
      "     |          The iteration index\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict of str to any\n",
      "     |          Equal to list(self)[ind]\n",
      "     |  \n",
      "     |  __init__(self, param_grid)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Iterate over the points in the grid.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : iterator over dict of str to any\n",
      "     |          Yields dictionaries mapping each estimator parameter to one of its\n",
      "     |          allowed values.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Number of points on the grid.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ParameterSampler(builtins.object)\n",
      "     |  ParameterSampler(param_distributions, n_iter, *, random_state=None)\n",
      "     |  \n",
      "     |  Generator on parameters sampled from given distributions.\n",
      "     |  \n",
      "     |  Non-deterministic iterable over random candidate combinations for hyper-\n",
      "     |  parameter search. If all parameters are presented as a list,\n",
      "     |  sampling without replacement is performed. If at least one parameter\n",
      "     |  is given as a distribution, sampling with replacement is used.\n",
      "     |  It is highly recommended to use continuous distributions for continuous\n",
      "     |  parameters.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <grid_search>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  param_distributions : dict\n",
      "     |      Dictionary with parameters names (`str`) as keys and distributions\n",
      "     |      or lists of parameters to try. Distributions must provide a ``rvs``\n",
      "     |      method for sampling (such as those from scipy.stats.distributions).\n",
      "     |      If a list is given, it is sampled uniformly.\n",
      "     |      If a list of dicts is given, first a dict is sampled uniformly, and\n",
      "     |      then a parameter is sampled using that dict as above.\n",
      "     |  \n",
      "     |  n_iter : int\n",
      "     |      Number of parameter settings that are produced.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Pseudo random number generator state used for random uniform sampling\n",
      "     |      from lists of possible values instead of scipy.stats distributions.\n",
      "     |      Pass an int for reproducible output across multiple\n",
      "     |      function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Returns\n",
      "     |  -------\n",
      "     |  params : dict of str to any\n",
      "     |      **Yields** dictionaries mapping each estimator parameter to\n",
      "     |      as sampled value.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.model_selection import ParameterSampler\n",
      "     |  >>> from scipy.stats.distributions import expon\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> rng = np.random.RandomState(0)\n",
      "     |  >>> param_grid = {'a':[1, 2], 'b': expon()}\n",
      "     |  >>> param_list = list(ParameterSampler(param_grid, n_iter=4,\n",
      "     |  ...                                    random_state=rng))\n",
      "     |  >>> rounded_list = [dict((k, round(v, 6)) for (k, v) in d.items())\n",
      "     |  ...                 for d in param_list]\n",
      "     |  >>> rounded_list == [{'b': 0.89856, 'a': 1},\n",
      "     |  ...                  {'b': 0.923223, 'a': 1},\n",
      "     |  ...                  {'b': 1.878964, 'a': 2},\n",
      "     |  ...                  {'b': 1.038159, 'a': 2}]\n",
      "     |  True\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, param_distributions, n_iter, *, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Number of points that will be sampled.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class PredefinedSplit(BaseCrossValidator)\n",
      "     |  PredefinedSplit(test_fold)\n",
      "     |  \n",
      "     |  Predefined split cross-validator\n",
      "     |  \n",
      "     |  Provides train/test indices to split data into train/test sets using a\n",
      "     |  predefined scheme specified by the user with the ``test_fold`` parameter.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <predefined_split>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.16\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  test_fold : array-like of shape (n_samples,)\n",
      "     |      The entry ``test_fold[i]`` represents the index of the test set that\n",
      "     |      sample ``i`` belongs to. It is possible to exclude sample ``i`` from\n",
      "     |      any test set (i.e. include sample ``i`` in every training set) by\n",
      "     |      setting ``test_fold[i]`` equal to -1.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import PredefinedSplit\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([0, 0, 1, 1])\n",
      "     |  >>> test_fold = [0, 1, -1, 1]\n",
      "     |  >>> ps = PredefinedSplit(test_fold)\n",
      "     |  >>> ps.get_n_splits()\n",
      "     |  2\n",
      "     |  >>> print(ps)\n",
      "     |  PredefinedSplit(test_fold=array([ 0,  1, -1,  1]))\n",
      "     |  >>> for train_index, test_index in ps.split():\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  TRAIN: [1 2 3] TEST: [0]\n",
      "     |  TRAIN: [0 2] TEST: [1 3]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PredefinedSplit\n",
      "     |      BaseCrossValidator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, test_fold)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  split(self, X=None, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class RandomizedSearchCV(BaseSearchCV)\n",
      "     |  RandomizedSearchCV(estimator, param_distributions, *, n_iter=10, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', random_state=None, error_score=nan, return_train_score=False)\n",
      "     |  \n",
      "     |  Randomized search on hyper parameters.\n",
      "     |  \n",
      "     |  RandomizedSearchCV implements a \"fit\" and a \"score\" method.\n",
      "     |  It also implements \"score_samples\", \"predict\", \"predict_proba\",\n",
      "     |  \"decision_function\", \"transform\" and \"inverse_transform\" if they are\n",
      "     |  implemented in the estimator used.\n",
      "     |  \n",
      "     |  The parameters of the estimator used to apply these methods are optimized\n",
      "     |  by cross-validated search over parameter settings.\n",
      "     |  \n",
      "     |  In contrast to GridSearchCV, not all parameter values are tried out, but\n",
      "     |  rather a fixed number of parameter settings is sampled from the specified\n",
      "     |  distributions. The number of parameter settings that are tried is\n",
      "     |  given by n_iter.\n",
      "     |  \n",
      "     |  If all parameters are presented as a list,\n",
      "     |  sampling without replacement is performed. If at least one parameter\n",
      "     |  is given as a distribution, sampling with replacement is used.\n",
      "     |  It is highly recommended to use continuous distributions for continuous\n",
      "     |  parameters.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <randomized_parameter_search>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.14\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimator : estimator object.\n",
      "     |      A object of that type is instantiated for each grid point.\n",
      "     |      This is assumed to implement the scikit-learn estimator interface.\n",
      "     |      Either estimator needs to provide a ``score`` function,\n",
      "     |      or ``scoring`` must be passed.\n",
      "     |  \n",
      "     |  param_distributions : dict or list of dicts\n",
      "     |      Dictionary with parameters names (`str`) as keys and distributions\n",
      "     |      or lists of parameters to try. Distributions must provide a ``rvs``\n",
      "     |      method for sampling (such as those from scipy.stats.distributions).\n",
      "     |      If a list is given, it is sampled uniformly.\n",
      "     |      If a list of dicts is given, first a dict is sampled uniformly, and\n",
      "     |      then a parameter is sampled using that dict as above.\n",
      "     |  \n",
      "     |  n_iter : int, default=10\n",
      "     |      Number of parameter settings that are sampled. n_iter trades\n",
      "     |      off runtime vs quality of the solution.\n",
      "     |  \n",
      "     |  scoring : str, callable, list, tuple or dict, default=None\n",
      "     |      Strategy to evaluate the performance of the cross-validated model on\n",
      "     |      the test set.\n",
      "     |  \n",
      "     |      If `scoring` represents a single score, one can use:\n",
      "     |  \n",
      "     |      - a single string (see :ref:`scoring_parameter`);\n",
      "     |      - a callable (see :ref:`scoring`) that returns a single value.\n",
      "     |  \n",
      "     |      If `scoring` represents multiple scores, one can use:\n",
      "     |  \n",
      "     |      - a list or tuple of unique strings;\n",
      "     |      - a callable returning a dictionary where the keys are the metric\n",
      "     |        names and the values are the metric scores;\n",
      "     |      - a dictionary with metric names as keys and callables a values.\n",
      "     |  \n",
      "     |      See :ref:`multimetric_grid_search` for an example.\n",
      "     |  \n",
      "     |      If None, the estimator's score method is used.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of jobs to run in parallel.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |      .. versionchanged:: v0.20\n",
      "     |         `n_jobs` default changed from 1 to None\n",
      "     |  \n",
      "     |  refit : bool, str, or callable, default=True\n",
      "     |      Refit an estimator using the best found parameters on the whole\n",
      "     |      dataset.\n",
      "     |  \n",
      "     |      For multiple metric evaluation, this needs to be a `str` denoting the\n",
      "     |      scorer that would be used to find the best parameters for refitting\n",
      "     |      the estimator at the end.\n",
      "     |  \n",
      "     |      Where there are considerations other than maximum score in\n",
      "     |      choosing a best estimator, ``refit`` can be set to a function which\n",
      "     |      returns the selected ``best_index_`` given the ``cv_results``. In that\n",
      "     |      case, the ``best_estimator_`` and ``best_params_`` will be set\n",
      "     |      according to the returned ``best_index_`` while the ``best_score_``\n",
      "     |      attribute will not be available.\n",
      "     |  \n",
      "     |      The refitted estimator is made available at the ``best_estimator_``\n",
      "     |      attribute and permits using ``predict`` directly on this\n",
      "     |      ``RandomizedSearchCV`` instance.\n",
      "     |  \n",
      "     |      Also for multiple metric evaluation, the attributes ``best_index_``,\n",
      "     |      ``best_score_`` and ``best_params_`` will only be available if\n",
      "     |      ``refit`` is set and all of them will be determined w.r.t this specific\n",
      "     |      scorer.\n",
      "     |  \n",
      "     |      See ``scoring`` parameter to know more about multiple metric\n",
      "     |      evaluation.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.20\n",
      "     |          Support for callable added.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross validation,\n",
      "     |      - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      "     |      either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "     |      other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "     |      with `shuffle=False` so the splits will be the same across calls.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  verbose : int\n",
      "     |      Controls the verbosity: the higher, the more messages.\n",
      "     |  \n",
      "     |  pre_dispatch : int, or str, default=None\n",
      "     |      Controls the number of jobs that get dispatched during parallel\n",
      "     |      execution. Reducing this number can be useful to avoid an\n",
      "     |      explosion of memory consumption when more jobs get dispatched\n",
      "     |      than CPUs can process. This parameter can be:\n",
      "     |  \n",
      "     |          - None, in which case all the jobs are immediately\n",
      "     |            created and spawned. Use this for lightweight and\n",
      "     |            fast-running jobs, to avoid delays due to on-demand\n",
      "     |            spawning of the jobs\n",
      "     |  \n",
      "     |          - An int, giving the exact number of total jobs that are\n",
      "     |            spawned\n",
      "     |  \n",
      "     |          - A str, giving an expression as a function of n_jobs,\n",
      "     |            as in '2*n_jobs'\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Pseudo random number generator state used for random uniform sampling\n",
      "     |      from lists of possible values instead of scipy.stats distributions.\n",
      "     |      Pass an int for reproducible output across multiple\n",
      "     |      function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  error_score : 'raise' or numeric, default=np.nan\n",
      "     |      Value to assign to the score if an error occurs in estimator fitting.\n",
      "     |      If set to 'raise', the error is raised. If a numeric value is given,\n",
      "     |      FitFailedWarning is raised. This parameter does not affect the refit\n",
      "     |      step, which will always raise the error.\n",
      "     |  \n",
      "     |  return_train_score : bool, default=False\n",
      "     |      If ``False``, the ``cv_results_`` attribute will not include training\n",
      "     |      scores.\n",
      "     |      Computing training scores is used to get insights on how different\n",
      "     |      parameter settings impact the overfitting/underfitting trade-off.\n",
      "     |      However computing the scores on the training set can be computationally\n",
      "     |      expensive and is not strictly required to select the parameters that\n",
      "     |      yield the best generalization performance.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.21\n",
      "     |          Default value was changed from ``True`` to ``False``\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  cv_results_ : dict of numpy (masked) ndarrays\n",
      "     |      A dict with keys as column headers and values as columns, that can be\n",
      "     |      imported into a pandas ``DataFrame``.\n",
      "     |  \n",
      "     |      For instance the below given table\n",
      "     |  \n",
      "     |      +--------------+-------------+-------------------+---+---------------+\n",
      "     |      | param_kernel | param_gamma | split0_test_score |...|rank_test_score|\n",
      "     |      +==============+=============+===================+===+===============+\n",
      "     |      |    'rbf'     |     0.1     |       0.80        |...|       1       |\n",
      "     |      +--------------+-------------+-------------------+---+---------------+\n",
      "     |      |    'rbf'     |     0.2     |       0.84        |...|       3       |\n",
      "     |      +--------------+-------------+-------------------+---+---------------+\n",
      "     |      |    'rbf'     |     0.3     |       0.70        |...|       2       |\n",
      "     |      +--------------+-------------+-------------------+---+---------------+\n",
      "     |  \n",
      "     |      will be represented by a ``cv_results_`` dict of::\n",
      "     |  \n",
      "     |          {\n",
      "     |          'param_kernel' : masked_array(data = ['rbf', 'rbf', 'rbf'],\n",
      "     |                                        mask = False),\n",
      "     |          'param_gamma'  : masked_array(data = [0.1 0.2 0.3], mask = False),\n",
      "     |          'split0_test_score'  : [0.80, 0.84, 0.70],\n",
      "     |          'split1_test_score'  : [0.82, 0.50, 0.70],\n",
      "     |          'mean_test_score'    : [0.81, 0.67, 0.70],\n",
      "     |          'std_test_score'     : [0.01, 0.24, 0.00],\n",
      "     |          'rank_test_score'    : [1, 3, 2],\n",
      "     |          'split0_train_score' : [0.80, 0.92, 0.70],\n",
      "     |          'split1_train_score' : [0.82, 0.55, 0.70],\n",
      "     |          'mean_train_score'   : [0.81, 0.74, 0.70],\n",
      "     |          'std_train_score'    : [0.01, 0.19, 0.00],\n",
      "     |          'mean_fit_time'      : [0.73, 0.63, 0.43],\n",
      "     |          'std_fit_time'       : [0.01, 0.02, 0.01],\n",
      "     |          'mean_score_time'    : [0.01, 0.06, 0.04],\n",
      "     |          'std_score_time'     : [0.00, 0.00, 0.00],\n",
      "     |          'params'             : [{'kernel' : 'rbf', 'gamma' : 0.1}, ...],\n",
      "     |          }\n",
      "     |  \n",
      "     |      NOTE\n",
      "     |  \n",
      "     |      The key ``'params'`` is used to store a list of parameter\n",
      "     |      settings dicts for all the parameter candidates.\n",
      "     |  \n",
      "     |      The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n",
      "     |      ``std_score_time`` are all in seconds.\n",
      "     |  \n",
      "     |      For multi-metric evaluation, the scores for all the scorers are\n",
      "     |      available in the ``cv_results_`` dict at the keys ending with that\n",
      "     |      scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n",
      "     |      above. ('split0_test_precision', 'mean_train_precision' etc.)\n",
      "     |  \n",
      "     |  best_estimator_ : estimator\n",
      "     |      Estimator that was chosen by the search, i.e. estimator\n",
      "     |      which gave highest score (or smallest loss if specified)\n",
      "     |      on the left out data. Not available if ``refit=False``.\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this attribute is present only if\n",
      "     |      ``refit`` is specified.\n",
      "     |  \n",
      "     |      See ``refit`` parameter for more information on allowed values.\n",
      "     |  \n",
      "     |  best_score_ : float\n",
      "     |      Mean cross-validated score of the best_estimator.\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this is not available if ``refit`` is\n",
      "     |      ``False``. See ``refit`` parameter for more information.\n",
      "     |  \n",
      "     |      This attribute is not available if ``refit`` is a function.\n",
      "     |  \n",
      "     |  best_params_ : dict\n",
      "     |      Parameter setting that gave the best results on the hold out data.\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this is not available if ``refit`` is\n",
      "     |      ``False``. See ``refit`` parameter for more information.\n",
      "     |  \n",
      "     |  best_index_ : int\n",
      "     |      The index (of the ``cv_results_`` arrays) which corresponds to the best\n",
      "     |      candidate parameter setting.\n",
      "     |  \n",
      "     |      The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n",
      "     |      the parameter setting for the best model, that gives the highest\n",
      "     |      mean score (``search.best_score_``).\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this is not available if ``refit`` is\n",
      "     |      ``False``. See ``refit`` parameter for more information.\n",
      "     |  \n",
      "     |  scorer_ : function or a dict\n",
      "     |      Scorer function used on the held out data to choose the best\n",
      "     |      parameters for the model.\n",
      "     |  \n",
      "     |      For multi-metric evaluation, this attribute holds the validated\n",
      "     |      ``scoring`` dict which maps the scorer key to the scorer callable.\n",
      "     |  \n",
      "     |  n_splits_ : int\n",
      "     |      The number of cross-validation splits (folds/iterations).\n",
      "     |  \n",
      "     |  refit_time_ : float\n",
      "     |      Seconds used for refitting the best model on the whole dataset.\n",
      "     |  \n",
      "     |      This is present only if ``refit`` is not False.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  multimetric_ : bool\n",
      "     |      Whether or not the scorers compute several metrics.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The parameters selected are those that maximize the score of the held-out\n",
      "     |  data, according to the scoring parameter.\n",
      "     |  \n",
      "     |  If `n_jobs` was set to a value higher than one, the data is copied for each\n",
      "     |  parameter setting(and not `n_jobs` times). This is done for efficiency\n",
      "     |  reasons if individual jobs take very little time, but may raise errors if\n",
      "     |  the dataset is large and not enough memory is available.  A workaround in\n",
      "     |  this case is to set `pre_dispatch`. Then, the memory is copied only\n",
      "     |  `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n",
      "     |  n_jobs`.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  GridSearchCV : Does exhaustive search over a grid of parameters.\n",
      "     |  ParameterSampler : A generator over parameter settings, constructed from\n",
      "     |      param_distributions.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_iris\n",
      "     |  >>> from sklearn.linear_model import LogisticRegression\n",
      "     |  >>> from sklearn.model_selection import RandomizedSearchCV\n",
      "     |  >>> from scipy.stats import uniform\n",
      "     |  >>> iris = load_iris()\n",
      "     |  >>> logistic = LogisticRegression(solver='saga', tol=1e-2, max_iter=200,\n",
      "     |  ...                               random_state=0)\n",
      "     |  >>> distributions = dict(C=uniform(loc=0, scale=4),\n",
      "     |  ...                      penalty=['l2', 'l1'])\n",
      "     |  >>> clf = RandomizedSearchCV(logistic, distributions, random_state=0)\n",
      "     |  >>> search = clf.fit(iris.data, iris.target)\n",
      "     |  >>> search.best_params_\n",
      "     |  {'C': 2..., 'penalty': 'l1'}\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RandomizedSearchCV\n",
      "     |      BaseSearchCV\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimator, param_distributions, *, n_iter=10, scoring=None, n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs', random_state=None, error_score=nan, return_train_score=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseSearchCV:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Call decision_function on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``decision_function``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, *, groups=None, **fit_params)\n",
      "     |      Run fit with all sets of parameters.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      \n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training vector, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples, n_output)             or (n_samples,), default=None\n",
      "     |          Target relative to X for classification or regression;\n",
      "     |          None for unsupervised learning.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "     |          instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).\n",
      "     |      \n",
      "     |      **fit_params : dict of str -> object\n",
      "     |          Parameters passed to the ``fit`` method of the estimator\n",
      "     |  \n",
      "     |  inverse_transform(self, Xt)\n",
      "     |      Call inverse_transform on the estimator with the best found params.\n",
      "     |      \n",
      "     |      Only available if the underlying estimator implements\n",
      "     |      ``inverse_transform`` and ``refit=True``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      Xt : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Call predict on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``predict``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Call predict_log_proba on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``predict_log_proba``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Call predict_proba on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``predict_proba``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |  \n",
      "     |  score(self, X, y=None)\n",
      "     |      Returns the score on the given data, if the estimator has been refit.\n",
      "     |      \n",
      "     |      This uses the score defined by ``scoring`` where provided, and the\n",
      "     |      ``best_estimator_.score`` method otherwise.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input data, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples, n_output)             or (n_samples,), default=None\n",
      "     |          Target relative to X for classification or regression;\n",
      "     |          None for unsupervised learning.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |  \n",
      "     |  score_samples(self, X)\n",
      "     |      Call score_samples on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if ``refit=True`` and the underlying estimator supports\n",
      "     |      ``score_samples``.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.24\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : iterable\n",
      "     |          Data to predict on. Must fulfill input requirements\n",
      "     |          of the underlying estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_score : ndarray of shape (n_samples,)\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Call transform on the estimator with the best found parameters.\n",
      "     |      \n",
      "     |      Only available if the underlying estimator supports ``transform`` and\n",
      "     |      ``refit=True``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : indexable, length n_samples\n",
      "     |          Must fulfill the input assumptions of the\n",
      "     |          underlying estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseSearchCV:\n",
      "     |  \n",
      "     |  classes_\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MetaEstimatorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RepeatedKFold(_RepeatedSplits)\n",
      "     |  RepeatedKFold(*, n_splits=5, n_repeats=10, random_state=None)\n",
      "     |  \n",
      "     |  Repeated K-Fold cross validator.\n",
      "     |  \n",
      "     |  Repeats K-Fold n times with different randomization in each repetition.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <repeated_k_fold>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=5\n",
      "     |      Number of folds. Must be at least 2.\n",
      "     |  \n",
      "     |  n_repeats : int, default=10\n",
      "     |      Number of times cross-validator needs to be repeated.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the randomness of each repeated cross-validation instance.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import RepeatedKFold\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([0, 0, 1, 1])\n",
      "     |  >>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)\n",
      "     |  >>> for train_index, test_index in rkf.split(X):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  ...\n",
      "     |  TRAIN: [0 1] TEST: [2 3]\n",
      "     |  TRAIN: [2 3] TEST: [0 1]\n",
      "     |  TRAIN: [1 2] TEST: [0 3]\n",
      "     |  TRAIN: [0 3] TEST: [1 2]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Randomized CV splitters may return different results for each call of\n",
      "     |  split. You can make the results identical by setting `random_state`\n",
      "     |  to an integer.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  RepeatedStratifiedKFold : Repeats Stratified K-Fold n times.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RepeatedKFold\n",
      "     |      _RepeatedSplits\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, n_splits=5, n_repeats=10, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _RepeatedSplits:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |          ``np.zeros(n_samples)`` may be used as a placeholder.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |          ``np.zeros(n_samples)`` may be used as a placeholder.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generates indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _RepeatedSplits:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class RepeatedStratifiedKFold(_RepeatedSplits)\n",
      "     |  RepeatedStratifiedKFold(*, n_splits=5, n_repeats=10, random_state=None)\n",
      "     |  \n",
      "     |  Repeated Stratified K-Fold cross validator.\n",
      "     |  \n",
      "     |  Repeats Stratified K-Fold n times with different randomization in each\n",
      "     |  repetition.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <repeated_k_fold>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=5\n",
      "     |      Number of folds. Must be at least 2.\n",
      "     |  \n",
      "     |  n_repeats : int, default=10\n",
      "     |      Number of times cross-validator needs to be repeated.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the generation of the random states for each repetition.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import RepeatedStratifiedKFold\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([0, 0, 1, 1])\n",
      "     |  >>> rskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2,\n",
      "     |  ...     random_state=36851234)\n",
      "     |  >>> for train_index, test_index in rskf.split(X, y):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  ...\n",
      "     |  TRAIN: [1 2] TEST: [0 3]\n",
      "     |  TRAIN: [0 3] TEST: [1 2]\n",
      "     |  TRAIN: [1 3] TEST: [0 2]\n",
      "     |  TRAIN: [0 2] TEST: [1 3]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Randomized CV splitters may return different results for each call of\n",
      "     |  split. You can make the results identical by setting `random_state`\n",
      "     |  to an integer.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  RepeatedKFold : Repeats K-Fold n times.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RepeatedStratifiedKFold\n",
      "     |      _RepeatedSplits\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, n_splits=5, n_repeats=10, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _RepeatedSplits:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |          ``np.zeros(n_samples)`` may be used as a placeholder.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |          ``np.zeros(n_samples)`` may be used as a placeholder.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generates indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from _RepeatedSplits:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ShuffleSplit(BaseShuffleSplit)\n",
      "     |  ShuffleSplit(n_splits=10, *, test_size=None, train_size=None, random_state=None)\n",
      "     |  \n",
      "     |  Random permutation cross-validator\n",
      "     |  \n",
      "     |  Yields indices to split data into training and test sets.\n",
      "     |  \n",
      "     |  Note: contrary to other cross-validation strategies, random splits\n",
      "     |  do not guarantee that all folds will be different, although this is\n",
      "     |  still very likely for sizeable datasets.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <ShuffleSplit>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=10\n",
      "     |      Number of re-shuffling & splitting iterations.\n",
      "     |  \n",
      "     |  test_size : float or int, default=None\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "     |      of the dataset to include in the test split. If int, represents the\n",
      "     |      absolute number of test samples. If None, the value is set to the\n",
      "     |      complement of the train size. If ``train_size`` is also None, it will\n",
      "     |      be set to 0.1.\n",
      "     |  \n",
      "     |  train_size : float or int, default=None\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the\n",
      "     |      proportion of the dataset to include in the train split. If\n",
      "     |      int, represents the absolute number of train samples. If None,\n",
      "     |      the value is automatically set to the complement of the test size.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the randomness of the training and testing indices produced.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import ShuffleSplit\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [3, 4], [5, 6]])\n",
      "     |  >>> y = np.array([1, 2, 1, 2, 1, 2])\n",
      "     |  >>> rs = ShuffleSplit(n_splits=5, test_size=.25, random_state=0)\n",
      "     |  >>> rs.get_n_splits(X)\n",
      "     |  5\n",
      "     |  >>> print(rs)\n",
      "     |  ShuffleSplit(n_splits=5, random_state=0, test_size=0.25, train_size=None)\n",
      "     |  >>> for train_index, test_index in rs.split(X):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  TRAIN: [1 3 0 4] TEST: [5 2]\n",
      "     |  TRAIN: [4 0 2 5] TEST: [1 3]\n",
      "     |  TRAIN: [1 2 4 0] TEST: [3 5]\n",
      "     |  TRAIN: [3 4 1 0] TEST: [5 2]\n",
      "     |  TRAIN: [3 5 1 0] TEST: [2 4]\n",
      "     |  >>> rs = ShuffleSplit(n_splits=5, train_size=0.5, test_size=.25,\n",
      "     |  ...                   random_state=0)\n",
      "     |  >>> for train_index, test_index in rs.split(X):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  TRAIN: [1 3 0] TEST: [5 2]\n",
      "     |  TRAIN: [4 0 2] TEST: [1 3]\n",
      "     |  TRAIN: [1 2 4] TEST: [3 5]\n",
      "     |  TRAIN: [3 4 1] TEST: [5 2]\n",
      "     |  TRAIN: [3 5 1] TEST: [2 4]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ShuffleSplit\n",
      "     |      BaseShuffleSplit\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=10, *, test_size=None, train_size=None, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseShuffleSplit:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,), default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Randomized CV splitters may return different results for each call of\n",
      "     |      split. You can make the results identical by setting `random_state`\n",
      "     |      to an integer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseShuffleSplit:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class StratifiedKFold(_BaseKFold)\n",
      "     |  StratifiedKFold(n_splits=5, *, shuffle=False, random_state=None)\n",
      "     |  \n",
      "     |  Stratified K-Folds cross-validator.\n",
      "     |  \n",
      "     |  Provides train/test indices to split data in train/test sets.\n",
      "     |  \n",
      "     |  This cross-validation object is a variation of KFold that returns\n",
      "     |  stratified folds. The folds are made by preserving the percentage of\n",
      "     |  samples for each class.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <stratified_k_fold>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=5\n",
      "     |      Number of folds. Must be at least 2.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``n_splits`` default value changed from 3 to 5.\n",
      "     |  \n",
      "     |  shuffle : bool, default=False\n",
      "     |      Whether to shuffle each class's samples before splitting into batches.\n",
      "     |      Note that the samples within each split will not be shuffled.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      When `shuffle` is True, `random_state` affects the ordering of the\n",
      "     |      indices, which controls the randomness of each fold for each class.\n",
      "     |      Otherwise, leave `random_state` as `None`.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import StratifiedKFold\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([0, 0, 1, 1])\n",
      "     |  >>> skf = StratifiedKFold(n_splits=2)\n",
      "     |  >>> skf.get_n_splits(X, y)\n",
      "     |  2\n",
      "     |  >>> print(skf)\n",
      "     |  StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n",
      "     |  >>> for train_index, test_index in skf.split(X, y):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  TRAIN: [1 3] TEST: [0 2]\n",
      "     |  TRAIN: [0 2] TEST: [1 3]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The implementation is designed to:\n",
      "     |  \n",
      "     |  * Generate test sets such that all contain the same distribution of\n",
      "     |    classes, or as close as possible.\n",
      "     |  * Be invariant to class label: relabelling ``y = [\"Happy\", \"Sad\"]`` to\n",
      "     |    ``y = [1, 0]`` should not change the indices generated.\n",
      "     |  * Preserve order dependencies in the dataset ordering, when\n",
      "     |    ``shuffle=False``: all samples from class k in some test set were\n",
      "     |    contiguous in y, or separated in y by samples from classes other than k.\n",
      "     |  * Generate test sets where the smallest and largest differ by at most one\n",
      "     |    sample.\n",
      "     |  \n",
      "     |  .. versionchanged:: 0.22\n",
      "     |      The previous implementation did not follow the last constraint.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  RepeatedStratifiedKFold : Repeats Stratified K-Fold n times.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StratifiedKFold\n",
      "     |      _BaseKFold\n",
      "     |      BaseCrossValidator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=5, *, shuffle=False, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  split(self, X, y, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |      \n",
      "     |          Note that providing ``y`` is sufficient to generate the splits and\n",
      "     |          hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
      "     |          ``X`` instead of actual training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |          Stratification is done based on the y labels.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Randomized CV splitters may return different results for each call of\n",
      "     |      split. You can make the results identical by setting `random_state`\n",
      "     |      to an integer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseKFold:\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class StratifiedShuffleSplit(BaseShuffleSplit)\n",
      "     |  StratifiedShuffleSplit(n_splits=10, *, test_size=None, train_size=None, random_state=None)\n",
      "     |  \n",
      "     |  Stratified ShuffleSplit cross-validator\n",
      "     |  \n",
      "     |  Provides train/test indices to split data in train/test sets.\n",
      "     |  \n",
      "     |  This cross-validation object is a merge of StratifiedKFold and\n",
      "     |  ShuffleSplit, which returns stratified randomized folds. The folds\n",
      "     |  are made by preserving the percentage of samples for each class.\n",
      "     |  \n",
      "     |  Note: like the ShuffleSplit strategy, stratified random splits\n",
      "     |  do not guarantee that all folds will be different, although this is\n",
      "     |  still very likely for sizeable datasets.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <stratified_shuffle_split>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=10\n",
      "     |      Number of re-shuffling & splitting iterations.\n",
      "     |  \n",
      "     |  test_size : float or int, default=None\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "     |      of the dataset to include in the test split. If int, represents the\n",
      "     |      absolute number of test samples. If None, the value is set to the\n",
      "     |      complement of the train size. If ``train_size`` is also None, it will\n",
      "     |      be set to 0.1.\n",
      "     |  \n",
      "     |  train_size : float or int, default=None\n",
      "     |      If float, should be between 0.0 and 1.0 and represent the\n",
      "     |      proportion of the dataset to include in the train split. If\n",
      "     |      int, represents the absolute number of train samples. If None,\n",
      "     |      the value is automatically set to the complement of the test size.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the randomness of the training and testing indices produced.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import StratifiedShuffleSplit\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([0, 0, 0, 1, 1, 1])\n",
      "     |  >>> sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)\n",
      "     |  >>> sss.get_n_splits(X, y)\n",
      "     |  5\n",
      "     |  >>> print(sss)\n",
      "     |  StratifiedShuffleSplit(n_splits=5, random_state=0, ...)\n",
      "     |  >>> for train_index, test_index in sss.split(X, y):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  TRAIN: [5 2 3] TEST: [4 1 0]\n",
      "     |  TRAIN: [5 1 4] TEST: [0 2 3]\n",
      "     |  TRAIN: [5 0 2] TEST: [4 3 1]\n",
      "     |  TRAIN: [4 1 0] TEST: [2 3 5]\n",
      "     |  TRAIN: [0 5 1] TEST: [3 4 2]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StratifiedShuffleSplit\n",
      "     |      BaseShuffleSplit\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=10, *, test_size=None, train_size=None, random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  split(self, X, y, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |      \n",
      "     |          Note that providing ``y`` is sufficient to generate the splits and\n",
      "     |          hence ``np.zeros(n_samples)`` may be used as a placeholder for\n",
      "     |          ``X`` instead of actual training data.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_labels)\n",
      "     |          The target variable for supervised learning problems.\n",
      "     |          Stratification is done based on the y labels.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      Randomized CV splitters may return different results for each call of\n",
      "     |      split. You can make the results identical by setting `random_state`\n",
      "     |      to an integer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseShuffleSplit:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseShuffleSplit:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TimeSeriesSplit(_BaseKFold)\n",
      "     |  TimeSeriesSplit(n_splits=5, *, max_train_size=None, test_size=None, gap=0)\n",
      "     |  \n",
      "     |  Time Series cross-validator\n",
      "     |  \n",
      "     |  Provides train/test indices to split time series data samples\n",
      "     |  that are observed at fixed time intervals, in train/test sets.\n",
      "     |  In each split, test indices must be higher than before, and thus shuffling\n",
      "     |  in cross validator is inappropriate.\n",
      "     |  \n",
      "     |  This cross-validation object is a variation of :class:`KFold`.\n",
      "     |  In the kth split, it returns first k folds as train set and the\n",
      "     |  (k+1)th fold as test set.\n",
      "     |  \n",
      "     |  Note that unlike standard cross-validation methods, successive\n",
      "     |  training sets are supersets of those that come before them.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <time_series_split>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_splits : int, default=5\n",
      "     |      Number of splits. Must be at least 2.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``n_splits`` default value changed from 3 to 5.\n",
      "     |  \n",
      "     |  max_train_size : int, default=None\n",
      "     |      Maximum size for a single training set.\n",
      "     |  \n",
      "     |  test_size : int, default=None\n",
      "     |      Used to limit the size of the test set. Defaults to\n",
      "     |      ``n_samples // (n_splits + 1)``, which is the maximum allowed value\n",
      "     |      with ``gap=0``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  gap : int, default=0\n",
      "     |      Number of samples to exclude from the end of each train set before\n",
      "     |      the test set.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.model_selection import TimeSeriesSplit\n",
      "     |  >>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
      "     |  >>> y = np.array([1, 2, 3, 4, 5, 6])\n",
      "     |  >>> tscv = TimeSeriesSplit()\n",
      "     |  >>> print(tscv)\n",
      "     |  TimeSeriesSplit(gap=0, max_train_size=None, n_splits=5, test_size=None)\n",
      "     |  >>> for train_index, test_index in tscv.split(X):\n",
      "     |  ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...     X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...     y_train, y_test = y[train_index], y[test_index]\n",
      "     |  TRAIN: [0] TEST: [1]\n",
      "     |  TRAIN: [0 1] TEST: [2]\n",
      "     |  TRAIN: [0 1 2] TEST: [3]\n",
      "     |  TRAIN: [0 1 2 3] TEST: [4]\n",
      "     |  TRAIN: [0 1 2 3 4] TEST: [5]\n",
      "     |  >>> # Fix test_size to 2 with 12 samples\n",
      "     |  >>> X = np.random.randn(12, 2)\n",
      "     |  >>> y = np.random.randint(0, 2, 12)\n",
      "     |  >>> tscv = TimeSeriesSplit(n_splits=3, test_size=2)\n",
      "     |  >>> for train_index, test_index in tscv.split(X):\n",
      "     |  ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...    X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...    y_train, y_test = y[train_index], y[test_index]\n",
      "     |  TRAIN: [0 1 2 3 4 5] TEST: [6 7]\n",
      "     |  TRAIN: [0 1 2 3 4 5 6 7] TEST: [8 9]\n",
      "     |  TRAIN: [0 1 2 3 4 5 6 7 8 9] TEST: [10 11]\n",
      "     |  >>> # Add in a 2 period gap\n",
      "     |  >>> tscv = TimeSeriesSplit(n_splits=3, test_size=2, gap=2)\n",
      "     |  >>> for train_index, test_index in tscv.split(X):\n",
      "     |  ...    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
      "     |  ...    X_train, X_test = X[train_index], X[test_index]\n",
      "     |  ...    y_train, y_test = y[train_index], y[test_index]\n",
      "     |  TRAIN: [0 1 2 3] TEST: [6 7]\n",
      "     |  TRAIN: [0 1 2 3 4 5] TEST: [8 9]\n",
      "     |  TRAIN: [0 1 2 3 4 5 6 7] TEST: [10 11]\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The training set has size ``i * n_samples // (n_splits + 1)\n",
      "     |  + n_samples % (n_splits + 1)`` in the ``i`` th split,\n",
      "     |  with a test set of size ``n_samples//(n_splits + 1)`` by default,\n",
      "     |  where ``n_samples`` is the number of samples.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TimeSeriesSplit\n",
      "     |      _BaseKFold\n",
      "     |      BaseCrossValidator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_splits=5, *, max_train_size=None, test_size=None, gap=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  split(self, X, y=None, groups=None)\n",
      "     |      Generate indices to split data into training and test set.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training data, where n_samples is the number of samples\n",
      "     |          and n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,)\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      train : ndarray\n",
      "     |          The training set indices for that split.\n",
      "     |      \n",
      "     |      test : ndarray\n",
      "     |          The testing set indices for that split.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseKFold:\n",
      "     |  \n",
      "     |  get_n_splits(self, X=None, y=None, groups=None)\n",
      "     |      Returns the number of splitting iterations in the cross-validator\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      y : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      groups : object\n",
      "     |          Always ignored, exists for compatibility.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      n_splits : int\n",
      "     |          Returns the number of splitting iterations in the cross-validator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseCrossValidator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    check_cv(cv=5, y=None, *, classifier=False)\n",
      "        Input checker utility for building a cross-validator\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cv : int, cross-validation generator or an iterable, default=None\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "            - None, to use the default 5-fold cross validation,\n",
      "            - integer, to specify the number of folds.\n",
      "            - :term:`CV splitter`,\n",
      "            - An iterable yielding (train, test) splits as arrays of indices.\n",
      "        \n",
      "            For integer/None inputs, if classifier is True and ``y`` is either\n",
      "            binary or multiclass, :class:`StratifiedKFold` is used. In all other\n",
      "            cases, :class:`KFold` is used.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "                ``cv`` default value changed from 3-fold to 5-fold.\n",
      "        \n",
      "        y : array-like, default=None\n",
      "            The target variable for supervised learning problems.\n",
      "        \n",
      "        classifier : bool, default=False\n",
      "            Whether the task is a classification task, in which case\n",
      "            stratified KFold will be used.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        checked_cv : a cross-validator instance.\n",
      "            The return value is a cross-validator which generates the train/test\n",
      "            splits via the ``split`` method.\n",
      "    \n",
      "    cross_val_predict(estimator, X, y=None, *, groups=None, cv=None, n_jobs=None, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', method='predict')\n",
      "        Generate cross-validated estimates for each input data point\n",
      "        \n",
      "        The data is split according to the cv parameter. Each sample belongs\n",
      "        to exactly one test set, and its prediction is computed with an\n",
      "        estimator fitted on the corresponding training set.\n",
      "        \n",
      "        Passing these predictions into an evaluation metric may not be a valid\n",
      "        way to measure generalization performance. Results can differ from\n",
      "        :func:`cross_validate` and :func:`cross_val_score` unless all tests sets\n",
      "        have equal size and the metric decomposes over samples.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cross_validation>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object implementing 'fit' and 'predict'\n",
      "            The object to use to fit the data.\n",
      "        \n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            The data to fit. Can be, for example a list, or an array at least 2d.\n",
      "        \n",
      "        y : array-like of shape (n_samples,) or (n_samples, n_outputs),             default=None\n",
      "            The target variable to try to predict in the case of\n",
      "            supervised learning.\n",
      "        \n",
      "        groups : array-like of shape (n_samples,), default=None\n",
      "            Group labels for the samples used while splitting the dataset into\n",
      "            train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "            instance (e.g., :class:`GroupKFold`).\n",
      "        \n",
      "        cv : int, cross-validation generator or an iterable, default=None\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "        \n",
      "            - None, to use the default 5-fold cross validation,\n",
      "            - int, to specify the number of folds in a `(Stratified)KFold`,\n",
      "            - :term:`CV splitter`,\n",
      "            - An iterable yielding (train, test) splits as arrays of indices.\n",
      "        \n",
      "            For int/None inputs, if the estimator is a classifier and ``y`` is\n",
      "            either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "            other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "            with `shuffle=False` so the splits will be the same across calls.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "                ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            Number of jobs to run in parallel. Training the estimator and\n",
      "            predicting are parallelized over the cross-validation splits.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            The verbosity level.\n",
      "        \n",
      "        fit_params : dict, default=None\n",
      "            Parameters to pass to the fit method of the estimator.\n",
      "        \n",
      "        pre_dispatch : int or str, default='2*n_jobs'\n",
      "            Controls the number of jobs that get dispatched during parallel\n",
      "            execution. Reducing this number can be useful to avoid an\n",
      "            explosion of memory consumption when more jobs get dispatched\n",
      "            than CPUs can process. This parameter can be:\n",
      "        \n",
      "                - None, in which case all the jobs are immediately\n",
      "                  created and spawned. Use this for lightweight and\n",
      "                  fast-running jobs, to avoid delays due to on-demand\n",
      "                  spawning of the jobs\n",
      "        \n",
      "                - An int, giving the exact number of total jobs that are\n",
      "                  spawned\n",
      "        \n",
      "                - A str, giving an expression as a function of n_jobs,\n",
      "                  as in '2*n_jobs'\n",
      "        \n",
      "        method : {'predict', 'predict_proba', 'predict_log_proba',               'decision_function'}, default='predict'\n",
      "            The method to be invoked by `estimator`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        predictions : ndarray\n",
      "            This is the result of calling `method`. Shape:\n",
      "        \n",
      "                - When `method` is 'predict' and in special case where `method` is\n",
      "                  'decision_function' and the target is binary: (n_samples,)\n",
      "                - When `method` is one of {'predict_proba', 'predict_log_proba',\n",
      "                  'decision_function'} (unless special case above):\n",
      "                  (n_samples, n_classes)\n",
      "                - If `estimator` is :term:`multioutput`, an extra dimension\n",
      "                  'n_outputs' is added to the end of each shape above.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        cross_val_score : Calculate score for each CV split.\n",
      "        cross_validate : Calculate one or more scores and timings for each CV\n",
      "            split.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In the case that one or more classes are absent in a training portion, a\n",
      "        default score needs to be assigned to all instances for that class if\n",
      "        ``method`` produces columns per class, as in {'decision_function',\n",
      "        'predict_proba', 'predict_log_proba'}.  For ``predict_proba`` this value is\n",
      "        0.  In order to ensure finite output, we approximate negative infinity by\n",
      "        the minimum finite float value for the dtype in other cases.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn import datasets, linear_model\n",
      "        >>> from sklearn.model_selection import cross_val_predict\n",
      "        >>> diabetes = datasets.load_diabetes()\n",
      "        >>> X = diabetes.data[:150]\n",
      "        >>> y = diabetes.target[:150]\n",
      "        >>> lasso = linear_model.Lasso()\n",
      "        >>> y_pred = cross_val_predict(lasso, X, y, cv=3)\n",
      "    \n",
      "    cross_val_score(estimator, X, y=None, *, groups=None, scoring=None, cv=None, n_jobs=None, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', error_score=nan)\n",
      "        Evaluate a score by cross-validation\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cross_validation>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object implementing 'fit'\n",
      "            The object to use to fit the data.\n",
      "        \n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            The data to fit. Can be for example a list, or an array.\n",
      "        \n",
      "        y : array-like of shape (n_samples,) or (n_samples, n_outputs),             default=None\n",
      "            The target variable to try to predict in the case of\n",
      "            supervised learning.\n",
      "        \n",
      "        groups : array-like of shape (n_samples,), default=None\n",
      "            Group labels for the samples used while splitting the dataset into\n",
      "            train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "            instance (e.g., :class:`GroupKFold`).\n",
      "        \n",
      "        scoring : str or callable, default=None\n",
      "            A str (see model evaluation documentation) or\n",
      "            a scorer callable object / function with signature\n",
      "            ``scorer(estimator, X, y)`` which should return only\n",
      "            a single value.\n",
      "        \n",
      "            Similar to :func:`cross_validate`\n",
      "            but only a single metric is permitted.\n",
      "        \n",
      "            If None, the estimator's default scorer (if available) is used.\n",
      "        \n",
      "        cv : int, cross-validation generator or an iterable, default=None\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "        \n",
      "            - None, to use the default 5-fold cross validation,\n",
      "            - int, to specify the number of folds in a `(Stratified)KFold`,\n",
      "            - :term:`CV splitter`,\n",
      "            - An iterable yielding (train, test) splits as arrays of indices.\n",
      "        \n",
      "            For int/None inputs, if the estimator is a classifier and ``y`` is\n",
      "            either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "            other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "            with `shuffle=False` so the splits will be the same across calls.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "                ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            Number of jobs to run in parallel. Training the estimator and computing\n",
      "            the score are parallelized over the cross-validation splits.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            The verbosity level.\n",
      "        \n",
      "        fit_params : dict, default=None\n",
      "            Parameters to pass to the fit method of the estimator.\n",
      "        \n",
      "        pre_dispatch : int or str, default='2*n_jobs'\n",
      "            Controls the number of jobs that get dispatched during parallel\n",
      "            execution. Reducing this number can be useful to avoid an\n",
      "            explosion of memory consumption when more jobs get dispatched\n",
      "            than CPUs can process. This parameter can be:\n",
      "        \n",
      "                - None, in which case all the jobs are immediately\n",
      "                  created and spawned. Use this for lightweight and\n",
      "                  fast-running jobs, to avoid delays due to on-demand\n",
      "                  spawning of the jobs\n",
      "        \n",
      "                - An int, giving the exact number of total jobs that are\n",
      "                  spawned\n",
      "        \n",
      "                - A str, giving an expression as a function of n_jobs,\n",
      "                  as in '2*n_jobs'\n",
      "        \n",
      "        error_score : 'raise' or numeric, default=np.nan\n",
      "            Value to assign to the score if an error occurs in estimator fitting.\n",
      "            If set to 'raise', the error is raised.\n",
      "            If a numeric value is given, FitFailedWarning is raised.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scores : ndarray of float of shape=(len(list(cv)),)\n",
      "            Array of scores of the estimator for each run of the cross validation.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn import datasets, linear_model\n",
      "        >>> from sklearn.model_selection import cross_val_score\n",
      "        >>> diabetes = datasets.load_diabetes()\n",
      "        >>> X = diabetes.data[:150]\n",
      "        >>> y = diabetes.target[:150]\n",
      "        >>> lasso = linear_model.Lasso()\n",
      "        >>> print(cross_val_score(lasso, X, y, cv=3))\n",
      "        [0.33150734 0.08022311 0.03531764]\n",
      "        \n",
      "        See Also\n",
      "        ---------\n",
      "        cross_validate : To run cross-validation on multiple metrics and also to\n",
      "            return train scores, fit times and score times.\n",
      "        \n",
      "        cross_val_predict : Get predictions from each split of cross-validation for\n",
      "            diagnostic purposes.\n",
      "        \n",
      "        sklearn.metrics.make_scorer : Make a scorer from a performance metric or\n",
      "            loss function.\n",
      "    \n",
      "    cross_validate(estimator, X, y=None, *, groups=None, scoring=None, cv=None, n_jobs=None, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', return_train_score=False, return_estimator=False, error_score=nan)\n",
      "        Evaluate metric(s) by cross-validation and also record fit/score times.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <multimetric_cross_validation>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object implementing 'fit'\n",
      "            The object to use to fit the data.\n",
      "        \n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            The data to fit. Can be for example a list, or an array.\n",
      "        \n",
      "        y : array-like of shape (n_samples,) or (n_samples, n_outputs),             default=None\n",
      "            The target variable to try to predict in the case of\n",
      "            supervised learning.\n",
      "        \n",
      "        groups : array-like of shape (n_samples,), default=None\n",
      "            Group labels for the samples used while splitting the dataset into\n",
      "            train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "            instance (e.g., :class:`GroupKFold`).\n",
      "        \n",
      "        scoring : str, callable, list, tuple, or dict, default=None\n",
      "            Strategy to evaluate the performance of the cross-validated model on\n",
      "            the test set.\n",
      "        \n",
      "            If `scoring` represents a single score, one can use:\n",
      "        \n",
      "            - a single string (see :ref:`scoring_parameter`);\n",
      "            - a callable (see :ref:`scoring`) that returns a single value.\n",
      "        \n",
      "            If `scoring` represents multiple scores, one can use:\n",
      "        \n",
      "            - a list or tuple of unique strings;\n",
      "            - a callable returning a dictionary where the keys are the metric\n",
      "              names and the values are the metric scores;\n",
      "            - a dictionary with metric names as keys and callables a values.\n",
      "        \n",
      "            See :ref:`multimetric_grid_search` for an example.\n",
      "        \n",
      "        cv : int, cross-validation generator or an iterable, default=None\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "        \n",
      "            - None, to use the default 5-fold cross validation,\n",
      "            - int, to specify the number of folds in a `(Stratified)KFold`,\n",
      "            - :term:`CV splitter`,\n",
      "            - An iterable yielding (train, test) splits as arrays of indices.\n",
      "        \n",
      "            For int/None inputs, if the estimator is a classifier and ``y`` is\n",
      "            either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "            other cases, :class:`.Fold` is used. These splitters are instantiated\n",
      "            with `shuffle=False` so the splits will be the same across calls.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "                ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            Number of jobs to run in parallel. Training the estimator and computing\n",
      "            the score are parallelized over the cross-validation splits.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            The verbosity level.\n",
      "        \n",
      "        fit_params : dict, default=None\n",
      "            Parameters to pass to the fit method of the estimator.\n",
      "        \n",
      "        pre_dispatch : int or str, default='2*n_jobs'\n",
      "            Controls the number of jobs that get dispatched during parallel\n",
      "            execution. Reducing this number can be useful to avoid an\n",
      "            explosion of memory consumption when more jobs get dispatched\n",
      "            than CPUs can process. This parameter can be:\n",
      "        \n",
      "                - None, in which case all the jobs are immediately\n",
      "                  created and spawned. Use this for lightweight and\n",
      "                  fast-running jobs, to avoid delays due to on-demand\n",
      "                  spawning of the jobs\n",
      "        \n",
      "                - An int, giving the exact number of total jobs that are\n",
      "                  spawned\n",
      "        \n",
      "                - A str, giving an expression as a function of n_jobs,\n",
      "                  as in '2*n_jobs'\n",
      "        \n",
      "        return_train_score : bool, default=False\n",
      "            Whether to include train scores.\n",
      "            Computing training scores is used to get insights on how different\n",
      "            parameter settings impact the overfitting/underfitting trade-off.\n",
      "            However computing the scores on the training set can be computationally\n",
      "            expensive and is not strictly required to select the parameters that\n",
      "            yield the best generalization performance.\n",
      "        \n",
      "            .. versionadded:: 0.19\n",
      "        \n",
      "            .. versionchanged:: 0.21\n",
      "                Default value was changed from ``True`` to ``False``\n",
      "        \n",
      "        return_estimator : bool, default=False\n",
      "            Whether to return the estimators fitted on each split.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        error_score : 'raise' or numeric, default=np.nan\n",
      "            Value to assign to the score if an error occurs in estimator fitting.\n",
      "            If set to 'raise', the error is raised.\n",
      "            If a numeric value is given, FitFailedWarning is raised.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scores : dict of float arrays of shape (n_splits,)\n",
      "            Array of scores of the estimator for each run of the cross validation.\n",
      "        \n",
      "            A dict of arrays containing the score/time arrays for each scorer is\n",
      "            returned. The possible keys for this ``dict`` are:\n",
      "        \n",
      "                ``test_score``\n",
      "                    The score array for test scores on each cv split.\n",
      "                    Suffix ``_score`` in ``test_score`` changes to a specific\n",
      "                    metric like ``test_r2`` or ``test_auc`` if there are\n",
      "                    multiple scoring metrics in the scoring parameter.\n",
      "                ``train_score``\n",
      "                    The score array for train scores on each cv split.\n",
      "                    Suffix ``_score`` in ``train_score`` changes to a specific\n",
      "                    metric like ``train_r2`` or ``train_auc`` if there are\n",
      "                    multiple scoring metrics in the scoring parameter.\n",
      "                    This is available only if ``return_train_score`` parameter\n",
      "                    is ``True``.\n",
      "                ``fit_time``\n",
      "                    The time for fitting the estimator on the train\n",
      "                    set for each cv split.\n",
      "                ``score_time``\n",
      "                    The time for scoring the estimator on the test set for each\n",
      "                    cv split. (Note time for scoring on the train set is not\n",
      "                    included even if ``return_train_score`` is set to ``True``\n",
      "                ``estimator``\n",
      "                    The estimator objects for each cv split.\n",
      "                    This is available only if ``return_estimator`` parameter\n",
      "                    is set to ``True``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn import datasets, linear_model\n",
      "        >>> from sklearn.model_selection import cross_validate\n",
      "        >>> from sklearn.metrics import make_scorer\n",
      "        >>> from sklearn.metrics import confusion_matrix\n",
      "        >>> from sklearn.svm import LinearSVC\n",
      "        >>> diabetes = datasets.load_diabetes()\n",
      "        >>> X = diabetes.data[:150]\n",
      "        >>> y = diabetes.target[:150]\n",
      "        >>> lasso = linear_model.Lasso()\n",
      "        \n",
      "        Single metric evaluation using ``cross_validate``\n",
      "        \n",
      "        >>> cv_results = cross_validate(lasso, X, y, cv=3)\n",
      "        >>> sorted(cv_results.keys())\n",
      "        ['fit_time', 'score_time', 'test_score']\n",
      "        >>> cv_results['test_score']\n",
      "        array([0.33150734, 0.08022311, 0.03531764])\n",
      "        \n",
      "        Multiple metric evaluation using ``cross_validate``\n",
      "        (please refer the ``scoring`` parameter doc for more information)\n",
      "        \n",
      "        >>> scores = cross_validate(lasso, X, y, cv=3,\n",
      "        ...                         scoring=('r2', 'neg_mean_squared_error'),\n",
      "        ...                         return_train_score=True)\n",
      "        >>> print(scores['test_neg_mean_squared_error'])\n",
      "        [-3635.5... -3573.3... -6114.7...]\n",
      "        >>> print(scores['train_r2'])\n",
      "        [0.28010158 0.39088426 0.22784852]\n",
      "        \n",
      "        See Also\n",
      "        ---------\n",
      "        cross_val_score : Run cross-validation for single metric evaluation.\n",
      "        \n",
      "        cross_val_predict : Get predictions from each split of cross-validation for\n",
      "            diagnostic purposes.\n",
      "        \n",
      "        sklearn.metrics.make_scorer : Make a scorer from a performance metric or\n",
      "            loss function.\n",
      "    \n",
      "    fit_grid_point(X, y, estimator, parameters, train, test, scorer, verbose, error_score=nan, **fit_params)\n",
      "        DEPRECATED: fit_grid_point is deprecated in version 0.23 and will be removed in version 1.0 (renaming of 0.25)\n",
      "        \n",
      "        Run fit on one set of parameters.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like, sparse matrix or list\n",
      "            Input data.\n",
      "        \n",
      "        y : array-like or None\n",
      "            Targets for input data.\n",
      "        \n",
      "        estimator : estimator object\n",
      "            A object of that type is instantiated for each grid point.\n",
      "            This is assumed to implement the scikit-learn estimator interface.\n",
      "            Either estimator needs to provide a ``score`` function,\n",
      "            or ``scoring`` must be passed.\n",
      "        \n",
      "        parameters : dict\n",
      "            Parameters to be set on estimator for this grid point.\n",
      "        \n",
      "        train : ndarray, dtype int or bool\n",
      "            Boolean mask or indices for training set.\n",
      "        \n",
      "        test : ndarray, dtype int or bool\n",
      "            Boolean mask or indices for test set.\n",
      "        \n",
      "        scorer : callable or None\n",
      "            The scorer callable object / function must have its signature as\n",
      "            ``scorer(estimator, X, y)``.\n",
      "        \n",
      "            If ``None`` the estimator's score method is used.\n",
      "        \n",
      "        verbose : int\n",
      "            Verbosity level.\n",
      "        \n",
      "        **fit_params : kwargs\n",
      "            Additional parameter passed to the fit function of the estimator.\n",
      "        \n",
      "        error_score : 'raise' or numeric, default=np.nan\n",
      "            Value to assign to the score if an error occurs in estimator fitting.\n",
      "            If set to 'raise', the error is raised. If a numeric value is given,\n",
      "            FitFailedWarning is raised. This parameter does not affect the refit\n",
      "            step, which will always raise the error.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "             Score of this parameter setting on given test split.\n",
      "        \n",
      "        parameters : dict\n",
      "            The parameters that have been evaluated.\n",
      "        \n",
      "        n_samples_test : int\n",
      "            Number of test samples in this split.\n",
      "    \n",
      "    learning_curve(estimator, X, y, *, groups=None, train_sizes=array([0.1  , 0.325, 0.55 , 0.775, 1.   ]), cv=None, scoring=None, exploit_incremental_learning=False, n_jobs=None, pre_dispatch='all', verbose=0, shuffle=False, random_state=None, error_score=nan, return_times=False, fit_params=None)\n",
      "        Learning curve.\n",
      "        \n",
      "        Determines cross-validated training and test scores for different training\n",
      "        set sizes.\n",
      "        \n",
      "        A cross-validation generator splits the whole dataset k times in training\n",
      "        and test data. Subsets of the training set with varying sizes will be used\n",
      "        to train the estimator and a score for each training subset size and the\n",
      "        test set will be computed. Afterwards, the scores will be averaged over\n",
      "        all k runs for each training subset size.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <learning_curve>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : object type that implements the \"fit\" and \"predict\" methods\n",
      "            An object of that type which is cloned for each validation.\n",
      "        \n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            Training vector, where n_samples is the number of samples and\n",
      "            n_features is the number of features.\n",
      "        \n",
      "        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Target relative to X for classification or regression;\n",
      "            None for unsupervised learning.\n",
      "        \n",
      "        groups : array-like of  shape (n_samples,), default=None\n",
      "            Group labels for the samples used while splitting the dataset into\n",
      "            train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "            instance (e.g., :class:`GroupKFold`).\n",
      "        \n",
      "        train_sizes : array-like of shape (n_ticks,),             default=np.linspace(0.1, 1.0, 5)\n",
      "            Relative or absolute numbers of training examples that will be used to\n",
      "            generate the learning curve. If the dtype is float, it is regarded as a\n",
      "            fraction of the maximum size of the training set (that is determined\n",
      "            by the selected validation method), i.e. it has to be within (0, 1].\n",
      "            Otherwise it is interpreted as absolute sizes of the training sets.\n",
      "            Note that for classification the number of samples usually have to\n",
      "            be big enough to contain at least one sample from each class.\n",
      "        \n",
      "        cv : int, cross-validation generator or an iterable, default=None\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "        \n",
      "            - None, to use the default 5-fold cross validation,\n",
      "            - int, to specify the number of folds in a `(Stratified)KFold`,\n",
      "            - :term:`CV splitter`,\n",
      "            - An iterable yielding (train, test) splits as arrays of indices.\n",
      "        \n",
      "            For int/None inputs, if the estimator is a classifier and ``y`` is\n",
      "            either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "            other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "            with `shuffle=False` so the splits will be the same across calls.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "                ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "        \n",
      "        scoring : str or callable, default=None\n",
      "            A str (see model evaluation documentation) or\n",
      "            a scorer callable object / function with signature\n",
      "            ``scorer(estimator, X, y)``.\n",
      "        \n",
      "        exploit_incremental_learning : bool, default=False\n",
      "            If the estimator supports incremental learning, this will be\n",
      "            used to speed up fitting for different training set sizes.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            Number of jobs to run in parallel. Training the estimator and computing\n",
      "            the score are parallelized over the different training and test sets.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        pre_dispatch : int or str, default='all'\n",
      "            Number of predispatched jobs for parallel execution (default is\n",
      "            all). The option can reduce the allocated memory. The str can\n",
      "            be an expression like '2*n_jobs'.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            Controls the verbosity: the higher, the more messages.\n",
      "        \n",
      "        shuffle : bool, default=False\n",
      "            Whether to shuffle training data before taking prefixes of it\n",
      "            based on``train_sizes``.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Used when ``shuffle`` is True. Pass an int for reproducible\n",
      "            output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        error_score : 'raise' or numeric, default=np.nan\n",
      "            Value to assign to the score if an error occurs in estimator fitting.\n",
      "            If set to 'raise', the error is raised.\n",
      "            If a numeric value is given, FitFailedWarning is raised.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        return_times : bool, default=False\n",
      "            Whether to return the fit and score times.\n",
      "        \n",
      "        fit_params : dict, default=None\n",
      "            Parameters to pass to the fit method of the estimator.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        train_sizes_abs : array of shape (n_unique_ticks,)\n",
      "            Numbers of training examples that has been used to generate the\n",
      "            learning curve. Note that the number of ticks might be less\n",
      "            than n_ticks because duplicate entries will be removed.\n",
      "        \n",
      "        train_scores : array of shape (n_ticks, n_cv_folds)\n",
      "            Scores on training sets.\n",
      "        \n",
      "        test_scores : array of shape (n_ticks, n_cv_folds)\n",
      "            Scores on test set.\n",
      "        \n",
      "        fit_times : array of shape (n_ticks, n_cv_folds)\n",
      "            Times spent for fitting in seconds. Only present if ``return_times``\n",
      "            is True.\n",
      "        \n",
      "        score_times : array of shape (n_ticks, n_cv_folds)\n",
      "            Times spent for scoring in seconds. Only present if ``return_times``\n",
      "            is True.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        See :ref:`examples/model_selection/plot_learning_curve.py\n",
      "        <sphx_glr_auto_examples_model_selection_plot_learning_curve.py>`\n",
      "    \n",
      "    permutation_test_score(estimator, X, y, *, groups=None, cv=None, n_permutations=100, n_jobs=None, random_state=0, verbose=0, scoring=None, fit_params=None)\n",
      "        Evaluate the significance of a cross-validated score with permutations\n",
      "        \n",
      "        Permutes targets to generate 'randomized data' and compute the empirical\n",
      "        p-value against the null hypothesis that features and targets are\n",
      "        independent.\n",
      "        \n",
      "        The p-value represents the fraction of randomized data sets where the\n",
      "        estimator performed as well or better than in the original data. A small\n",
      "        p-value suggests that there is a real dependency between features and\n",
      "        targets which has been used by the estimator to give good predictions.\n",
      "        A large p-value may be due to lack of real dependency between features\n",
      "        and targets or the estimator was not able to use the dependency to\n",
      "        give good predictions.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <permutation_test_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object implementing 'fit'\n",
      "            The object to use to fit the data.\n",
      "        \n",
      "        X : array-like of shape at least 2D\n",
      "            The data to fit.\n",
      "        \n",
      "        y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None\n",
      "            The target variable to try to predict in the case of\n",
      "            supervised learning.\n",
      "        \n",
      "        groups : array-like of shape (n_samples,), default=None\n",
      "            Labels to constrain permutation within groups, i.e. ``y`` values\n",
      "            are permuted among samples with the same group identifier.\n",
      "            When not specified, ``y`` values are permuted among all samples.\n",
      "        \n",
      "            When a grouped cross-validator is used, the group labels are\n",
      "            also passed on to the ``split`` method of the cross-validator. The\n",
      "            cross-validator uses them for grouping the samples  while splitting\n",
      "            the dataset into train/test set.\n",
      "        \n",
      "        scoring : str or callable, default=None\n",
      "            A single str (see :ref:`scoring_parameter`) or a callable\n",
      "            (see :ref:`scoring`) to evaluate the predictions on the test set.\n",
      "        \n",
      "            If None the estimator's score method is used.\n",
      "        \n",
      "        cv : int, cross-validation generator or an iterable, default=None\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "        \n",
      "            - None, to use the default 5-fold cross validation,\n",
      "            - int, to specify the number of folds in a `(Stratified)KFold`,\n",
      "            - :term:`CV splitter`,\n",
      "            - An iterable yielding (train, test) splits as arrays of indices.\n",
      "        \n",
      "            For int/None inputs, if the estimator is a classifier and ``y`` is\n",
      "            either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "            other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "            with `shuffle=False` so the splits will be the same across calls.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "                ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "        \n",
      "        n_permutations : int, default=100\n",
      "            Number of times to permute ``y``.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            Number of jobs to run in parallel. Training the estimator and computing\n",
      "            the cross-validated score are parallelized over the permutations.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=0\n",
      "            Pass an int for reproducible output for permutation of\n",
      "            ``y`` values among samples. See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            The verbosity level.\n",
      "        \n",
      "        fit_params : dict, default=None\n",
      "            Parameters to pass to the fit method of the estimator.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            The true score without permuting targets.\n",
      "        \n",
      "        permutation_scores : array of shape (n_permutations,)\n",
      "            The scores obtained for each permutations.\n",
      "        \n",
      "        pvalue : float\n",
      "            The p-value, which approximates the probability that the score would\n",
      "            be obtained by chance. This is calculated as:\n",
      "        \n",
      "            `(C + 1) / (n_permutations + 1)`\n",
      "        \n",
      "            Where C is the number of permutations whose score >= the true score.\n",
      "        \n",
      "            The best possible p-value is 1/(n_permutations + 1), the worst is 1.0.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function implements Test 1 in:\n",
      "        \n",
      "            Ojala and Garriga. `Permutation Tests for Studying Classifier\n",
      "            Performance\n",
      "            <http://www.jmlr.org/papers/volume11/ojala10a/ojala10a.pdf>`_. The\n",
      "            Journal of Machine Learning Research (2010) vol. 11\n",
      "    \n",
      "    train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
      "        Split arrays or matrices into random train and test subsets\n",
      "        \n",
      "        Quick utility that wraps input validation and\n",
      "        ``next(ShuffleSplit().split(X, y))`` and application to input data\n",
      "        into a single call for splitting (and optionally subsampling) data in a\n",
      "        oneliner.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cross_validation>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        *arrays : sequence of indexables with same length / shape[0]\n",
      "            Allowed inputs are lists, numpy arrays, scipy-sparse\n",
      "            matrices or pandas dataframes.\n",
      "        \n",
      "        test_size : float or int, default=None\n",
      "            If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "            of the dataset to include in the test split. If int, represents the\n",
      "            absolute number of test samples. If None, the value is set to the\n",
      "            complement of the train size. If ``train_size`` is also None, it will\n",
      "            be set to 0.25.\n",
      "        \n",
      "        train_size : float or int, default=None\n",
      "            If float, should be between 0.0 and 1.0 and represent the\n",
      "            proportion of the dataset to include in the train split. If\n",
      "            int, represents the absolute number of train samples. If None,\n",
      "            the value is automatically set to the complement of the test size.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Controls the shuffling applied to the data before applying the split.\n",
      "            Pass an int for reproducible output across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        \n",
      "        shuffle : bool, default=True\n",
      "            Whether or not to shuffle the data before splitting. If shuffle=False\n",
      "            then stratify must be None.\n",
      "        \n",
      "        stratify : array-like, default=None\n",
      "            If not None, data is split in a stratified fashion, using this as\n",
      "            the class labels.\n",
      "            Read more in the :ref:`User Guide <stratification>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        splitting : list, length=2 * len(arrays)\n",
      "            List containing train-test split of inputs.\n",
      "        \n",
      "            .. versionadded:: 0.16\n",
      "                If the input is sparse, the output will be a\n",
      "                ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
      "                input type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.model_selection import train_test_split\n",
      "        >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
      "        >>> X\n",
      "        array([[0, 1],\n",
      "               [2, 3],\n",
      "               [4, 5],\n",
      "               [6, 7],\n",
      "               [8, 9]])\n",
      "        >>> list(y)\n",
      "        [0, 1, 2, 3, 4]\n",
      "        \n",
      "        >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "        ...     X, y, test_size=0.33, random_state=42)\n",
      "        ...\n",
      "        >>> X_train\n",
      "        array([[4, 5],\n",
      "               [0, 1],\n",
      "               [6, 7]])\n",
      "        >>> y_train\n",
      "        [2, 0, 3]\n",
      "        >>> X_test\n",
      "        array([[2, 3],\n",
      "               [8, 9]])\n",
      "        >>> y_test\n",
      "        [1, 4]\n",
      "        \n",
      "        >>> train_test_split(y, shuffle=False)\n",
      "        [[0, 1, 2], [3, 4]]\n",
      "    \n",
      "    validation_curve(estimator, X, y, *, param_name, param_range, groups=None, cv=None, scoring=None, n_jobs=None, pre_dispatch='all', verbose=0, error_score=nan, fit_params=None)\n",
      "        Validation curve.\n",
      "        \n",
      "        Determine training and test scores for varying parameter values.\n",
      "        \n",
      "        Compute scores for an estimator with different values of a specified\n",
      "        parameter. This is similar to grid search with one parameter. However, this\n",
      "        will also compute training scores and is merely a utility for plotting the\n",
      "        results.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <validation_curve>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : object type that implements the \"fit\" and \"predict\" methods\n",
      "            An object of that type which is cloned for each validation.\n",
      "        \n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            Training vector, where n_samples is the number of samples and\n",
      "            n_features is the number of features.\n",
      "        \n",
      "        y : array-like of shape (n_samples,) or (n_samples, n_outputs) or None\n",
      "            Target relative to X for classification or regression;\n",
      "            None for unsupervised learning.\n",
      "        \n",
      "        param_name : str\n",
      "            Name of the parameter that will be varied.\n",
      "        \n",
      "        param_range : array-like of shape (n_values,)\n",
      "            The values of the parameter that will be evaluated.\n",
      "        \n",
      "        groups : array-like of shape (n_samples,), default=None\n",
      "            Group labels for the samples used while splitting the dataset into\n",
      "            train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "            instance (e.g., :class:`GroupKFold`).\n",
      "        \n",
      "        cv : int, cross-validation generator or an iterable, default=None\n",
      "            Determines the cross-validation splitting strategy.\n",
      "            Possible inputs for cv are:\n",
      "        \n",
      "            - None, to use the default 5-fold cross validation,\n",
      "            - int, to specify the number of folds in a `(Stratified)KFold`,\n",
      "            - :term:`CV splitter`,\n",
      "            - An iterable yielding (train, test) splits as arrays of indices.\n",
      "        \n",
      "            For int/None inputs, if the estimator is a classifier and ``y`` is\n",
      "            either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "            other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "            with `shuffle=False` so the splits will be the same across calls.\n",
      "        \n",
      "            Refer :ref:`User Guide <cross_validation>` for the various\n",
      "            cross-validation strategies that can be used here.\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "                ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "        \n",
      "        scoring : str or callable, default=None\n",
      "            A str (see model evaluation documentation) or\n",
      "            a scorer callable object / function with signature\n",
      "            ``scorer(estimator, X, y)``.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            Number of jobs to run in parallel. Training the estimator and computing\n",
      "            the score are parallelized over the combinations of each parameter\n",
      "            value and each cross-validation split.\n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        pre_dispatch : int or str, default='all'\n",
      "            Number of predispatched jobs for parallel execution (default is\n",
      "            all). The option can reduce the allocated memory. The str can\n",
      "            be an expression like '2*n_jobs'.\n",
      "        \n",
      "        verbose : int, default=0\n",
      "            Controls the verbosity: the higher, the more messages.\n",
      "        \n",
      "        fit_params : dict, default=None\n",
      "            Parameters to pass to the fit method of the estimator.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        error_score : 'raise' or numeric, default=np.nan\n",
      "            Value to assign to the score if an error occurs in estimator fitting.\n",
      "            If set to 'raise', the error is raised.\n",
      "            If a numeric value is given, FitFailedWarning is raised.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        train_scores : array of shape (n_ticks, n_cv_folds)\n",
      "            Scores on training sets.\n",
      "        \n",
      "        test_scores : array of shape (n_ticks, n_cv_folds)\n",
      "            Scores on test set.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        See :ref:`sphx_glr_auto_examples_model_selection_plot_validation_curve.py`\n",
      "\n",
      "DATA\n",
      "    __all__ = ['BaseCrossValidator', 'GridSearchCV', 'TimeSeriesSplit', 'K...\n",
      "\n",
      "FILE\n",
      "    /opt/anaconda3/envs/ksunsupervised/lib/python3.9/site-packages/sklearn/model_selection/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d10ad72",
   "metadata": {},
   "source": [
    "```model_selection.train_test_split:``` --> Split arrays or matrices into random train and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6de3b27b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train_test_split in module sklearn.model_selection._split:\n",
      "\n",
      "train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
      "    Split arrays or matrices into random train and test subsets\n",
      "    \n",
      "    Quick utility that wraps input validation and\n",
      "    ``next(ShuffleSplit().split(X, y))`` and application to input data\n",
      "    into a single call for splitting (and optionally subsampling) data in a\n",
      "    oneliner.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <cross_validation>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    *arrays : sequence of indexables with same length / shape[0]\n",
      "        Allowed inputs are lists, numpy arrays, scipy-sparse\n",
      "        matrices or pandas dataframes.\n",
      "    \n",
      "    test_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "        of the dataset to include in the test split. If int, represents the\n",
      "        absolute number of test samples. If None, the value is set to the\n",
      "        complement of the train size. If ``train_size`` is also None, it will\n",
      "        be set to 0.25.\n",
      "    \n",
      "    train_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the\n",
      "        proportion of the dataset to include in the train split. If\n",
      "        int, represents the absolute number of train samples. If None,\n",
      "        the value is automatically set to the complement of the test size.\n",
      "    \n",
      "    random_state : int, RandomState instance or None, default=None\n",
      "        Controls the shuffling applied to the data before applying the split.\n",
      "        Pass an int for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "    \n",
      "    \n",
      "    shuffle : bool, default=True\n",
      "        Whether or not to shuffle the data before splitting. If shuffle=False\n",
      "        then stratify must be None.\n",
      "    \n",
      "    stratify : array-like, default=None\n",
      "        If not None, data is split in a stratified fashion, using this as\n",
      "        the class labels.\n",
      "        Read more in the :ref:`User Guide <stratification>`.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    splitting : list, length=2 * len(arrays)\n",
      "        List containing train-test split of inputs.\n",
      "    \n",
      "        .. versionadded:: 0.16\n",
      "            If the input is sparse, the output will be a\n",
      "            ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
      "            input type.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.model_selection import train_test_split\n",
      "    >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
      "    >>> X\n",
      "    array([[0, 1],\n",
      "           [2, 3],\n",
      "           [4, 5],\n",
      "           [6, 7],\n",
      "           [8, 9]])\n",
      "    >>> list(y)\n",
      "    [0, 1, 2, 3, 4]\n",
      "    \n",
      "    >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "    ...     X, y, test_size=0.33, random_state=42)\n",
      "    ...\n",
      "    >>> X_train\n",
      "    array([[4, 5],\n",
      "           [0, 1],\n",
      "           [6, 7]])\n",
      "    >>> y_train\n",
      "    [2, 0, 3]\n",
      "    >>> X_test\n",
      "    array([[2, 3],\n",
      "           [8, 9]])\n",
      "    >>> y_test\n",
      "    [1, 4]\n",
      "    \n",
      "    >>> train_test_split(y, shuffle=False)\n",
      "    [[0, 1, 2], [3, 4]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_selection.train_test_split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e9e00d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f300e",
   "metadata": {},
   "source": [
    "LinearRegression fits a linear model with coefficients w = (w1, ..., wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "650882f2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LinearRegression in module sklearn.linear_model._base:\n",
      "\n",
      "class LinearRegression(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, LinearModel)\n",
      " |  LinearRegression(*, fit_intercept=True, normalize=False, copy_X=True, n_jobs=None, positive=False)\n",
      " |  \n",
      " |  Ordinary least squares Linear Regression.\n",
      " |  \n",
      " |  LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\n",
      " |  to minimize the residual sum of squares between the observed targets in\n",
      " |  the dataset, and the targets predicted by the linear approximation.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  fit_intercept : bool, default=True\n",
      " |      Whether to calculate the intercept for this model. If set\n",
      " |      to False, no intercept will be used in calculations\n",
      " |      (i.e. data is expected to be centered).\n",
      " |  \n",
      " |  normalize : bool, default=False\n",
      " |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      " |      If True, the regressors X will be normalized before regression by\n",
      " |      subtracting the mean and dividing by the l2-norm.\n",
      " |      If you wish to standardize, please use\n",
      " |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      " |      on an estimator with ``normalize=False``.\n",
      " |  \n",
      " |  copy_X : bool, default=True\n",
      " |      If True, X will be copied; else, it may be overwritten.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to use for the computation. This will only provide\n",
      " |      speedup for n_targets > 1 and sufficient large problems.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  positive : bool, default=False\n",
      " |      When set to ``True``, forces the coefficients to be positive. This\n",
      " |      option is only supported for dense arrays.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : array of shape (n_features, ) or (n_targets, n_features)\n",
      " |      Estimated coefficients for the linear regression problem.\n",
      " |      If multiple targets are passed during the fit (y 2D), this\n",
      " |      is a 2D array of shape (n_targets, n_features), while if only\n",
      " |      one target is passed, this is a 1D array of length n_features.\n",
      " |  \n",
      " |  rank_ : int\n",
      " |      Rank of matrix `X`. Only available when `X` is dense.\n",
      " |  \n",
      " |  singular_ : array of shape (min(X, y),)\n",
      " |      Singular values of `X`. Only available when `X` is dense.\n",
      " |  \n",
      " |  intercept_ : float or array of shape (n_targets,)\n",
      " |      Independent term in the linear model. Set to 0.0 if\n",
      " |      `fit_intercept = False`.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  Ridge : Ridge regression addresses some of the\n",
      " |      problems of Ordinary Least Squares by imposing a penalty on the\n",
      " |      size of the coefficients with l2 regularization.\n",
      " |  Lasso : The Lasso is a linear model that estimates\n",
      " |      sparse coefficients with l1 regularization.\n",
      " |  ElasticNet : Elastic-Net is a linear regression\n",
      " |      model trained with both l1 and l2 -norm regularization of the\n",
      " |      coefficients.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  From the implementation point of view, this is just plain Ordinary\n",
      " |  Least Squares (scipy.linalg.lstsq) or Non Negative Least Squares\n",
      " |  (scipy.optimize.nnls) wrapped as a predictor object.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.linear_model import LinearRegression\n",
      " |  >>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
      " |  >>> # y = 1 * x_0 + 2 * x_1 + 3\n",
      " |  >>> y = np.dot(X, np.array([1, 2])) + 3\n",
      " |  >>> reg = LinearRegression().fit(X, y)\n",
      " |  >>> reg.score(X, y)\n",
      " |  1.0\n",
      " |  >>> reg.coef_\n",
      " |  array([1., 2.])\n",
      " |  >>> reg.intercept_\n",
      " |  3.0...\n",
      " |  >>> reg.predict(np.array([[3, 5]]))\n",
      " |  array([16.])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LinearRegression\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      LinearModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, fit_intercept=True, normalize=False, copy_X=True, n_jobs=None, positive=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit linear model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training data\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      " |          Target values. Will be cast to X's dtype if necessary\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Individual weights for each sample\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             parameter *sample_weight* support to LinearRegression.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination :math:`R^2` of the\n",
      " |      prediction.\n",
      " |      \n",
      " |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      " |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      " |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      " |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      " |      can be negative (because the model can be arbitrarily worse). A\n",
      " |      constant model that always predicts the expected value of `y`,\n",
      " |      disregarding the input features, would get a :math:`R^2` score of\n",
      " |      0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a precomputed\n",
      " |          kernel matrix or a list of generic objects instead with shape\n",
      " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      " |          is the number of samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from LinearModel:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the linear model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape (n_samples,)\n",
      " |          Returns predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LinearRegression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fd5dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c8022d",
   "metadata": {},
   "source": [
    "`metrics` includes score functions, performance metrics and pairwise metrics and distance computations. Some of these metrics (like mean absolute error, mean squared error or root mean squared error) help to identify if our model is good or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09be101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9e37235",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.metrics in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.metrics\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.metrics` module includes score functions, performance metrics\n",
      "    and pairwise metrics and distance computations.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _base\n",
      "    _classification\n",
      "    _pairwise_fast\n",
      "    _plot (package)\n",
      "    _ranking\n",
      "    _regression\n",
      "    _scorer\n",
      "    cluster (package)\n",
      "    pairwise\n",
      "    setup\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay\n",
      "        sklearn.metrics._plot.det_curve.DetCurveDisplay\n",
      "        sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay\n",
      "        sklearn.metrics._plot.roc_curve.RocCurveDisplay\n",
      "    \n",
      "    class ConfusionMatrixDisplay(builtins.object)\n",
      "     |  ConfusionMatrixDisplay(confusion_matrix, *, display_labels=None)\n",
      "     |  \n",
      "     |  Confusion Matrix visualization.\n",
      "     |  \n",
      "     |  It is recommend to use :func:`~sklearn.metrics.plot_confusion_matrix` to\n",
      "     |  create a :class:`ConfusionMatrixDisplay`. All parameters are stored as\n",
      "     |  attributes.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  confusion_matrix : ndarray of shape (n_classes, n_classes)\n",
      "     |      Confusion matrix.\n",
      "     |  \n",
      "     |  display_labels : ndarray of shape (n_classes,), default=None\n",
      "     |      Display labels for plot. If None, display labels are set from 0 to\n",
      "     |      `n_classes - 1`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  im_ : matplotlib AxesImage\n",
      "     |      Image representing the confusion matrix.\n",
      "     |  \n",
      "     |  text_ : ndarray of shape (n_classes, n_classes), dtype=matplotlib Text,             or None\n",
      "     |      Array of matplotlib axes. `None` if `include_values` is false.\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with confusion matrix.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the confusion matrix.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  confusion_matrix : Compute Confusion Matrix to evaluate the accuracy of a\n",
      "     |      classification.\n",
      "     |  ConfusionMatrixDisplay.from_estimator : Plot the confusion matrix\n",
      "     |      given an estimator, the data, and the label.\n",
      "     |  ConfusionMatrixDisplay.from_predictions : Plot the confusion matrix\n",
      "     |      given the true and predicted labels.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> from sklearn.svm import SVC\n",
      "     |  >>> X, y = make_classification(random_state=0)\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
      "     |  ...                                                     random_state=0)\n",
      "     |  >>> clf = SVC(random_state=0)\n",
      "     |  >>> clf.fit(X_train, y_train)\n",
      "     |  SVC(random_state=0)\n",
      "     |  >>> predictions = clf.predict(X_test)\n",
      "     |  >>> cm = confusion_matrix(y_test, predictions, labels=clf.classes_)\n",
      "     |  >>> disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
      "     |  ...                               display_labels=clf.classes_)\n",
      "     |  >>> disp.plot() # doctest: +SKIP\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, confusion_matrix, *, display_labels=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, *, include_values=True, cmap='viridis', xticks_rotation='horizontal', values_format=None, ax=None, colorbar=True)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      include_values : bool, default=True\n",
      "     |          Includes values in confusion matrix.\n",
      "     |      \n",
      "     |      cmap : str or matplotlib Colormap, default='viridis'\n",
      "     |          Colormap recognized by matplotlib.\n",
      "     |      \n",
      "     |      xticks_rotation : {'vertical', 'horizontal'} or float,                          default='horizontal'\n",
      "     |          Rotation of xtick labels.\n",
      "     |      \n",
      "     |      values_format : str, default=None\n",
      "     |          Format specification for values in confusion matrix. If `None`,\n",
      "     |          the format specification is 'd' or '.2g' whichever is shorter.\n",
      "     |      \n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      colorbar : bool, default=True\n",
      "     |          Whether or not to add a colorbar to the plot.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.ConfusionMatrixDisplay`\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DetCurveDisplay(builtins.object)\n",
      "     |  DetCurveDisplay(*, fpr, fnr, estimator_name=None, pos_label=None)\n",
      "     |  \n",
      "     |  DET curve visualization.\n",
      "     |  \n",
      "     |  It is recommend to use :func:`~sklearn.metrics.plot_det_curve` to create a\n",
      "     |  visualizer. All parameters are stored as attributes.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fpr : ndarray\n",
      "     |      False positive rate.\n",
      "     |  \n",
      "     |  fnr : ndarray\n",
      "     |      False negative rate.\n",
      "     |  \n",
      "     |  estimator_name : str, default=None\n",
      "     |      Name of estimator. If None, the estimator name is not shown.\n",
      "     |  \n",
      "     |  pos_label : str or int, default=None\n",
      "     |      The label of the positive class.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  line_ : matplotlib Artist\n",
      "     |      DET Curve.\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with DET Curve.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the curve.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  det_curve : Compute error rates for different probability thresholds.\n",
      "     |  plot_det_curve : Plot detection error tradeoff (DET) curve.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import matplotlib.pyplot as plt  # doctest: +SKIP\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn import metrics\n",
      "     |  >>> y = np.array([0, 0, 1, 1])\n",
      "     |  >>> pred = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "     |  >>> fpr, fnr, thresholds = metrics.det_curve(y, pred)\n",
      "     |  >>> display = metrics.DetCurveDisplay(\n",
      "     |  ...     fpr=fpr, fnr=fnr, estimator_name='example estimator'\n",
      "     |  ... )\n",
      "     |  >>> display.plot()  # doctest: +SKIP\n",
      "     |  >>> plt.show()      # doctest: +SKIP\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fpr, fnr, estimator_name=None, pos_label=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, ax=None, *, name=None, **kwargs)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of DET curve for labeling. If `None`, use the name of the\n",
      "     |          estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.plot.DetCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class PrecisionRecallDisplay(builtins.object)\n",
      "     |  PrecisionRecallDisplay(precision, recall, *, average_precision=None, estimator_name=None, pos_label=None)\n",
      "     |  \n",
      "     |  Precision Recall visualization.\n",
      "     |  \n",
      "     |  It is recommend to use :func:`~sklearn.metrics.plot_precision_recall_curve`\n",
      "     |  to create a visualizer. All parameters are stored as attributes.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  -----------\n",
      "     |  precision : ndarray\n",
      "     |      Precision values.\n",
      "     |  \n",
      "     |  recall : ndarray\n",
      "     |      Recall values.\n",
      "     |  \n",
      "     |  average_precision : float, default=None\n",
      "     |      Average precision. If None, the average precision is not shown.\n",
      "     |  \n",
      "     |  estimator_name : str, default=None\n",
      "     |      Name of estimator. If None, then the estimator name is not shown.\n",
      "     |  \n",
      "     |  pos_label : str or int, default=None\n",
      "     |      The class considered as the positive class. If None, the class will not\n",
      "     |      be shown in the legend.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  line_ : matplotlib Artist\n",
      "     |      Precision recall curve.\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with precision recall curve.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the curve.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  precision_recall_curve : Compute precision-recall pairs for different\n",
      "     |      probability thresholds.\n",
      "     |  plot_precision_recall_curve : Plot Precision Recall Curve for binary\n",
      "     |      classifiers.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> from sklearn.metrics import (precision_recall_curve,\n",
      "     |  ...                              PrecisionRecallDisplay)\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> from sklearn.svm import SVC\n",
      "     |  >>> X, y = make_classification(random_state=0)\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
      "     |  ...                                                     random_state=0)\n",
      "     |  >>> clf = SVC(random_state=0)\n",
      "     |  >>> clf.fit(X_train, y_train)\n",
      "     |  SVC(random_state=0)\n",
      "     |  >>> predictions = clf.predict(X_test)\n",
      "     |  >>> precision, recall, _ = precision_recall_curve(y_test, predictions)\n",
      "     |  >>> disp = PrecisionRecallDisplay(precision=precision, recall=recall)\n",
      "     |  >>> disp.plot() # doctest: +SKIP\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, precision, recall, *, average_precision=None, estimator_name=None, pos_label=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, ax=None, *, name=None, **kwargs)\n",
      "     |      Plot visualization.\n",
      "     |      \n",
      "     |      Extra keyword arguments will be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ax : Matplotlib Axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of precision recall curve for labeling. If `None`, use the\n",
      "     |          name of the estimator.\n",
      "     |      \n",
      "     |      **kwargs : dict\n",
      "     |          Keyword arguments to be passed to matplotlib's `plot`.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.PrecisionRecallDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class RocCurveDisplay(builtins.object)\n",
      "     |  RocCurveDisplay(*, fpr, tpr, roc_auc=None, estimator_name=None, pos_label=None)\n",
      "     |  \n",
      "     |  ROC Curve visualization.\n",
      "     |  \n",
      "     |  It is recommend to use :func:`~sklearn.metrics.plot_roc_curve` to create a\n",
      "     |  visualizer. All parameters are stored as attributes.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <visualizations>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  fpr : ndarray\n",
      "     |      False positive rate.\n",
      "     |  \n",
      "     |  tpr : ndarray\n",
      "     |      True positive rate.\n",
      "     |  \n",
      "     |  roc_auc : float, default=None\n",
      "     |      Area under ROC curve. If None, the roc_auc score is not shown.\n",
      "     |  \n",
      "     |  estimator_name : str, default=None\n",
      "     |      Name of estimator. If None, the estimator name is not shown.\n",
      "     |  \n",
      "     |  pos_label : str or int, default=None\n",
      "     |      The class considered as the positive class when computing the roc auc\n",
      "     |      metrics. By default, `estimators.classes_[1]` is considered\n",
      "     |      as the positive class.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  line_ : matplotlib Artist\n",
      "     |      ROC Curve.\n",
      "     |  \n",
      "     |  ax_ : matplotlib Axes\n",
      "     |      Axes with ROC Curve.\n",
      "     |  \n",
      "     |  figure_ : matplotlib Figure\n",
      "     |      Figure containing the curve.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "     |  plot_roc_curve : Plot Receiver operating characteristic (ROC) curve.\n",
      "     |  roc_auc_score : Compute the area under the ROC curve.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import matplotlib.pyplot as plt  # doctest: +SKIP\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn import metrics\n",
      "     |  >>> y = np.array([0, 0, 1, 1])\n",
      "     |  >>> pred = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "     |  >>> fpr, tpr, thresholds = metrics.roc_curve(y, pred)\n",
      "     |  >>> roc_auc = metrics.auc(fpr, tpr)\n",
      "     |  >>> display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,                                          estimator_name='example estimator')\n",
      "     |  >>> display.plot()  # doctest: +SKIP\n",
      "     |  >>> plt.show()      # doctest: +SKIP\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, fpr, tpr, roc_auc=None, estimator_name=None, pos_label=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  plot(self, ax=None, *, name=None, **kwargs)\n",
      "     |      Plot visualization\n",
      "     |      \n",
      "     |      Extra keyword arguments will be passed to matplotlib's ``plot``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      ax : matplotlib axes, default=None\n",
      "     |          Axes object to plot on. If `None`, a new figure and axes is\n",
      "     |          created.\n",
      "     |      \n",
      "     |      name : str, default=None\n",
      "     |          Name of ROC Curve for labeling. If `None`, use the name of the\n",
      "     |          estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      display : :class:`~sklearn.metrics.plot.RocCurveDisplay`\n",
      "     |          Object that stores computed values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None)\n",
      "        Accuracy classification score.\n",
      "        \n",
      "        In multilabel classification, this function computes subset accuracy:\n",
      "        the set of labels predicted for a sample must *exactly* match the\n",
      "        corresponding set of labels in y_true.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <accuracy_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If ``False``, return the number of correctly classified samples.\n",
      "            Otherwise, return the fraction of correctly classified samples.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            If ``normalize == True``, return the fraction of correctly\n",
      "            classified samples (float), else returns the number of correctly\n",
      "            classified samples (int).\n",
      "        \n",
      "            The best performance is 1 with ``normalize == True`` and the number\n",
      "            of samples with ``normalize == False``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        jaccard_score, hamming_loss, zero_one_loss\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In binary and multiclass classification, this function is equal\n",
      "        to the ``jaccard_score`` function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import accuracy_score\n",
      "        >>> y_pred = [0, 2, 1, 3]\n",
      "        >>> y_true = [0, 1, 2, 3]\n",
      "        >>> accuracy_score(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> accuracy_score(y_true, y_pred, normalize=False)\n",
      "        2\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
      "        0.5\n",
      "    \n",
      "    adjusted_mutual_info_score(labels_true, labels_pred, *, average_method='arithmetic')\n",
      "        Adjusted Mutual Information between two clusterings.\n",
      "        \n",
      "        Adjusted Mutual Information (AMI) is an adjustment of the Mutual\n",
      "        Information (MI) score to account for chance. It accounts for the fact that\n",
      "        the MI is generally higher for two clusterings with a larger number of\n",
      "        clusters, regardless of whether there is actually more information shared.\n",
      "        For two clusterings :math:`U` and :math:`V`, the AMI is given as::\n",
      "        \n",
      "            AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [avg(H(U), H(V)) - E(MI(U, V))]\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Be mindful that this function is an order of magnitude slower than other\n",
      "        metrics, such as the Adjusted Rand Index.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : int array-like of shape (n_samples,)\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        average_method : str, default='arithmetic'\n",
      "            How to compute the normalizer in the denominator. Possible options\n",
      "            are 'min', 'geometric', 'arithmetic', and 'max'.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "               The default value of ``average_method`` changed from 'max' to\n",
      "               'arithmetic'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ami: float (upperlimited by 1.0)\n",
      "           The AMI returns a value of 1 when the two partitions are identical\n",
      "           (ie perfectly matched). Random partitions (independent labellings) have\n",
      "           an expected AMI around 0 on average hence can be negative.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        adjusted_rand_score : Adjusted Rand Index.\n",
      "        mutual_info_score : Mutual Information (not adjusted for chance).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import adjusted_mutual_info_score\n",
      "          >>> adjusted_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "          >>> adjusted_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally in-complete, hence the AMI is null::\n",
      "        \n",
      "          >>> adjusted_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          ... # doctest: +SKIP\n",
      "          0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for\n",
      "           Clusterings Comparison: Variants, Properties, Normalization and\n",
      "           Correction for Chance, JMLR\n",
      "           <http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Adjusted Mutual Information\n",
      "           <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_\n",
      "    \n",
      "    adjusted_rand_score(labels_true, labels_pred)\n",
      "        Rand index adjusted for chance.\n",
      "        \n",
      "        The Rand Index computes a similarity measure between two clusterings\n",
      "        by considering all pairs of samples and counting pairs that are\n",
      "        assigned in the same or different clusters in the predicted and\n",
      "        true clusterings.\n",
      "        \n",
      "        The raw RI score is then \"adjusted for chance\" into the ARI score\n",
      "        using the following scheme::\n",
      "        \n",
      "            ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)\n",
      "        \n",
      "        The adjusted Rand index is thus ensured to have a value close to\n",
      "        0.0 for random labeling independently of the number of clusters and\n",
      "        samples and exactly 1.0 when the clusterings are identical (up to\n",
      "        a permutation).\n",
      "        \n",
      "        ARI is a symmetric measure::\n",
      "        \n",
      "            adjusted_rand_score(a, b) == adjusted_rand_score(b, a)\n",
      "        \n",
      "        Read more in the :ref:`User Guide <adjusted_rand_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            Ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            Cluster labels to evaluate\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ARI : float\n",
      "           Similarity score between -1.0 and 1.0. Random labelings have an ARI\n",
      "           close to 0.0. 1.0 stands for perfect match.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Perfectly matching labelings have a score of 1 even\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import adjusted_rand_score\n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete but may not always be pure, hence penalized::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 1, 2], [0, 0, 1, 1])\n",
      "          0.57...\n",
      "        \n",
      "        ARI is symmetric, so labelings that have pure clusters with members\n",
      "        coming from the same classes but unnecessary splits are penalized::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 1, 1], [0, 0, 1, 2])\n",
      "          0.57...\n",
      "        \n",
      "        If classes members are completely split across different clusters, the\n",
      "        assignment is totally incomplete, hence the ARI is very low::\n",
      "        \n",
      "          >>> adjusted_rand_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [Hubert1985] L. Hubert and P. Arabie, Comparing Partitions,\n",
      "          Journal of Classification 1985\n",
      "          https://link.springer.com/article/10.1007%2FBF01908075\n",
      "        \n",
      "        .. [Steinley2004] D. Steinley, Properties of the Hubert-Arabie\n",
      "          adjusted Rand index, Psychological Methods 2004\n",
      "        \n",
      "        .. [wk] https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        adjusted_mutual_info_score : Adjusted Mutual Information.\n",
      "    \n",
      "    auc(x, y)\n",
      "        Compute Area Under the Curve (AUC) using the trapezoidal rule.\n",
      "        \n",
      "        This is a general function, given points on a curve.  For computing the\n",
      "        area under the ROC-curve, see :func:`roc_auc_score`.  For an alternative\n",
      "        way to summarize a precision-recall curve, see\n",
      "        :func:`average_precision_score`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : ndarray of shape (n,)\n",
      "            x coordinates. These must be either monotonic increasing or monotonic\n",
      "            decreasing.\n",
      "        y : ndarray of shape, (n,)\n",
      "            y coordinates.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        auc : float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roc_auc_score : Compute the area under the ROC curve.\n",
      "        average_precision_score : Compute average precision from prediction scores.\n",
      "        precision_recall_curve : Compute precision-recall pairs for different\n",
      "            probability thresholds.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn import metrics\n",
      "        >>> y = np.array([1, 1, 2, 2])\n",
      "        >>> pred = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> fpr, tpr, thresholds = metrics.roc_curve(y, pred, pos_label=2)\n",
      "        >>> metrics.auc(fpr, tpr)\n",
      "        0.75\n",
      "    \n",
      "    average_precision_score(y_true, y_score, *, average='macro', pos_label=1, sample_weight=None)\n",
      "        Compute average precision (AP) from prediction scores.\n",
      "        \n",
      "        AP summarizes a precision-recall curve as the weighted mean of precisions\n",
      "        achieved at each threshold, with the increase in recall from the previous\n",
      "        threshold used as the weight:\n",
      "        \n",
      "        .. math::\n",
      "            \\text{AP} = \\sum_n (R_n - R_{n-1}) P_n\n",
      "        \n",
      "        where :math:`P_n` and :math:`R_n` are the precision and recall at the nth\n",
      "        threshold [1]_. This implementation is not interpolated and is different\n",
      "        from computing the area under the precision-recall curve with the\n",
      "        trapezoidal rule, which uses linear interpolation and can be too\n",
      "        optimistic.\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task\n",
      "        or multilabel classification task.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "            True binary labels or binary label indicators.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by :term:`decision_function` on some classifiers).\n",
      "        \n",
      "        average : {'micro', 'samples', 'weighted', 'macro'} or None,             default='macro'\n",
      "            If ``None``, the scores for each class are returned. Otherwise,\n",
      "            this determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by considering each element of the label\n",
      "                indicator matrix as a label.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label).\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average.\n",
      "        \n",
      "            Will be ignored when ``y_true`` is binary.\n",
      "        \n",
      "        pos_label : int or str, default=1\n",
      "            The label of the positive class. Only applied to binary ``y_true``.\n",
      "            For multilabel-indicator ``y_true``, ``pos_label`` is fixed to 1.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        average_precision : float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roc_auc_score : Compute the area under the ROC curve.\n",
      "        precision_recall_curve : Compute precision-recall pairs for different\n",
      "            probability thresholds.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. versionchanged:: 0.19\n",
      "          Instead of linearly interpolating between operating points, precisions\n",
      "          are weighted by the change in recall since the last operating point.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Average precision\n",
      "               <https://en.wikipedia.org/w/index.php?title=Information_retrieval&\n",
      "               oldid=793358396#Average_precision>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import average_precision_score\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> average_precision_score(y_true, y_scores)\n",
      "        0.83...\n",
      "    \n",
      "    balanced_accuracy_score(y_true, y_pred, *, sample_weight=None, adjusted=False)\n",
      "        Compute the balanced accuracy.\n",
      "        \n",
      "        The balanced accuracy in binary and multiclass classification problems to\n",
      "        deal with imbalanced datasets. It is defined as the average of recall\n",
      "        obtained on each class.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0 when ``adjusted=False``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <balanced_accuracy_score>`.\n",
      "        \n",
      "        .. versionadded:: 0.20\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        adjusted : bool, default=False\n",
      "            When true, the result is adjusted for chance, so that random\n",
      "            performance would score 0, while keeping perfect performance at a score\n",
      "            of 1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        balanced_accuracy : float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        recall_score, roc_auc_score\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Some literature promotes alternative definitions of balanced accuracy. Our\n",
      "        definition is equivalent to :func:`accuracy_score` with class-balanced\n",
      "        sample weights, and shares desirable properties with the binary case.\n",
      "        See the :ref:`User Guide <balanced_accuracy_score>`.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010).\n",
      "               The balanced accuracy and its posterior distribution.\n",
      "               Proceedings of the 20th International Conference on Pattern\n",
      "               Recognition, 3121-24.\n",
      "        .. [2] John. D. Kelleher, Brian Mac Namee, Aoife D'Arcy, (2015).\n",
      "               `Fundamentals of Machine Learning for Predictive Data Analytics:\n",
      "               Algorithms, Worked Examples, and Case Studies\n",
      "               <https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import balanced_accuracy_score\n",
      "        >>> y_true = [0, 1, 0, 0, 1, 0]\n",
      "        >>> y_pred = [0, 1, 0, 0, 0, 1]\n",
      "        >>> balanced_accuracy_score(y_true, y_pred)\n",
      "        0.625\n",
      "    \n",
      "    brier_score_loss(y_true, y_prob, *, sample_weight=None, pos_label=None)\n",
      "        Compute the Brier score loss.\n",
      "        \n",
      "        The smaller the Brier score loss, the better, hence the naming with \"loss\".\n",
      "        The Brier score measures the mean squared difference between the predicted\n",
      "        probability and the actual outcome. The Brier score always\n",
      "        takes on a value between zero and one, since this is the largest\n",
      "        possible difference between a predicted probability (which must be\n",
      "        between zero and one) and the actual outcome (which can take on values\n",
      "        of only 0 and 1). It can be decomposed is the sum of refinement loss and\n",
      "        calibration loss.\n",
      "        \n",
      "        The Brier score is appropriate for binary and categorical outcomes that\n",
      "        can be structured as true or false, but is inappropriate for ordinal\n",
      "        variables which can take on three or more values (this is because the\n",
      "        Brier score assumes that all possible outcomes are equivalently\n",
      "        \"distant\" from one another). Which label is considered to be the positive\n",
      "        label is controlled via the parameter `pos_label`, which defaults to\n",
      "        the greater label unless `y_true` is all 0 or all -1, in which case\n",
      "        `pos_label` defaults to 1.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <brier_score_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array of shape (n_samples,)\n",
      "            True targets.\n",
      "        \n",
      "        y_prob : array of shape (n_samples,)\n",
      "            Probabilities of the positive class.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            Label of the positive class. `pos_label` will be infered in the\n",
      "            following manner:\n",
      "        \n",
      "            * if `y_true` in {-1, 1} or {0, 1}, `pos_label` defaults to 1;\n",
      "            * else if `y_true` contains string, an error will be raised and\n",
      "              `pos_label` should be explicitely specified;\n",
      "            * otherwise, `pos_label` defaults to the greater label,\n",
      "              i.e. `np.unique(y_true)[-1]`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            Brier score loss.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import brier_score_loss\n",
      "        >>> y_true = np.array([0, 1, 1, 0])\n",
      "        >>> y_true_categorical = np.array([\"spam\", \"ham\", \"ham\", \"spam\"])\n",
      "        >>> y_prob = np.array([0.1, 0.9, 0.8, 0.3])\n",
      "        >>> brier_score_loss(y_true, y_prob)\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true, 1-y_prob, pos_label=0)\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true_categorical, y_prob, pos_label=\"ham\")\n",
      "        0.037...\n",
      "        >>> brier_score_loss(y_true, np.array(y_prob) > 0.5)\n",
      "        0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Brier score\n",
      "                <https://en.wikipedia.org/wiki/Brier_score>`_.\n",
      "    \n",
      "    calinski_harabasz_score(X, labels)\n",
      "        Compute the Calinski and Harabasz score.\n",
      "        \n",
      "        It is also known as the Variance Ratio Criterion.\n",
      "        \n",
      "        The score is defined as ratio between the within-cluster dispersion and\n",
      "        the between-cluster dispersion.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <calinski_harabasz_index>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            A list of ``n_features``-dimensional data points. Each row corresponds\n",
      "            to a single data point.\n",
      "        \n",
      "        labels : array-like of shape (n_samples,)\n",
      "            Predicted labels for each sample.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            The resulting Calinski-Harabasz score.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `T. Calinski and J. Harabasz, 1974. \"A dendrite method for cluster\n",
      "           analysis\". Communications in Statistics\n",
      "           <https://www.tandfonline.com/doi/abs/10.1080/03610927408827101>`_\n",
      "    \n",
      "    check_scoring(estimator, scoring=None, *, allow_none=False)\n",
      "        Determine scorer from user options.\n",
      "        \n",
      "        A TypeError will be thrown if the estimator cannot be scored.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator object implementing 'fit'\n",
      "            The object to use to fit the data.\n",
      "        \n",
      "        scoring : str or callable, default=None\n",
      "            A string (see model evaluation documentation) or\n",
      "            a scorer callable object / function with signature\n",
      "            ``scorer(estimator, X, y)``.\n",
      "        \n",
      "        allow_none : bool, default=False\n",
      "            If no scoring is specified and the estimator has no score function, we\n",
      "            can either return None or raise an exception.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scoring : callable\n",
      "            A scorer callable object / function with signature\n",
      "            ``scorer(estimator, X, y)``.\n",
      "    \n",
      "    classification_report(y_true, y_pred, *, labels=None, target_names=None, sample_weight=None, digits=2, output_dict=False, zero_division='warn')\n",
      "        Build a text report showing the main classification metrics.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <classification_report>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like of shape (n_labels,), default=None\n",
      "            Optional list of label indices to include in the report.\n",
      "        \n",
      "        target_names : list of str of shape (n_labels,), default=None\n",
      "            Optional display names matching the labels (same order).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        digits : int, default=2\n",
      "            Number of digits for formatting output floating point values.\n",
      "            When ``output_dict`` is ``True``, this will be ignored and the\n",
      "            returned values will not be rounded.\n",
      "        \n",
      "        output_dict : bool, default=False\n",
      "            If True, return output as dict.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division. If set to\n",
      "            \"warn\", this acts as 0, but warnings are also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        report : string / dict\n",
      "            Text summary of the precision, recall, F1 score for each class.\n",
      "            Dictionary returned if output_dict is True. Dictionary has the\n",
      "            following structure::\n",
      "        \n",
      "                {'label 1': {'precision':0.5,\n",
      "                             'recall':1.0,\n",
      "                             'f1-score':0.67,\n",
      "                             'support':1},\n",
      "                 'label 2': { ... },\n",
      "                  ...\n",
      "                }\n",
      "        \n",
      "            The reported averages include macro average (averaging the unweighted\n",
      "            mean per label), weighted average (averaging the support-weighted mean\n",
      "            per label), and sample average (only for multilabel classification).\n",
      "            Micro average (averaging the total true positives, false negatives and\n",
      "            false positives) is only shown for multi-label or multi-class\n",
      "            with a subset of classes, because it corresponds to accuracy\n",
      "            otherwise and would be the same for all metrics.\n",
      "            See also :func:`precision_recall_fscore_support` for more details\n",
      "            on averages.\n",
      "        \n",
      "            Note that in binary classification, recall of the positive class\n",
      "            is also known as \"sensitivity\"; recall of the negative class is\n",
      "            \"specificity\".\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_fscore_support, confusion_matrix,\n",
      "        multilabel_confusion_matrix\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import classification_report\n",
      "        >>> y_true = [0, 1, 2, 2, 2]\n",
      "        >>> y_pred = [0, 0, 2, 2, 1]\n",
      "        >>> target_names = ['class 0', 'class 1', 'class 2']\n",
      "        >>> print(classification_report(y_true, y_pred, target_names=target_names))\n",
      "                      precision    recall  f1-score   support\n",
      "        <BLANKLINE>\n",
      "             class 0       0.50      1.00      0.67         1\n",
      "             class 1       0.00      0.00      0.00         1\n",
      "             class 2       1.00      0.67      0.80         3\n",
      "        <BLANKLINE>\n",
      "            accuracy                           0.60         5\n",
      "           macro avg       0.50      0.56      0.49         5\n",
      "        weighted avg       0.70      0.60      0.61         5\n",
      "        <BLANKLINE>\n",
      "        >>> y_pred = [1, 1, 0]\n",
      "        >>> y_true = [1, 1, 1]\n",
      "        >>> print(classification_report(y_true, y_pred, labels=[1, 2, 3]))\n",
      "                      precision    recall  f1-score   support\n",
      "        <BLANKLINE>\n",
      "                   1       1.00      0.67      0.80         3\n",
      "                   2       0.00      0.00      0.00         0\n",
      "                   3       0.00      0.00      0.00         0\n",
      "        <BLANKLINE>\n",
      "           micro avg       1.00      0.67      0.80         3\n",
      "           macro avg       0.33      0.22      0.27         3\n",
      "        weighted avg       1.00      0.67      0.80         3\n",
      "        <BLANKLINE>\n",
      "    \n",
      "    cohen_kappa_score(y1, y2, *, labels=None, weights=None, sample_weight=None)\n",
      "        Cohen's kappa: a statistic that measures inter-annotator agreement.\n",
      "        \n",
      "        This function computes Cohen's kappa [1]_, a score that expresses the level\n",
      "        of agreement between two annotators on a classification problem. It is\n",
      "        defined as\n",
      "        \n",
      "        .. math::\n",
      "            \\kappa = (p_o - p_e) / (1 - p_e)\n",
      "        \n",
      "        where :math:`p_o` is the empirical probability of agreement on the label\n",
      "        assigned to any sample (the observed agreement ratio), and :math:`p_e` is\n",
      "        the expected agreement when both annotators assign labels randomly.\n",
      "        :math:`p_e` is estimated using a per-annotator empirical prior over the\n",
      "        class labels [2]_.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <cohen_kappa>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y1 : array of shape (n_samples,)\n",
      "            Labels assigned by the first annotator.\n",
      "        \n",
      "        y2 : array of shape (n_samples,)\n",
      "            Labels assigned by the second annotator. The kappa statistic is\n",
      "            symmetric, so swapping ``y1`` and ``y2`` doesn't change the value.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            List of labels to index the matrix. This may be used to select a\n",
      "            subset of labels. If None, all labels that appear at least once in\n",
      "            ``y1`` or ``y2`` are used.\n",
      "        \n",
      "        weights : {'linear', 'quadratic'}, default=None\n",
      "            Weighting type to calculate the score. None means no weighted;\n",
      "            \"linear\" means linear weighted; \"quadratic\" means quadratic weighted.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        kappa : float\n",
      "            The kappa statistic, which is a number between -1 and 1. The maximum\n",
      "            value means complete agreement; zero or lower means chance agreement.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] J. Cohen (1960). \"A coefficient of agreement for nominal scales\".\n",
      "               Educational and Psychological Measurement 20(1):37-46.\n",
      "               doi:10.1177/001316446002000104.\n",
      "        .. [2] `R. Artstein and M. Poesio (2008). \"Inter-coder agreement for\n",
      "               computational linguistics\". Computational Linguistics 34(4):555-596\n",
      "               <https://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-034-R2>`_.\n",
      "        .. [3] `Wikipedia entry for the Cohen's kappa\n",
      "                <https://en.wikipedia.org/wiki/Cohen%27s_kappa>`_.\n",
      "    \n",
      "    completeness_score(labels_true, labels_pred)\n",
      "        Completeness metric of a cluster labeling given a ground truth.\n",
      "        \n",
      "        A clustering result satisfies completeness if all the data points\n",
      "        that are members of a given class are elements of the same cluster.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is not symmetric: switching ``label_true`` with ``label_pred``\n",
      "        will return the :func:`homogeneity_score` which will be different in\n",
      "        general.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            cluster labels to evaluate\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        completeness : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        homogeneity_score\n",
      "        v_measure_score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are complete::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import completeness_score\n",
      "          >>> completeness_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Non-perfect labelings that assign all classes members to the same clusters\n",
      "        are still complete::\n",
      "        \n",
      "          >>> print(completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          1.0\n",
      "          >>> print(completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))\n",
      "          0.999...\n",
      "        \n",
      "        If classes members are split across different clusters, the\n",
      "        assignment cannot be complete::\n",
      "        \n",
      "          >>> print(completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))\n",
      "          0.0\n",
      "          >>> print(completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))\n",
      "          0.0\n",
      "    \n",
      "    confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)\n",
      "        Compute confusion matrix to evaluate the accuracy of a classification.\n",
      "        \n",
      "        By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\n",
      "        is equal to the number of observations known to be in group :math:`i` and\n",
      "        predicted to be in group :math:`j`.\n",
      "        \n",
      "        Thus in binary classification, the count of true negatives is\n",
      "        :math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is\n",
      "        :math:`C_{1,1}` and false positives is :math:`C_{0,1}`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like of shape (n_classes), default=None\n",
      "            List of labels to index the matrix. This may be used to reorder\n",
      "            or select a subset of labels.\n",
      "            If ``None`` is given, those that appear at least once\n",
      "            in ``y_true`` or ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        normalize : {'true', 'pred', 'all'}, default=None\n",
      "            Normalizes confusion matrix over the true (rows), predicted (columns)\n",
      "            conditions or all the population. If None, confusion matrix will not be\n",
      "            normalized.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        C : ndarray of shape (n_classes, n_classes)\n",
      "            Confusion matrix whose i-th row and j-th\n",
      "            column entry indicates the number of\n",
      "            samples with true label being i-th class\n",
      "            and predicted label being j-th class.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ConfusionMatrixDisplay.from_estimator : Plot the confusion matrix\n",
      "            given an estimator, the data, and the label.\n",
      "        ConfusionMatrixDisplay.from_predictions : Plot the confusion matrix\n",
      "            given the true and predicted labels.\n",
      "        ConfusionMatrixDisplay : Confusion Matrix visualization.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Confusion matrix\n",
      "               <https://en.wikipedia.org/wiki/Confusion_matrix>`_\n",
      "               (Wikipedia and other references may use a different\n",
      "               convention for axes).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import confusion_matrix\n",
      "        >>> y_true = [2, 0, 2, 2, 0, 1]\n",
      "        >>> y_pred = [0, 0, 2, 2, 0, 2]\n",
      "        >>> confusion_matrix(y_true, y_pred)\n",
      "        array([[2, 0, 0],\n",
      "               [0, 0, 1],\n",
      "               [1, 0, 2]])\n",
      "        \n",
      "        >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
      "        >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
      "        >>> confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\n",
      "        array([[2, 0, 0],\n",
      "               [0, 0, 1],\n",
      "               [1, 0, 2]])\n",
      "        \n",
      "        In the binary case, we can extract true positives, etc as follows:\n",
      "        \n",
      "        >>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
      "        >>> (tn, fp, fn, tp)\n",
      "        (0, 2, 1, 1)\n",
      "    \n",
      "    consensus_score(a, b, *, similarity='jaccard')\n",
      "        The similarity of two sets of biclusters.\n",
      "        \n",
      "        Similarity between individual biclusters is computed. Then the\n",
      "        best matching between sets is found using the Hungarian algorithm.\n",
      "        The final score is the sum of similarities divided by the size of\n",
      "        the larger set.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <biclustering>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        a : (rows, columns)\n",
      "            Tuple of row and column indicators for a set of biclusters.\n",
      "        \n",
      "        b : (rows, columns)\n",
      "            Another set of biclusters like ``a``.\n",
      "        \n",
      "        similarity : 'jaccard' or callable, default='jaccard'\n",
      "            May be the string \"jaccard\" to use the Jaccard coefficient, or\n",
      "            any function that takes four arguments, each of which is a 1d\n",
      "            indicator vector: (a_rows, a_columns, b_rows, b_columns).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        * Hochreiter, Bodenhofer, et. al., 2010. `FABIA: factor analysis\n",
      "          for bicluster acquisition\n",
      "          <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/>`__.\n",
      "    \n",
      "    coverage_error(y_true, y_score, *, sample_weight=None)\n",
      "        Coverage error measure.\n",
      "        \n",
      "        Compute how far we need to go through the ranked scores to cover all\n",
      "        true labels. The best value is equal to the average number\n",
      "        of labels in ``y_true`` per sample.\n",
      "        \n",
      "        Ties in ``y_scores`` are broken by giving maximal rank that would have\n",
      "        been assigned to all tied values.\n",
      "        \n",
      "        Note: Our implementation's score is 1 greater than the one given in\n",
      "        Tsoumakas et al., 2010. This extends it to handle the degenerate case\n",
      "        in which an instance has 0 true labels.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <coverage_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples, n_labels)\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        coverage_error : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n",
      "               Mining multi-label data. In Data mining and knowledge discovery\n",
      "               handbook (pp. 667-685). Springer US.\n",
      "    \n",
      "    davies_bouldin_score(X, labels)\n",
      "        Computes the Davies-Bouldin score.\n",
      "        \n",
      "        The score is defined as the average similarity measure of each cluster with\n",
      "        its most similar cluster, where similarity is the ratio of within-cluster\n",
      "        distances to between-cluster distances. Thus, clusters which are farther\n",
      "        apart and less dispersed will result in a better score.\n",
      "        \n",
      "        The minimum score is zero, with lower values indicating better clustering.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <davies-bouldin_index>`.\n",
      "        \n",
      "        .. versionadded:: 0.20\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples, n_features)\n",
      "            A list of ``n_features``-dimensional data points. Each row corresponds\n",
      "            to a single data point.\n",
      "        \n",
      "        labels : array-like of shape (n_samples,)\n",
      "            Predicted labels for each sample.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score: float\n",
      "            The resulting Davies-Bouldin score.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Davies, David L.; Bouldin, Donald W. (1979).\n",
      "           `\"A Cluster Separation Measure\"\n",
      "           <https://ieeexplore.ieee.org/document/4766909>`__.\n",
      "           IEEE Transactions on Pattern Analysis and Machine Intelligence.\n",
      "           PAMI-1 (2): 224-227\n",
      "    \n",
      "    dcg_score(y_true, y_score, *, k=None, log_base=2, sample_weight=None, ignore_ties=False)\n",
      "        Compute Discounted Cumulative Gain.\n",
      "        \n",
      "        Sum the true scores ranked in the order induced by the predicted scores,\n",
      "        after applying a logarithmic discount.\n",
      "        \n",
      "        This ranking metric yields a high value if true labels are ranked high by\n",
      "        ``y_score``.\n",
      "        \n",
      "        Usually the Normalized Discounted Cumulative Gain (NDCG, computed by\n",
      "        ndcg_score) is preferred.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples, n_labels)\n",
      "            True targets of multilabel classification, or true scores of entities\n",
      "            to be ranked.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates, confidence values,\n",
      "            or non-thresholded measure of decisions (as returned by\n",
      "            \"decision_function\" on some classifiers).\n",
      "        \n",
      "        k : int, default=None\n",
      "            Only consider the highest k scores in the ranking. If None, use all\n",
      "            outputs.\n",
      "        \n",
      "        log_base : float, default=2\n",
      "            Base of the logarithm used for the discount. A low value means a\n",
      "            sharper discount (top results are more important).\n",
      "        \n",
      "        sample_weight : ndarray of shape (n_samples,), default=None\n",
      "            Sample weights. If None, all samples are given the same weight.\n",
      "        \n",
      "        ignore_ties : bool, default=False\n",
      "            Assume that there are no ties in y_score (which is likely to be the\n",
      "            case if y_score is continuous) for efficiency gains.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        discounted_cumulative_gain : float\n",
      "            The averaged sample DCG scores.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ndcg_score : The Discounted Cumulative Gain divided by the Ideal Discounted\n",
      "            Cumulative Gain (the DCG obtained for a perfect ranking), in order to\n",
      "            have a score between 0 and 1.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        `Wikipedia entry for Discounted Cumulative Gain\n",
      "        <https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_.\n",
      "        \n",
      "        Jarvelin, K., & Kekalainen, J. (2002).\n",
      "        Cumulated gain-based evaluation of IR techniques. ACM Transactions on\n",
      "        Information Systems (TOIS), 20(4), 422-446.\n",
      "        \n",
      "        Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).\n",
      "        A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th\n",
      "        Annual Conference on Learning Theory (COLT 2013).\n",
      "        \n",
      "        McSherry, F., & Najork, M. (2008, March). Computing information retrieval\n",
      "        performance measures efficiently in the presence of tied scores. In\n",
      "        European conference on information retrieval (pp. 414-421). Springer,\n",
      "        Berlin, Heidelberg.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import dcg_score\n",
      "        >>> # we have groud-truth relevance of some answers to a query:\n",
      "        >>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])\n",
      "        >>> # we predict scores for the answers\n",
      "        >>> scores = np.asarray([[.1, .2, .3, 4, 70]])\n",
      "        >>> dcg_score(true_relevance, scores)\n",
      "        9.49...\n",
      "        >>> # we can set k to truncate the sum; only top k answers contribute\n",
      "        >>> dcg_score(true_relevance, scores, k=2)\n",
      "        5.63...\n",
      "        >>> # now we have some ties in our prediction\n",
      "        >>> scores = np.asarray([[1, 0, 0, 0, 1]])\n",
      "        >>> # by default ties are averaged, so here we get the average true\n",
      "        >>> # relevance of our top predictions: (10 + 5) / 2 = 7.5\n",
      "        >>> dcg_score(true_relevance, scores, k=1)\n",
      "        7.5\n",
      "        >>> # we can choose to ignore ties for faster results, but only\n",
      "        >>> # if we know there aren't ties in our scores, otherwise we get\n",
      "        >>> # wrong results:\n",
      "        >>> dcg_score(true_relevance,\n",
      "        ...           scores, k=1, ignore_ties=True)\n",
      "        5.0\n",
      "    \n",
      "    det_curve(y_true, y_score, pos_label=None, sample_weight=None)\n",
      "        Compute error rates for different probability thresholds.\n",
      "        \n",
      "        .. note::\n",
      "           This metric is used for evaluation of ranking and error tradeoffs of\n",
      "           a binary classification task.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <det_curve>`.\n",
      "        \n",
      "        .. versionadded:: 0.24\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples,)\n",
      "            True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n",
      "            pos_label should be explicitly given.\n",
      "        \n",
      "        y_score : ndarray of shape of (n_samples,)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            The label of the positive class.\n",
      "            When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},\n",
      "            ``pos_label`` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fpr : ndarray of shape (n_thresholds,)\n",
      "            False positive rate (FPR) such that element i is the false positive\n",
      "            rate of predictions with score >= thresholds[i]. This is occasionally\n",
      "            referred to as false acceptance propability or fall-out.\n",
      "        \n",
      "        fnr : ndarray of shape (n_thresholds,)\n",
      "            False negative rate (FNR) such that element i is the false negative\n",
      "            rate of predictions with score >= thresholds[i]. This is occasionally\n",
      "            referred to as false rejection or miss rate.\n",
      "        \n",
      "        thresholds : ndarray of shape (n_thresholds,)\n",
      "            Decreasing score values.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        plot_det_curve : Plot detection error tradeoff (DET) curve.\n",
      "        DetCurveDisplay : DET curve visualization.\n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "        precision_recall_curve : Compute precision-recall curve.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import det_curve\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> fpr, fnr, thresholds = det_curve(y_true, y_scores)\n",
      "        >>> fpr\n",
      "        array([0.5, 0.5, 0. ])\n",
      "        >>> fnr\n",
      "        array([0. , 0.5, 0.5])\n",
      "        >>> thresholds\n",
      "        array([0.35, 0.4 , 0.8 ])\n",
      "    \n",
      "    euclidean_distances(X, Y=None, *, Y_norm_squared=None, squared=False, X_norm_squared=None)\n",
      "        Considering the rows of X (and Y=X) as vectors, compute the\n",
      "        distance matrix between each pair of vectors.\n",
      "        \n",
      "        For efficiency reasons, the euclidean distance between a pair of row\n",
      "        vector x and y is computed as::\n",
      "        \n",
      "            dist(x, y) = sqrt(dot(x, x) - 2 * dot(x, y) + dot(y, y))\n",
      "        \n",
      "        This formulation has two advantages over other ways of computing distances.\n",
      "        First, it is computationally efficient when dealing with sparse data.\n",
      "        Second, if one argument varies but the other remains unchanged, then\n",
      "        `dot(x, x)` and/or `dot(y, y)` can be pre-computed.\n",
      "        \n",
      "        However, this is not the most precise way of doing this computation,\n",
      "        because this equation potentially suffers from \"catastrophic cancellation\".\n",
      "        Also, the distance matrix returned by this function may not be exactly\n",
      "        symmetric as required by, e.g., ``scipy.spatial.distance`` functions.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features),             default=None\n",
      "        \n",
      "        Y_norm_squared : array-like of shape (n_samples_Y,), default=None\n",
      "            Pre-computed dot-products of vectors in Y (e.g.,\n",
      "            ``(Y**2).sum(axis=1)``)\n",
      "            May be ignored in some cases, see the note below.\n",
      "        \n",
      "        squared : bool, default=False\n",
      "            Return squared Euclidean distances.\n",
      "        \n",
      "        X_norm_squared : array-like of shape (n_samples,), default=None\n",
      "            Pre-computed dot-products of vectors in X (e.g.,\n",
      "            ``(X**2).sum(axis=1)``)\n",
      "            May be ignored in some cases, see the note below.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        To achieve better accuracy, `X_norm_squared`and `Y_norm_squared` may be\n",
      "        unused if they are passed as ``float32``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : ndarray of shape (n_samples_X, n_samples_Y)\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        paired_distances : Distances betweens pairs of elements of X and Y.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import euclidean_distances\n",
      "        >>> X = [[0, 1], [1, 1]]\n",
      "        >>> # distance between rows of X\n",
      "        >>> euclidean_distances(X, X)\n",
      "        array([[0., 1.],\n",
      "               [1., 0.]])\n",
      "        >>> # get distance to origin\n",
      "        >>> euclidean_distances(X, [[0, 0]])\n",
      "        array([[1.        ],\n",
      "               [1.41421356]])\n",
      "    \n",
      "    explained_variance_score(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        Explained variance regression score function.\n",
      "        \n",
      "        Best possible score is 1.0, lower values are worse.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <explained_variance_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average', 'variance_weighted'} or             array-like of shape (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output scores.\n",
      "            Array-like value defines weights used to average scores.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of scores in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Scores of all outputs are averaged with uniform weight.\n",
      "        \n",
      "            'variance_weighted' :\n",
      "                Scores of all outputs are averaged, weighted by the variances\n",
      "                of each individual output.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float or ndarray of floats\n",
      "            The explained variance or ndarray if 'multioutput' is 'raw_values'.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is not a symmetric function.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import explained_variance_score\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> explained_variance_score(y_true, y_pred)\n",
      "        0.957...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> explained_variance_score(y_true, y_pred, multioutput='uniform_average')\n",
      "        0.983...\n",
      "    \n",
      "    f1_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Compute the F1 score, also known as balanced F-score or F-measure.\n",
      "        \n",
      "        The F1 score can be interpreted as a weighted average of the precision and\n",
      "        recall, where an F1 score reaches its best value at 1 and worst score at 0.\n",
      "        The relative contribution of precision and recall to the F1 score are\n",
      "        equal. The formula for the F1 score is::\n",
      "        \n",
      "            F1 = 2 * (precision * recall) / (precision + recall)\n",
      "        \n",
      "        In the multi-class and multi-label case, this is the average of\n",
      "        the F1 score of each class with weighting depending on the ``average``\n",
      "        parameter.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, default=1\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples','weighted', 'binary'} or None,             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division, i.e. when all\n",
      "            predictions and labels are negative. If set to \"warn\", this acts as 0,\n",
      "            but warnings are also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        f1_score : float or array of float, shape = [n_unique_labels]\n",
      "            F1 score of the positive class in binary classification or weighted\n",
      "            average of the F1 scores of each class for the multiclass task.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        fbeta_score, precision_recall_fscore_support, jaccard_score,\n",
      "        multilabel_confusion_matrix\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import f1_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> f1_score(y_true, y_pred, average='macro')\n",
      "        0.26...\n",
      "        >>> f1_score(y_true, y_pred, average='micro')\n",
      "        0.33...\n",
      "        >>> f1_score(y_true, y_pred, average='weighted')\n",
      "        0.26...\n",
      "        >>> f1_score(y_true, y_pred, average=None)\n",
      "        array([0.8, 0. , 0. ])\n",
      "        >>> y_true = [0, 0, 0, 0, 0, 0]\n",
      "        >>> y_pred = [0, 0, 0, 0, 0, 0]\n",
      "        >>> f1_score(y_true, y_pred, zero_division=1)\n",
      "        1.0...\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0``, precision is undefined.\n",
      "        When ``true positive + false negative == 0``, recall is undefined.\n",
      "        In such cases, by default the metric will be set to 0, as will f-score,\n",
      "        and ``UndefinedMetricWarning`` will be raised. This behavior can be\n",
      "        modified with ``zero_division``.\n",
      "    \n",
      "    fbeta_score(y_true, y_pred, *, beta, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Compute the F-beta score.\n",
      "        \n",
      "        The F-beta score is the weighted harmonic mean of precision and recall,\n",
      "        reaching its optimal value at 1 and its worst value at 0.\n",
      "        \n",
      "        The `beta` parameter determines the weight of recall in the combined\n",
      "        score. ``beta < 1`` lends more weight to precision, while ``beta > 1``\n",
      "        favors recall (``beta -> 0`` considers only precision, ``beta -> +inf``\n",
      "        only recall).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        beta : float\n",
      "            Determines the weight of recall in the combined score.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, default=1\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division, i.e. when all\n",
      "            predictions and labels are negative. If set to \"warn\", this acts as 0,\n",
      "            but warnings are also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "            F-beta score of the positive class in binary classification or weighted\n",
      "            average of the F-beta score of each class for the multiclass task.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_fscore_support, multilabel_confusion_matrix\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0`` or\n",
      "        ``true positive + false negative == 0``, f-score returns 0 and raises\n",
      "        ``UndefinedMetricWarning``. This behavior can be\n",
      "        modified with ``zero_division``.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] R. Baeza-Yates and B. Ribeiro-Neto (2011).\n",
      "               Modern Information Retrieval. Addison Wesley, pp. 327-328.\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import fbeta_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> fbeta_score(y_true, y_pred, average='macro', beta=0.5)\n",
      "        0.23...\n",
      "        >>> fbeta_score(y_true, y_pred, average='micro', beta=0.5)\n",
      "        0.33...\n",
      "        >>> fbeta_score(y_true, y_pred, average='weighted', beta=0.5)\n",
      "        0.23...\n",
      "        >>> fbeta_score(y_true, y_pred, average=None, beta=0.5)\n",
      "        array([0.71..., 0.        , 0.        ])\n",
      "    \n",
      "    fowlkes_mallows_score(labels_true, labels_pred, *, sparse=False)\n",
      "        Measure the similarity of two clusterings of a set of points.\n",
      "        \n",
      "        .. versionadded:: 0.18\n",
      "        \n",
      "        The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of\n",
      "        the precision and recall::\n",
      "        \n",
      "            FMI = TP / sqrt((TP + FP) * (TP + FN))\n",
      "        \n",
      "        Where ``TP`` is the number of **True Positive** (i.e. the number of pair of\n",
      "        points that belongs in the same clusters in both ``labels_true`` and\n",
      "        ``labels_pred``), ``FP`` is the number of **False Positive** (i.e. the\n",
      "        number of pair of points that belongs in the same clusters in\n",
      "        ``labels_true`` and not in ``labels_pred``) and ``FN`` is the number of\n",
      "        **False Negative** (i.e the number of pair of points that belongs in the\n",
      "        same clusters in ``labels_pred`` and not in ``labels_True``).\n",
      "        \n",
      "        The score ranges from 0 to 1. A high value indicates a good similarity\n",
      "        between two clusters.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <fowlkes_mallows_scores>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = (``n_samples``,)\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : array, shape = (``n_samples``, )\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        sparse : bool, default=False\n",
      "            Compute contingency matrix internally with sparse matrix.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "           The resulting Fowlkes-Mallows score.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import fowlkes_mallows_score\n",
      "          >>> fowlkes_mallows_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> fowlkes_mallows_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally random, hence the FMI is null::\n",
      "        \n",
      "          >>> fowlkes_mallows_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          0.0\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `E. B. Fowkles and C. L. Mallows, 1983. \"A method for comparing two\n",
      "           hierarchical clusterings\". Journal of the American Statistical\n",
      "           Association\n",
      "           <http://wildfire.stat.ucla.edu/pdflibrary/fowlkes.pdf>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Fowlkes-Mallows Index\n",
      "               <https://en.wikipedia.org/wiki/Fowlkes-Mallows_index>`_\n",
      "    \n",
      "    get_scorer(scoring)\n",
      "        Get a scorer from string.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <scoring_parameter>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        scoring : str or callable\n",
      "            Scoring method as string. If callable it is returned as is.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scorer : callable\n",
      "            The scorer.\n",
      "    \n",
      "    hamming_loss(y_true, y_pred, *, sample_weight=None)\n",
      "        Compute the average Hamming loss.\n",
      "        \n",
      "        The Hamming loss is the fraction of labels that are incorrectly predicted.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <hamming_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or int\n",
      "            Return the average Hamming loss between element of ``y_true`` and\n",
      "            ``y_pred``.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score, jaccard_score, zero_one_loss\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In multiclass classification, the Hamming loss corresponds to the Hamming\n",
      "        distance between ``y_true`` and ``y_pred`` which is equivalent to the\n",
      "        subset ``zero_one_loss`` function, when `normalize` parameter is set to\n",
      "        True.\n",
      "        \n",
      "        In multilabel classification, the Hamming loss is different from the\n",
      "        subset zero-one loss. The zero-one loss considers the entire set of labels\n",
      "        for a given sample incorrect if it does not entirely match the true set of\n",
      "        labels. Hamming loss is more forgiving in that it penalizes only the\n",
      "        individual labels.\n",
      "        \n",
      "        The Hamming loss is upperbounded by the subset zero-one loss, when\n",
      "        `normalize` parameter is set to True. It is always between 0 and 1,\n",
      "        lower being better.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification:\n",
      "               An Overview. International Journal of Data Warehousing & Mining,\n",
      "               3(3), 1-13, July-September 2007.\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Hamming distance\n",
      "               <https://en.wikipedia.org/wiki/Hamming_distance>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import hamming_loss\n",
      "        >>> y_pred = [1, 2, 3, 4]\n",
      "        >>> y_true = [2, 2, 3, 4]\n",
      "        >>> hamming_loss(y_true, y_pred)\n",
      "        0.25\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))\n",
      "        0.75\n",
      "    \n",
      "    hinge_loss(y_true, pred_decision, *, labels=None, sample_weight=None)\n",
      "        Average hinge loss (non-regularized).\n",
      "        \n",
      "        In binary class case, assuming labels in y_true are encoded with +1 and -1,\n",
      "        when a prediction mistake is made, ``margin = y_true * pred_decision`` is\n",
      "        always negative (since the signs disagree), implying ``1 - margin`` is\n",
      "        always greater than 1.  The cumulated hinge loss is therefore an upper\n",
      "        bound of the number of mistakes made by the classifier.\n",
      "        \n",
      "        In multiclass case, the function expects that either all the labels are\n",
      "        included in y_true or an optional labels argument is provided which\n",
      "        contains all the labels. The multilabel margin is calculated according\n",
      "        to Crammer-Singer's method. As in the binary case, the cumulated hinge loss\n",
      "        is an upper bound of the number of mistakes made by the classifier.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <hinge_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array of shape (n_samples,)\n",
      "            True target, consisting of integers of two values. The positive label\n",
      "            must be greater than the negative label.\n",
      "        \n",
      "        pred_decision : array of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Predicted decisions, as output by decision_function (floats).\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            Contains all the labels for the problem. Used in multiclass hinge loss.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry on the Hinge loss\n",
      "               <https://en.wikipedia.org/wiki/Hinge_loss>`_.\n",
      "        \n",
      "        .. [2] Koby Crammer, Yoram Singer. On the Algorithmic\n",
      "               Implementation of Multiclass Kernel-based Vector\n",
      "               Machines. Journal of Machine Learning Research 2,\n",
      "               (2001), 265-292.\n",
      "        \n",
      "        .. [3] `L1 AND L2 Regularization for Multiclass Hinge Loss Models\n",
      "               by Robert C. Moore, John DeNero\n",
      "               <http://www.ttic.edu/sigml/symposium2011/papers/\n",
      "               Moore+DeNero_Regularization.pdf>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn import svm\n",
      "        >>> from sklearn.metrics import hinge_loss\n",
      "        >>> X = [[0], [1]]\n",
      "        >>> y = [-1, 1]\n",
      "        >>> est = svm.LinearSVC(random_state=0)\n",
      "        >>> est.fit(X, y)\n",
      "        LinearSVC(random_state=0)\n",
      "        >>> pred_decision = est.decision_function([[-2], [3], [0.5]])\n",
      "        >>> pred_decision\n",
      "        array([-2.18...,  2.36...,  0.09...])\n",
      "        >>> hinge_loss([-1, 1, 1], pred_decision)\n",
      "        0.30...\n",
      "        \n",
      "        In the multiclass case:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> X = np.array([[0], [1], [2], [3]])\n",
      "        >>> Y = np.array([0, 1, 2, 3])\n",
      "        >>> labels = np.array([0, 1, 2, 3])\n",
      "        >>> est = svm.LinearSVC()\n",
      "        >>> est.fit(X, Y)\n",
      "        LinearSVC()\n",
      "        >>> pred_decision = est.decision_function([[-1], [2], [3]])\n",
      "        >>> y_true = [0, 2, 3]\n",
      "        >>> hinge_loss(y_true, pred_decision, labels=labels)\n",
      "        0.56...\n",
      "    \n",
      "    homogeneity_completeness_v_measure(labels_true, labels_pred, *, beta=1.0)\n",
      "        Compute the homogeneity and completeness and V-Measure scores at once.\n",
      "        \n",
      "        Those metrics are based on normalized conditional entropy measures of\n",
      "        the clustering labeling to evaluate given the knowledge of a Ground\n",
      "        Truth class labels of the same samples.\n",
      "        \n",
      "        A clustering result satisfies homogeneity if all of its clusters\n",
      "        contain only data points which are members of a single class.\n",
      "        \n",
      "        A clustering result satisfies completeness if all the data points\n",
      "        that are members of a given class are elements of the same cluster.\n",
      "        \n",
      "        Both scores have positive values between 0.0 and 1.0, larger values\n",
      "        being desirable.\n",
      "        \n",
      "        Those 3 metrics are independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score values in any way.\n",
      "        \n",
      "        V-Measure is furthermore symmetric: swapping ``labels_true`` and\n",
      "        ``label_pred`` will give the same score. This does not hold for\n",
      "        homogeneity and completeness. V-Measure is identical to\n",
      "        :func:`normalized_mutual_info_score` with the arithmetic averaging\n",
      "        method.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            cluster labels to evaluate\n",
      "        \n",
      "        beta : float, default=1.0\n",
      "            Ratio of weight attributed to ``homogeneity`` vs ``completeness``.\n",
      "            If ``beta`` is greater than 1, ``completeness`` is weighted more\n",
      "            strongly in the calculation. If ``beta`` is less than 1,\n",
      "            ``homogeneity`` is weighted more strongly.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        homogeneity : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling\n",
      "        \n",
      "        completeness : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n",
      "        \n",
      "        v_measure : float\n",
      "            harmonic mean of the first two\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        homogeneity_score\n",
      "        completeness_score\n",
      "        v_measure_score\n",
      "    \n",
      "    homogeneity_score(labels_true, labels_pred)\n",
      "        Homogeneity metric of a cluster labeling given a ground truth.\n",
      "        \n",
      "        A clustering result satisfies homogeneity if all of its clusters\n",
      "        contain only data points which are members of a single class.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is not symmetric: switching ``label_true`` with ``label_pred``\n",
      "        will return the :func:`completeness_score` which will be different in\n",
      "        general.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            cluster labels to evaluate\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        homogeneity : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        completeness_score\n",
      "        v_measure_score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are homogeneous::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import homogeneity_score\n",
      "          >>> homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Non-perfect labelings that further split classes into more clusters can be\n",
      "        perfectly homogeneous::\n",
      "        \n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))\n",
      "          1.000000\n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))\n",
      "          1.000000\n",
      "        \n",
      "        Clusters that include samples from different classes do not make for an\n",
      "        homogeneous labeling::\n",
      "        \n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))\n",
      "          0.0...\n",
      "          >>> print(\"%.6f\" % homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          0.0...\n",
      "    \n",
      "    jaccard_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Jaccard similarity coefficient score.\n",
      "        \n",
      "        The Jaccard index [1], or Jaccard similarity coefficient, defined as\n",
      "        the size of the intersection divided by the size of the union of two label\n",
      "        sets, is used to compare set of predicted labels for a sample to the\n",
      "        corresponding set of labels in ``y_true``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <jaccard_similarity_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        pos_label : str or int, default=1\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : {None, 'micro', 'macro', 'samples', 'weighted',             'binary'}, default='binary'\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", {0.0, 1.0}, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division, i.e. when there\n",
      "            there are no negative values in predictions and labels. If set to\n",
      "            \"warn\", this acts like 0, but a warning is also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float (if average is not None) or array of floats, shape =            [n_unique_labels]\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score, f_score, multilabel_confusion_matrix\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        :func:`jaccard_score` may be a poor metric if there are no\n",
      "        positives for some samples or classes. Jaccard is undefined if there are\n",
      "        no true or predicted labels, and our implementation will return a score\n",
      "        of 0 with a warning.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Jaccard index\n",
      "               <https://en.wikipedia.org/wiki/Jaccard_index>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import jaccard_score\n",
      "        >>> y_true = np.array([[0, 1, 1],\n",
      "        ...                    [1, 1, 0]])\n",
      "        >>> y_pred = np.array([[1, 1, 1],\n",
      "        ...                    [1, 0, 0]])\n",
      "        \n",
      "        In the binary case:\n",
      "        \n",
      "        >>> jaccard_score(y_true[0], y_pred[0])\n",
      "        0.6666...\n",
      "        \n",
      "        In the multilabel case:\n",
      "        \n",
      "        >>> jaccard_score(y_true, y_pred, average='samples')\n",
      "        0.5833...\n",
      "        >>> jaccard_score(y_true, y_pred, average='macro')\n",
      "        0.6666...\n",
      "        >>> jaccard_score(y_true, y_pred, average=None)\n",
      "        array([0.5, 0.5, 1. ])\n",
      "        \n",
      "        In the multiclass case:\n",
      "        \n",
      "        >>> y_pred = [0, 2, 1, 2]\n",
      "        >>> y_true = [0, 1, 2, 2]\n",
      "        >>> jaccard_score(y_true, y_pred, average=None)\n",
      "        array([1. , 0. , 0.33...])\n",
      "    \n",
      "    label_ranking_average_precision_score(y_true, y_score, *, sample_weight=None)\n",
      "        Compute ranking-based average precision.\n",
      "        \n",
      "        Label ranking average precision (LRAP) is the average over each ground\n",
      "        truth label assigned to each sample, of the ratio of true vs. total\n",
      "        labels with lower score.\n",
      "        \n",
      "        This metric is used in multilabel ranking problem, where the goal\n",
      "        is to give better rank to the labels associated to each sample.\n",
      "        \n",
      "        The obtained score is always strictly greater than 0 and\n",
      "        the best value is 1.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <label_ranking_average_precision>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : {ndarray, sparse matrix} of shape (n_samples, n_labels)\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import label_ranking_average_precision_score\n",
      "        >>> y_true = np.array([[1, 0, 0], [0, 0, 1]])\n",
      "        >>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])\n",
      "        >>> label_ranking_average_precision_score(y_true, y_score)\n",
      "        0.416...\n",
      "    \n",
      "    label_ranking_loss(y_true, y_score, *, sample_weight=None)\n",
      "        Compute Ranking loss measure.\n",
      "        \n",
      "        Compute the average number of label pairs that are incorrectly ordered\n",
      "        given y_score weighted by the size of the label set and the number of\n",
      "        labels not in the label set.\n",
      "        \n",
      "        This is similar to the error set size, but weighted by the number of\n",
      "        relevant and irrelevant labels. The best performance is achieved with\n",
      "        a ranking loss of zero.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <label_ranking_loss>`.\n",
      "        \n",
      "        .. versionadded:: 0.17\n",
      "           A function *label_ranking_loss*\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : {ndarray, sparse matrix} of shape (n_samples, n_labels)\n",
      "            True binary labels in binary indicator format.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010).\n",
      "               Mining multi-label data. In Data mining and knowledge discovery\n",
      "               handbook (pp. 667-685). Springer US.\n",
      "    \n",
      "    log_loss(y_true, y_pred, *, eps=1e-15, normalize=True, sample_weight=None, labels=None)\n",
      "        Log loss, aka logistic loss or cross-entropy loss.\n",
      "        \n",
      "        This is the loss function used in (multinomial) logistic regression\n",
      "        and extensions of it such as neural networks, defined as the negative\n",
      "        log-likelihood of a logistic model that returns ``y_pred`` probabilities\n",
      "        for its training data ``y_true``.\n",
      "        The log loss is only defined for two or more labels.\n",
      "        For a single sample with true label :math:`y \\in \\{0,1\\}` and\n",
      "        and a probability estimate :math:`p = \\operatorname{Pr}(y = 1)`, the log\n",
      "        loss is:\n",
      "        \n",
      "        .. math::\n",
      "            L_{\\log}(y, p) = -(y \\log (p) + (1 - y) \\log (1 - p))\n",
      "        \n",
      "        Read more in the :ref:`User Guide <log_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like or label indicator matrix\n",
      "            Ground truth (correct) labels for n_samples samples.\n",
      "        \n",
      "        y_pred : array-like of float, shape = (n_samples, n_classes) or (n_samples,)\n",
      "            Predicted probabilities, as returned by a classifier's\n",
      "            predict_proba method. If ``y_pred.shape = (n_samples,)``\n",
      "            the probabilities provided are assumed to be that of the\n",
      "            positive class. The labels in ``y_pred`` are assumed to be\n",
      "            ordered alphabetically, as done by\n",
      "            :class:`preprocessing.LabelBinarizer`.\n",
      "        \n",
      "        eps : float, default=1e-15\n",
      "            Log loss is undefined for p=0 or p=1, so probabilities are\n",
      "            clipped to max(eps, min(1 - eps, p)).\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If true, return the mean loss per sample.\n",
      "            Otherwise, return the sum of the per-sample losses.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            If not provided, labels will be inferred from y_true. If ``labels``\n",
      "            is ``None`` and ``y_pred`` has shape (n_samples,) the labels are\n",
      "            assumed to be binary and are inferred from ``y_true``.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The logarithm used is the natural logarithm (base-e).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import log_loss\n",
      "        >>> log_loss([\"spam\", \"ham\", \"ham\", \"spam\"],\n",
      "        ...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])\n",
      "        0.21616...\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer,\n",
      "        p. 209.\n",
      "    \n",
      "    make_scorer(score_func, *, greater_is_better=True, needs_proba=False, needs_threshold=False, **kwargs)\n",
      "        Make a scorer from a performance metric or loss function.\n",
      "        \n",
      "        This factory function wraps scoring functions for use in\n",
      "        :class:`~sklearn.model_selection.GridSearchCV` and\n",
      "        :func:`~sklearn.model_selection.cross_val_score`.\n",
      "        It takes a score function, such as :func:`~sklearn.metrics.accuracy_score`,\n",
      "        :func:`~sklearn.metrics.mean_squared_error`,\n",
      "        :func:`~sklearn.metrics.adjusted_rand_index` or\n",
      "        :func:`~sklearn.metrics.average_precision`\n",
      "        and returns a callable that scores an estimator's output.\n",
      "        The signature of the call is `(estimator, X, y)` where `estimator`\n",
      "        is the model to be evaluated, `X` is the data and `y` is the\n",
      "        ground truth labeling (or `None` in the case of unsupervised models).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <scoring>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        score_func : callable\n",
      "            Score function (or loss function) with signature\n",
      "            ``score_func(y, y_pred, **kwargs)``.\n",
      "        \n",
      "        greater_is_better : bool, default=True\n",
      "            Whether score_func is a score function (default), meaning high is good,\n",
      "            or a loss function, meaning low is good. In the latter case, the\n",
      "            scorer object will sign-flip the outcome of the score_func.\n",
      "        \n",
      "        needs_proba : bool, default=False\n",
      "            Whether score_func requires predict_proba to get probability estimates\n",
      "            out of a classifier.\n",
      "        \n",
      "            If True, for binary `y_true`, the score function is supposed to accept\n",
      "            a 1D `y_pred` (i.e., probability of the positive class, shape\n",
      "            `(n_samples,)`).\n",
      "        \n",
      "        needs_threshold : bool, default=False\n",
      "            Whether score_func takes a continuous decision certainty.\n",
      "            This only works for binary classification using estimators that\n",
      "            have either a decision_function or predict_proba method.\n",
      "        \n",
      "            If True, for binary `y_true`, the score function is supposed to accept\n",
      "            a 1D `y_pred` (i.e., probability of the positive class or the decision\n",
      "            function, shape `(n_samples,)`).\n",
      "        \n",
      "            For example ``average_precision`` or the area under the roc curve\n",
      "            can not be computed using discrete predictions alone.\n",
      "        \n",
      "        **kwargs : additional arguments\n",
      "            Additional parameters to be passed to score_func.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        scorer : callable\n",
      "            Callable object that returns a scalar score; greater is better.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import fbeta_score, make_scorer\n",
      "        >>> ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
      "        >>> ftwo_scorer\n",
      "        make_scorer(fbeta_score, beta=2)\n",
      "        >>> from sklearn.model_selection import GridSearchCV\n",
      "        >>> from sklearn.svm import LinearSVC\n",
      "        >>> grid = GridSearchCV(LinearSVC(), param_grid={'C': [1, 10]},\n",
      "        ...                     scoring=ftwo_scorer)\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If `needs_proba=False` and `needs_threshold=False`, the score\n",
      "        function is supposed to accept the output of :term:`predict`. If\n",
      "        `needs_proba=True`, the score function is supposed to accept the\n",
      "        output of :term:`predict_proba` (For binary `y_true`, the score function is\n",
      "        supposed to accept probability of the positive class). If\n",
      "        `needs_threshold=True`, the score function is supposed to accept the\n",
      "        output of :term:`decision_function`.\n",
      "    \n",
      "    matthews_corrcoef(y_true, y_pred, *, sample_weight=None)\n",
      "        Compute the Matthews correlation coefficient (MCC).\n",
      "        \n",
      "        The Matthews correlation coefficient is used in machine learning as a\n",
      "        measure of the quality of binary and multiclass classifications. It takes\n",
      "        into account true and false positives and negatives and is generally\n",
      "        regarded as a balanced measure which can be used even if the classes are of\n",
      "        very different sizes. The MCC is in essence a correlation coefficient value\n",
      "        between -1 and +1. A coefficient of +1 represents a perfect prediction, 0\n",
      "        an average random prediction and -1 an inverse prediction.  The statistic\n",
      "        is also known as the phi coefficient. [source: Wikipedia]\n",
      "        \n",
      "        Binary and multiclass labels are supported.  Only in the binary case does\n",
      "        this relate to information about true and false positives and negatives.\n",
      "        See references below.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <matthews_corrcoef>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array, shape = [n_samples]\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array, shape = [n_samples]\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.18\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mcc : float\n",
      "            The Matthews correlation coefficient (+1 represents a perfect\n",
      "            prediction, 0 an average random prediction and -1 and inverse\n",
      "            prediction).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the\n",
      "           accuracy of prediction algorithms for classification: an overview\n",
      "           <https://doi.org/10.1093/bioinformatics/16.5.412>`_.\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the Matthews Correlation Coefficient\n",
      "           <https://en.wikipedia.org/wiki/Matthews_correlation_coefficient>`_.\n",
      "        \n",
      "        .. [3] `Gorodkin, (2004). Comparing two K-category assignments by a\n",
      "            K-category correlation coefficient\n",
      "            <https://www.sciencedirect.com/science/article/pii/S1476927104000799>`_.\n",
      "        \n",
      "        .. [4] `Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC and CEN\n",
      "            Error Measures in MultiClass Prediction\n",
      "            <https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0041882>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import matthews_corrcoef\n",
      "        >>> y_true = [+1, +1, +1, -1]\n",
      "        >>> y_pred = [+1, -1, +1, +1]\n",
      "        >>> matthews_corrcoef(y_true, y_pred)\n",
      "        -0.33...\n",
      "    \n",
      "    max_error(y_true, y_pred)\n",
      "        max_error metric calculates the maximum residual error.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <max_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        max_error : float\n",
      "            A positive floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import max_error\n",
      "        >>> y_true = [3, 2, 7, 1]\n",
      "        >>> y_pred = [4, 2, 7, 1]\n",
      "        >>> max_error(y_true, y_pred)\n",
      "        1\n",
      "    \n",
      "    mean_absolute_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean absolute error regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_absolute_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'}  or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            If multioutput is 'raw_values', then mean absolute error is returned\n",
      "            for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "            MAE output is non-negative floating point. The best value is 0.0.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_absolute_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_absolute_error(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> mean_absolute_error(y_true, y_pred)\n",
      "        0.75\n",
      "        >>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.5, 1. ])\n",
      "        >>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.85...\n",
      "    \n",
      "    mean_absolute_percentage_error(y_true, y_pred, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean absolute percentage error regression loss.\n",
      "        \n",
      "        Note here that we do not represent the output as a percentage in range\n",
      "        [0, 100]. Instead, we represent it in range [0, 1/eps]. Read more in the\n",
      "        :ref:`User Guide <mean_absolute_percentage_error>`.\n",
      "        \n",
      "        .. versionadded:: 0.24\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "            If input is list then the shape must be (n_outputs,).\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats in the range [0, 1/eps]\n",
      "            If multioutput is 'raw_values', then mean absolute percentage error\n",
      "            is returned for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "            MAPE output is non-negative floating point. The best value is 0.0.\n",
      "            But note the fact that bad predictions can lead to arbitarily large\n",
      "            MAPE values, especially if some y_true values are very close to zero.\n",
      "            Note that we return a large value instead of `inf` when y_true is zero.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_absolute_percentage_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_absolute_percentage_error(y_true, y_pred)\n",
      "        0.3273...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> mean_absolute_percentage_error(y_true, y_pred)\n",
      "        0.5515...\n",
      "        >>> mean_absolute_percentage_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.6198...\n",
      "    \n",
      "    mean_gamma_deviance(y_true, y_pred, *, sample_weight=None)\n",
      "        Mean Gamma deviance regression loss.\n",
      "        \n",
      "        Gamma deviance is equivalent to the Tweedie deviance with\n",
      "        the power parameter `power=2`. It is invariant to scaling of\n",
      "        the target variable, and measures relative errors.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_tweedie_deviance>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values. Requires y_true > 0.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values. Requires y_pred > 0.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            A non-negative floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_gamma_deviance\n",
      "        >>> y_true = [2, 0.5, 1, 4]\n",
      "        >>> y_pred = [0.5, 0.5, 2., 2.]\n",
      "        >>> mean_gamma_deviance(y_true, y_pred)\n",
      "        1.0568...\n",
      "    \n",
      "    mean_poisson_deviance(y_true, y_pred, *, sample_weight=None)\n",
      "        Mean Poisson deviance regression loss.\n",
      "        \n",
      "        Poisson deviance is equivalent to the Tweedie deviance with\n",
      "        the power parameter `power=1`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_tweedie_deviance>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values. Requires y_true >= 0.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values. Requires y_pred > 0.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            A non-negative floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_poisson_deviance\n",
      "        >>> y_true = [2, 0, 1, 4]\n",
      "        >>> y_pred = [0.5, 0.5, 2., 2.]\n",
      "        >>> mean_poisson_deviance(y_true, y_pred)\n",
      "        1.4260...\n",
      "    \n",
      "    mean_squared_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average', squared=True)\n",
      "        Mean squared error regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_squared_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        squared : bool, default=True\n",
      "            If True returns MSE value, if False returns RMSE value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            A non-negative floating point value (the best value is 0.0), or an\n",
      "            array of floating point values, one for each individual target.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_squared_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_squared_error(y_true, y_pred)\n",
      "        0.375\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> mean_squared_error(y_true, y_pred, squared=False)\n",
      "        0.612...\n",
      "        >>> y_true = [[0.5, 1],[-1, 1],[7, -6]]\n",
      "        >>> y_pred = [[0, 2],[-1, 2],[8, -5]]\n",
      "        >>> mean_squared_error(y_true, y_pred)\n",
      "        0.708...\n",
      "        >>> mean_squared_error(y_true, y_pred, squared=False)\n",
      "        0.822...\n",
      "        >>> mean_squared_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.41666667, 1.        ])\n",
      "        >>> mean_squared_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.825...\n",
      "    \n",
      "    mean_squared_log_error(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        Mean squared logarithmic error regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_squared_log_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "        \n",
      "            Defines aggregating of multiple output values.\n",
      "            Array-like value defines weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors when the input is of multioutput\n",
      "                format.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            A non-negative floating point value (the best value is 0.0), or an\n",
      "            array of floating point values, one for each individual target.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_squared_log_error\n",
      "        >>> y_true = [3, 5, 2.5, 7]\n",
      "        >>> y_pred = [2.5, 5, 4, 8]\n",
      "        >>> mean_squared_log_error(y_true, y_pred)\n",
      "        0.039...\n",
      "        >>> y_true = [[0.5, 1], [1, 2], [7, 6]]\n",
      "        >>> y_pred = [[0.5, 2], [1, 2.5], [8, 8]]\n",
      "        >>> mean_squared_log_error(y_true, y_pred)\n",
      "        0.044...\n",
      "        >>> mean_squared_log_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.00462428, 0.08377444])\n",
      "        >>> mean_squared_log_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.060...\n",
      "    \n",
      "    mean_tweedie_deviance(y_true, y_pred, *, sample_weight=None, power=0)\n",
      "        Mean Tweedie deviance regression loss.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mean_tweedie_deviance>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        power : float, default=0\n",
      "            Tweedie power parameter. Either power <= 0 or power >= 1.\n",
      "        \n",
      "            The higher `p` the less weight is given to extreme\n",
      "            deviations between true and predicted targets.\n",
      "        \n",
      "            - power < 0: Extreme stable distribution. Requires: y_pred > 0.\n",
      "            - power = 0 : Normal distribution, output corresponds to\n",
      "              mean_squared_error. y_true and y_pred can be any real numbers.\n",
      "            - power = 1 : Poisson distribution. Requires: y_true >= 0 and\n",
      "              y_pred > 0.\n",
      "            - 1 < p < 2 : Compound Poisson distribution. Requires: y_true >= 0\n",
      "              and y_pred > 0.\n",
      "            - power = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.\n",
      "            - power = 3 : Inverse Gaussian distribution. Requires: y_true > 0\n",
      "              and y_pred > 0.\n",
      "            - otherwise : Positive stable distribution. Requires: y_true > 0\n",
      "              and y_pred > 0.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float\n",
      "            A non-negative floating point value (the best value is 0.0).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import mean_tweedie_deviance\n",
      "        >>> y_true = [2, 0, 1, 4]\n",
      "        >>> y_pred = [0.5, 0.5, 2., 2.]\n",
      "        >>> mean_tweedie_deviance(y_true, y_pred, power=1)\n",
      "        1.4260...\n",
      "    \n",
      "    median_absolute_error(y_true, y_pred, *, multioutput='uniform_average', sample_weight=None)\n",
      "        Median absolute error regression loss.\n",
      "        \n",
      "        Median absolute error output is non-negative floating point. The best value\n",
      "        is 0.0. Read more in the :ref:`User Guide <median_absolute_error>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average'} or array-like of shape             (n_outputs,), default='uniform_average'\n",
      "            Defines aggregating of multiple output values. Array-like value defines\n",
      "            weights used to average errors.\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of errors in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Errors of all outputs are averaged with uniform weight.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or ndarray of floats\n",
      "            If multioutput is 'raw_values', then mean absolute error is returned\n",
      "            for each output separately.\n",
      "            If multioutput is 'uniform_average' or an ndarray of weights, then the\n",
      "            weighted average of all output errors is returned.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import median_absolute_error\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> median_absolute_error(y_true, y_pred)\n",
      "        0.5\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> median_absolute_error(y_true, y_pred)\n",
      "        0.75\n",
      "        >>> median_absolute_error(y_true, y_pred, multioutput='raw_values')\n",
      "        array([0.5, 1. ])\n",
      "        >>> median_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\n",
      "        0.85\n",
      "    \n",
      "    multilabel_confusion_matrix(y_true, y_pred, *, sample_weight=None, labels=None, samplewise=False)\n",
      "        Compute a confusion matrix for each class or sample.\n",
      "        \n",
      "        .. versionadded:: 0.21\n",
      "        \n",
      "        Compute class-wise (default) or sample-wise (samplewise=True) multilabel\n",
      "        confusion matrix to evaluate the accuracy of a classification, and output\n",
      "        confusion matrices for each class or sample.\n",
      "        \n",
      "        In multilabel confusion matrix :math:`MCM`, the count of true negatives\n",
      "        is :math:`MCM_{:,0,0}`, false negatives is :math:`MCM_{:,1,0}`,\n",
      "        true positives is :math:`MCM_{:,1,1}` and false positives is\n",
      "        :math:`MCM_{:,0,1}`.\n",
      "        \n",
      "        Multiclass data will be treated as if binarized under a one-vs-rest\n",
      "        transformation. Returned confusion matrices will be in the order of\n",
      "        sorted unique labels in the union of (y_true, y_pred).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <multilabel_confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : {array-like, sparse matrix} of shape (n_samples, n_outputs) or             (n_samples,)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : {array-like, sparse matrix} of shape (n_samples, n_outputs) or             (n_samples,)\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            A list of classes or column indices to select some (or to force\n",
      "            inclusion of classes absent from the data).\n",
      "        \n",
      "        samplewise : bool, default=False\n",
      "            In the multilabel case, this calculates a confusion matrix per sample.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        multi_confusion : ndarray of shape (n_outputs, 2, 2)\n",
      "            A 2x2 confusion matrix corresponding to each output in the input.\n",
      "            When calculating class-wise multi_confusion (default), then\n",
      "            n_outputs = n_labels; when calculating sample-wise multi_confusion\n",
      "            (samplewise=True), n_outputs = n_samples. If ``labels`` is defined,\n",
      "            the results will be returned in the order specified in ``labels``,\n",
      "            otherwise the results will be returned in sorted order by default.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        confusion_matrix\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The multilabel_confusion_matrix calculates class-wise or sample-wise\n",
      "        multilabel confusion matrices, and in multiclass tasks, labels are\n",
      "        binarized under a one-vs-rest way; while confusion_matrix calculates\n",
      "        one confusion matrix for confusion between every two classes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Multilabel-indicator case:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import multilabel_confusion_matrix\n",
      "        >>> y_true = np.array([[1, 0, 1],\n",
      "        ...                    [0, 1, 0]])\n",
      "        >>> y_pred = np.array([[1, 0, 0],\n",
      "        ...                    [0, 1, 1]])\n",
      "        >>> multilabel_confusion_matrix(y_true, y_pred)\n",
      "        array([[[1, 0],\n",
      "                [0, 1]],\n",
      "        <BLANKLINE>\n",
      "               [[1, 0],\n",
      "                [0, 1]],\n",
      "        <BLANKLINE>\n",
      "               [[0, 1],\n",
      "                [1, 0]]])\n",
      "        \n",
      "        Multiclass case:\n",
      "        \n",
      "        >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
      "        >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
      "        >>> multilabel_confusion_matrix(y_true, y_pred,\n",
      "        ...                             labels=[\"ant\", \"bird\", \"cat\"])\n",
      "        array([[[3, 1],\n",
      "                [0, 2]],\n",
      "        <BLANKLINE>\n",
      "               [[5, 0],\n",
      "                [1, 0]],\n",
      "        <BLANKLINE>\n",
      "               [[2, 1],\n",
      "                [1, 2]]])\n",
      "    \n",
      "    mutual_info_score(labels_true, labels_pred, *, contingency=None)\n",
      "        Mutual Information between two clusterings.\n",
      "        \n",
      "        The Mutual Information is a measure of the similarity between two labels of\n",
      "        the same data. Where :math:`|U_i|` is the number of the samples\n",
      "        in cluster :math:`U_i` and :math:`|V_j|` is the number of the\n",
      "        samples in cluster :math:`V_j`, the Mutual Information\n",
      "        between clusterings :math:`U` and :math:`V` is given as:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "            MI(U,V)=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i\\cap V_j|}{N}\n",
      "            \\log\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : int array-like of shape (n_samples,)\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        contingency : {ndarray, sparse matrix} of shape             (n_classes_true, n_classes_pred), default=None\n",
      "            A contingency matrix given by the :func:`contingency_matrix` function.\n",
      "            If value is ``None``, it will be computed, otherwise the given value is\n",
      "            used, with ``labels_true`` and ``labels_pred`` ignored.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mi : float\n",
      "           Mutual information, a non-negative value\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The logarithm used is the natural logarithm (base-e).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        adjusted_mutual_info_score : Adjusted against chance Mutual Information.\n",
      "        normalized_mutual_info_score : Normalized Mutual Information.\n",
      "    \n",
      "    nan_euclidean_distances(X, Y=None, *, squared=False, missing_values=nan, copy=True)\n",
      "        Calculate the euclidean distances in the presence of missing values.\n",
      "        \n",
      "        Compute the euclidean distance between each pair of samples in X and Y,\n",
      "        where Y=X is assumed if Y=None. When calculating the distance between a\n",
      "        pair of samples, this formulation ignores feature coordinates with a\n",
      "        missing value in either sample and scales up the weight of the remaining\n",
      "        coordinates:\n",
      "        \n",
      "            dist(x,y) = sqrt(weight * sq. distance from present coordinates)\n",
      "            where,\n",
      "            weight = Total # of coordinates / # of present coordinates\n",
      "        \n",
      "        For example, the distance between ``[3, na, na, 6]`` and ``[1, na, 4, 5]``\n",
      "        is:\n",
      "        \n",
      "            .. math::\n",
      "                \\sqrt{\\frac{4}{2}((3-1)^2 + (6-5)^2)}\n",
      "        \n",
      "        If all the coordinates are missing or if there are no common present\n",
      "        coordinates then NaN is returned for that pair.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        .. versionadded:: 0.22\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape=(n_samples_X, n_features)\n",
      "        \n",
      "        Y : array-like of shape=(n_samples_Y, n_features), default=None\n",
      "        \n",
      "        squared : bool, default=False\n",
      "            Return squared Euclidean distances.\n",
      "        \n",
      "        missing_values : np.nan or int, default=np.nan\n",
      "            Representation of missing value.\n",
      "        \n",
      "        copy : bool, default=True\n",
      "            Make and use a deep copy of X and Y (if Y exists).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        distances : ndarray of shape (n_samples_X, n_samples_Y)\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        paired_distances : Distances between pairs of elements of X and Y.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics.pairwise import nan_euclidean_distances\n",
      "        >>> nan = float(\"NaN\")\n",
      "        >>> X = [[0, 1], [1, nan]]\n",
      "        >>> nan_euclidean_distances(X, X) # distance between rows of X\n",
      "        array([[0.        , 1.41421356],\n",
      "               [1.41421356, 0.        ]])\n",
      "        \n",
      "        >>> # get distance to origin\n",
      "        >>> nan_euclidean_distances(X, [[0, 0]])\n",
      "        array([[1.        ],\n",
      "               [1.41421356]])\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        * John K. Dixon, \"Pattern Recognition with Partly Missing Data\",\n",
      "          IEEE Transactions on Systems, Man, and Cybernetics, Volume: 9, Issue:\n",
      "          10, pp. 617 - 621, Oct. 1979.\n",
      "          http://ieeexplore.ieee.org/abstract/document/4310090/\n",
      "    \n",
      "    ndcg_score(y_true, y_score, *, k=None, sample_weight=None, ignore_ties=False)\n",
      "        Compute Normalized Discounted Cumulative Gain.\n",
      "        \n",
      "        Sum the true scores ranked in the order induced by the predicted scores,\n",
      "        after applying a logarithmic discount. Then divide by the best possible\n",
      "        score (Ideal DCG, obtained for a perfect ranking) to obtain a score between\n",
      "        0 and 1.\n",
      "        \n",
      "        This ranking metric yields a high value if true labels are ranked high by\n",
      "        ``y_score``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples, n_labels)\n",
      "            True targets of multilabel classification, or true scores of entities\n",
      "            to be ranked.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples, n_labels)\n",
      "            Target scores, can either be probability estimates, confidence values,\n",
      "            or non-thresholded measure of decisions (as returned by\n",
      "            \"decision_function\" on some classifiers).\n",
      "        \n",
      "        k : int, default=None\n",
      "            Only consider the highest k scores in the ranking. If None, use all\n",
      "            outputs.\n",
      "        \n",
      "        sample_weight : ndarray of shape (n_samples,), default=None\n",
      "            Sample weights. If None, all samples are given the same weight.\n",
      "        \n",
      "        ignore_ties : bool, default=False\n",
      "            Assume that there are no ties in y_score (which is likely to be the\n",
      "            case if y_score is continuous) for efficiency gains.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        normalized_discounted_cumulative_gain : float in [0., 1.]\n",
      "            The averaged NDCG scores for all samples.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        dcg_score : Discounted Cumulative Gain (not normalized).\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        `Wikipedia entry for Discounted Cumulative Gain\n",
      "        <https://en.wikipedia.org/wiki/Discounted_cumulative_gain>`_\n",
      "        \n",
      "        Jarvelin, K., & Kekalainen, J. (2002).\n",
      "        Cumulated gain-based evaluation of IR techniques. ACM Transactions on\n",
      "        Information Systems (TOIS), 20(4), 422-446.\n",
      "        \n",
      "        Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).\n",
      "        A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th\n",
      "        Annual Conference on Learning Theory (COLT 2013)\n",
      "        \n",
      "        McSherry, F., & Najork, M. (2008, March). Computing information retrieval\n",
      "        performance measures efficiently in the presence of tied scores. In\n",
      "        European conference on information retrieval (pp. 414-421). Springer,\n",
      "        Berlin, Heidelberg.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import ndcg_score\n",
      "        >>> # we have groud-truth relevance of some answers to a query:\n",
      "        >>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])\n",
      "        >>> # we predict some scores (relevance) for the answers\n",
      "        >>> scores = np.asarray([[.1, .2, .3, 4, 70]])\n",
      "        >>> ndcg_score(true_relevance, scores)\n",
      "        0.69...\n",
      "        >>> scores = np.asarray([[.05, 1.1, 1., .5, .0]])\n",
      "        >>> ndcg_score(true_relevance, scores)\n",
      "        0.49...\n",
      "        >>> # we can set k to truncate the sum; only top k answers contribute.\n",
      "        >>> ndcg_score(true_relevance, scores, k=4)\n",
      "        0.35...\n",
      "        >>> # the normalization takes k into account so a perfect answer\n",
      "        >>> # would still get 1.0\n",
      "        >>> ndcg_score(true_relevance, true_relevance, k=4)\n",
      "        1.0\n",
      "        >>> # now we have some ties in our prediction\n",
      "        >>> scores = np.asarray([[1, 0, 0, 0, 1]])\n",
      "        >>> # by default ties are averaged, so here we get the average (normalized)\n",
      "        >>> # true relevance of our top predictions: (10 / 10 + 5 / 10) / 2 = .75\n",
      "        >>> ndcg_score(true_relevance, scores, k=1)\n",
      "        0.75\n",
      "        >>> # we can choose to ignore ties for faster results, but only\n",
      "        >>> # if we know there aren't ties in our scores, otherwise we get\n",
      "        >>> # wrong results:\n",
      "        >>> ndcg_score(true_relevance,\n",
      "        ...           scores, k=1, ignore_ties=True)\n",
      "        0.5\n",
      "    \n",
      "    normalized_mutual_info_score(labels_true, labels_pred, *, average_method='arithmetic')\n",
      "        Normalized Mutual Information between two clusterings.\n",
      "        \n",
      "        Normalized Mutual Information (NMI) is a normalization of the Mutual\n",
      "        Information (MI) score to scale the results between 0 (no mutual\n",
      "        information) and 1 (perfect correlation). In this function, mutual\n",
      "        information is normalized by some generalized mean of ``H(labels_true)``\n",
      "        and ``H(labels_pred))``, defined by the `average_method`.\n",
      "        \n",
      "        This measure is not adjusted for chance. Therefore\n",
      "        :func:`adjusted_mutual_info_score` might be preferred.\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <mutual_info_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        labels_pred : int array-like of shape (n_samples,)\n",
      "            A clustering of the data into disjoint subsets.\n",
      "        \n",
      "        average_method : str, default='arithmetic'\n",
      "            How to compute the normalizer in the denominator. Possible options\n",
      "            are 'min', 'geometric', 'arithmetic', and 'max'.\n",
      "        \n",
      "            .. versionadded:: 0.20\n",
      "        \n",
      "            .. versionchanged:: 0.22\n",
      "               The default value of ``average_method`` changed from 'geometric' to\n",
      "               'arithmetic'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        nmi : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        v_measure_score : V-Measure (NMI with arithmetic mean option).\n",
      "        adjusted_rand_score : Adjusted Rand Index.\n",
      "        adjusted_mutual_info_score : Adjusted Mutual Information (adjusted\n",
      "            against chance).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have\n",
      "        score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import normalized_mutual_info_score\n",
      "          >>> normalized_mutual_info_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "          >>> normalized_mutual_info_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          ... # doctest: +SKIP\n",
      "          1.0\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally in-complete, hence the NMI is null::\n",
      "        \n",
      "          >>> normalized_mutual_info_score([0, 0, 0, 0], [0, 1, 2, 3])\n",
      "          ... # doctest: +SKIP\n",
      "          0.0\n",
      "    \n",
      "    pair_confusion_matrix(labels_true, labels_pred)\n",
      "        Pair confusion matrix arising from two clusterings.\n",
      "        \n",
      "        The pair confusion matrix :math:`C` computes a 2 by 2 similarity matrix\n",
      "        between two clusterings by considering all pairs of samples and counting\n",
      "        pairs that are assigned into the same or into different clusters under\n",
      "        the true and predicted clusterings.\n",
      "        \n",
      "        Considering a pair of samples that is clustered together a positive pair,\n",
      "        then as in binary classification the count of true negatives is\n",
      "        :math:`C_{00}`, false negatives is :math:`C_{10}`, true positives is\n",
      "        :math:`C_{11}` and false positives is :math:`C_{01}`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <pair_confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,), dtype=integral\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,), dtype=integral\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        C : ndarray of shape (2, 2), dtype=np.int64\n",
      "            The contingency matrix.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        rand_score: Rand Score\n",
      "        adjusted_rand_score: Adjusted Rand Score\n",
      "        adjusted_mutual_info_score: Adjusted Mutual Information\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Perfectly matching labelings have all non-zero entries on the\n",
      "        diagonal regardless of actual label values:\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import pair_confusion_matrix\n",
      "          >>> pair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          array([[8, 0],\n",
      "                 [0, 4]]...\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete but may be not always pure, hence penalized, and\n",
      "        have some off-diagonal non-zero entries:\n",
      "        \n",
      "          >>> pair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])\n",
      "          array([[8, 2],\n",
      "                 [0, 2]]...\n",
      "        \n",
      "        Note that the matrix is not symmetric.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. L. Hubert and P. Arabie, Comparing Partitions, Journal of\n",
      "          Classification 1985\n",
      "          https://link.springer.com/article/10.1007%2FBF01908075\n",
      "    \n",
      "    pairwise_distances(X, Y=None, metric='euclidean', *, n_jobs=None, force_all_finite=True, **kwds)\n",
      "        Compute the distance matrix from a vector array X and optional Y.\n",
      "        \n",
      "        This method takes either a vector array or a distance matrix, and returns\n",
      "        a distance matrix. If the input is a vector array, the distances are\n",
      "        computed. If the input is a distances matrix, it is returned instead.\n",
      "        \n",
      "        This method provides a safe way to take a distance matrix as input, while\n",
      "        preserving compatibility with many other algorithms that take a vector\n",
      "        array.\n",
      "        \n",
      "        If Y is given (default is None), then the returned matrix is the pairwise\n",
      "        distance between the arrays from both X and Y.\n",
      "        \n",
      "        Valid values for metric are:\n",
      "        \n",
      "        - From scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "          'manhattan']. These metrics support sparse matrix\n",
      "          inputs.\n",
      "          ['nan_euclidean'] but it does not yet support sparse matrices.\n",
      "        \n",
      "        - From scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "          'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis',\n",
      "          'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',\n",
      "          'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']\n",
      "          See the documentation for scipy.spatial.distance for details on these\n",
      "          metrics. These metrics do not support sparse matrix inputs.\n",
      "        \n",
      "        Note that in the case of 'cityblock', 'cosine' and 'euclidean' (which are\n",
      "        valid scipy.spatial.distance metrics), the scikit-learn implementation\n",
      "        will be used, which is faster and has support for sparse matrices (except\n",
      "        for 'cityblock'). For a verbose description of the metrics from\n",
      "        scikit-learn, see the __doc__ of the sklearn.pairwise.distance_metrics\n",
      "        function.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "            The shape of the array should be (n_samples_X, n_samples_X) if\n",
      "            metric == \"precomputed\" and (n_samples_X, n_features) otherwise.\n",
      "        \n",
      "        Y : ndarray of shape (n_samples_Y, n_features), default=None\n",
      "            An optional second feature array. Only allowed if\n",
      "            metric != \"precomputed\".\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by scipy.spatial.distance.pdist for its metric parameter, or\n",
      "            a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``.\n",
      "            If metric is \"precomputed\", X is assumed to be a distance matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays from X as input and return a value indicating\n",
      "            the distance between them.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        force_all_finite : bool or 'allow-nan', default=True\n",
      "            Whether to raise an error on np.inf, np.nan, pd.NA in array. Ignored\n",
      "            for a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``. The\n",
      "            possibilities are:\n",
      "        \n",
      "            - True: Force all values of array to be finite.\n",
      "            - False: accepts np.inf, np.nan, pd.NA in array.\n",
      "            - 'allow-nan': accepts only np.nan and pd.NA values in array. Values\n",
      "              cannot be infinite.\n",
      "        \n",
      "            .. versionadded:: 0.22\n",
      "               ``force_all_finite`` accepts the string ``'allow-nan'``.\n",
      "        \n",
      "            .. versionchanged:: 0.23\n",
      "               Accepts `pd.NA` and converts it into `np.nan`.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        D : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_samples_Y)\n",
      "            A distance matrix D such that D_{i, j} is the distance between the\n",
      "            ith and jth vectors of the given matrix X, if Y is None.\n",
      "            If Y is not None, then D_{i, j} is the distance between the ith array\n",
      "            from X and the jth array from Y.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        pairwise_distances_chunked : Performs the same calculation as this\n",
      "            function, but returns a generator of chunks of the distance matrix, in\n",
      "            order to limit memory usage.\n",
      "        paired_distances : Computes the distances between corresponding elements\n",
      "            of two arrays.\n",
      "    \n",
      "    pairwise_distances_argmin(X, Y, *, axis=1, metric='euclidean', metric_kwargs=None)\n",
      "        Compute minimum distances between one point and a set of points.\n",
      "        \n",
      "        This function computes for each row in X, the index of the row of Y which\n",
      "        is closest (according to the specified distance).\n",
      "        \n",
      "        This is mostly equivalent to calling:\n",
      "        \n",
      "            pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis)\n",
      "        \n",
      "        but uses much less memory, and is faster for large arrays.\n",
      "        \n",
      "        This function works with dense 2D arrays only.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples_X, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        Y : array-like of shape (n_samples_Y, n_features)\n",
      "            Arrays containing points.\n",
      "        \n",
      "        axis : int, default=1\n",
      "            Axis along which the argmin and distances are to be computed.\n",
      "        \n",
      "        metric : str or callable, default=\"euclidean\"\n",
      "            Metric to use for distance computation. Any metric from scikit-learn\n",
      "            or scipy.spatial.distance can be used.\n",
      "        \n",
      "            If metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays as input and return one value indicating the\n",
      "            distance between them. This works for Scipy's metrics, but is less\n",
      "            efficient than passing the metric name as a string.\n",
      "        \n",
      "            Distance matrices are not supported.\n",
      "        \n",
      "            Valid values for metric are:\n",
      "        \n",
      "            - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "              'manhattan']\n",
      "        \n",
      "            - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "              'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "              'yule']\n",
      "        \n",
      "            See the documentation for scipy.spatial.distance for details on these\n",
      "            metrics.\n",
      "        \n",
      "        metric_kwargs : dict, default=None\n",
      "            Keyword arguments to pass to specified metric function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        argmin : numpy.ndarray\n",
      "            Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        sklearn.metrics.pairwise_distances\n",
      "        sklearn.metrics.pairwise_distances_argmin_min\n",
      "    \n",
      "    pairwise_distances_argmin_min(X, Y, *, axis=1, metric='euclidean', metric_kwargs=None)\n",
      "        Compute minimum distances between one point and a set of points.\n",
      "        \n",
      "        This function computes for each row in X, the index of the row of Y which\n",
      "        is closest (according to the specified distance). The minimal distances are\n",
      "        also returned.\n",
      "        \n",
      "        This is mostly equivalent to calling:\n",
      "        \n",
      "            (pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis),\n",
      "             pairwise_distances(X, Y=Y, metric=metric).min(axis=axis))\n",
      "        \n",
      "        but uses much less memory, and is faster for large arrays.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\n",
      "            Array containing points.\n",
      "        \n",
      "        axis : int, default=1\n",
      "            Axis along which the argmin and distances are to be computed.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            Metric to use for distance computation. Any metric from scikit-learn\n",
      "            or scipy.spatial.distance can be used.\n",
      "        \n",
      "            If metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays as input and return one value indicating the\n",
      "            distance between them. This works for Scipy's metrics, but is less\n",
      "            efficient than passing the metric name as a string.\n",
      "        \n",
      "            Distance matrices are not supported.\n",
      "        \n",
      "            Valid values for metric are:\n",
      "        \n",
      "            - from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',\n",
      "              'manhattan']\n",
      "        \n",
      "            - from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',\n",
      "              'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski',\n",
      "              'mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao',\n",
      "              'seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean',\n",
      "              'yule']\n",
      "        \n",
      "            See the documentation for scipy.spatial.distance for details on these\n",
      "            metrics.\n",
      "        \n",
      "        metric_kwargs : dict, default=None\n",
      "            Keyword arguments to pass to specified metric function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        argmin : ndarray\n",
      "            Y[argmin[i], :] is the row in Y that is closest to X[i, :].\n",
      "        \n",
      "        distances : ndarray\n",
      "            distances[i] is the distance between the i-th row in X and the\n",
      "            argmin[i]-th row in Y.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        sklearn.metrics.pairwise_distances\n",
      "        sklearn.metrics.pairwise_distances_argmin\n",
      "    \n",
      "    pairwise_distances_chunked(X, Y=None, *, reduce_func=None, metric='euclidean', n_jobs=None, working_memory=None, **kwds)\n",
      "        Generate a distance matrix chunk by chunk with optional reduction.\n",
      "        \n",
      "        In cases where not all of a pairwise distance matrix needs to be stored at\n",
      "        once, this is used to calculate pairwise distances in\n",
      "        ``working_memory``-sized chunks.  If ``reduce_func`` is given, it is run\n",
      "        on each chunk and its return values are concatenated into lists, arrays\n",
      "        or sparse matrices.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n",
      "            Array of pairwise distances between samples, or a feature array.\n",
      "            The shape the array should be (n_samples_X, n_samples_X) if\n",
      "            metric='precomputed' and (n_samples_X, n_features) otherwise.\n",
      "        \n",
      "        Y : ndarray of shape (n_samples_Y, n_features), default=None\n",
      "            An optional second feature array. Only allowed if\n",
      "            metric != \"precomputed\".\n",
      "        \n",
      "        reduce_func : callable, default=None\n",
      "            The function which is applied on each chunk of the distance matrix,\n",
      "            reducing it to needed values.  ``reduce_func(D_chunk, start)``\n",
      "            is called repeatedly, where ``D_chunk`` is a contiguous vertical\n",
      "            slice of the pairwise distance matrix, starting at row ``start``.\n",
      "            It should return one of: None; an array, a list, or a sparse matrix\n",
      "            of length ``D_chunk.shape[0]``; or a tuple of such objects. Returning\n",
      "            None is useful for in-place operations, rather than reductions.\n",
      "        \n",
      "            If None, pairwise_distances_chunked returns a generator of vertical\n",
      "            chunks of the distance matrix.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by scipy.spatial.distance.pdist for its metric parameter, or\n",
      "            a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\n",
      "            If metric is \"precomputed\", X is assumed to be a distance matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two arrays from X as input and return a value indicating\n",
      "            the distance between them.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        working_memory : int, default=None\n",
      "            The sought maximum memory for temporary distance matrix chunks.\n",
      "            When None (default), the value of\n",
      "            ``sklearn.get_config()['working_memory']`` is used.\n",
      "        \n",
      "        `**kwds` : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Yields\n",
      "        ------\n",
      "        D_chunk : {ndarray, sparse matrix}\n",
      "            A contiguous slice of distance matrix, optionally processed by\n",
      "            ``reduce_func``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Without reduce_func:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import pairwise_distances_chunked\n",
      "        >>> X = np.random.RandomState(0).rand(5, 3)\n",
      "        >>> D_chunk = next(pairwise_distances_chunked(X))\n",
      "        >>> D_chunk\n",
      "        array([[0.  ..., 0.29..., 0.41..., 0.19..., 0.57...],\n",
      "               [0.29..., 0.  ..., 0.57..., 0.41..., 0.76...],\n",
      "               [0.41..., 0.57..., 0.  ..., 0.44..., 0.90...],\n",
      "               [0.19..., 0.41..., 0.44..., 0.  ..., 0.51...],\n",
      "               [0.57..., 0.76..., 0.90..., 0.51..., 0.  ...]])\n",
      "        \n",
      "        Retrieve all neighbors and average distance within radius r:\n",
      "        \n",
      "        >>> r = .2\n",
      "        >>> def reduce_func(D_chunk, start):\n",
      "        ...     neigh = [np.flatnonzero(d < r) for d in D_chunk]\n",
      "        ...     avg_dist = (D_chunk * (D_chunk < r)).mean(axis=1)\n",
      "        ...     return neigh, avg_dist\n",
      "        >>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func)\n",
      "        >>> neigh, avg_dist = next(gen)\n",
      "        >>> neigh\n",
      "        [array([0, 3]), array([1]), array([2]), array([0, 3]), array([4])]\n",
      "        >>> avg_dist\n",
      "        array([0.039..., 0.        , 0.        , 0.039..., 0.        ])\n",
      "        \n",
      "        Where r is defined per sample, we need to make use of ``start``:\n",
      "        \n",
      "        >>> r = [.2, .4, .4, .3, .1]\n",
      "        >>> def reduce_func(D_chunk, start):\n",
      "        ...     neigh = [np.flatnonzero(d < r[i])\n",
      "        ...              for i, d in enumerate(D_chunk, start)]\n",
      "        ...     return neigh\n",
      "        >>> neigh = next(pairwise_distances_chunked(X, reduce_func=reduce_func))\n",
      "        >>> neigh\n",
      "        [array([0, 3]), array([0, 1]), array([2]), array([0, 3]), array([4])]\n",
      "        \n",
      "        Force row-by-row generation by reducing ``working_memory``:\n",
      "        \n",
      "        >>> gen = pairwise_distances_chunked(X, reduce_func=reduce_func,\n",
      "        ...                                  working_memory=0)\n",
      "        >>> next(gen)\n",
      "        [array([0, 3])]\n",
      "        >>> next(gen)\n",
      "        [array([0, 1])]\n",
      "    \n",
      "    pairwise_kernels(X, Y=None, metric='linear', *, filter_params=False, n_jobs=None, **kwds)\n",
      "        Compute the kernel between arrays X and optional array Y.\n",
      "        \n",
      "        This method takes either a vector array or a kernel matrix, and returns\n",
      "        a kernel matrix. If the input is a vector array, the kernels are\n",
      "        computed. If the input is a kernel matrix, it is returned instead.\n",
      "        \n",
      "        This method provides a safe way to take a kernel matrix as input, while\n",
      "        preserving compatibility with many other algorithms that take a vector\n",
      "        array.\n",
      "        \n",
      "        If Y is given (default is None), then the returned matrix is the pairwise\n",
      "        kernel between the arrays from both X and Y.\n",
      "        \n",
      "        Valid values for metric are:\n",
      "            ['additive_chi2', 'chi2', 'linear', 'poly', 'polynomial', 'rbf',\n",
      "            'laplacian', 'sigmoid', 'cosine']\n",
      "        \n",
      "        Read more in the :ref:`User Guide <metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_features)\n",
      "            Array of pairwise kernels between samples, or a feature array.\n",
      "            The shape of the array should be (n_samples_X, n_samples_X) if\n",
      "            metric == \"precomputed\" and (n_samples_X, n_features) otherwise.\n",
      "        \n",
      "        Y : ndarray of shape (n_samples_Y, n_features), default=None\n",
      "            A second feature array only if X has shape (n_samples_X, n_features).\n",
      "        \n",
      "        metric : str or callable, default=\"linear\"\n",
      "            The metric to use when calculating kernel between instances in a\n",
      "            feature array. If metric is a string, it must be one of the metrics\n",
      "            in pairwise.PAIRWISE_KERNEL_FUNCTIONS.\n",
      "            If metric is \"precomputed\", X is assumed to be a kernel matrix.\n",
      "            Alternatively, if metric is a callable function, it is called on each\n",
      "            pair of instances (rows) and the resulting value recorded. The callable\n",
      "            should take two rows from X as input and return the corresponding\n",
      "            kernel value as a single number. This means that callables from\n",
      "            :mod:`sklearn.metrics.pairwise` are not allowed, as they operate on\n",
      "            matrices, not single samples. Use the string identifying the kernel\n",
      "            instead.\n",
      "        \n",
      "        filter_params : bool, default=False\n",
      "            Whether to filter invalid parameters or not.\n",
      "        \n",
      "        n_jobs : int, default=None\n",
      "            The number of jobs to use for the computation. This works by breaking\n",
      "            down the pairwise matrix into n_jobs even slices and computing them in\n",
      "            parallel.\n",
      "        \n",
      "            ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "            ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "            for more details.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the kernel function.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        K : ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_samples_Y)\n",
      "            A kernel matrix K such that K_{i, j} is the kernel between the\n",
      "            ith and jth vectors of the given matrix X, if Y is None.\n",
      "            If Y is not None, then K_{i, j} is the kernel between the ith array\n",
      "            from X and the jth array from Y.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If metric is 'precomputed', Y is ignored and X is returned.\n",
      "    \n",
      "    plot_confusion_matrix(estimator, X, y_true, *, labels=None, sample_weight=None, normalize=None, display_labels=None, include_values=True, xticks_rotation='horizontal', values_format=None, cmap='viridis', ax=None, colorbar=True)\n",
      "        Plot Confusion Matrix.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <confusion_matrix>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator instance\n",
      "            Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "            in which the last estimator is a classifier.\n",
      "        \n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Input values.\n",
      "        \n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            Target values.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            List of labels to index the matrix. This may be used to reorder or\n",
      "            select a subset of labels. If `None` is given, those that appear at\n",
      "            least once in `y_true` or `y_pred` are used in sorted order.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        normalize : {'true', 'pred', 'all'}, default=None\n",
      "            Normalizes confusion matrix over the true (rows), predicted (columns)\n",
      "            conditions or all the population. If None, confusion matrix will not be\n",
      "            normalized.\n",
      "        \n",
      "        display_labels : array-like of shape (n_classes,), default=None\n",
      "            Target names used for plotting. By default, `labels` will be used if\n",
      "            it is defined, otherwise the unique labels of `y_true` and `y_pred`\n",
      "            will be used.\n",
      "        \n",
      "        include_values : bool, default=True\n",
      "            Includes values in confusion matrix.\n",
      "        \n",
      "        xticks_rotation : {'vertical', 'horizontal'} or float,                         default='horizontal'\n",
      "            Rotation of xtick labels.\n",
      "        \n",
      "        values_format : str, default=None\n",
      "            Format specification for values in confusion matrix. If `None`,\n",
      "            the format specification is 'd' or '.2g' whichever is shorter.\n",
      "        \n",
      "        cmap : str or matplotlib Colormap, default='viridis'\n",
      "            Colormap recognized by matplotlib.\n",
      "        \n",
      "        ax : matplotlib Axes, default=None\n",
      "            Axes object to plot on. If `None`, a new figure and axes is\n",
      "            created.\n",
      "        \n",
      "        colorbar : bool, default=True\n",
      "            Whether or not to add a colorbar to the plot.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        display : :class:`~sklearn.metrics.ConfusionMatrixDisplay`\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        confusion_matrix : Compute Confusion Matrix to evaluate the accuracy of a\n",
      "            classification.\n",
      "        ConfusionMatrixDisplay : Confusion Matrix visualization.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import matplotlib.pyplot as plt  # doctest: +SKIP\n",
      "        >>> from sklearn.datasets import make_classification\n",
      "        >>> from sklearn.metrics import plot_confusion_matrix\n",
      "        >>> from sklearn.model_selection import train_test_split\n",
      "        >>> from sklearn.svm import SVC\n",
      "        >>> X, y = make_classification(random_state=0)\n",
      "        >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "        ...         X, y, random_state=0)\n",
      "        >>> clf = SVC(random_state=0)\n",
      "        >>> clf.fit(X_train, y_train)\n",
      "        SVC(random_state=0)\n",
      "        >>> plot_confusion_matrix(clf, X_test, y_test)  # doctest: +SKIP\n",
      "        >>> plt.show()  # doctest: +SKIP\n",
      "    \n",
      "    plot_det_curve(estimator, X, y, *, sample_weight=None, response_method='auto', name=None, ax=None, pos_label=None, **kwargs)\n",
      "        Plot detection error tradeoff (DET) curve.\n",
      "        \n",
      "        Extra keyword arguments will be passed to matplotlib's `plot`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <visualizations>`.\n",
      "        \n",
      "        .. versionadded:: 0.24\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator instance\n",
      "            Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "            in which the last estimator is a classifier.\n",
      "        \n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Input values.\n",
      "        \n",
      "        y : array-like of shape (n_samples,)\n",
      "            Target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        response_method : {'predict_proba', 'decision_function', 'auto'}             default='auto'\n",
      "            Specifies whether to use :term:`predict_proba` or\n",
      "            :term:`decision_function` as the predicted target response. If set to\n",
      "            'auto', :term:`predict_proba` is tried first and if it does not exist\n",
      "            :term:`decision_function` is tried next.\n",
      "        \n",
      "        name : str, default=None\n",
      "            Name of DET curve for labeling. If `None`, use the name of the\n",
      "            estimator.\n",
      "        \n",
      "        ax : matplotlib axes, default=None\n",
      "            Axes object to plot on. If `None`, a new figure and axes is created.\n",
      "        \n",
      "        pos_label : str or int, default=None\n",
      "            The label of the positive class.\n",
      "            When `pos_label=None`, if `y_true` is in {-1, 1} or {0, 1},\n",
      "            `pos_label` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        display : :class:`~sklearn.metrics.DetCurveDisplay`\n",
      "            Object that stores computed values.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        det_curve : Compute error rates for different probability thresholds.\n",
      "        DetCurveDisplay : DET curve visualization.\n",
      "        plot_roc_curve : Plot Receiver operating characteristic (ROC) curve.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import matplotlib.pyplot as plt  # doctest: +SKIP\n",
      "        >>> from sklearn import datasets, metrics, model_selection, svm\n",
      "        >>> X, y = datasets.make_classification(random_state=0)\n",
      "        >>> X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
      "        ...     X, y, random_state=0)\n",
      "        >>> clf = svm.SVC(random_state=0)\n",
      "        >>> clf.fit(X_train, y_train)\n",
      "        SVC(random_state=0)\n",
      "        >>> metrics.plot_det_curve(clf, X_test, y_test)  # doctest: +SKIP\n",
      "        >>> plt.show()                                   # doctest: +SKIP\n",
      "    \n",
      "    plot_precision_recall_curve(estimator, X, y, *, sample_weight=None, response_method='auto', name=None, ax=None, pos_label=None, **kwargs)\n",
      "        Plot Precision Recall Curve for binary classifiers.\n",
      "        \n",
      "        Extra keyword arguments will be passed to matplotlib's `plot`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator instance\n",
      "            Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "            in which the last estimator is a classifier.\n",
      "        \n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Input values.\n",
      "        \n",
      "        y : array-like of shape (n_samples,)\n",
      "            Binary target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        response_method : {'predict_proba', 'decision_function', 'auto'},                       default='auto'\n",
      "            Specifies whether to use :term:`predict_proba` or\n",
      "            :term:`decision_function` as the target response. If set to 'auto',\n",
      "            :term:`predict_proba` is tried first and if it does not exist\n",
      "            :term:`decision_function` is tried next.\n",
      "        \n",
      "        name : str, default=None\n",
      "            Name for labeling curve. If `None`, the name of the\n",
      "            estimator is used.\n",
      "        \n",
      "        ax : matplotlib axes, default=None\n",
      "            Axes object to plot on. If `None`, a new figure and axes is created.\n",
      "        \n",
      "        pos_label : str or int, default=None\n",
      "            The class considered as the positive class when computing the precision\n",
      "            and recall metrics. By default, `estimators.classes_[1]` is considered\n",
      "            as the positive class.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        **kwargs : dict\n",
      "            Keyword arguments to be passed to matplotlib's `plot`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        display : :class:`~sklearn.metrics.PrecisionRecallDisplay`\n",
      "            Object that stores computed values.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_curve : Compute precision-recall pairs for different\n",
      "            probability thresholds.\n",
      "        PrecisionRecallDisplay : Precision Recall visualization.\n",
      "    \n",
      "    plot_roc_curve(estimator, X, y, *, sample_weight=None, drop_intermediate=True, response_method='auto', name=None, ax=None, pos_label=None, **kwargs)\n",
      "        Plot Receiver operating characteristic (ROC) curve.\n",
      "        \n",
      "        Extra keyword arguments will be passed to matplotlib's `plot`.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <visualizations>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        estimator : estimator instance\n",
      "            Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`\n",
      "            in which the last estimator is a classifier.\n",
      "        \n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Input values.\n",
      "        \n",
      "        y : array-like of shape (n_samples,)\n",
      "            Target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        drop_intermediate : boolean, default=True\n",
      "            Whether to drop some suboptimal thresholds which would not appear\n",
      "            on a plotted ROC curve. This is useful in order to create lighter\n",
      "            ROC curves.\n",
      "        \n",
      "        response_method : {'predict_proba', 'decision_function', 'auto'}     default='auto'\n",
      "            Specifies whether to use :term:`predict_proba` or\n",
      "            :term:`decision_function` as the target response. If set to 'auto',\n",
      "            :term:`predict_proba` is tried first and if it does not exist\n",
      "            :term:`decision_function` is tried next.\n",
      "        \n",
      "        name : str, default=None\n",
      "            Name of ROC Curve for labeling. If `None`, use the name of the\n",
      "            estimator.\n",
      "        \n",
      "        ax : matplotlib axes, default=None\n",
      "            Axes object to plot on. If `None`, a new figure and axes is created.\n",
      "        \n",
      "        pos_label : str or int, default=None\n",
      "            The class considered as the positive class when computing the roc auc\n",
      "            metrics. By default, `estimators.classes_[1]` is considered\n",
      "            as the positive class.\n",
      "        \n",
      "            .. versionadded:: 0.24\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        display : :class:`~sklearn.metrics.RocCurveDisplay`\n",
      "            Object that stores computed values.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "        RocCurveDisplay : ROC Curve visualization.\n",
      "        roc_auc_score : Compute the area under the ROC curve.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import matplotlib.pyplot as plt  # doctest: +SKIP\n",
      "        >>> from sklearn import datasets, metrics, model_selection, svm\n",
      "        >>> X, y = datasets.make_classification(random_state=0)\n",
      "        >>> X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
      "        ...     X, y, random_state=0)\n",
      "        >>> clf = svm.SVC(random_state=0)\n",
      "        >>> clf.fit(X_train, y_train)\n",
      "        SVC(random_state=0)\n",
      "        >>> metrics.plot_roc_curve(clf, X_test, y_test)  # doctest: +SKIP\n",
      "        >>> plt.show()                                   # doctest: +SKIP\n",
      "    \n",
      "    precision_recall_curve(y_true, probas_pred, *, pos_label=None, sample_weight=None)\n",
      "        Compute precision-recall pairs for different probability thresholds.\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task.\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The last precision and recall values are 1. and 0. respectively and do not\n",
      "        have a corresponding threshold. This ensures that the graph starts on the\n",
      "        y axis.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples,)\n",
      "            True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n",
      "            pos_label should be explicitly given.\n",
      "        \n",
      "        probas_pred : ndarray of shape (n_samples,)\n",
      "            Estimated probabilities or output of a decision function.\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            The label of the positive class.\n",
      "            When ``pos_label=None``, if y_true is in {-1, 1} or {0, 1},\n",
      "            ``pos_label`` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : ndarray of shape (n_thresholds + 1,)\n",
      "            Precision values such that element i is the precision of\n",
      "            predictions with score >= thresholds[i] and the last element is 1.\n",
      "        \n",
      "        recall : ndarray of shape (n_thresholds + 1,)\n",
      "            Decreasing recall values such that element i is the recall of\n",
      "            predictions with score >= thresholds[i] and the last element is 0.\n",
      "        \n",
      "        thresholds : ndarray of shape (n_thresholds,)\n",
      "            Increasing thresholds on the decision function used to compute\n",
      "            precision and recall. n_thresholds <= len(np.unique(probas_pred)).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        plot_precision_recall_curve : Plot Precision Recall Curve for binary\n",
      "            classifiers.\n",
      "        PrecisionRecallDisplay : Precision Recall visualization.\n",
      "        average_precision_score : Compute average precision from prediction scores.\n",
      "        det_curve: Compute error rates for different probability thresholds.\n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import precision_recall_curve\n",
      "        >>> y_true = np.array([0, 0, 1, 1])\n",
      "        >>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> precision, recall, thresholds = precision_recall_curve(\n",
      "        ...     y_true, y_scores)\n",
      "        >>> precision\n",
      "        array([0.66666667, 0.5       , 1.        , 1.        ])\n",
      "        >>> recall\n",
      "        array([1. , 0.5, 0.5, 0. ])\n",
      "        >>> thresholds\n",
      "        array([0.35, 0.4 , 0.8 ])\n",
      "    \n",
      "    precision_recall_fscore_support(y_true, y_pred, *, beta=1.0, labels=None, pos_label=1, average=None, warn_for=('precision', 'recall', 'f-score'), sample_weight=None, zero_division='warn')\n",
      "        Compute precision, recall, F-measure and support for each class.\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The F-beta score can be interpreted as a weighted harmonic mean of\n",
      "        the precision and recall, where an F-beta score reaches its best\n",
      "        value at 1 and worst score at 0.\n",
      "        \n",
      "        The F-beta score weights recall more than precision by a factor of\n",
      "        ``beta``. ``beta == 1.0`` means recall and precision are equally important.\n",
      "        \n",
      "        The support is the number of occurrences of each class in ``y_true``.\n",
      "        \n",
      "        If ``pos_label is None`` and in binary classification, this function\n",
      "        returns the average precision, recall and F-measure if ``average``\n",
      "        is one of ``'micro'``, ``'macro'``, ``'weighted'`` or ``'samples'``.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        beta : float, default=1.0\n",
      "            The strength of recall versus precision in the F-score.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "        pos_label : str or int, default=1\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : {'binary', 'micro', 'macro', 'samples','weighted'},             default=None\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        warn_for : tuple or set, for internal use\n",
      "            This determines which warnings will be made in the case that this\n",
      "            function is being used to return only one of its metrics.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division:\n",
      "               - recall: when there are no positive labels\n",
      "               - precision: when there are no positive predictions\n",
      "               - f-score: both\n",
      "        \n",
      "            If set to \"warn\", this acts as 0, but warnings are also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "        \n",
      "        recall : float (if average is not None) or array of float, , shape =        [n_unique_labels]\n",
      "        \n",
      "        fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels]\n",
      "        \n",
      "        support : None (if average is not None) or array of int, shape =        [n_unique_labels]\n",
      "            The number of occurrences of each label in ``y_true``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0``, precision is undefined.\n",
      "        When ``true positive + false negative == 0``, recall is undefined.\n",
      "        In such cases, by default the metric will be set to 0, as will f-score,\n",
      "        and ``UndefinedMetricWarning`` will be raised. This behavior can be\n",
      "        modified with ``zero_division``.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Precision and recall\n",
      "               <https://en.wikipedia.org/wiki/Precision_and_recall>`_.\n",
      "        \n",
      "        .. [2] `Wikipedia entry for the F1-score\n",
      "               <https://en.wikipedia.org/wiki/F1_score>`_.\n",
      "        \n",
      "        .. [3] `Discriminative Methods for Multi-labeled Classification Advances\n",
      "               in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu\n",
      "               Godbole, Sunita Sarawagi\n",
      "               <http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf>`_.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import precision_recall_fscore_support\n",
      "        >>> y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])\n",
      "        >>> y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
      "        (0.22..., 0.33..., 0.26..., None)\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
      "        (0.33..., 0.33..., 0.33..., None)\n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
      "        (0.22..., 0.33..., 0.26..., None)\n",
      "        \n",
      "        It is possible to compute per-label precisions, recalls, F1-scores and\n",
      "        supports instead of averaging:\n",
      "        \n",
      "        >>> precision_recall_fscore_support(y_true, y_pred, average=None,\n",
      "        ... labels=['pig', 'dog', 'cat'])\n",
      "        (array([0.        , 0.        , 0.66...]),\n",
      "         array([0., 0., 1.]), array([0. , 0. , 0.8]),\n",
      "         array([2, 2, 2]))\n",
      "    \n",
      "    precision_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Compute the precision.\n",
      "        \n",
      "        The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
      "        true positives and ``fp`` the number of false positives. The precision is\n",
      "        intuitively the ability of the classifier not to label as positive a sample\n",
      "        that is negative.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, default=1\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted', 'binary'}             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division. If set to\n",
      "            \"warn\", this acts as 0, but warnings are also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        precision : float (if average is not None) or array of float of shape\n",
      "            (n_unique_labels,)\n",
      "            Precision of the positive class in binary classification or weighted\n",
      "            average of the precision of each class for the multiclass task.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_fscore_support, multilabel_confusion_matrix\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false positive == 0``, precision returns 0 and\n",
      "        raises ``UndefinedMetricWarning``. This behavior can be\n",
      "        modified with ``zero_division``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import precision_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> precision_score(y_true, y_pred, average='macro')\n",
      "        0.22...\n",
      "        >>> precision_score(y_true, y_pred, average='micro')\n",
      "        0.33...\n",
      "        >>> precision_score(y_true, y_pred, average='weighted')\n",
      "        0.22...\n",
      "        >>> precision_score(y_true, y_pred, average=None)\n",
      "        array([0.66..., 0.        , 0.        ])\n",
      "        >>> y_pred = [0, 0, 0, 0, 0, 0]\n",
      "        >>> precision_score(y_true, y_pred, average=None)\n",
      "        array([0.33..., 0.        , 0.        ])\n",
      "        >>> precision_score(y_true, y_pred, average=None, zero_division=1)\n",
      "        array([0.33..., 1.        , 1.        ])\n",
      "    \n",
      "    r2_score(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average')\n",
      "        :math:`R^2` (coefficient of determination) regression score function.\n",
      "        \n",
      "        Best possible score is 1.0 and it can be negative (because the\n",
      "        model can be arbitrarily worse). A constant model that always\n",
      "        predicts the expected value of y, disregarding the input features,\n",
      "        would get a :math:`R^2` score of 0.0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <r2_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "            Estimated target values.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        multioutput : {'raw_values', 'uniform_average', 'variance_weighted'},             array-like of shape (n_outputs,) or None, default='uniform_average'\n",
      "        \n",
      "            Defines aggregating of multiple output scores.\n",
      "            Array-like value defines weights used to average scores.\n",
      "            Default is \"uniform_average\".\n",
      "        \n",
      "            'raw_values' :\n",
      "                Returns a full set of scores in case of multioutput input.\n",
      "        \n",
      "            'uniform_average' :\n",
      "                Scores of all outputs are averaged with uniform weight.\n",
      "        \n",
      "            'variance_weighted' :\n",
      "                Scores of all outputs are averaged, weighted by the variances\n",
      "                of each individual output.\n",
      "        \n",
      "            .. versionchanged:: 0.19\n",
      "                Default value of multioutput is 'uniform_average'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        z : float or ndarray of floats\n",
      "            The :math:`R^2` score or ndarray of scores if 'multioutput' is\n",
      "            'raw_values'.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is not a symmetric function.\n",
      "        \n",
      "        Unlike most other scores, :math:`R^2` score may be negative (it need not\n",
      "        actually be the square of a quantity R).\n",
      "        \n",
      "        This metric is not well-defined for single samples and will return a NaN\n",
      "        value if n_samples is less than two.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry on the Coefficient of determination\n",
      "                <https://en.wikipedia.org/wiki/Coefficient_of_determination>`_\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import r2_score\n",
      "        >>> y_true = [3, -0.5, 2, 7]\n",
      "        >>> y_pred = [2.5, 0.0, 2, 8]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        0.948...\n",
      "        >>> y_true = [[0.5, 1], [-1, 1], [7, -6]]\n",
      "        >>> y_pred = [[0, 2], [-1, 2], [8, -5]]\n",
      "        >>> r2_score(y_true, y_pred,\n",
      "        ...          multioutput='variance_weighted')\n",
      "        0.938...\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [1, 2, 3]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        1.0\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [2, 2, 2]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        0.0\n",
      "        >>> y_true = [1, 2, 3]\n",
      "        >>> y_pred = [3, 2, 1]\n",
      "        >>> r2_score(y_true, y_pred)\n",
      "        -3.0\n",
      "    \n",
      "    rand_score(labels_true, labels_pred)\n",
      "        Rand index.\n",
      "        \n",
      "        The Rand Index computes a similarity measure between two clusterings\n",
      "        by considering all pairs of samples and counting pairs that are\n",
      "        assigned in the same or different clusters in the predicted and\n",
      "        true clusterings.\n",
      "        \n",
      "        The raw RI score is:\n",
      "        \n",
      "            RI = (number of agreeing pairs) / (number of pairs)\n",
      "        \n",
      "        Read more in the :ref:`User Guide <rand_score>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : array-like of shape (n_samples,), dtype=integral\n",
      "            Ground truth class labels to be used as a reference.\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,), dtype=integral\n",
      "            Cluster labels to evaluate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        RI : float\n",
      "           Similarity score between 0.0 and 1.0, inclusive, 1.0 stands for\n",
      "           perfect match.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        adjusted_rand_score: Adjusted Rand Score\n",
      "        adjusted_mutual_info_score: Adjusted Mutual Information\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Perfectly matching labelings have a score of 1 even\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import rand_score\n",
      "          >>> rand_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete but may not always be pure, hence penalized:\n",
      "        \n",
      "          >>> rand_score([0, 0, 1, 2], [0, 0, 1, 1])\n",
      "          0.83...\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. L. Hubert and P. Arabie, Comparing Partitions, Journal of\n",
      "          Classification 1985\n",
      "          https://link.springer.com/article/10.1007%2FBF01908075\n",
      "        \n",
      "        .. https://en.wikipedia.org/wiki/Simple_matching_coefficient\n",
      "        \n",
      "        .. https://en.wikipedia.org/wiki/Rand_index\n",
      "    \n",
      "    recall_score(y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')\n",
      "        Compute the recall.\n",
      "        \n",
      "        The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
      "        true positives and ``fn`` the number of false negatives. The recall is\n",
      "        intuitively the ability of the classifier to find all the positive samples.\n",
      "        \n",
      "        The best value is 1 and the worst value is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) target values.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Estimated targets as returned by a classifier.\n",
      "        \n",
      "        labels : array-like, default=None\n",
      "            The set of labels to include when ``average != 'binary'``, and their\n",
      "            order if ``average is None``. Labels present in the data can be\n",
      "            excluded, for example to calculate a multiclass average ignoring a\n",
      "            majority negative class, while labels not present in the data will\n",
      "            result in 0 components in a macro average. For multilabel targets,\n",
      "            labels are column indices. By default, all labels in ``y_true`` and\n",
      "            ``y_pred`` are used in sorted order.\n",
      "        \n",
      "            .. versionchanged:: 0.17\n",
      "               Parameter `labels` improved for multiclass problem.\n",
      "        \n",
      "        pos_label : str or int, default=1\n",
      "            The class to report if ``average='binary'`` and the data is binary.\n",
      "            If the data are multiclass or multilabel, this will be ignored;\n",
      "            setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
      "            scores for that label only.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted', 'binary'}             default='binary'\n",
      "            This parameter is required for multiclass/multilabel targets.\n",
      "            If ``None``, the scores for each class are returned. Otherwise, this\n",
      "            determines the type of averaging performed on the data:\n",
      "        \n",
      "            ``'binary'``:\n",
      "                Only report results for the class specified by ``pos_label``.\n",
      "                This is applicable only if targets (``y_{true,pred}``) are binary.\n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by counting the total true positives,\n",
      "                false negatives and false positives.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average weighted\n",
      "                by support (the number of true instances for each label). This\n",
      "                alters 'macro' to account for label imbalance; it can result in an\n",
      "                F-score that is not between precision and recall.\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average (only\n",
      "                meaningful for multilabel classification where this differs from\n",
      "                :func:`accuracy_score`).\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
      "            Sets the value to return when there is a zero division. If set to\n",
      "            \"warn\", this acts as 0, but warnings are also raised.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        recall : float (if average is not None) or array of float of shape\n",
      "            (n_unique_labels,)\n",
      "            Recall of the positive class in binary classification or weighted\n",
      "            average of the recall of each class for the multiclass task.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        precision_recall_fscore_support, balanced_accuracy_score,\n",
      "        multilabel_confusion_matrix\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        When ``true positive + false negative == 0``, recall returns 0 and raises\n",
      "        ``UndefinedMetricWarning``. This behavior can be modified with\n",
      "        ``zero_division``.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import recall_score\n",
      "        >>> y_true = [0, 1, 2, 0, 1, 2]\n",
      "        >>> y_pred = [0, 2, 1, 0, 0, 1]\n",
      "        >>> recall_score(y_true, y_pred, average='macro')\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average='micro')\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average='weighted')\n",
      "        0.33...\n",
      "        >>> recall_score(y_true, y_pred, average=None)\n",
      "        array([1., 0., 0.])\n",
      "        >>> y_true = [0, 0, 0, 0, 0, 0]\n",
      "        >>> recall_score(y_true, y_pred, average=None)\n",
      "        array([0.5, 0. , 0. ])\n",
      "        >>> recall_score(y_true, y_pred, average=None, zero_division=1)\n",
      "        array([0.5, 1. , 1. ])\n",
      "    \n",
      "    roc_auc_score(y_true, y_score, *, average='macro', sample_weight=None, max_fpr=None, multi_class='raise', labels=None)\n",
      "        Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)\n",
      "        from prediction scores.\n",
      "        \n",
      "        Note: this implementation can be used with binary, multiclass and\n",
      "        multilabel classification, but some restrictions apply (see Parameters).\n",
      "        \n",
      "        Read more in the :ref:`User Guide <roc_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            True labels or binary label indicators. The binary and multiclass cases\n",
      "            expect labels with shape (n_samples,) while the multilabel case expects\n",
      "            binary label indicators with shape (n_samples, n_classes).\n",
      "        \n",
      "        y_score : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Target scores.\n",
      "        \n",
      "            * In the binary case, it corresponds to an array of shape\n",
      "              `(n_samples,)`. Both probability estimates and non-thresholded\n",
      "              decision values can be provided. The probability estimates correspond\n",
      "              to the **probability of the class with the greater label**,\n",
      "              i.e. `estimator.classes_[1]` and thus\n",
      "              `estimator.predict_proba(X, y)[:, 1]`. The decision values\n",
      "              corresponds to the output of `estimator.decision_function(X, y)`.\n",
      "              See more information in the :ref:`User guide <roc_auc_binary>`;\n",
      "            * In the multiclass case, it corresponds to an array of shape\n",
      "              `(n_samples, n_classes)` of probability estimates provided by the\n",
      "              `predict_proba` method. The probability estimates **must**\n",
      "              sum to 1 across the possible classes. In addition, the order of the\n",
      "              class scores must correspond to the order of ``labels``,\n",
      "              if provided, or else to the numerical or lexicographical order of\n",
      "              the labels in ``y_true``. See more information in the\n",
      "              :ref:`User guide <roc_auc_multiclass>`;\n",
      "            * In the multilabel case, it corresponds to an array of shape\n",
      "              `(n_samples, n_classes)`. Probability estimates are provided by the\n",
      "              `predict_proba` method and the non-thresholded decision values by\n",
      "              the `decision_function` method. The probability estimates correspond\n",
      "              to the **probability of the class with the greater label for each\n",
      "              output** of the classifier. See more information in the\n",
      "              :ref:`User guide <roc_auc_multilabel>`.\n",
      "        \n",
      "        average : {'micro', 'macro', 'samples', 'weighted'} or None,             default='macro'\n",
      "            If ``None``, the scores for each class are returned. Otherwise,\n",
      "            this determines the type of averaging performed on the data:\n",
      "            Note: multiclass ROC AUC currently only handles the 'macro' and\n",
      "            'weighted' averages.\n",
      "        \n",
      "            ``'micro'``:\n",
      "                Calculate metrics globally by considering each element of the label\n",
      "                indicator matrix as a label.\n",
      "            ``'macro'``:\n",
      "                Calculate metrics for each label, and find their unweighted\n",
      "                mean.  This does not take label imbalance into account.\n",
      "            ``'weighted'``:\n",
      "                Calculate metrics for each label, and find their average, weighted\n",
      "                by support (the number of true instances for each label).\n",
      "            ``'samples'``:\n",
      "                Calculate metrics for each instance, and find their average.\n",
      "        \n",
      "            Will be ignored when ``y_true`` is binary.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        max_fpr : float > 0 and <= 1, default=None\n",
      "            If not ``None``, the standardized partial AUC [2]_ over the range\n",
      "            [0, max_fpr] is returned. For the multiclass case, ``max_fpr``,\n",
      "            should be either equal to ``None`` or ``1.0`` as AUC ROC partial\n",
      "            computation currently is not supported for multiclass.\n",
      "        \n",
      "        multi_class : {'raise', 'ovr', 'ovo'}, default='raise'\n",
      "            Only used for multiclass targets. Determines the type of configuration\n",
      "            to use. The default value raises an error, so either\n",
      "            ``'ovr'`` or ``'ovo'`` must be passed explicitly.\n",
      "        \n",
      "            ``'ovr'``:\n",
      "                Stands for One-vs-rest. Computes the AUC of each class\n",
      "                against the rest [3]_ [4]_. This\n",
      "                treats the multiclass case in the same way as the multilabel case.\n",
      "                Sensitive to class imbalance even when ``average == 'macro'``,\n",
      "                because class imbalance affects the composition of each of the\n",
      "                'rest' groupings.\n",
      "            ``'ovo'``:\n",
      "                Stands for One-vs-one. Computes the average AUC of all\n",
      "                possible pairwise combinations of classes [5]_.\n",
      "                Insensitive to class imbalance when\n",
      "                ``average == 'macro'``.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            Only used for multiclass targets. List of labels that index the\n",
      "            classes in ``y_score``. If ``None``, the numerical or lexicographical\n",
      "            order of the labels in ``y_true`` is used.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        auc : float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Receiver operating characteristic\n",
      "                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n",
      "        \n",
      "        .. [2] `Analyzing a portion of the ROC curve. McClish, 1989\n",
      "                <https://www.ncbi.nlm.nih.gov/pubmed/2668680>`_\n",
      "        \n",
      "        .. [3] Provost, F., Domingos, P. (2000). Well-trained PETs: Improving\n",
      "               probability estimation trees (Section 6.2), CeDER Working Paper\n",
      "               #IS-00-04, Stern School of Business, New York University.\n",
      "        \n",
      "        .. [4] `Fawcett, T. (2006). An introduction to ROC analysis. Pattern\n",
      "                Recognition Letters, 27(8), 861-874.\n",
      "                <https://www.sciencedirect.com/science/article/pii/S016786550500303X>`_\n",
      "        \n",
      "        .. [5] `Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area\n",
      "                Under the ROC Curve for Multiple Class Classification Problems.\n",
      "                Machine Learning, 45(2), 171-186.\n",
      "                <http://link.springer.com/article/10.1023/A:1010920819831>`_\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        average_precision_score : Area under the precision-recall curve.\n",
      "        roc_curve : Compute Receiver operating characteristic (ROC) curve.\n",
      "        plot_roc_curve : Plot Receiver operating characteristic (ROC) curve.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Binary case:\n",
      "        \n",
      "        >>> from sklearn.datasets import load_breast_cancer\n",
      "        >>> from sklearn.linear_model import LogisticRegression\n",
      "        >>> from sklearn.metrics import roc_auc_score\n",
      "        >>> X, y = load_breast_cancer(return_X_y=True)\n",
      "        >>> clf = LogisticRegression(solver=\"liblinear\", random_state=0).fit(X, y)\n",
      "        >>> roc_auc_score(y, clf.predict_proba(X)[:, 1])\n",
      "        0.99...\n",
      "        >>> roc_auc_score(y, clf.decision_function(X))\n",
      "        0.99...\n",
      "        \n",
      "        Multiclass case:\n",
      "        \n",
      "        >>> from sklearn.datasets import load_iris\n",
      "        >>> X, y = load_iris(return_X_y=True)\n",
      "        >>> clf = LogisticRegression(solver=\"liblinear\").fit(X, y)\n",
      "        >>> roc_auc_score(y, clf.predict_proba(X), multi_class='ovr')\n",
      "        0.99...\n",
      "        \n",
      "        Multilabel case:\n",
      "        \n",
      "        >>> from sklearn.datasets import make_multilabel_classification\n",
      "        >>> from sklearn.multioutput import MultiOutputClassifier\n",
      "        >>> X, y = make_multilabel_classification(random_state=0)\n",
      "        >>> clf = MultiOutputClassifier(clf).fit(X, y)\n",
      "        >>> # get a list of n_output containing probability arrays of shape\n",
      "        >>> # (n_samples, n_classes)\n",
      "        >>> y_pred = clf.predict_proba(X)\n",
      "        >>> # extract the positive columns for each output\n",
      "        >>> y_pred = np.transpose([pred[:, 1] for pred in y_pred])\n",
      "        >>> roc_auc_score(y, y_pred, average=None)\n",
      "        array([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])\n",
      "        >>> from sklearn.linear_model import RidgeClassifierCV\n",
      "        >>> clf = RidgeClassifierCV().fit(X, y)\n",
      "        >>> roc_auc_score(y, clf.decision_function(X), average=None)\n",
      "        array([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])\n",
      "    \n",
      "    roc_curve(y_true, y_score, *, pos_label=None, sample_weight=None, drop_intermediate=True)\n",
      "        Compute Receiver operating characteristic (ROC).\n",
      "        \n",
      "        Note: this implementation is restricted to the binary classification task.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <roc_metrics>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : ndarray of shape (n_samples,)\n",
      "            True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n",
      "            pos_label should be explicitly given.\n",
      "        \n",
      "        y_score : ndarray of shape (n_samples,)\n",
      "            Target scores, can either be probability estimates of the positive\n",
      "            class, confidence values, or non-thresholded measure of decisions\n",
      "            (as returned by \"decision_function\" on some classifiers).\n",
      "        \n",
      "        pos_label : int or str, default=None\n",
      "            The label of the positive class.\n",
      "            When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},\n",
      "            ``pos_label`` is set to 1, otherwise an error will be raised.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        drop_intermediate : bool, default=True\n",
      "            Whether to drop some suboptimal thresholds which would not appear\n",
      "            on a plotted ROC curve. This is useful in order to create lighter\n",
      "            ROC curves.\n",
      "        \n",
      "            .. versionadded:: 0.17\n",
      "               parameter *drop_intermediate*.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fpr : ndarray of shape (>2,)\n",
      "            Increasing false positive rates such that element i is the false\n",
      "            positive rate of predictions with score >= `thresholds[i]`.\n",
      "        \n",
      "        tpr : ndarray of shape (>2,)\n",
      "            Increasing true positive rates such that element `i` is the true\n",
      "            positive rate of predictions with score >= `thresholds[i]`.\n",
      "        \n",
      "        thresholds : ndarray of shape = (n_thresholds,)\n",
      "            Decreasing thresholds on the decision function used to compute\n",
      "            fpr and tpr. `thresholds[0]` represents no instances being predicted\n",
      "            and is arbitrarily set to `max(y_score) + 1`.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        plot_roc_curve : Plot Receiver operating characteristic (ROC) curve.\n",
      "        RocCurveDisplay : ROC Curve visualization.\n",
      "        det_curve: Compute error rates for different probability thresholds.\n",
      "        roc_auc_score : Compute the area under the ROC curve.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Since the thresholds are sorted from low to high values, they\n",
      "        are reversed upon returning them to ensure they correspond to both ``fpr``\n",
      "        and ``tpr``, which are sorted in reversed order during their calculation.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Wikipedia entry for the Receiver operating characteristic\n",
      "                <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n",
      "        \n",
      "        .. [2] Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition\n",
      "               Letters, 2006, 27(8):861-874.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn import metrics\n",
      "        >>> y = np.array([1, 1, 2, 2])\n",
      "        >>> scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
      "        >>> fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)\n",
      "        >>> fpr\n",
      "        array([0. , 0. , 0.5, 0.5, 1. ])\n",
      "        >>> tpr\n",
      "        array([0. , 0.5, 0.5, 1. , 1. ])\n",
      "        >>> thresholds\n",
      "        array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])\n",
      "    \n",
      "    silhouette_samples(X, labels, *, metric='euclidean', **kwds)\n",
      "        Compute the Silhouette Coefficient for each sample.\n",
      "        \n",
      "        The Silhouette Coefficient is a measure of how well samples are clustered\n",
      "        with samples that are similar to themselves. Clustering models with a high\n",
      "        Silhouette Coefficient are said to be dense, where samples in the same\n",
      "        cluster are similar to each other, and well separated, where samples in\n",
      "        different clusters are not very similar to each other.\n",
      "        \n",
      "        The Silhouette Coefficient is calculated using the mean intra-cluster\n",
      "        distance (``a``) and the mean nearest-cluster distance (``b``) for each\n",
      "        sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\n",
      "        b)``.\n",
      "        Note that Silhouette Coefficient is only defined if number of labels\n",
      "        is 2 ``<= n_labels <= n_samples - 1``.\n",
      "        \n",
      "        This function returns the Silhouette Coefficient for each sample.\n",
      "        \n",
      "        The best value is 1 and the worst value is -1. Values near 0 indicate\n",
      "        overlapping clusters.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <silhouette_coefficient>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples_a, n_samples_a) if metric ==             \"precomputed\" or (n_samples_a, n_features) otherwise\n",
      "            An array of pairwise distances between samples, or a feature array.\n",
      "        \n",
      "        labels : array-like of shape (n_samples,)\n",
      "            Label values for each sample.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by :func:`sklearn.metrics.pairwise.pairwise_distances`.\n",
      "            If ``X`` is the distance array itself, use \"precomputed\" as the metric.\n",
      "            Precomputed distance matrices must have 0 along the diagonal.\n",
      "        \n",
      "        `**kwds` : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a ``scipy.spatial.distance`` metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        silhouette : array-like of shape (n_samples,)\n",
      "            Silhouette Coefficients for each sample.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the\n",
      "           Interpretation and Validation of Cluster Analysis\". Computational\n",
      "           and Applied Mathematics 20: 53-65.\n",
      "           <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Silhouette Coefficient\n",
      "           <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_\n",
      "    \n",
      "    silhouette_score(X, labels, *, metric='euclidean', sample_size=None, random_state=None, **kwds)\n",
      "        Compute the mean Silhouette Coefficient of all samples.\n",
      "        \n",
      "        The Silhouette Coefficient is calculated using the mean intra-cluster\n",
      "        distance (``a``) and the mean nearest-cluster distance (``b``) for each\n",
      "        sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a,\n",
      "        b)``.  To clarify, ``b`` is the distance between a sample and the nearest\n",
      "        cluster that the sample is not a part of.\n",
      "        Note that Silhouette Coefficient is only defined if number of labels\n",
      "        is ``2 <= n_labels <= n_samples - 1``.\n",
      "        \n",
      "        This function returns the mean Silhouette Coefficient over all samples.\n",
      "        To obtain the values for each sample, use :func:`silhouette_samples`.\n",
      "        \n",
      "        The best value is 1 and the worst value is -1. Values near 0 indicate\n",
      "        overlapping clusters. Negative values generally indicate that a sample has\n",
      "        been assigned to the wrong cluster, as a different cluster is more similar.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <silhouette_coefficient>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like of shape (n_samples_a, n_samples_a) if metric ==             \"precomputed\" or (n_samples_a, n_features) otherwise\n",
      "            An array of pairwise distances between samples, or a feature array.\n",
      "        \n",
      "        labels : array-like of shape (n_samples,)\n",
      "            Predicted labels for each sample.\n",
      "        \n",
      "        metric : str or callable, default='euclidean'\n",
      "            The metric to use when calculating distance between instances in a\n",
      "            feature array. If metric is a string, it must be one of the options\n",
      "            allowed by :func:`metrics.pairwise.pairwise_distances\n",
      "            <sklearn.metrics.pairwise.pairwise_distances>`. If ``X`` is\n",
      "            the distance array itself, use ``metric=\"precomputed\"``.\n",
      "        \n",
      "        sample_size : int, default=None\n",
      "            The size of the sample to use when computing the Silhouette Coefficient\n",
      "            on a random subset of the data.\n",
      "            If ``sample_size is None``, no sampling is used.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for selecting a subset of samples.\n",
      "            Used when ``sample_size is not None``.\n",
      "            Pass an int for reproducible results across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        **kwds : optional keyword parameters\n",
      "            Any further parameters are passed directly to the distance function.\n",
      "            If using a scipy.spatial.distance metric, the parameters are still\n",
      "            metric dependent. See the scipy docs for usage examples.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        silhouette : float\n",
      "            Mean Silhouette Coefficient for all samples.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the\n",
      "           Interpretation and Validation of Cluster Analysis\". Computational\n",
      "           and Applied Mathematics 20: 53-65.\n",
      "           <https://www.sciencedirect.com/science/article/pii/0377042787901257>`_\n",
      "        \n",
      "        .. [2] `Wikipedia entry on the Silhouette Coefficient\n",
      "               <https://en.wikipedia.org/wiki/Silhouette_(clustering)>`_\n",
      "    \n",
      "    top_k_accuracy_score(y_true, y_score, *, k=2, normalize=True, sample_weight=None, labels=None)\n",
      "        Top-k Accuracy classification score.\n",
      "        \n",
      "        This metric computes the number of times where the correct label is among\n",
      "        the top `k` labels predicted (ranked by predicted scores). Note that the\n",
      "        multilabel case isn't covered here.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <top_k_accuracy_score>`\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : array-like of shape (n_samples,)\n",
      "            True labels.\n",
      "        \n",
      "        y_score : array-like of shape (n_samples,) or (n_samples, n_classes)\n",
      "            Target scores. These can be either probability estimates or\n",
      "            non-thresholded decision values (as returned by\n",
      "            :term:`decision_function` on some classifiers). The binary case expects\n",
      "            scores with shape (n_samples,) while the multiclass case expects scores\n",
      "            with shape (n_samples, n_classes). In the multiclass case, the order of\n",
      "            the class scores must correspond to the order of ``labels``, if\n",
      "            provided, or else to the numerical or lexicographical order of the\n",
      "            labels in ``y_true``.\n",
      "        \n",
      "        k : int, default=2\n",
      "            Number of most likely outcomes considered to find the correct label.\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If `True`, return the fraction of correctly classified samples.\n",
      "            Otherwise, return the number of correctly classified samples.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights. If `None`, all samples are given the same weight.\n",
      "        \n",
      "        labels : array-like of shape (n_classes,), default=None\n",
      "            Multiclass only. List of labels that index the classes in ``y_score``.\n",
      "            If ``None``, the numerical or lexicographical order of the labels in\n",
      "            ``y_true`` is used.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        score : float\n",
      "            The top-k accuracy score. The best performance is 1 with\n",
      "            `normalize == True` and the number of samples with\n",
      "            `normalize == False`.\n",
      "        \n",
      "        See also\n",
      "        --------\n",
      "        accuracy_score\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In cases where two or more labels are assigned equal predicted scores,\n",
      "        the labels with the highest indices will be chosen first. This might\n",
      "        impact the result if the correct label falls after the threshold because\n",
      "        of that.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import numpy as np\n",
      "        >>> from sklearn.metrics import top_k_accuracy_score\n",
      "        >>> y_true = np.array([0, 1, 2, 2])\n",
      "        >>> y_score = np.array([[0.5, 0.2, 0.2],  # 0 is in top 2\n",
      "        ...                     [0.3, 0.4, 0.2],  # 1 is in top 2\n",
      "        ...                     [0.2, 0.4, 0.3],  # 2 is in top 2\n",
      "        ...                     [0.7, 0.2, 0.1]]) # 2 isn't in top 2\n",
      "        >>> top_k_accuracy_score(y_true, y_score, k=2)\n",
      "        0.75\n",
      "        >>> # Not normalizing gives the number of \"correctly\" classified samples\n",
      "        >>> top_k_accuracy_score(y_true, y_score, k=2, normalize=False)\n",
      "        3\n",
      "    \n",
      "    v_measure_score(labels_true, labels_pred, *, beta=1.0)\n",
      "        V-measure cluster labeling given a ground truth.\n",
      "        \n",
      "        This score is identical to :func:`normalized_mutual_info_score` with\n",
      "        the ``'arithmetic'`` option for averaging.\n",
      "        \n",
      "        The V-measure is the harmonic mean between homogeneity and completeness::\n",
      "        \n",
      "            v = (1 + beta) * homogeneity * completeness\n",
      "                 / (beta * homogeneity + completeness)\n",
      "        \n",
      "        This metric is independent of the absolute values of the labels:\n",
      "        a permutation of the class or cluster label values won't change the\n",
      "        score value in any way.\n",
      "        \n",
      "        This metric is furthermore symmetric: switching ``label_true`` with\n",
      "        ``label_pred`` will return the same score value. This can be useful to\n",
      "        measure the agreement of two independent label assignments strategies\n",
      "        on the same dataset when the real ground truth is not known.\n",
      "        \n",
      "        \n",
      "        Read more in the :ref:`User Guide <homogeneity_completeness>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        labels_true : int array, shape = [n_samples]\n",
      "            ground truth class labels to be used as a reference\n",
      "        \n",
      "        labels_pred : array-like of shape (n_samples,)\n",
      "            cluster labels to evaluate\n",
      "        \n",
      "        beta : float, default=1.0\n",
      "            Ratio of weight attributed to ``homogeneity`` vs ``completeness``.\n",
      "            If ``beta`` is greater than 1, ``completeness`` is weighted more\n",
      "            strongly in the calculation. If ``beta`` is less than 1,\n",
      "            ``homogeneity`` is weighted more strongly.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        v_measure : float\n",
      "           score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] `Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A\n",
      "           conditional entropy-based external cluster evaluation measure\n",
      "           <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        homogeneity_score\n",
      "        completeness_score\n",
      "        normalized_mutual_info_score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        \n",
      "        Perfect labelings are both homogeneous and complete, hence have score 1.0::\n",
      "        \n",
      "          >>> from sklearn.metrics.cluster import v_measure_score\n",
      "          >>> v_measure_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
      "          1.0\n",
      "          >>> v_measure_score([0, 0, 1, 1], [1, 1, 0, 0])\n",
      "          1.0\n",
      "        \n",
      "        Labelings that assign all classes members to the same clusters\n",
      "        are complete be not homogeneous, hence penalized::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))\n",
      "          0.8...\n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))\n",
      "          0.66...\n",
      "        \n",
      "        Labelings that have pure clusters with members coming from the same\n",
      "        classes are homogeneous but un-necessary splits harms completeness\n",
      "        and thus penalize V-measure as well::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))\n",
      "          0.8...\n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))\n",
      "          0.66...\n",
      "        \n",
      "        If classes members are completely split across different clusters,\n",
      "        the assignment is totally incomplete, hence the V-Measure is null::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))\n",
      "          0.0...\n",
      "        \n",
      "        Clusters that include samples from totally different classes totally\n",
      "        destroy the homogeneity of the labeling, hence::\n",
      "        \n",
      "          >>> print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))\n",
      "          0.0...\n",
      "    \n",
      "    zero_one_loss(y_true, y_pred, *, normalize=True, sample_weight=None)\n",
      "        Zero-one classification loss.\n",
      "        \n",
      "        If normalize is ``True``, return the fraction of misclassifications\n",
      "        (float), else it returns the number of misclassifications (int). The best\n",
      "        performance is 0.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <zero_one_loss>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "            Ground truth (correct) labels.\n",
      "        \n",
      "        y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "            Predicted labels, as returned by a classifier.\n",
      "        \n",
      "        normalize : bool, default=True\n",
      "            If ``False``, return the number of misclassifications.\n",
      "            Otherwise, return the fraction of misclassifications.\n",
      "        \n",
      "        sample_weight : array-like of shape (n_samples,), default=None\n",
      "            Sample weights.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        loss : float or int,\n",
      "            If ``normalize == True``, return the fraction of misclassifications\n",
      "            (float), else it returns the number of misclassifications (int).\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        In multilabel classification, the zero_one_loss function corresponds to\n",
      "        the subset zero-one loss: for each sample, the entire set of labels must be\n",
      "        correctly predicted, otherwise the loss for that sample is equal to one.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        accuracy_score, hamming_loss, jaccard_score\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from sklearn.metrics import zero_one_loss\n",
      "        >>> y_pred = [1, 2, 3, 4]\n",
      "        >>> y_true = [2, 2, 3, 4]\n",
      "        >>> zero_one_loss(y_true, y_pred)\n",
      "        0.25\n",
      "        >>> zero_one_loss(y_true, y_pred, normalize=False)\n",
      "        1\n",
      "        \n",
      "        In the multilabel case with binary label indicators:\n",
      "        \n",
      "        >>> import numpy as np\n",
      "        >>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
      "        0.5\n",
      "\n",
      "DATA\n",
      "    SCORERS = {'accuracy': make_scorer(accuracy_score), 'adjusted_mutual_i...\n",
      "    __all__ = ['accuracy_score', 'adjusted_mutual_info_score', 'adjusted_r...\n",
      "\n",
      "FILE\n",
      "    /opt/anaconda3/envs/ksunsupervised/lib/python3.9/site-packages/sklearn/metrics/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90420b7d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'explained_variance': make_scorer(explained_variance_score),\n",
       " 'r2': make_scorer(r2_score),\n",
       " 'max_error': make_scorer(max_error, greater_is_better=False),\n",
       " 'neg_median_absolute_error': make_scorer(median_absolute_error, greater_is_better=False),\n",
       " 'neg_mean_absolute_error': make_scorer(mean_absolute_error, greater_is_better=False),\n",
       " 'neg_mean_absolute_percentage_error': make_scorer(mean_absolute_percentage_error, greater_is_better=False),\n",
       " 'neg_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False),\n",
       " 'neg_mean_squared_log_error': make_scorer(mean_squared_log_error, greater_is_better=False),\n",
       " 'neg_root_mean_squared_error': make_scorer(mean_squared_error, greater_is_better=False, squared=False),\n",
       " 'neg_mean_poisson_deviance': make_scorer(mean_poisson_deviance, greater_is_better=False),\n",
       " 'neg_mean_gamma_deviance': make_scorer(mean_gamma_deviance, greater_is_better=False),\n",
       " 'accuracy': make_scorer(accuracy_score),\n",
       " 'top_k_accuracy': make_scorer(top_k_accuracy_score, needs_threshold=True),\n",
       " 'roc_auc': make_scorer(roc_auc_score, needs_threshold=True),\n",
       " 'roc_auc_ovr': make_scorer(roc_auc_score, needs_proba=True, multi_class=ovr),\n",
       " 'roc_auc_ovo': make_scorer(roc_auc_score, needs_proba=True, multi_class=ovo),\n",
       " 'roc_auc_ovr_weighted': make_scorer(roc_auc_score, needs_proba=True, multi_class=ovr, average=weighted),\n",
       " 'roc_auc_ovo_weighted': make_scorer(roc_auc_score, needs_proba=True, multi_class=ovo, average=weighted),\n",
       " 'balanced_accuracy': make_scorer(balanced_accuracy_score),\n",
       " 'average_precision': make_scorer(average_precision_score, needs_threshold=True),\n",
       " 'neg_log_loss': make_scorer(log_loss, greater_is_better=False, needs_proba=True),\n",
       " 'neg_brier_score': make_scorer(brier_score_loss, greater_is_better=False, needs_proba=True),\n",
       " 'adjusted_rand_score': make_scorer(adjusted_rand_score),\n",
       " 'rand_score': make_scorer(rand_score),\n",
       " 'homogeneity_score': make_scorer(homogeneity_score),\n",
       " 'completeness_score': make_scorer(completeness_score),\n",
       " 'v_measure_score': make_scorer(v_measure_score),\n",
       " 'mutual_info_score': make_scorer(mutual_info_score),\n",
       " 'adjusted_mutual_info_score': make_scorer(adjusted_mutual_info_score),\n",
       " 'normalized_mutual_info_score': make_scorer(normalized_mutual_info_score),\n",
       " 'fowlkes_mallows_score': make_scorer(fowlkes_mallows_score),\n",
       " 'precision': make_scorer(precision_score, average=binary),\n",
       " 'precision_macro': make_scorer(precision_score, pos_label=None, average=macro),\n",
       " 'precision_micro': make_scorer(precision_score, pos_label=None, average=micro),\n",
       " 'precision_samples': make_scorer(precision_score, pos_label=None, average=samples),\n",
       " 'precision_weighted': make_scorer(precision_score, pos_label=None, average=weighted),\n",
       " 'recall': make_scorer(recall_score, average=binary),\n",
       " 'recall_macro': make_scorer(recall_score, pos_label=None, average=macro),\n",
       " 'recall_micro': make_scorer(recall_score, pos_label=None, average=micro),\n",
       " 'recall_samples': make_scorer(recall_score, pos_label=None, average=samples),\n",
       " 'recall_weighted': make_scorer(recall_score, pos_label=None, average=weighted),\n",
       " 'f1': make_scorer(f1_score, average=binary),\n",
       " 'f1_macro': make_scorer(f1_score, pos_label=None, average=macro),\n",
       " 'f1_micro': make_scorer(f1_score, pos_label=None, average=micro),\n",
       " 'f1_samples': make_scorer(f1_score, pos_label=None, average=samples),\n",
       " 'f1_weighted': make_scorer(f1_score, pos_label=None, average=weighted),\n",
       " 'jaccard': make_scorer(jaccard_score, average=binary),\n",
       " 'jaccard_macro': make_scorer(jaccard_score, pos_label=None, average=macro),\n",
       " 'jaccard_micro': make_scorer(jaccard_score, pos_label=None, average=micro),\n",
       " 'jaccard_samples': make_scorer(jaccard_score, pos_label=None, average=samples),\n",
       " 'jaccard_weighted': make_scorer(jaccard_score, pos_label=None, average=weighted)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.SCORERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f22b7cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fead7429",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.feature_selection in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.feature_selection\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.feature_selection` module implements feature selection\n",
      "    algorithms. It currently includes univariate filter selection methods and the\n",
      "    recursive feature elimination algorithm.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _base\n",
      "    _from_model\n",
      "    _mutual_info\n",
      "    _rfe\n",
      "    _sequential\n",
      "    _univariate_selection\n",
      "    _variance_threshold\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    sklearn.base.MetaEstimatorMixin(builtins.object)\n",
      "        sklearn.feature_selection._from_model.SelectFromModel(sklearn.base.MetaEstimatorMixin, sklearn.feature_selection._base.SelectorMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.TransformerMixin(builtins.object)\n",
      "        sklearn.feature_selection._base.SelectorMixin\n",
      "            sklearn.feature_selection._from_model.SelectFromModel(sklearn.base.MetaEstimatorMixin, sklearn.feature_selection._base.SelectorMixin, sklearn.base.BaseEstimator)\n",
      "            sklearn.feature_selection._rfe.RFE(sklearn.feature_selection._base.SelectorMixin, sklearn.base.MetaEstimatorMixin, sklearn.base.BaseEstimator)\n",
      "                sklearn.feature_selection._rfe.RFECV\n",
      "            sklearn.feature_selection._sequential.SequentialFeatureSelector(sklearn.feature_selection._base.SelectorMixin, sklearn.base.MetaEstimatorMixin, sklearn.base.BaseEstimator)\n",
      "            sklearn.feature_selection._variance_threshold.VarianceThreshold(sklearn.feature_selection._base.SelectorMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.feature_selection._univariate_selection._BaseFilter(sklearn.feature_selection._base.SelectorMixin, sklearn.base.BaseEstimator)\n",
      "        sklearn.feature_selection._univariate_selection.GenericUnivariateSelect\n",
      "        sklearn.feature_selection._univariate_selection.SelectFdr\n",
      "        sklearn.feature_selection._univariate_selection.SelectFpr\n",
      "        sklearn.feature_selection._univariate_selection.SelectFwe\n",
      "        sklearn.feature_selection._univariate_selection.SelectKBest\n",
      "        sklearn.feature_selection._univariate_selection.SelectPercentile\n",
      "    \n",
      "    class GenericUnivariateSelect(_BaseFilter)\n",
      "     |  GenericUnivariateSelect(score_func=<function f_classif at 0x1170c3820>, *, mode='percentile', param=1e-05)\n",
      "     |  \n",
      "     |  Univariate feature selector with configurable strategy.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <univariate_feature_selection>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  score_func : callable, default=f_classif\n",
      "     |      Function taking two arrays X and y, and returning a pair of arrays\n",
      "     |      (scores, pvalues). For modes 'percentile' or 'kbest' it can return\n",
      "     |      a single array scores.\n",
      "     |  \n",
      "     |  mode : {'percentile', 'k_best', 'fpr', 'fdr', 'fwe'}, default='percentile'\n",
      "     |      Feature selection mode.\n",
      "     |  \n",
      "     |  param : float or int depending on the feature selection mode, default=1e-5\n",
      "     |      Parameter of the corresponding mode.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  scores_ : array-like of shape (n_features,)\n",
      "     |      Scores of features.\n",
      "     |  \n",
      "     |  pvalues_ : array-like of shape (n_features,)\n",
      "     |      p-values of feature scores, None if `score_func` returned scores only.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_breast_cancer\n",
      "     |  >>> from sklearn.feature_selection import GenericUnivariateSelect, chi2\n",
      "     |  >>> X, y = load_breast_cancer(return_X_y=True)\n",
      "     |  >>> X.shape\n",
      "     |  (569, 30)\n",
      "     |  >>> transformer = GenericUnivariateSelect(chi2, mode='k_best', param=20)\n",
      "     |  >>> X_new = transformer.fit_transform(X, y)\n",
      "     |  >>> X_new.shape\n",
      "     |  (569, 20)\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  f_classif : ANOVA F-value between label/feature for classification tasks.\n",
      "     |  mutual_info_classif : Mutual information for a discrete target.\n",
      "     |  chi2 : Chi-squared stats of non-negative features for classification tasks.\n",
      "     |  f_regression : F-value between label/feature for regression tasks.\n",
      "     |  mutual_info_regression : Mutual information for a continuous target.\n",
      "     |  SelectPercentile : Select features based on percentile of the highest\n",
      "     |      scores.\n",
      "     |  SelectKBest : Select features based on the k highest scores.\n",
      "     |  SelectFpr : Select features based on a false positive rate test.\n",
      "     |  SelectFdr : Select features based on an estimated false discovery rate.\n",
      "     |  SelectFwe : Select features based on family-wise error rate.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GenericUnivariateSelect\n",
      "     |      _BaseFilter\n",
      "     |      sklearn.feature_selection._base.SelectorMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, score_func=<function f_classif at 0x1170c3820>, *, mode='percentile', param=1e-05)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseFilter:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Run score function on (X, y) and get the appropriate features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The training input samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.feature_selection._base.SelectorMixin:\n",
      "     |  \n",
      "     |  get_support(self, indices=False)\n",
      "     |      Get a mask, or integer index, of the features selected\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      indices : bool, default=False\n",
      "     |          If True, the return value will be an array of integers, rather\n",
      "     |          than a boolean mask.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      support : array\n",
      "     |          An index that selects the retained features from a feature vector.\n",
      "     |          If `indices` is False, this is a boolean array of shape\n",
      "     |          [# input features], in which an element is True iff its\n",
      "     |          corresponding feature is selected for retention. If `indices` is\n",
      "     |          True, this is an integer array of shape [# output features] whose\n",
      "     |          values are indices into the input feature vector.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Reverse the transformation operation\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_original_features]\n",
      "     |          `X` with columns of zeros inserted where features would have\n",
      "     |          been removed by :meth:`transform`.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Reduce X to the selected features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples with only the selected features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RFE(sklearn.feature_selection._base.SelectorMixin, sklearn.base.MetaEstimatorMixin, sklearn.base.BaseEstimator)\n",
      "     |  RFE(estimator, *, n_features_to_select=None, step=1, verbose=0, importance_getter='auto')\n",
      "     |  \n",
      "     |  Feature ranking with recursive feature elimination.\n",
      "     |  \n",
      "     |  Given an external estimator that assigns weights to features (e.g., the\n",
      "     |  coefficients of a linear model), the goal of recursive feature elimination\n",
      "     |  (RFE) is to select features by recursively considering smaller and smaller\n",
      "     |  sets of features. First, the estimator is trained on the initial set of\n",
      "     |  features and the importance of each feature is obtained either through\n",
      "     |  any specific attribute or callable.\n",
      "     |  Then, the least important features are pruned from current set of features.\n",
      "     |  That procedure is recursively repeated on the pruned set until the desired\n",
      "     |  number of features to select is eventually reached.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <rfe>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimator : ``Estimator`` instance\n",
      "     |      A supervised learning estimator with a ``fit`` method that provides\n",
      "     |      information about feature importance\n",
      "     |      (e.g. `coef_`, `feature_importances_`).\n",
      "     |  \n",
      "     |  n_features_to_select : int or float, default=None\n",
      "     |      The number of features to select. If `None`, half of the features are\n",
      "     |      selected. If integer, the parameter is the absolute number of features\n",
      "     |      to select. If float between 0 and 1, it is the fraction of features to\n",
      "     |      select.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.24\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  step : int or float, default=1\n",
      "     |      If greater than or equal to 1, then ``step`` corresponds to the\n",
      "     |      (integer) number of features to remove at each iteration.\n",
      "     |      If within (0.0, 1.0), then ``step`` corresponds to the percentage\n",
      "     |      (rounded down) of features to remove at each iteration.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Controls verbosity of output.\n",
      "     |  \n",
      "     |  importance_getter : str or callable, default='auto'\n",
      "     |      If 'auto', uses the feature importance either through a `coef_`\n",
      "     |      or `feature_importances_` attributes of estimator.\n",
      "     |  \n",
      "     |      Also accepts a string that specifies an attribute name/path\n",
      "     |      for extracting feature importance (implemented with `attrgetter`).\n",
      "     |      For example, give `regressor_.coef_` in case of\n",
      "     |      :class:`~sklearn.compose.TransformedTargetRegressor`  or\n",
      "     |      `named_steps.clf.feature_importances_` in case of\n",
      "     |      class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n",
      "     |  \n",
      "     |      If `callable`, overrides the default feature importance getter.\n",
      "     |      The callable is passed with the fitted estimator and it should\n",
      "     |      return importance for each feature.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimator_ : ``Estimator`` instance\n",
      "     |      The fitted estimator used to select features.\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of selected features.\n",
      "     |  \n",
      "     |  ranking_ : ndarray of shape (n_features,)\n",
      "     |      The feature ranking, such that ``ranking_[i]`` corresponds to the\n",
      "     |      ranking position of the i-th feature. Selected (i.e., estimated\n",
      "     |      best) features are assigned rank 1.\n",
      "     |  \n",
      "     |  support_ : ndarray of shape (n_features,)\n",
      "     |      The mask of selected features.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  The following example shows how to retrieve the 5 most informative\n",
      "     |  features in the Friedman #1 dataset.\n",
      "     |  \n",
      "     |  >>> from sklearn.datasets import make_friedman1\n",
      "     |  >>> from sklearn.feature_selection import RFE\n",
      "     |  >>> from sklearn.svm import SVR\n",
      "     |  >>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n",
      "     |  >>> estimator = SVR(kernel=\"linear\")\n",
      "     |  >>> selector = RFE(estimator, n_features_to_select=5, step=1)\n",
      "     |  >>> selector = selector.fit(X, y)\n",
      "     |  >>> selector.support_\n",
      "     |  array([ True,  True,  True,  True,  True, False, False, False, False,\n",
      "     |         False])\n",
      "     |  >>> selector.ranking_\n",
      "     |  array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Allows NaN/Inf in the input if the underlying estimator does as well.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  RFECV : Recursive feature elimination with built-in cross-validated\n",
      "     |      selection of the best number of features.\n",
      "     |  SelectFromModel : Feature selection based on thresholds of importance\n",
      "     |      weights.\n",
      "     |  SequentialFeatureSelector : Sequential cross-validation based feature\n",
      "     |      selection. Does not rely on importance weights.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n",
      "     |         for cancer classification using support vector machines\",\n",
      "     |         Mach. Learn., 46(1-3), 389--422, 2002.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RFE\n",
      "     |      sklearn.feature_selection._base.SelectorMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimator, *, n_features_to_select=None, step=1, verbose=0, importance_getter='auto')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Compute the decision function of ``X``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like or sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : array, shape = [n_samples, n_classes] or [n_samples]\n",
      "     |          The decision function of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |          Regression and binary classification produce an array of shape\n",
      "     |          [n_samples].\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Fit the RFE model and then the underlying estimator on the selected\n",
      "     |         features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target values.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Reduce X to the selected features and then predict using the\n",
      "     |         underlying estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape [n_samples]\n",
      "     |          The predicted target values.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape (n_samples, n_classes)\n",
      "     |          The class log-probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like or sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape (n_samples, n_classes)\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  score(self, X, y)\n",
      "     |      Reduce X to the selected features and then return the score of the\n",
      "     |         underlying estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      y : array of shape [n_samples]\n",
      "     |          The target values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  classes_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.feature_selection._base.SelectorMixin:\n",
      "     |  \n",
      "     |  get_support(self, indices=False)\n",
      "     |      Get a mask, or integer index, of the features selected\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      indices : bool, default=False\n",
      "     |          If True, the return value will be an array of integers, rather\n",
      "     |          than a boolean mask.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      support : array\n",
      "     |          An index that selects the retained features from a feature vector.\n",
      "     |          If `indices` is False, this is a boolean array of shape\n",
      "     |          [# input features], in which an element is True iff its\n",
      "     |          corresponding feature is selected for retention. If `indices` is\n",
      "     |          True, this is an integer array of shape [# output features] whose\n",
      "     |          values are indices into the input feature vector.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Reverse the transformation operation\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_original_features]\n",
      "     |          `X` with columns of zeros inserted where features would have\n",
      "     |          been removed by :meth:`transform`.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Reduce X to the selected features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples with only the selected features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RFECV(RFE)\n",
      "     |  RFECV(estimator, *, step=1, min_features_to_select=1, cv=None, scoring=None, verbose=0, n_jobs=None, importance_getter='auto')\n",
      "     |  \n",
      "     |  Feature ranking with recursive feature elimination and cross-validated\n",
      "     |  selection of the best number of features.\n",
      "     |  \n",
      "     |  See glossary entry for :term:`cross-validation estimator`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <rfe>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimator : ``Estimator`` instance\n",
      "     |      A supervised learning estimator with a ``fit`` method that provides\n",
      "     |      information about feature importance either through a ``coef_``\n",
      "     |      attribute or through a ``feature_importances_`` attribute.\n",
      "     |  \n",
      "     |  step : int or float, default=1\n",
      "     |      If greater than or equal to 1, then ``step`` corresponds to the\n",
      "     |      (integer) number of features to remove at each iteration.\n",
      "     |      If within (0.0, 1.0), then ``step`` corresponds to the percentage\n",
      "     |      (rounded down) of features to remove at each iteration.\n",
      "     |      Note that the last iteration may remove fewer than ``step`` features in\n",
      "     |      order to reach ``min_features_to_select``.\n",
      "     |  \n",
      "     |  min_features_to_select : int, default=1\n",
      "     |      The minimum number of features to be selected. This number of features\n",
      "     |      will always be scored, even if the difference between the original\n",
      "     |      feature count and ``min_features_to_select`` isn't divisible by\n",
      "     |      ``step``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross-validation,\n",
      "     |      - integer, to specify the number of folds.\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For integer/None inputs, if ``y`` is binary or multiclass,\n",
      "     |      :class:`~sklearn.model_selection.StratifiedKFold` is used. If the\n",
      "     |      estimator is a classifier or if ``y`` is neither binary nor multiclass,\n",
      "     |      :class:`~sklearn.model_selection.KFold` is used.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |          ``cv`` default value of None changed from 3-fold to 5-fold.\n",
      "     |  \n",
      "     |  scoring : string, callable or None, default=None\n",
      "     |      A string (see model evaluation documentation) or\n",
      "     |      a scorer callable object / function with signature\n",
      "     |      ``scorer(estimator, X, y)``.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Controls verbosity of output.\n",
      "     |  \n",
      "     |  n_jobs : int or None, default=None\n",
      "     |      Number of cores to run in parallel while fitting across folds.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  importance_getter : str or callable, default='auto'\n",
      "     |      If 'auto', uses the feature importance either through a `coef_`\n",
      "     |      or `feature_importances_` attributes of estimator.\n",
      "     |  \n",
      "     |      Also accepts a string that specifies an attribute name/path\n",
      "     |      for extracting feature importance.\n",
      "     |      For example, give `regressor_.coef_` in case of\n",
      "     |      :class:`~sklearn.compose.TransformedTargetRegressor`  or\n",
      "     |      `named_steps.clf.feature_importances_` in case of\n",
      "     |      :class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n",
      "     |  \n",
      "     |      If `callable`, overrides the default feature importance getter.\n",
      "     |      The callable is passed with the fitted estimator and it should\n",
      "     |      return importance for each feature.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimator_ : ``Estimator`` instance\n",
      "     |      The fitted estimator used to select features.\n",
      "     |  \n",
      "     |  grid_scores_ : ndarray of shape (n_subsets_of_features,)\n",
      "     |      The cross-validation scores such that\n",
      "     |      ``grid_scores_[i]`` corresponds to\n",
      "     |      the CV score of the i-th subset of features.\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of selected features with cross-validation.\n",
      "     |  \n",
      "     |  ranking_ : narray of shape (n_features,)\n",
      "     |      The feature ranking, such that `ranking_[i]`\n",
      "     |      corresponds to the ranking\n",
      "     |      position of the i-th feature.\n",
      "     |      Selected (i.e., estimated best)\n",
      "     |      features are assigned rank 1.\n",
      "     |  \n",
      "     |  support_ : ndarray of shape (n_features,)\n",
      "     |      The mask of selected features.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The size of ``grid_scores_`` is equal to\n",
      "     |  ``ceil((n_features - min_features_to_select) / step) + 1``,\n",
      "     |  where step is the number of features removed at each iteration.\n",
      "     |  \n",
      "     |  Allows NaN/Inf in the input if the underlying estimator does as well.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  The following example shows how to retrieve the a-priori not known 5\n",
      "     |  informative features in the Friedman #1 dataset.\n",
      "     |  \n",
      "     |  >>> from sklearn.datasets import make_friedman1\n",
      "     |  >>> from sklearn.feature_selection import RFECV\n",
      "     |  >>> from sklearn.svm import SVR\n",
      "     |  >>> X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)\n",
      "     |  >>> estimator = SVR(kernel=\"linear\")\n",
      "     |  >>> selector = RFECV(estimator, step=1, cv=5)\n",
      "     |  >>> selector = selector.fit(X, y)\n",
      "     |  >>> selector.support_\n",
      "     |  array([ True,  True,  True,  True,  True, False, False, False, False,\n",
      "     |         False])\n",
      "     |  >>> selector.ranking_\n",
      "     |  array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  RFE : Recursive feature elimination.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., \"Gene selection\n",
      "     |         for cancer classification using support vector machines\",\n",
      "     |         Mach. Learn., 46(1-3), 389--422, 2002.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RFECV\n",
      "     |      RFE\n",
      "     |      sklearn.feature_selection._base.SelectorMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimator, *, step=1, min_features_to_select=1, cv=None, scoring=None, verbose=0, n_jobs=None, importance_getter='auto')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, groups=None)\n",
      "     |      Fit the RFE model and automatically tune the number of selected\n",
      "     |         features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vector, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the total number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values (integers for classification, real numbers for\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      groups : array-like of shape (n_samples,) or None, default=None\n",
      "     |          Group labels for the samples used while splitting the dataset into\n",
      "     |          train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "     |          instance (e.g., :class:`~sklearn.model_selection.GroupKFold`).\n",
      "     |      \n",
      "     |          .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from RFE:\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Compute the decision function of ``X``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like or sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : array, shape = [n_samples, n_classes] or [n_samples]\n",
      "     |          The decision function of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |          Regression and binary classification produce an array of shape\n",
      "     |          [n_samples].\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Reduce X to the selected features and then predict using the\n",
      "     |         underlying estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : array of shape [n_samples]\n",
      "     |          The predicted target values.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape (n_samples, n_classes)\n",
      "     |          The class log-probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like or sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : array of shape (n_samples, n_classes)\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  score(self, X, y)\n",
      "     |      Reduce X to the selected features and then return the score of the\n",
      "     |         underlying estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      y : array of shape [n_samples]\n",
      "     |          The target values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from RFE:\n",
      "     |  \n",
      "     |  classes_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.feature_selection._base.SelectorMixin:\n",
      "     |  \n",
      "     |  get_support(self, indices=False)\n",
      "     |      Get a mask, or integer index, of the features selected\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      indices : bool, default=False\n",
      "     |          If True, the return value will be an array of integers, rather\n",
      "     |          than a boolean mask.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      support : array\n",
      "     |          An index that selects the retained features from a feature vector.\n",
      "     |          If `indices` is False, this is a boolean array of shape\n",
      "     |          [# input features], in which an element is True iff its\n",
      "     |          corresponding feature is selected for retention. If `indices` is\n",
      "     |          True, this is an integer array of shape [# output features] whose\n",
      "     |          values are indices into the input feature vector.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Reverse the transformation operation\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_original_features]\n",
      "     |          `X` with columns of zeros inserted where features would have\n",
      "     |          been removed by :meth:`transform`.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Reduce X to the selected features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples with only the selected features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class SelectFdr(_BaseFilter)\n",
      "     |  SelectFdr(score_func=<function f_classif at 0x1170c3820>, *, alpha=0.05)\n",
      "     |  \n",
      "     |  Filter: Select the p-values for an estimated false discovery rate\n",
      "     |  \n",
      "     |  This uses the Benjamini-Hochberg procedure. ``alpha`` is an upper bound\n",
      "     |  on the expected false discovery rate.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <univariate_feature_selection>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  score_func : callable, default=f_classif\n",
      "     |      Function taking two arrays X and y, and returning a pair of arrays\n",
      "     |      (scores, pvalues).\n",
      "     |      Default is f_classif (see below \"See Also\"). The default function only\n",
      "     |      works with classification tasks.\n",
      "     |  \n",
      "     |  alpha : float, default=5e-2\n",
      "     |      The highest uncorrected p-value for features to keep.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_breast_cancer\n",
      "     |  >>> from sklearn.feature_selection import SelectFdr, chi2\n",
      "     |  >>> X, y = load_breast_cancer(return_X_y=True)\n",
      "     |  >>> X.shape\n",
      "     |  (569, 30)\n",
      "     |  >>> X_new = SelectFdr(chi2, alpha=0.01).fit_transform(X, y)\n",
      "     |  >>> X_new.shape\n",
      "     |  (569, 16)\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  scores_ : array-like of shape (n_features,)\n",
      "     |      Scores of features.\n",
      "     |  \n",
      "     |  pvalues_ : array-like of shape (n_features,)\n",
      "     |      p-values of feature scores.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  https://en.wikipedia.org/wiki/False_discovery_rate\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  f_classif : ANOVA F-value between label/feature for classification tasks.\n",
      "     |  mutual_info_classif : Mutual information for a discrete target.\n",
      "     |  chi2 : Chi-squared stats of non-negative features for classification tasks.\n",
      "     |  f_regression : F-value between label/feature for regression tasks.\n",
      "     |  mutual_info_regression : Mutual information for a contnuous target.\n",
      "     |  SelectPercentile : Select features based on percentile of the highest\n",
      "     |      scores.\n",
      "     |  SelectKBest : Select features based on the k highest scores.\n",
      "     |  SelectFpr : Select features based on a false positive rate test.\n",
      "     |  SelectFwe : Select features based on family-wise error rate.\n",
      "     |  GenericUnivariateSelect : Univariate feature selector with configurable\n",
      "     |      mode.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SelectFdr\n",
      "     |      _BaseFilter\n",
      "     |      sklearn.feature_selection._base.SelectorMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, score_func=<function f_classif at 0x1170c3820>, *, alpha=0.05)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseFilter:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Run score function on (X, y) and get the appropriate features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The training input samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.feature_selection._base.SelectorMixin:\n",
      "     |  \n",
      "     |  get_support(self, indices=False)\n",
      "     |      Get a mask, or integer index, of the features selected\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      indices : bool, default=False\n",
      "     |          If True, the return value will be an array of integers, rather\n",
      "     |          than a boolean mask.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      support : array\n",
      "     |          An index that selects the retained features from a feature vector.\n",
      "     |          If `indices` is False, this is a boolean array of shape\n",
      "     |          [# input features], in which an element is True iff its\n",
      "     |          corresponding feature is selected for retention. If `indices` is\n",
      "     |          True, this is an integer array of shape [# output features] whose\n",
      "     |          values are indices into the input feature vector.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Reverse the transformation operation\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_original_features]\n",
      "     |          `X` with columns of zeros inserted where features would have\n",
      "     |          been removed by :meth:`transform`.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Reduce X to the selected features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples with only the selected features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class SelectFpr(_BaseFilter)\n",
      "     |  SelectFpr(score_func=<function f_classif at 0x1170c3820>, *, alpha=0.05)\n",
      "     |  \n",
      "     |  Filter: Select the pvalues below alpha based on a FPR test.\n",
      "     |  \n",
      "     |  FPR test stands for False Positive Rate test. It controls the total\n",
      "     |  amount of false detections.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <univariate_feature_selection>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  score_func : callable, default=f_classif\n",
      "     |      Function taking two arrays X and y, and returning a pair of arrays\n",
      "     |      (scores, pvalues).\n",
      "     |      Default is f_classif (see below \"See Also\"). The default function only\n",
      "     |      works with classification tasks.\n",
      "     |  \n",
      "     |  alpha : float, default=5e-2\n",
      "     |      The highest p-value for features to be kept.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  scores_ : array-like of shape (n_features,)\n",
      "     |      Scores of features.\n",
      "     |  \n",
      "     |  pvalues_ : array-like of shape (n_features,)\n",
      "     |      p-values of feature scores.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_breast_cancer\n",
      "     |  >>> from sklearn.feature_selection import SelectFpr, chi2\n",
      "     |  >>> X, y = load_breast_cancer(return_X_y=True)\n",
      "     |  >>> X.shape\n",
      "     |  (569, 30)\n",
      "     |  >>> X_new = SelectFpr(chi2, alpha=0.01).fit_transform(X, y)\n",
      "     |  >>> X_new.shape\n",
      "     |  (569, 16)\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  f_classif : ANOVA F-value between label/feature for classification tasks.\n",
      "     |  chi2 : Chi-squared stats of non-negative features for classification tasks.\n",
      "     |  mutual_info_classif: Mutual information for a discrete target.\n",
      "     |  f_regression : F-value between label/feature for regression tasks.\n",
      "     |  mutual_info_regression : Mutual information for a continuous target.\n",
      "     |  SelectPercentile : Select features based on percentile of the highest\n",
      "     |      scores.\n",
      "     |  SelectKBest : Select features based on the k highest scores.\n",
      "     |  SelectFdr : Select features based on an estimated false discovery rate.\n",
      "     |  SelectFwe : Select features based on family-wise error rate.\n",
      "     |  GenericUnivariateSelect : Univariate feature selector with configurable\n",
      "     |      mode.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SelectFpr\n",
      "     |      _BaseFilter\n",
      "     |      sklearn.feature_selection._base.SelectorMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, score_func=<function f_classif at 0x1170c3820>, *, alpha=0.05)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseFilter:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Run score function on (X, y) and get the appropriate features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The training input samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.feature_selection._base.SelectorMixin:\n",
      "     |  \n",
      "     |  get_support(self, indices=False)\n",
      "     |      Get a mask, or integer index, of the features selected\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      indices : bool, default=False\n",
      "     |          If True, the return value will be an array of integers, rather\n",
      "     |          than a boolean mask.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      support : array\n",
      "     |          An index that selects the retained features from a feature vector.\n",
      "     |          If `indices` is False, this is a boolean array of shape\n",
      "     |          [# input features], in which an element is True iff its\n",
      "     |          corresponding feature is selected for retention. If `indices` is\n",
      "     |          True, this is an integer array of shape [# output features] whose\n",
      "     |          values are indices into the input feature vector.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Reverse the transformation operation\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_original_features]\n",
      "     |          `X` with columns of zeros inserted where features would have\n",
      "     |          been removed by :meth:`transform`.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Reduce X to the selected features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples with only the selected features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class SelectFromModel(sklearn.base.MetaEstimatorMixin, sklearn.feature_selection._base.SelectorMixin, sklearn.base.BaseEstimator)\n",
      "     |  SelectFromModel(estimator, *, threshold=None, prefit=False, norm_order=1, max_features=None, importance_getter='auto')\n",
      "     |  \n",
      "     |  Meta-transformer for selecting features based on importance weights.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.17\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <select_from_model>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimator : object\n",
      "     |      The base estimator from which the transformer is built.\n",
      "     |      This can be both a fitted (if ``prefit`` is set to True)\n",
      "     |      or a non-fitted estimator. The estimator should have a\n",
      "     |      ``feature_importances_`` or ``coef_`` attribute after fitting.\n",
      "     |      Otherwise, the ``importance_getter`` parameter should be used.\n",
      "     |  \n",
      "     |  threshold : string or float, default=None\n",
      "     |      The threshold value to use for feature selection. Features whose\n",
      "     |      importance is greater or equal are kept while the others are\n",
      "     |      discarded. If \"median\" (resp. \"mean\"), then the ``threshold`` value is\n",
      "     |      the median (resp. the mean) of the feature importances. A scaling\n",
      "     |      factor (e.g., \"1.25*mean\") may also be used. If None and if the\n",
      "     |      estimator has a parameter penalty set to l1, either explicitly\n",
      "     |      or implicitly (e.g, Lasso), the threshold used is 1e-5.\n",
      "     |      Otherwise, \"mean\" is used by default.\n",
      "     |  \n",
      "     |  prefit : bool, default=False\n",
      "     |      Whether a prefit model is expected to be passed into the constructor\n",
      "     |      directly or not. If True, ``transform`` must be called directly\n",
      "     |      and SelectFromModel cannot be used with ``cross_val_score``,\n",
      "     |      ``GridSearchCV`` and similar utilities that clone the estimator.\n",
      "     |      Otherwise train the model using ``fit`` and then ``transform`` to do\n",
      "     |      feature selection.\n",
      "     |  \n",
      "     |  norm_order : non-zero int, inf, -inf, default=1\n",
      "     |      Order of the norm used to filter the vectors of coefficients below\n",
      "     |      ``threshold`` in the case where the ``coef_`` attribute of the\n",
      "     |      estimator is of dimension 2.\n",
      "     |  \n",
      "     |  max_features : int, default=None\n",
      "     |      The maximum number of features to select.\n",
      "     |      To only select based on ``max_features``, set ``threshold=-np.inf``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  importance_getter : str or callable, default='auto'\n",
      "     |      If 'auto', uses the feature importance either through a ``coef_``\n",
      "     |      attribute or ``feature_importances_`` attribute of estimator.\n",
      "     |  \n",
      "     |      Also accepts a string that specifies an attribute name/path\n",
      "     |      for extracting feature importance (implemented with `attrgetter`).\n",
      "     |      For example, give `regressor_.coef_` in case of\n",
      "     |      :class:`~sklearn.compose.TransformedTargetRegressor`  or\n",
      "     |      `named_steps.clf.feature_importances_` in case of\n",
      "     |      :class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.\n",
      "     |  \n",
      "     |      If `callable`, overrides the default feature importance getter.\n",
      "     |      The callable is passed with the fitted estimator and it should\n",
      "     |      return importance for each feature.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimator_ : an estimator\n",
      "     |      The base estimator from which the transformer is built.\n",
      "     |      This is stored only when a non-fitted estimator is passed to the\n",
      "     |      ``SelectFromModel``, i.e when prefit is False.\n",
      "     |  \n",
      "     |  threshold_ : float\n",
      "     |      The threshold value used for feature selection.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Allows NaN/Inf in the input if the underlying estimator does as well.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.feature_selection import SelectFromModel\n",
      "     |  >>> from sklearn.linear_model import LogisticRegression\n",
      "     |  >>> X = [[ 0.87, -1.34,  0.31 ],\n",
      "     |  ...      [-2.79, -0.02, -0.85 ],\n",
      "     |  ...      [-1.34, -0.48, -2.55 ],\n",
      "     |  ...      [ 1.92,  1.48,  0.65 ]]\n",
      "     |  >>> y = [0, 1, 0, 1]\n",
      "     |  >>> selector = SelectFromModel(estimator=LogisticRegression()).fit(X, y)\n",
      "     |  >>> selector.estimator_.coef_\n",
      "     |  array([[-0.3252302 ,  0.83462377,  0.49750423]])\n",
      "     |  >>> selector.threshold_\n",
      "     |  0.55245...\n",
      "     |  >>> selector.get_support()\n",
      "     |  array([False,  True, False])\n",
      "     |  >>> selector.transform(X)\n",
      "     |  array([[-1.34],\n",
      "     |         [-0.02],\n",
      "     |         [-0.48],\n",
      "     |         [ 1.48]])\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  RFE : Recursive feature elimination based on importance weights.\n",
      "     |  RFECV : Recursive feature elimination with built-in cross-validated\n",
      "     |      selection of the best number of features.\n",
      "     |  SequentialFeatureSelector : Sequential cross-validation based feature\n",
      "     |      selection. Does not rely on importance weights.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SelectFromModel\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.feature_selection._base.SelectorMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimator, *, threshold=None, prefit=False, norm_order=1, max_features=None, importance_getter='auto')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, **fit_params)\n",
      "     |      Fit the SelectFromModel meta-transformer.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The training input samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,), default=None\n",
      "     |          The target values (integers that correspond to classes in\n",
      "     |          classification, real numbers in regression).\n",
      "     |      \n",
      "     |      **fit_params : Other estimator specific parameters\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  partial_fit(self, X, y=None, **fit_params)\n",
      "     |      Fit the SelectFromModel meta-transformer only once.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The training input samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,), default=None\n",
      "     |          The target values (integers that correspond to classes in\n",
      "     |          classification, real numbers in regression).\n",
      "     |      \n",
      "     |      **fit_params : Other estimator specific parameters\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |  \n",
      "     |  threshold_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MetaEstimatorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.feature_selection._base.SelectorMixin:\n",
      "     |  \n",
      "     |  get_support(self, indices=False)\n",
      "     |      Get a mask, or integer index, of the features selected\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      indices : bool, default=False\n",
      "     |          If True, the return value will be an array of integers, rather\n",
      "     |          than a boolean mask.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      support : array\n",
      "     |          An index that selects the retained features from a feature vector.\n",
      "     |          If `indices` is False, this is a boolean array of shape\n",
      "     |          [# input features], in which an element is True iff its\n",
      "     |          corresponding feature is selected for retention. If `indices` is\n",
      "     |          True, this is an integer array of shape [# output features] whose\n",
      "     |          values are indices into the input feature vector.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Reverse the transformation operation\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_original_features]\n",
      "     |          `X` with columns of zeros inserted where features would have\n",
      "     |          been removed by :meth:`transform`.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Reduce X to the selected features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples with only the selected features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class SelectFwe(_BaseFilter)\n",
      "     |  SelectFwe(score_func=<function f_classif at 0x1170c3820>, *, alpha=0.05)\n",
      "     |  \n",
      "     |  Filter: Select the p-values corresponding to Family-wise error rate\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <univariate_feature_selection>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  score_func : callable, default=f_classif\n",
      "     |      Function taking two arrays X and y, and returning a pair of arrays\n",
      "     |      (scores, pvalues).\n",
      "     |      Default is f_classif (see below \"See Also\"). The default function only\n",
      "     |      works with classification tasks.\n",
      "     |  \n",
      "     |  alpha : float, default=5e-2\n",
      "     |      The highest uncorrected p-value for features to keep.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_breast_cancer\n",
      "     |  >>> from sklearn.feature_selection import SelectFwe, chi2\n",
      "     |  >>> X, y = load_breast_cancer(return_X_y=True)\n",
      "     |  >>> X.shape\n",
      "     |  (569, 30)\n",
      "     |  >>> X_new = SelectFwe(chi2, alpha=0.01).fit_transform(X, y)\n",
      "     |  >>> X_new.shape\n",
      "     |  (569, 15)\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  scores_ : array-like of shape (n_features,)\n",
      "     |      Scores of features.\n",
      "     |  \n",
      "     |  pvalues_ : array-like of shape (n_features,)\n",
      "     |      p-values of feature scores.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  f_classif : ANOVA F-value between label/feature for classification tasks.\n",
      "     |  chi2 : Chi-squared stats of non-negative features for classification tasks.\n",
      "     |  f_regression : F-value between label/feature for regression tasks.\n",
      "     |  SelectPercentile : Select features based on percentile of the highest\n",
      "     |      scores.\n",
      "     |  SelectKBest : Select features based on the k highest scores.\n",
      "     |  SelectFpr : Select features based on a false positive rate test.\n",
      "     |  SelectFdr : Select features based on an estimated false discovery rate.\n",
      "     |  GenericUnivariateSelect : Univariate feature selector with configurable\n",
      "     |      mode.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SelectFwe\n",
      "     |      _BaseFilter\n",
      "     |      sklearn.feature_selection._base.SelectorMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, score_func=<function f_classif at 0x1170c3820>, *, alpha=0.05)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseFilter:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Run score function on (X, y) and get the appropriate features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The training input samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.feature_selection._base.SelectorMixin:\n",
      "     |  \n",
      "     |  get_support(self, indices=False)\n",
      "     |      Get a mask, or integer index, of the features selected\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      indices : bool, default=False\n",
      "     |          If True, the return value will be an array of integers, rather\n",
      "     |          than a boolean mask.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      support : array\n",
      "     |          An index that selects the retained features from a feature vector.\n",
      "     |          If `indices` is False, this is a boolean array of shape\n",
      "     |          [# input features], in which an element is True iff its\n",
      "     |          corresponding feature is selected for retention. If `indices` is\n",
      "     |          True, this is an integer array of shape [# output features] whose\n",
      "     |          values are indices into the input feature vector.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Reverse the transformation operation\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_original_features]\n",
      "     |          `X` with columns of zeros inserted where features would have\n",
      "     |          been removed by :meth:`transform`.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Reduce X to the selected features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples with only the selected features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class SelectKBest(_BaseFilter)\n",
      "     |  SelectKBest(score_func=<function f_classif at 0x1170c3820>, *, k=10)\n",
      "     |  \n",
      "     |  Select features according to the k highest scores.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <univariate_feature_selection>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  score_func : callable, default=f_classif\n",
      "     |      Function taking two arrays X and y, and returning a pair of arrays\n",
      "     |      (scores, pvalues) or a single array with scores.\n",
      "     |      Default is f_classif (see below \"See Also\"). The default function only\n",
      "     |      works with classification tasks.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  k : int or \"all\", default=10\n",
      "     |      Number of top features to select.\n",
      "     |      The \"all\" option bypasses selection, for use in a parameter search.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  scores_ : array-like of shape (n_features,)\n",
      "     |      Scores of features.\n",
      "     |  \n",
      "     |  pvalues_ : array-like of shape (n_features,)\n",
      "     |      p-values of feature scores, None if `score_func` returned only scores.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_digits\n",
      "     |  >>> from sklearn.feature_selection import SelectKBest, chi2\n",
      "     |  >>> X, y = load_digits(return_X_y=True)\n",
      "     |  >>> X.shape\n",
      "     |  (1797, 64)\n",
      "     |  >>> X_new = SelectKBest(chi2, k=20).fit_transform(X, y)\n",
      "     |  >>> X_new.shape\n",
      "     |  (1797, 20)\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Ties between features with equal scores will be broken in an unspecified\n",
      "     |  way.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  f_classif : ANOVA F-value between label/feature for classification tasks.\n",
      "     |  mutual_info_classif : Mutual information for a discrete target.\n",
      "     |  chi2 : Chi-squared stats of non-negative features for classification tasks.\n",
      "     |  f_regression : F-value between label/feature for regression tasks.\n",
      "     |  mutual_info_regression : Mutual information for a continuous target.\n",
      "     |  SelectPercentile : Select features based on percentile of the highest\n",
      "     |      scores.\n",
      "     |  SelectFpr : Select features based on a false positive rate test.\n",
      "     |  SelectFdr : Select features based on an estimated false discovery rate.\n",
      "     |  SelectFwe : Select features based on family-wise error rate.\n",
      "     |  GenericUnivariateSelect : Univariate feature selector with configurable\n",
      "     |      mode.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SelectKBest\n",
      "     |      _BaseFilter\n",
      "     |      sklearn.feature_selection._base.SelectorMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, score_func=<function f_classif at 0x1170c3820>, *, k=10)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseFilter:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Run score function on (X, y) and get the appropriate features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The training input samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.feature_selection._base.SelectorMixin:\n",
      "     |  \n",
      "     |  get_support(self, indices=False)\n",
      "     |      Get a mask, or integer index, of the features selected\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      indices : bool, default=False\n",
      "     |          If True, the return value will be an array of integers, rather\n",
      "     |          than a boolean mask.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      support : array\n",
      "     |          An index that selects the retained features from a feature vector.\n",
      "     |          If `indices` is False, this is a boolean array of shape\n",
      "     |          [# input features], in which an element is True iff its\n",
      "     |          corresponding feature is selected for retention. If `indices` is\n",
      "     |          True, this is an integer array of shape [# output features] whose\n",
      "     |          values are indices into the input feature vector.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Reverse the transformation operation\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_original_features]\n",
      "     |          `X` with columns of zeros inserted where features would have\n",
      "     |          been removed by :meth:`transform`.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Reduce X to the selected features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples with only the selected features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class SelectPercentile(_BaseFilter)\n",
      "     |  SelectPercentile(score_func=<function f_classif at 0x1170c3820>, *, percentile=10)\n",
      "     |  \n",
      "     |  Select features according to a percentile of the highest scores.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <univariate_feature_selection>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  score_func : callable, default=f_classif\n",
      "     |      Function taking two arrays X and y, and returning a pair of arrays\n",
      "     |      (scores, pvalues) or a single array with scores.\n",
      "     |      Default is f_classif (see below \"See Also\"). The default function only\n",
      "     |      works with classification tasks.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  percentile : int, default=10\n",
      "     |      Percent of features to keep.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  scores_ : array-like of shape (n_features,)\n",
      "     |      Scores of features.\n",
      "     |  \n",
      "     |  pvalues_ : array-like of shape (n_features,)\n",
      "     |      p-values of feature scores, None if `score_func` returned only scores.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_digits\n",
      "     |  >>> from sklearn.feature_selection import SelectPercentile, chi2\n",
      "     |  >>> X, y = load_digits(return_X_y=True)\n",
      "     |  >>> X.shape\n",
      "     |  (1797, 64)\n",
      "     |  >>> X_new = SelectPercentile(chi2, percentile=10).fit_transform(X, y)\n",
      "     |  >>> X_new.shape\n",
      "     |  (1797, 7)\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Ties between features with equal scores will be broken in an unspecified\n",
      "     |  way.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  f_classif : ANOVA F-value between label/feature for classification tasks.\n",
      "     |  mutual_info_classif : Mutual information for a discrete target.\n",
      "     |  chi2 : Chi-squared stats of non-negative features for classification tasks.\n",
      "     |  f_regression : F-value between label/feature for regression tasks.\n",
      "     |  mutual_info_regression : Mutual information for a continuous target.\n",
      "     |  SelectKBest : Select features based on the k highest scores.\n",
      "     |  SelectFpr : Select features based on a false positive rate test.\n",
      "     |  SelectFdr : Select features based on an estimated false discovery rate.\n",
      "     |  SelectFwe : Select features based on family-wise error rate.\n",
      "     |  GenericUnivariateSelect : Univariate feature selector with configurable\n",
      "     |      mode.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SelectPercentile\n",
      "     |      _BaseFilter\n",
      "     |      sklearn.feature_selection._base.SelectorMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, score_func=<function f_classif at 0x1170c3820>, *, percentile=10)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseFilter:\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Run score function on (X, y) and get the appropriate features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The training input samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.feature_selection._base.SelectorMixin:\n",
      "     |  \n",
      "     |  get_support(self, indices=False)\n",
      "     |      Get a mask, or integer index, of the features selected\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      indices : bool, default=False\n",
      "     |          If True, the return value will be an array of integers, rather\n",
      "     |          than a boolean mask.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      support : array\n",
      "     |          An index that selects the retained features from a feature vector.\n",
      "     |          If `indices` is False, this is a boolean array of shape\n",
      "     |          [# input features], in which an element is True iff its\n",
      "     |          corresponding feature is selected for retention. If `indices` is\n",
      "     |          True, this is an integer array of shape [# output features] whose\n",
      "     |          values are indices into the input feature vector.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Reverse the transformation operation\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_original_features]\n",
      "     |          `X` with columns of zeros inserted where features would have\n",
      "     |          been removed by :meth:`transform`.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Reduce X to the selected features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples with only the selected features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class SelectorMixin(sklearn.base.TransformerMixin)\n",
      "     |  Transformer mixin that performs feature selection given a support mask\n",
      "     |  \n",
      "     |  This mixin provides a feature selector implementation with `transform` and\n",
      "     |  `inverse_transform` functionality given an implementation of\n",
      "     |  `_get_support_mask`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SelectorMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  get_support(self, indices=False)\n",
      "     |      Get a mask, or integer index, of the features selected\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      indices : bool, default=False\n",
      "     |          If True, the return value will be an array of integers, rather\n",
      "     |          than a boolean mask.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      support : array\n",
      "     |          An index that selects the retained features from a feature vector.\n",
      "     |          If `indices` is False, this is a boolean array of shape\n",
      "     |          [# input features], in which an element is True iff its\n",
      "     |          corresponding feature is selected for retention. If `indices` is\n",
      "     |          True, this is an integer array of shape [# output features] whose\n",
      "     |          values are indices into the input feature vector.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Reverse the transformation operation\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_original_features]\n",
      "     |          `X` with columns of zeros inserted where features would have\n",
      "     |          been removed by :meth:`transform`.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Reduce X to the selected features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples with only the selected features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'_get_support_mask'})\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SequentialFeatureSelector(sklearn.feature_selection._base.SelectorMixin, sklearn.base.MetaEstimatorMixin, sklearn.base.BaseEstimator)\n",
      "     |  SequentialFeatureSelector(estimator, *, n_features_to_select=None, direction='forward', scoring=None, cv=5, n_jobs=None)\n",
      "     |  \n",
      "     |  Transformer that performs Sequential Feature Selection.\n",
      "     |  \n",
      "     |  This Sequential Feature Selector adds (forward selection) or\n",
      "     |  removes (backward selection) features to form a feature subset in a\n",
      "     |  greedy fashion. At each stage, this estimator chooses the best feature to\n",
      "     |  add or remove based on the cross-validation score of an estimator.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <sequential_feature_selection>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.24\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimator : estimator instance\n",
      "     |      An unfitted estimator.\n",
      "     |  \n",
      "     |  n_features_to_select : int or float, default=None\n",
      "     |      The number of features to select. If `None`, half of the features are\n",
      "     |      selected. If integer, the parameter is the absolute number of features\n",
      "     |      to select. If float between 0 and 1, it is the fraction of features to\n",
      "     |      select.\n",
      "     |  \n",
      "     |  direction : {'forward', 'backward'}, default='forward'\n",
      "     |      Whether to perform forward selection or backward selection.\n",
      "     |  \n",
      "     |  scoring : str, callable, list/tuple or dict, default=None\n",
      "     |      A single str (see :ref:`scoring_parameter`) or a callable\n",
      "     |      (see :ref:`scoring`) to evaluate the predictions on the test set.\n",
      "     |  \n",
      "     |      NOTE that when using custom scorers, each scorer should return a single\n",
      "     |      value. Metric functions returning a list/array of values can be wrapped\n",
      "     |      into multiple scorers that return one value each.\n",
      "     |  \n",
      "     |      If None, the estimator's score method is used.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy.\n",
      "     |      Possible inputs for cv are:\n",
      "     |  \n",
      "     |      - None, to use the default 5-fold cross validation,\n",
      "     |      - integer, to specify the number of folds in a `(Stratified)KFold`,\n",
      "     |      - :term:`CV splitter`,\n",
      "     |      - An iterable yielding (train, test) splits as arrays of indices.\n",
      "     |  \n",
      "     |      For integer/None inputs, if the estimator is a classifier and ``y`` is\n",
      "     |      either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "     |      other cases, :class:`KFold` is used. These splitters are instantiated\n",
      "     |      with `shuffle=False` so the splits will be the same across calls.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      Number of jobs to run in parallel. When evaluating a new feature to\n",
      "     |      add or remove, the cross-validation procedure is parallel over the\n",
      "     |      folds.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  n_features_to_select_ : int\n",
      "     |      The number of features that were selected.\n",
      "     |  \n",
      "     |  support_ : ndarray of shape (n_features,), dtype=bool\n",
      "     |      The mask of selected features.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  RFE : Recursive feature elimination based on importance weights.\n",
      "     |  RFECV : Recursive feature elimination based on importance weights, with\n",
      "     |      automatic selection of the number of features.\n",
      "     |  SelectFromModel : Feature selection based on thresholds of importance\n",
      "     |      weights.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.feature_selection import SequentialFeatureSelector\n",
      "     |  >>> from sklearn.neighbors import KNeighborsClassifier\n",
      "     |  >>> from sklearn.datasets import load_iris\n",
      "     |  >>> X, y = load_iris(return_X_y=True)\n",
      "     |  >>> knn = KNeighborsClassifier(n_neighbors=3)\n",
      "     |  >>> sfs = SequentialFeatureSelector(knn, n_features_to_select=3)\n",
      "     |  >>> sfs.fit(X, y)\n",
      "     |  SequentialFeatureSelector(estimator=KNeighborsClassifier(n_neighbors=3),\n",
      "     |                            n_features_to_select=3)\n",
      "     |  >>> sfs.get_support()\n",
      "     |  array([ True, False,  True,  True])\n",
      "     |  >>> sfs.transform(X).shape\n",
      "     |  (150, 3)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SequentialFeatureSelector\n",
      "     |      sklearn.feature_selection._base.SelectorMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimator, *, n_features_to_select=None, direction='forward', scoring=None, cv=5, n_jobs=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y)\n",
      "     |      Learn the features to select.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Training vectors.\n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.feature_selection._base.SelectorMixin:\n",
      "     |  \n",
      "     |  get_support(self, indices=False)\n",
      "     |      Get a mask, or integer index, of the features selected\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      indices : bool, default=False\n",
      "     |          If True, the return value will be an array of integers, rather\n",
      "     |          than a boolean mask.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      support : array\n",
      "     |          An index that selects the retained features from a feature vector.\n",
      "     |          If `indices` is False, this is a boolean array of shape\n",
      "     |          [# input features], in which an element is True iff its\n",
      "     |          corresponding feature is selected for retention. If `indices` is\n",
      "     |          True, this is an integer array of shape [# output features] whose\n",
      "     |          values are indices into the input feature vector.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Reverse the transformation operation\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_original_features]\n",
      "     |          `X` with columns of zeros inserted where features would have\n",
      "     |          been removed by :meth:`transform`.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Reduce X to the selected features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples with only the selected features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class VarianceThreshold(sklearn.feature_selection._base.SelectorMixin, sklearn.base.BaseEstimator)\n",
      "     |  VarianceThreshold(threshold=0.0)\n",
      "     |  \n",
      "     |  Feature selector that removes all low-variance features.\n",
      "     |  \n",
      "     |  This feature selection algorithm looks only at the features (X), not the\n",
      "     |  desired outputs (y), and can thus be used for unsupervised learning.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <variance_threshold>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  threshold : float, default=0\n",
      "     |      Features with a training-set variance lower than this threshold will\n",
      "     |      be removed. The default is to keep all features with non-zero variance,\n",
      "     |      i.e. remove the features that have the same value in all samples.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  variances_ : array, shape (n_features,)\n",
      "     |      Variances of individual features.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  Allows NaN in the input.\n",
      "     |  Raises ValueError if no feature in X meets the variance threshold.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  The following dataset has integer features, two of which are the same\n",
      "     |  in every sample. These are removed with the default setting for threshold::\n",
      "     |  \n",
      "     |      >>> X = [[0, 2, 0, 3], [0, 1, 4, 3], [0, 1, 1, 3]]\n",
      "     |      >>> selector = VarianceThreshold()\n",
      "     |      >>> selector.fit_transform(X)\n",
      "     |      array([[2, 0],\n",
      "     |             [1, 4],\n",
      "     |             [1, 1]])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      VarianceThreshold\n",
      "     |      sklearn.feature_selection._base.SelectorMixin\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, threshold=0.0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None)\n",
      "     |      Learn empirical variances from X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      "     |          Sample vectors from which to compute variances.\n",
      "     |      \n",
      "     |      y : any, default=None\n",
      "     |          Ignored. This parameter exists only for compatibility with\n",
      "     |          sklearn.pipeline.Pipeline.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.feature_selection._base.SelectorMixin:\n",
      "     |  \n",
      "     |  get_support(self, indices=False)\n",
      "     |      Get a mask, or integer index, of the features selected\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      indices : bool, default=False\n",
      "     |          If True, the return value will be an array of integers, rather\n",
      "     |          than a boolean mask.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      support : array\n",
      "     |          An index that selects the retained features from a feature vector.\n",
      "     |          If `indices` is False, this is a boolean array of shape\n",
      "     |          [# input features], in which an element is True iff its\n",
      "     |          corresponding feature is selected for retention. If `indices` is\n",
      "     |          True, this is an integer array of shape [# output features] whose\n",
      "     |          values are indices into the input feature vector.\n",
      "     |  \n",
      "     |  inverse_transform(self, X)\n",
      "     |      Reverse the transformation operation\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_original_features]\n",
      "     |          `X` with columns of zeros inserted where features would have\n",
      "     |          been removed by :meth:`transform`.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Reduce X to the selected features.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array of shape [n_samples, n_features]\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_r : array of shape [n_samples, n_selected_features]\n",
      "     |          The input samples with only the selected features.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "\n",
      "FUNCTIONS\n",
      "    chi2(X, y)\n",
      "        Compute chi-squared stats between each non-negative feature and class.\n",
      "        \n",
      "        This score can be used to select the n_features features with the\n",
      "        highest values for the test chi-squared statistic from X, which must\n",
      "        contain only non-negative features such as booleans or frequencies\n",
      "        (e.g., term counts in document classification), relative to the classes.\n",
      "        \n",
      "        Recall that the chi-square test measures dependence between stochastic\n",
      "        variables, so using this function \"weeds out\" the features that are the\n",
      "        most likely to be independent of class and therefore irrelevant for\n",
      "        classification.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <univariate_feature_selection>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            Sample vectors.\n",
      "        \n",
      "        y : array-like of shape (n_samples,)\n",
      "            Target vector (class labels).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        chi2 : ndarray of shape (n_features,)\n",
      "            Chi2 statistics for each feature.\n",
      "        \n",
      "        p_values : ndarray of shape (n_features,)\n",
      "            P-values for each feature.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Complexity of this algorithm is O(n_classes * n_features).\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        f_classif : ANOVA F-value between label/feature for classification tasks.\n",
      "        f_regression : F-value between label/feature for regression tasks.\n",
      "    \n",
      "    f_classif(X, y)\n",
      "        Compute the ANOVA F-value for the provided sample.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <univariate_feature_selection>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "            The set of regressors that will be tested sequentially.\n",
      "        \n",
      "        y : ndarray of shape (n_samples,)\n",
      "            The target vector.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        f_statistic : ndarray of shape (n_features,)\n",
      "            F-statistic for each feature.\n",
      "        \n",
      "        p_values : ndarray of shape (n_features,)\n",
      "            P-values associated with the F-statistic.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        chi2 : Chi-squared stats of non-negative features for classification tasks.\n",
      "        f_regression : F-value between label/feature for regression tasks.\n",
      "    \n",
      "    f_oneway(*args)\n",
      "        Performs a 1-way ANOVA.\n",
      "        \n",
      "        The one-way ANOVA tests the null hypothesis that 2 or more groups have\n",
      "        the same population mean. The test is applied to samples from two or\n",
      "        more groups, possibly with differing sizes.\n",
      "        \n",
      "        Read more in the :ref:`User Guide <univariate_feature_selection>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        *args : {array-like, sparse matrix}\n",
      "            sample1, sample2... The sample measurements should be given as\n",
      "            arguments.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        f_statistic : float\n",
      "            The computed F-value of the test.\n",
      "        p_value : float\n",
      "            The associated p-value from the F-distribution.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The ANOVA test has important assumptions that must be satisfied in order\n",
      "        for the associated p-value to be valid.\n",
      "        \n",
      "        1. The samples are independent\n",
      "        2. Each sample is from a normally distributed population\n",
      "        3. The population standard deviations of the groups are all equal. This\n",
      "           property is known as homoscedasticity.\n",
      "        \n",
      "        If these assumptions are not true for a given set of data, it may still be\n",
      "        possible to use the Kruskal-Wallis H-test (`scipy.stats.kruskal`_) although\n",
      "        with some loss of power.\n",
      "        \n",
      "        The algorithm is from Heiman[2], pp.394-7.\n",
      "        \n",
      "        See ``scipy.stats.f_oneway`` that should give the same results while\n",
      "        being less efficient.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        \n",
      "        .. [1] Lowry, Richard.  \"Concepts and Applications of Inferential\n",
      "               Statistics\". Chapter 14.\n",
      "               http://faculty.vassar.edu/lowry/ch14pt1.html\n",
      "        \n",
      "        .. [2] Heiman, G.W.  Research Methods in Statistics. 2002.\n",
      "    \n",
      "    f_regression(X, y, *, center=True)\n",
      "        Univariate linear regression tests.\n",
      "        \n",
      "        Linear model for testing the individual effect of each of many regressors.\n",
      "        This is a scoring function to be used in a feature selection procedure, not\n",
      "        a free standing feature selection procedure.\n",
      "        \n",
      "        This is done in 2 steps:\n",
      "        \n",
      "        1. The correlation between each regressor and the target is computed,\n",
      "           that is, ((X[:, i] - mean(X[:, i])) * (y - mean_y)) / (std(X[:, i]) *\n",
      "           std(y)).\n",
      "        2. It is converted to an F score then to a p-value.\n",
      "        \n",
      "        For more on usage see the :ref:`User Guide <univariate_feature_selection>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : {array-like, sparse matrix}  shape = (n_samples, n_features)\n",
      "            The set of regressors that will be tested sequentially.\n",
      "        \n",
      "        y : array of shape(n_samples).\n",
      "            The data matrix\n",
      "        \n",
      "        center : bool, default=True\n",
      "            If true, X and y will be centered.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        F : array, shape=(n_features,)\n",
      "            F values of features.\n",
      "        \n",
      "        pval : array, shape=(n_features,)\n",
      "            p-values of F-scores.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        mutual_info_regression : Mutual information for a continuous target.\n",
      "        f_classif : ANOVA F-value between label/feature for classification tasks.\n",
      "        chi2 : Chi-squared stats of non-negative features for classification tasks.\n",
      "        SelectKBest : Select features based on the k highest scores.\n",
      "        SelectFpr : Select features based on a false positive rate test.\n",
      "        SelectFdr : Select features based on an estimated false discovery rate.\n",
      "        SelectFwe : Select features based on family-wise error rate.\n",
      "        SelectPercentile : Select features based on percentile of the highest\n",
      "            scores.\n",
      "    \n",
      "    mutual_info_classif(X, y, *, discrete_features='auto', n_neighbors=3, copy=True, random_state=None)\n",
      "        Estimate mutual information for a discrete target variable.\n",
      "        \n",
      "        Mutual information (MI) [1]_ between two random variables is a non-negative\n",
      "        value, which measures the dependency between the variables. It is equal\n",
      "        to zero if and only if two random variables are independent, and higher\n",
      "        values mean higher dependency.\n",
      "        \n",
      "        The function relies on nonparametric methods based on entropy estimation\n",
      "        from k-nearest neighbors distances as described in [2]_ and [3]_. Both\n",
      "        methods are based on the idea originally proposed in [4]_.\n",
      "        \n",
      "        It can be used for univariate features selection, read more in the\n",
      "        :ref:`User Guide <univariate_feature_selection>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "            Feature matrix.\n",
      "        \n",
      "        y : array-like of shape (n_samples,)\n",
      "            Target vector.\n",
      "        \n",
      "        discrete_features : {'auto', bool, array-like}, default='auto'\n",
      "            If bool, then determines whether to consider all features discrete\n",
      "            or continuous. If array, then it should be either a boolean mask\n",
      "            with shape (n_features,) or array with indices of discrete features.\n",
      "            If 'auto', it is assigned to False for dense `X` and to True for\n",
      "            sparse `X`.\n",
      "        \n",
      "        n_neighbors : int, default=3\n",
      "            Number of neighbors to use for MI estimation for continuous variables,\n",
      "            see [2]_ and [3]_. Higher values reduce variance of the estimation, but\n",
      "            could introduce a bias.\n",
      "        \n",
      "        copy : bool, default=True\n",
      "            Whether to make a copy of the given data. If set to False, the initial\n",
      "            data will be overwritten.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for adding small noise to\n",
      "            continuous variables in order to remove repeated values.\n",
      "            Pass an int for reproducible results across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mi : ndarray, shape (n_features,)\n",
      "            Estimated mutual information between each feature and the target.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        1. The term \"discrete features\" is used instead of naming them\n",
      "           \"categorical\", because it describes the essence more accurately.\n",
      "           For example, pixel intensities of an image are discrete features\n",
      "           (but hardly categorical) and you will get better results if mark them\n",
      "           as such. Also note, that treating a continuous variable as discrete and\n",
      "           vice versa will usually give incorrect results, so be attentive about\n",
      "           that.\n",
      "        2. True mutual information can't be negative. If its estimate turns out\n",
      "           to be negative, it is replaced by zero.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Mutual Information\n",
      "               <https://en.wikipedia.org/wiki/Mutual_information>`_\n",
      "               on Wikipedia.\n",
      "        .. [2] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual\n",
      "               information\". Phys. Rev. E 69, 2004.\n",
      "        .. [3] B. C. Ross \"Mutual Information between Discrete and Continuous\n",
      "               Data Sets\". PLoS ONE 9(2), 2014.\n",
      "        .. [4] L. F. Kozachenko, N. N. Leonenko, \"Sample Estimate of the Entropy\n",
      "               of a Random Vector:, Probl. Peredachi Inf., 23:2 (1987), 9-16\n",
      "    \n",
      "    mutual_info_regression(X, y, *, discrete_features='auto', n_neighbors=3, copy=True, random_state=None)\n",
      "        Estimate mutual information for a continuous target variable.\n",
      "        \n",
      "        Mutual information (MI) [1]_ between two random variables is a non-negative\n",
      "        value, which measures the dependency between the variables. It is equal\n",
      "        to zero if and only if two random variables are independent, and higher\n",
      "        values mean higher dependency.\n",
      "        \n",
      "        The function relies on nonparametric methods based on entropy estimation\n",
      "        from k-nearest neighbors distances as described in [2]_ and [3]_. Both\n",
      "        methods are based on the idea originally proposed in [4]_.\n",
      "        \n",
      "        It can be used for univariate features selection, read more in the\n",
      "        :ref:`User Guide <univariate_feature_selection>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      "            Feature matrix.\n",
      "        \n",
      "        y : array-like of shape (n_samples,)\n",
      "            Target vector.\n",
      "        \n",
      "        discrete_features : {'auto', bool, array-like}, default='auto'\n",
      "            If bool, then determines whether to consider all features discrete\n",
      "            or continuous. If array, then it should be either a boolean mask\n",
      "            with shape (n_features,) or array with indices of discrete features.\n",
      "            If 'auto', it is assigned to False for dense `X` and to True for\n",
      "            sparse `X`.\n",
      "        \n",
      "        n_neighbors : int, default=3\n",
      "            Number of neighbors to use for MI estimation for continuous variables,\n",
      "            see [2]_ and [3]_. Higher values reduce variance of the estimation, but\n",
      "            could introduce a bias.\n",
      "        \n",
      "        copy : bool, default=True\n",
      "            Whether to make a copy of the given data. If set to False, the initial\n",
      "            data will be overwritten.\n",
      "        \n",
      "        random_state : int, RandomState instance or None, default=None\n",
      "            Determines random number generation for adding small noise to\n",
      "            continuous variables in order to remove repeated values.\n",
      "            Pass an int for reproducible results across multiple function calls.\n",
      "            See :term:`Glossary <random_state>`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        mi : ndarray, shape (n_features,)\n",
      "            Estimated mutual information between each feature and the target.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        1. The term \"discrete features\" is used instead of naming them\n",
      "           \"categorical\", because it describes the essence more accurately.\n",
      "           For example, pixel intensities of an image are discrete features\n",
      "           (but hardly categorical) and you will get better results if mark them\n",
      "           as such. Also note, that treating a continuous variable as discrete and\n",
      "           vice versa will usually give incorrect results, so be attentive about\n",
      "           that.\n",
      "        2. True mutual information can't be negative. If its estimate turns out\n",
      "           to be negative, it is replaced by zero.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        .. [1] `Mutual Information\n",
      "               <https://en.wikipedia.org/wiki/Mutual_information>`_\n",
      "               on Wikipedia.\n",
      "        .. [2] A. Kraskov, H. Stogbauer and P. Grassberger, \"Estimating mutual\n",
      "               information\". Phys. Rev. E 69, 2004.\n",
      "        .. [3] B. C. Ross \"Mutual Information between Discrete and Continuous\n",
      "               Data Sets\". PLoS ONE 9(2), 2014.\n",
      "        .. [4] L. F. Kozachenko, N. N. Leonenko, \"Sample Estimate of the Entropy\n",
      "               of a Random Vector\", Probl. Peredachi Inf., 23:2 (1987), 9-16\n",
      "\n",
      "DATA\n",
      "    __all__ = ['GenericUnivariateSelect', 'SequentialFeatureSelector', 'RF...\n",
      "\n",
      "FILE\n",
      "    /opt/anaconda3/envs/ksunsupervised/lib/python3.9/site-packages/sklearn/feature_selection/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a527c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6fb54f9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DecisionTreeRegressor in module sklearn.tree._classes:\n",
      "\n",
      "class DecisionTreeRegressor(sklearn.base.RegressorMixin, BaseDecisionTree)\n",
      " |  DecisionTreeRegressor(*, criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, ccp_alpha=0.0)\n",
      " |  \n",
      " |  A decision tree regressor.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <tree>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  criterion : {\"mse\", \"friedman_mse\", \"mae\", \"poisson\"}, default=\"mse\"\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are \"mse\" for the mean squared error, which is equal to variance\n",
      " |      reduction as feature selection criterion and minimizes the L2 loss\n",
      " |      using the mean of each terminal node, \"friedman_mse\", which uses mean\n",
      " |      squared error with Friedman's improvement score for potential splits,\n",
      " |      \"mae\" for the mean absolute error, which minimizes the L1 loss using\n",
      " |      the median of each terminal node, and \"poisson\" which uses reduction in\n",
      " |      Poisson deviance to find splits.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Mean Absolute Error (MAE) criterion.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |          Poisson deviance criterion.\n",
      " |  \n",
      " |  splitter : {\"best\", \"random\"}, default=\"best\"\n",
      " |      The strategy used to choose the split at each node. Supported\n",
      " |      strategies are \"best\" to choose the best split and \"random\" to choose\n",
      " |      the best random split.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=n_features`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the randomness of the estimator. The features are always\n",
      " |      randomly permuted at each split, even if ``splitter`` is set to\n",
      " |      ``\"best\"``. When ``max_features < n_features``, the algorithm will\n",
      " |      select ``max_features`` at random at each split before finding the best\n",
      " |      split among them. But the best found split may vary across different\n",
      " |      runs, even if ``max_features=n_features``. That is the case, if the\n",
      " |      improvement of the criterion is identical for several splits and one\n",
      " |      split has to be selected at random. To obtain a deterministic behaviour\n",
      " |      during fitting, ``random_state`` has to be fixed to an integer.\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, default=0\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 1.0 (renaming of 0.25).\n",
      " |         Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the\n",
      " |      (normalized) total reduction of the criterion brought\n",
      " |      by that feature. It is also known as the Gini importance [4]_.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  max_features_ : int\n",
      " |      The inferred value of max_features.\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  tree_ : Tree instance\n",
      " |      The underlying Tree object. Please refer to\n",
      " |      ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
      " |      :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
      " |      for basic usage of these attributes.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  DecisionTreeClassifier : A decision tree classifier.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      " |  \n",
      " |  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
      " |         and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
      " |  \n",
      " |  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
      " |         Learning\", Springer, 2009.\n",
      " |  \n",
      " |  .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
      " |         https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_diabetes\n",
      " |  >>> from sklearn.model_selection import cross_val_score\n",
      " |  >>> from sklearn.tree import DecisionTreeRegressor\n",
      " |  >>> X, y = load_diabetes(return_X_y=True)\n",
      " |  >>> regressor = DecisionTreeRegressor(random_state=0)\n",
      " |  >>> cross_val_score(regressor, X, y, cv=10)\n",
      " |  ...                    # doctest: +SKIP\n",
      " |  ...\n",
      " |  array([-0.39..., -0.46...,  0.02...,  0.06..., -0.50...,\n",
      " |         0.16...,  0.11..., -0.73..., -0.30..., -0.00...])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DecisionTreeRegressor\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      BaseDecisionTree\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, criterion='mse', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, ccp_alpha=0.0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted='deprecated')\n",
      " |      Build a decision tree regressor from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (real numbers). Use ``dtype=np.float64`` and\n",
      " |          ``order='C'`` for maximum efficiency.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      X_idx_sorted : deprecated, default=\"deprecated\"\n",
      " |          This parameter is deprecated and has no effect.\n",
      " |          It will be removed in 1.1 (renaming of 0.26).\n",
      " |      \n",
      " |          .. deprecated :: 0.24\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : DecisionTreeRegressor\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination :math:`R^2` of the\n",
      " |      prediction.\n",
      " |      \n",
      " |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      " |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      " |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      " |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      " |      can be negative (because the model can be arbitrarily worse). A\n",
      " |      constant model that always predicts the expected value of `y`,\n",
      " |      disregarding the input features, would get a :math:`R^2` score of\n",
      " |      0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a precomputed\n",
      " |          kernel matrix or a list of generic objects instead with shape\n",
      " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      " |          is the number of samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  apply(self, X, check_input=True)\n",
      " |      Return the index of the leaf that each sample is predicted as.\n",
      " |      \n",
      " |      .. versionadded:: 0.17\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array-like of shape (n_samples,)\n",
      " |          For each datapoint x in X, return the index of the leaf x\n",
      " |          ends up in. Leaves are numbered within\n",
      " |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
      " |          numbering.\n",
      " |  \n",
      " |  cost_complexity_pruning_path(self, X, y, sample_weight=None)\n",
      " |      Compute the pruning path during Minimal Cost-Complexity Pruning.\n",
      " |      \n",
      " |      See :ref:`minimal_cost_complexity_pruning` for details on the pruning\n",
      " |      process.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels) as integers or strings.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. Splits are also\n",
      " |          ignored if they would result in any single class carrying a\n",
      " |          negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ccp_path : :class:`~sklearn.utils.Bunch`\n",
      " |          Dictionary-like object, with the following attributes.\n",
      " |      \n",
      " |          ccp_alphas : ndarray\n",
      " |              Effective alphas of subtree during pruning.\n",
      " |      \n",
      " |          impurities : ndarray\n",
      " |              Sum of the impurities of the subtree leaves for the\n",
      " |              corresponding alpha value in ``ccp_alphas``.\n",
      " |  \n",
      " |  decision_path(self, X, check_input=True)\n",
      " |      Return the decision path in the tree.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator CSR matrix where non zero elements\n",
      " |          indicates that the samples goes through the nodes.\n",
      " |  \n",
      " |  get_depth(self)\n",
      " |      Return the depth of the decision tree.\n",
      " |      \n",
      " |      The depth of a tree is the maximum distance between the root\n",
      " |      and any leaf.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self.tree_.max_depth : int\n",
      " |          The maximum depth of the tree.\n",
      " |  \n",
      " |  get_n_leaves(self)\n",
      " |      Return the number of leaves of the decision tree.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self.tree_.n_leaves : int\n",
      " |          Number of leaves.\n",
      " |  \n",
      " |  predict(self, X, check_input=True)\n",
      " |      Predict class or regression value for X.\n",
      " |      \n",
      " |      For a classification model, the predicted class for each sample in X is\n",
      " |      returned. For a regression model, the predicted value based on X is\n",
      " |      returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted classes, or the predict values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances.\n",
      " |      \n",
      " |      The importance of a feature is computed as the (normalized) total\n",
      " |      reduction of the criterion brought by that feature.\n",
      " |      It is also known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          Normalized total reduction of criteria by feature\n",
      " |          (Gini importance).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DecisionTreeRegressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90d5ec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b5fb34c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KNeighborsRegressor in module sklearn.neighbors._regression:\n",
      "\n",
      "class KNeighborsRegressor(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.RegressorMixin, sklearn.neighbors._base.NeighborsBase)\n",
      " |  KNeighborsRegressor(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)\n",
      " |  \n",
      " |  Regression based on k-nearest neighbors.\n",
      " |  \n",
      " |  The target is predicted by local interpolation of the targets\n",
      " |  associated of the nearest neighbors in the training set.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <regression>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.9\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_neighbors : int, default=5\n",
      " |      Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
      " |  \n",
      " |  weights : {'uniform', 'distance'} or callable, default='uniform'\n",
      " |      weight function used in prediction.  Possible values:\n",
      " |  \n",
      " |      - 'uniform' : uniform weights.  All points in each neighborhood\n",
      " |        are weighted equally.\n",
      " |      - 'distance' : weight points by the inverse of their distance.\n",
      " |        in this case, closer neighbors of a query point will have a\n",
      " |        greater influence than neighbors which are further away.\n",
      " |      - [callable] : a user-defined function which accepts an\n",
      " |        array of distances, and returns an array of the same shape\n",
      " |        containing the weights.\n",
      " |  \n",
      " |      Uniform weights are used by default.\n",
      " |  \n",
      " |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      " |      Algorithm used to compute the nearest neighbors:\n",
      " |  \n",
      " |      - 'ball_tree' will use :class:`BallTree`\n",
      " |      - 'kd_tree' will use :class:`KDTree`\n",
      " |      - 'brute' will use a brute-force search.\n",
      " |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      " |        based on the values passed to :meth:`fit` method.\n",
      " |  \n",
      " |      Note: fitting on sparse input will override the setting of\n",
      " |      this parameter, using brute force.\n",
      " |  \n",
      " |  leaf_size : int, default=30\n",
      " |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      " |      speed of the construction and query, as well as the memory\n",
      " |      required to store the tree.  The optimal value depends on the\n",
      " |      nature of the problem.\n",
      " |  \n",
      " |  p : int, default=2\n",
      " |      Power parameter for the Minkowski metric. When p = 1, this is\n",
      " |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      " |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      " |  \n",
      " |  metric : str or callable, default='minkowski'\n",
      " |      the distance metric to use for the tree.  The default metric is\n",
      " |      minkowski, and with p=2 is equivalent to the standard Euclidean\n",
      " |      metric. See the documentation of :class:`DistanceMetric` for a\n",
      " |      list of available metrics.\n",
      " |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      " |      must be square during fit. X may be a :term:`sparse graph`,\n",
      " |      in which case only \"nonzero\" elements may be considered neighbors.\n",
      " |  \n",
      " |  metric_params : dict, default=None\n",
      " |      Additional keyword arguments for the metric function.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of parallel jobs to run for neighbors search.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |      Doesn't affect :meth:`fit` method.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  effective_metric_ : str or callable\n",
      " |      The distance metric to use. It will be same as the `metric` parameter\n",
      " |      or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
      " |      'minkowski' and `p` parameter set to 2.\n",
      " |  \n",
      " |  effective_metric_params_ : dict\n",
      " |      Additional keyword arguments for the metric function. For most metrics\n",
      " |      will be same with `metric_params` parameter, but may also contain the\n",
      " |      `p` parameter value if the `effective_metric_` attribute is set to\n",
      " |      'minkowski'.\n",
      " |  \n",
      " |  n_samples_fit_ : int\n",
      " |      Number of samples in the fitted data.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> X = [[0], [1], [2], [3]]\n",
      " |  >>> y = [0, 0, 1, 1]\n",
      " |  >>> from sklearn.neighbors import KNeighborsRegressor\n",
      " |  >>> neigh = KNeighborsRegressor(n_neighbors=2)\n",
      " |  >>> neigh.fit(X, y)\n",
      " |  KNeighborsRegressor(...)\n",
      " |  >>> print(neigh.predict([[1.5]]))\n",
      " |  [0.5]\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  NearestNeighbors\n",
      " |  RadiusNeighborsRegressor\n",
      " |  KNeighborsClassifier\n",
      " |  RadiusNeighborsClassifier\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      " |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      " |  \n",
      " |  .. warning::\n",
      " |  \n",
      " |     Regarding the Nearest Neighbors algorithms, if it is found that two\n",
      " |     neighbors, neighbor `k+1` and `k`, have identical distances but\n",
      " |     different labels, the results will depend on the ordering of the\n",
      " |     training data.\n",
      " |  \n",
      " |  https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KNeighborsRegressor\n",
      " |      sklearn.neighbors._base.KNeighborsMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      sklearn.neighbors._base.NeighborsBase\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit the k-nearest neighbors regressor from the training dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n",
      " |          Training data.\n",
      " |      \n",
      " |      y : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : KNeighborsRegressor\n",
      " |          The fitted k-nearest neighbors regressor.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict the target for the provided data\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n",
      " |          Test samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_queries,) or (n_queries, n_outputs), dtype=int\n",
      " |          Target values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      " |  \n",
      " |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
      " |      Finds the K-neighbors of a point.\n",
      " |      \n",
      " |      Returns indices of and distances to the neighbors of each point.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |      \n",
      " |      n_neighbors : int, default=None\n",
      " |          Number of neighbors required for each sample. The default is the\n",
      " |          value passed to the constructor.\n",
      " |      \n",
      " |      return_distance : bool, default=True\n",
      " |          Whether or not to return the distances.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      neigh_dist : ndarray of shape (n_queries, n_neighbors)\n",
      " |          Array representing the lengths to points, only present if\n",
      " |          return_distance=True\n",
      " |      \n",
      " |      neigh_ind : ndarray of shape (n_queries, n_neighbors)\n",
      " |          Indices of the nearest points in the population matrix.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      In the following example, we construct a NearestNeighbors\n",
      " |      class from an array representing our data set and ask who's\n",
      " |      the closest point to [1,1,1]\n",
      " |      \n",
      " |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
      " |      >>> neigh.fit(samples)\n",
      " |      NearestNeighbors(n_neighbors=1)\n",
      " |      >>> print(neigh.kneighbors([[1., 1., 1.]]))\n",
      " |      (array([[0.5]]), array([[2]]))\n",
      " |      \n",
      " |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
      " |      element is at distance 0.5 and is the third element of samples\n",
      " |      (indexes start at 0). You can also query for multiple points:\n",
      " |      \n",
      " |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
      " |      >>> neigh.kneighbors(X, return_distance=False)\n",
      " |      array([[1],\n",
      " |             [2]]...)\n",
      " |  \n",
      " |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
      " |      Computes the (weighted) graph of k-Neighbors for points in X\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |          For ``metric='precomputed'`` the shape should be\n",
      " |          (n_queries, n_indexed). Otherwise the shape should be\n",
      " |          (n_queries, n_features).\n",
      " |      \n",
      " |      n_neighbors : int, default=None\n",
      " |          Number of neighbors for each sample. The default is the value\n",
      " |          passed to the constructor.\n",
      " |      \n",
      " |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
      " |          Type of returned matrix: 'connectivity' will return the\n",
      " |          connectivity matrix with ones and zeros, in 'distance' the\n",
      " |          edges are Euclidean distance between points.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
      " |          `n_samples_fit` is the number of samples in the fitted data\n",
      " |          `A[i, j]` is assigned the weight of edge that connects `i` to `j`.\n",
      " |          The matrix is of CSR format.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> X = [[0], [3], [1]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
      " |      >>> neigh.fit(X)\n",
      " |      NearestNeighbors(n_neighbors=2)\n",
      " |      >>> A = neigh.kneighbors_graph(X)\n",
      " |      >>> A.toarray()\n",
      " |      array([[1., 0., 1.],\n",
      " |             [0., 1., 1.],\n",
      " |             [1., 0., 1.]])\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      NearestNeighbors.radius_neighbors_graph\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination :math:`R^2` of the\n",
      " |      prediction.\n",
      " |      \n",
      " |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      " |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      " |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      " |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      " |      can be negative (because the model can be arbitrarily worse). A\n",
      " |      constant model that always predicts the expected value of `y`,\n",
      " |      disregarding the input features, would get a :math:`R^2` score of\n",
      " |      0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a precomputed\n",
      " |          kernel matrix or a list of generic objects instead with shape\n",
      " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      " |          is the number of samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "KNeighborsRegressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "579518d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68f8d911",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LogisticRegression in module sklearn.linear_model._logistic:\n",
      "\n",
      "class LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      " |  LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |  \n",
      " |  Logistic Regression (aka logit, MaxEnt) classifier.\n",
      " |  \n",
      " |  In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
      " |  scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
      " |  cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
      " |  (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
      " |  'sag', 'saga' and 'newton-cg' solvers.)\n",
      " |  \n",
      " |  This class implements regularized logistic regression using the\n",
      " |  'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
      " |  that regularization is applied by default**. It can handle both dense\n",
      " |  and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
      " |  floats for optimal performance; any other input format will be converted\n",
      " |  (and copied).\n",
      " |  \n",
      " |  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
      " |  with primal formulation, or no regularization. The 'liblinear' solver\n",
      " |  supports both L1 and L2 regularization, with a dual formulation only for\n",
      " |  the L2 penalty. The Elastic-Net regularization is only supported by the\n",
      " |  'saga' solver.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <logistic_regression>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n",
      " |      Used to specify the norm used in the penalization. The 'newton-cg',\n",
      " |      'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n",
      " |      only supported by the 'saga' solver. If 'none' (not supported by the\n",
      " |      liblinear solver), no regularization is applied.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |         l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
      " |  \n",
      " |  dual : bool, default=False\n",
      " |      Dual or primal formulation. Dual formulation is only implemented for\n",
      " |      l2 penalty with liblinear solver. Prefer dual=False when\n",
      " |      n_samples > n_features.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for stopping criteria.\n",
      " |  \n",
      " |  C : float, default=1.0\n",
      " |      Inverse of regularization strength; must be a positive float.\n",
      " |      Like in support vector machines, smaller values specify stronger\n",
      " |      regularization.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Specifies if a constant (a.k.a. bias or intercept) should be\n",
      " |      added to the decision function.\n",
      " |  \n",
      " |  intercept_scaling : float, default=1\n",
      " |      Useful only when the solver 'liblinear' is used\n",
      " |      and self.fit_intercept is set to True. In this case, x becomes\n",
      " |      [x, self.intercept_scaling],\n",
      " |      i.e. a \"synthetic\" feature with constant value equal to\n",
      " |      intercept_scaling is appended to the instance vector.\n",
      " |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
      " |  \n",
      " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      " |      as all other features.\n",
      " |      To lessen the effect of regularization on synthetic feature weight\n",
      " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      " |  \n",
      " |  class_weight : dict or 'balanced', default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one.\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *class_weight='balanced'*\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
      " |      data. See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
      " |  \n",
      " |      Algorithm to use in the optimization problem.\n",
      " |  \n",
      " |      - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
      " |        'saga' are faster for large ones.\n",
      " |      - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
      " |        handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
      " |        schemes.\n",
      " |      - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n",
      " |      - 'liblinear' and 'saga' also handle L1 penalty\n",
      " |      - 'saga' also supports 'elasticnet' penalty\n",
      " |      - 'liblinear' does not support setting ``penalty='none'``\n",
      " |  \n",
      " |      Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
      " |      features with approximately the same scale. You can\n",
      " |      preprocess the data with a scaler from sklearn.preprocessing.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Stochastic Average Gradient descent solver.\n",
      " |      .. versionadded:: 0.19\n",
      " |         SAGA solver.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
      " |  \n",
      " |  max_iter : int, default=100\n",
      " |      Maximum number of iterations taken for the solvers to converge.\n",
      " |  \n",
      " |  multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
      " |      If the option chosen is 'ovr', then a binary problem is fit for each\n",
      " |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
      " |      across the entire probability distribution, *even when the data is\n",
      " |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
      " |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
      " |      and otherwise selects 'multinomial'.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Stochastic Average Gradient descent solver for 'multinomial' case.\n",
      " |      .. versionchanged:: 0.22\n",
      " |          Default changed from 'ovr' to 'auto' in 0.22.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      For the liblinear and lbfgs solvers set verbose to any positive\n",
      " |      number for verbosity.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous call to fit as\n",
      " |      initialization, otherwise, just erase the previous solution.\n",
      " |      Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      Number of CPU cores used when parallelizing over classes if\n",
      " |      multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
      " |      set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
      " |      not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors.\n",
      " |      See :term:`Glossary <n_jobs>` for more details.\n",
      " |  \n",
      " |  l1_ratio : float, default=None\n",
      " |      The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
      " |      used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
      " |      to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
      " |      to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
      " |      combination of L1 and L2.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes, )\n",
      " |      A list of class labels known to the classifier.\n",
      " |  \n",
      " |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
      " |      Coefficient of the features in the decision function.\n",
      " |  \n",
      " |      `coef_` is of shape (1, n_features) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
      " |      to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (1,) or (n_classes,)\n",
      " |      Intercept (a.k.a. bias) added to the decision function.\n",
      " |  \n",
      " |      If `fit_intercept` is set to False, the intercept is set to zero.\n",
      " |      `intercept_` is of shape (1,) when the given problem is binary.\n",
      " |      In particular, when `multi_class='multinomial'`, `intercept_`\n",
      " |      corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
      " |      outcome 0 (False).\n",
      " |  \n",
      " |  n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
      " |      Actual number of iterations for all classes. If binary or multinomial,\n",
      " |      it returns only 1 element. For liblinear solver, only the maximum\n",
      " |      number of iteration across all classes is given.\n",
      " |  \n",
      " |      .. versionchanged:: 0.20\n",
      " |  \n",
      " |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
      " |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  SGDClassifier : Incrementally trained logistic regression (when given\n",
      " |      the parameter ``loss=\"log\"``).\n",
      " |  LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The underlying C implementation uses a random number generator to\n",
      " |  select features when fitting the model. It is thus not uncommon,\n",
      " |  to have slightly different results for the same input data. If\n",
      " |  that happens, try with a smaller tol parameter.\n",
      " |  \n",
      " |  Predict output may not match that of standalone liblinear in certain\n",
      " |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      " |  in the narrative documentation.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
      " |      Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
      " |      http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
      " |  \n",
      " |  LIBLINEAR -- A Library for Large Linear Classification\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
      " |  \n",
      " |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
      " |      Minimizing Finite Sums with the Stochastic Average Gradient\n",
      " |      https://hal.inria.fr/hal-00860051/document\n",
      " |  \n",
      " |  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
      " |      SAGA: A Fast Incremental Gradient Method With Support\n",
      " |      for Non-Strongly Convex Composite Objectives\n",
      " |      https://arxiv.org/abs/1407.0202\n",
      " |  \n",
      " |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
      " |      methods for logistic regression and maximum entropy models.\n",
      " |      Machine Learning 85(1-2):41-75.\n",
      " |      https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.linear_model import LogisticRegression\n",
      " |  >>> X, y = load_iris(return_X_y=True)\n",
      " |  >>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
      " |  >>> clf.predict(X[:2, :])\n",
      " |  array([0, 0])\n",
      " |  >>> clf.predict_proba(X[:2, :])\n",
      " |  array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
      " |         [9.7...e-01, 2.8...e-02, ...e-08]])\n",
      " |  >>> clf.score(X, y)\n",
      " |  0.97...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LogisticRegression\n",
      " |      sklearn.linear_model._base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.linear_model._base.SparseCoefMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vector, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target vector relative to X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,) default=None\n",
      " |          Array of weights that are assigned to individual samples.\n",
      " |          If not provided, then each sample is given unit weight.\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             *sample_weight* support to LogisticRegression.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The SAGA solver supports both float64 and float32 bit arrays.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict logarithm of probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the sample for each class in the\n",
      " |          model, where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Probability estimates.\n",
      " |      \n",
      " |      The returned estimates for all classes are ordered by the\n",
      " |      label of classes.\n",
      " |      \n",
      " |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n",
      " |      the softmax function is used to find the predicted probability of\n",
      " |      each class.\n",
      " |      Else use a one-vs-rest approach, i.e calculate the probability\n",
      " |      of each class assuming it to be positive using the logistic function.\n",
      " |      and normalize these values across all the classes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Vector to be scored, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the sample for each class in the model,\n",
      " |          where classes are ordered as they are in ``self.classes_``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is proportional to the signed\n",
      " |      distance of that sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      " |          Confidence scores per (sample, class) combination. In the binary\n",
      " |          case, confidence score for self.classes_[1] where >0 means this\n",
      " |          class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape [n_samples]\n",
      " |          Predicted class label per sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LogisticRegression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00ceff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f6e6635",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GaussianNB in module sklearn.naive_bayes:\n",
      "\n",
      "class GaussianNB(_BaseNB)\n",
      " |  GaussianNB(*, priors=None, var_smoothing=1e-09)\n",
      " |  \n",
      " |  Gaussian Naive Bayes (GaussianNB)\n",
      " |  \n",
      " |  Can perform online updates to model parameters via :meth:`partial_fit`.\n",
      " |  For details on algorithm used to update feature means and variance online,\n",
      " |  see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:\n",
      " |  \n",
      " |      http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <gaussian_naive_bayes>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  priors : array-like of shape (n_classes,)\n",
      " |      Prior probabilities of the classes. If specified the priors are not\n",
      " |      adjusted according to the data.\n",
      " |  \n",
      " |  var_smoothing : float, default=1e-9\n",
      " |      Portion of the largest variance of all features that is added to\n",
      " |      variances for calculation stability.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  class_count_ : ndarray of shape (n_classes,)\n",
      " |      number of training samples observed in each class.\n",
      " |  \n",
      " |  class_prior_ : ndarray of shape (n_classes,)\n",
      " |      probability of each class.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      class labels known to the classifier\n",
      " |  \n",
      " |  epsilon_ : float\n",
      " |      absolute additive value to variances\n",
      " |  \n",
      " |  sigma_ : ndarray of shape (n_classes, n_features)\n",
      " |      variance of each feature per class\n",
      " |  \n",
      " |  theta_ : ndarray of shape (n_classes, n_features)\n",
      " |      mean of each feature per class\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      " |  >>> Y = np.array([1, 1, 1, 2, 2, 2])\n",
      " |  >>> from sklearn.naive_bayes import GaussianNB\n",
      " |  >>> clf = GaussianNB()\n",
      " |  >>> clf.fit(X, Y)\n",
      " |  GaussianNB()\n",
      " |  >>> print(clf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  >>> clf_pf = GaussianNB()\n",
      " |  >>> clf_pf.partial_fit(X, Y, np.unique(Y))\n",
      " |  GaussianNB()\n",
      " |  >>> print(clf_pf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GaussianNB\n",
      " |      _BaseNB\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, priors=None, var_smoothing=1e-09)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit Gaussian Naive Bayes according to X, y\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             Gaussian Naive Bayes supports fitting with *sample_weight*.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      " |      Incremental fit on a batch of samples.\n",
      " |      \n",
      " |      This method is expected to be called several times consecutively\n",
      " |      on different chunks of a dataset so as to implement out-of-core\n",
      " |      or online learning.\n",
      " |      \n",
      " |      This is especially useful when the whole dataset is too big to fit in\n",
      " |      memory at once.\n",
      " |      \n",
      " |      This method has some performance and numerical stability overhead,\n",
      " |      hence it is better to call partial_fit on chunks of data that are\n",
      " |      as large as possible (as long as fitting in the memory budget) to\n",
      " |      hide the overhead.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training vectors, where n_samples is the number of samples and\n",
      " |          n_features is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      classes : array-like of shape (n_classes,), default=None\n",
      " |          List of all the classes that can possibly appear in the y vector.\n",
      " |      \n",
      " |          Must be provided at the first call to partial_fit, can be omitted\n",
      " |          in subsequent calls.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Weights applied to individual samples (1. for unweighted).\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _BaseNB:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Perform classification on an array of test vectors X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : ndarray of shape (n_samples,)\n",
      " |          Predicted target values for X\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Return log-probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the log-probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Return probability estimates for the test vector X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array-like of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the samples for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GaussianNB?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba84af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a217322e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KNeighborsClassifier in module sklearn.neighbors._classification:\n",
      "\n",
      "class KNeighborsClassifier(sklearn.neighbors._base.KNeighborsMixin, sklearn.base.ClassifierMixin, sklearn.neighbors._base.NeighborsBase)\n",
      " |  KNeighborsClassifier(n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)\n",
      " |  \n",
      " |  Classifier implementing the k-nearest neighbors vote.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <classification>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_neighbors : int, default=5\n",
      " |      Number of neighbors to use by default for :meth:`kneighbors` queries.\n",
      " |  \n",
      " |  weights : {'uniform', 'distance'} or callable, default='uniform'\n",
      " |      weight function used in prediction.  Possible values:\n",
      " |  \n",
      " |      - 'uniform' : uniform weights.  All points in each neighborhood\n",
      " |        are weighted equally.\n",
      " |      - 'distance' : weight points by the inverse of their distance.\n",
      " |        in this case, closer neighbors of a query point will have a\n",
      " |        greater influence than neighbors which are further away.\n",
      " |      - [callable] : a user-defined function which accepts an\n",
      " |        array of distances, and returns an array of the same shape\n",
      " |        containing the weights.\n",
      " |  \n",
      " |  algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n",
      " |      Algorithm used to compute the nearest neighbors:\n",
      " |  \n",
      " |      - 'ball_tree' will use :class:`BallTree`\n",
      " |      - 'kd_tree' will use :class:`KDTree`\n",
      " |      - 'brute' will use a brute-force search.\n",
      " |      - 'auto' will attempt to decide the most appropriate algorithm\n",
      " |        based on the values passed to :meth:`fit` method.\n",
      " |  \n",
      " |      Note: fitting on sparse input will override the setting of\n",
      " |      this parameter, using brute force.\n",
      " |  \n",
      " |  leaf_size : int, default=30\n",
      " |      Leaf size passed to BallTree or KDTree.  This can affect the\n",
      " |      speed of the construction and query, as well as the memory\n",
      " |      required to store the tree.  The optimal value depends on the\n",
      " |      nature of the problem.\n",
      " |  \n",
      " |  p : int, default=2\n",
      " |      Power parameter for the Minkowski metric. When p = 1, this is\n",
      " |      equivalent to using manhattan_distance (l1), and euclidean_distance\n",
      " |      (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n",
      " |  \n",
      " |  metric : str or callable, default='minkowski'\n",
      " |      the distance metric to use for the tree.  The default metric is\n",
      " |      minkowski, and with p=2 is equivalent to the standard Euclidean\n",
      " |      metric. See the documentation of :class:`DistanceMetric` for a\n",
      " |      list of available metrics.\n",
      " |      If metric is \"precomputed\", X is assumed to be a distance matrix and\n",
      " |      must be square during fit. X may be a :term:`sparse graph`,\n",
      " |      in which case only \"nonzero\" elements may be considered neighbors.\n",
      " |  \n",
      " |  metric_params : dict, default=None\n",
      " |      Additional keyword arguments for the metric function.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of parallel jobs to run for neighbors search.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |      Doesn't affect :meth:`fit` method.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : array of shape (n_classes,)\n",
      " |      Class labels known to the classifier\n",
      " |  \n",
      " |  effective_metric_ : str or callble\n",
      " |      The distance metric used. It will be same as the `metric` parameter\n",
      " |      or a synonym of it, e.g. 'euclidean' if the `metric` parameter set to\n",
      " |      'minkowski' and `p` parameter set to 2.\n",
      " |  \n",
      " |  effective_metric_params_ : dict\n",
      " |      Additional keyword arguments for the metric function. For most metrics\n",
      " |      will be same with `metric_params` parameter, but may also contain the\n",
      " |      `p` parameter value if the `effective_metric_` attribute is set to\n",
      " |      'minkowski'.\n",
      " |  \n",
      " |  n_samples_fit_ : int\n",
      " |      Number of samples in the fitted data.\n",
      " |  \n",
      " |  outputs_2d_ : bool\n",
      " |      False when `y`'s shape is (n_samples, ) or (n_samples, 1) during fit\n",
      " |      otherwise True.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> X = [[0], [1], [2], [3]]\n",
      " |  >>> y = [0, 0, 1, 1]\n",
      " |  >>> from sklearn.neighbors import KNeighborsClassifier\n",
      " |  >>> neigh = KNeighborsClassifier(n_neighbors=3)\n",
      " |  >>> neigh.fit(X, y)\n",
      " |  KNeighborsClassifier(...)\n",
      " |  >>> print(neigh.predict([[1.1]]))\n",
      " |  [0]\n",
      " |  >>> print(neigh.predict_proba([[0.9]]))\n",
      " |  [[0.66666667 0.33333333]]\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  RadiusNeighborsClassifier\n",
      " |  KNeighborsRegressor\n",
      " |  RadiusNeighborsRegressor\n",
      " |  NearestNeighbors\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  See :ref:`Nearest Neighbors <neighbors>` in the online documentation\n",
      " |  for a discussion of the choice of ``algorithm`` and ``leaf_size``.\n",
      " |  \n",
      " |  .. warning::\n",
      " |  \n",
      " |     Regarding the Nearest Neighbors algorithms, if it is found that two\n",
      " |     neighbors, neighbor `k+1` and `k`, have identical distances\n",
      " |     but different labels, the results will depend on the ordering of the\n",
      " |     training data.\n",
      " |  \n",
      " |  https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KNeighborsClassifier\n",
      " |      sklearn.neighbors._base.KNeighborsMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.neighbors._base.NeighborsBase\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_neighbors=5, *, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='minkowski', metric_params=None, n_jobs=None, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit the k-nearest neighbors classifier from the training dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples, n_samples) if metric='precomputed'\n",
      " |          Training data.\n",
      " |      \n",
      " |      y : {array-like, sparse matrix} of shape (n_samples,) or                 (n_samples, n_outputs)\n",
      " |          Target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : KNeighborsClassifier\n",
      " |          The fitted k-nearest neighbors classifier.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict the class labels for the provided data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n",
      " |          Test samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_queries,) or (n_queries, n_outputs)\n",
      " |          Class labels for each data sample.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Return probability estimates for the test data X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed'\n",
      " |          Test samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_queries, n_classes), or a list of n_outputs\n",
      " |          of such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. Classes are ordered\n",
      " |          by lexicographic order.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      " |  \n",
      " |  kneighbors(self, X=None, n_neighbors=None, return_distance=True)\n",
      " |      Finds the K-neighbors of a point.\n",
      " |      \n",
      " |      Returns indices of and distances to the neighbors of each point.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_queries, n_features),             or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |      \n",
      " |      n_neighbors : int, default=None\n",
      " |          Number of neighbors required for each sample. The default is the\n",
      " |          value passed to the constructor.\n",
      " |      \n",
      " |      return_distance : bool, default=True\n",
      " |          Whether or not to return the distances.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      neigh_dist : ndarray of shape (n_queries, n_neighbors)\n",
      " |          Array representing the lengths to points, only present if\n",
      " |          return_distance=True\n",
      " |      \n",
      " |      neigh_ind : ndarray of shape (n_queries, n_neighbors)\n",
      " |          Indices of the nearest points in the population matrix.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      In the following example, we construct a NearestNeighbors\n",
      " |      class from an array representing our data set and ask who's\n",
      " |      the closest point to [1,1,1]\n",
      " |      \n",
      " |      >>> samples = [[0., 0., 0.], [0., .5, 0.], [1., 1., .5]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=1)\n",
      " |      >>> neigh.fit(samples)\n",
      " |      NearestNeighbors(n_neighbors=1)\n",
      " |      >>> print(neigh.kneighbors([[1., 1., 1.]]))\n",
      " |      (array([[0.5]]), array([[2]]))\n",
      " |      \n",
      " |      As you can see, it returns [[0.5]], and [[2]], which means that the\n",
      " |      element is at distance 0.5 and is the third element of samples\n",
      " |      (indexes start at 0). You can also query for multiple points:\n",
      " |      \n",
      " |      >>> X = [[0., 1., 0.], [1., 0., 1.]]\n",
      " |      >>> neigh.kneighbors(X, return_distance=False)\n",
      " |      array([[1],\n",
      " |             [2]]...)\n",
      " |  \n",
      " |  kneighbors_graph(self, X=None, n_neighbors=None, mode='connectivity')\n",
      " |      Computes the (weighted) graph of k-Neighbors for points in X\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_queries, n_features),                 or (n_queries, n_indexed) if metric == 'precomputed',                 default=None\n",
      " |          The query point or points.\n",
      " |          If not provided, neighbors of each indexed point are returned.\n",
      " |          In this case, the query point is not considered its own neighbor.\n",
      " |          For ``metric='precomputed'`` the shape should be\n",
      " |          (n_queries, n_indexed). Otherwise the shape should be\n",
      " |          (n_queries, n_features).\n",
      " |      \n",
      " |      n_neighbors : int, default=None\n",
      " |          Number of neighbors for each sample. The default is the value\n",
      " |          passed to the constructor.\n",
      " |      \n",
      " |      mode : {'connectivity', 'distance'}, default='connectivity'\n",
      " |          Type of returned matrix: 'connectivity' will return the\n",
      " |          connectivity matrix with ones and zeros, in 'distance' the\n",
      " |          edges are Euclidean distance between points.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A : sparse-matrix of shape (n_queries, n_samples_fit)\n",
      " |          `n_samples_fit` is the number of samples in the fitted data\n",
      " |          `A[i, j]` is assigned the weight of edge that connects `i` to `j`.\n",
      " |          The matrix is of CSR format.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> X = [[0], [3], [1]]\n",
      " |      >>> from sklearn.neighbors import NearestNeighbors\n",
      " |      >>> neigh = NearestNeighbors(n_neighbors=2)\n",
      " |      >>> neigh.fit(X)\n",
      " |      NearestNeighbors(n_neighbors=2)\n",
      " |      >>> A = neigh.kneighbors_graph(X)\n",
      " |      >>> A.toarray()\n",
      " |      array([[1., 0., 1.],\n",
      " |             [0., 1., 1.],\n",
      " |             [1., 0., 1.]])\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      NearestNeighbors.radius_neighbors_graph\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.neighbors._base.KNeighborsMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "KNeighborsClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "325e6723",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3031089",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DecisionTreeClassifier in module sklearn.tree._classes:\n",
      "\n",
      "class DecisionTreeClassifier(sklearn.base.ClassifierMixin, BaseDecisionTree)\n",
      " |  DecisionTreeClassifier(*, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, ccp_alpha=0.0)\n",
      " |  \n",
      " |  A decision tree classifier.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <tree>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |  \n",
      " |  splitter : {\"best\", \"random\"}, default=\"best\"\n",
      " |      The strategy used to choose the split at each node. Supported\n",
      " |      strategies are \"best\" to choose the best split and \"random\" to choose\n",
      " |      the best random split.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : int, float or {\"auto\", \"sqrt\", \"log2\"}, default=None\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |          - If int, then consider `max_features` features at each split.\n",
      " |          - If float, then `max_features` is a fraction and\n",
      " |            `int(max_features * n_features)` features are considered at each\n",
      " |            split.\n",
      " |          - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |          - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |          - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |          - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the randomness of the estimator. The features are always\n",
      " |      randomly permuted at each split, even if ``splitter`` is set to\n",
      " |      ``\"best\"``. When ``max_features < n_features``, the algorithm will\n",
      " |      select ``max_features`` at random at each split before finding the best\n",
      " |      split among them. But the best found split may vary across different\n",
      " |      runs, even if ``max_features=n_features``. That is the case, if the\n",
      " |      improvement of the criterion is identical for several splits and one\n",
      " |      split has to be selected at random. To obtain a deterministic behaviour\n",
      " |      during fitting, ``random_state`` has to be fixed to an integer.\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow a tree with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, default=0\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 1.0 (renaming of 0.25).\n",
      " |         Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  class_weight : dict, list of dict or \"balanced\", default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If None, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : ndarray of shape (n_classes,) or list of ndarray\n",
      " |      The classes labels (single output problem),\n",
      " |      or a list of arrays of class labels (multi-output problem).\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance [4]_.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  max_features_ : int\n",
      " |      The inferred value of max_features.\n",
      " |  \n",
      " |  n_classes_ : int or list of int\n",
      " |      The number of classes (for single output problems),\n",
      " |      or a list containing the number of classes for each\n",
      " |      output (for multi-output problems).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  tree_ : Tree instance\n",
      " |      The underlying Tree object. Please refer to\n",
      " |      ``help(sklearn.tree._tree.Tree)`` for attributes of Tree object and\n",
      " |      :ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`\n",
      " |      for basic usage of these attributes.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  DecisionTreeRegressor : A decision tree regressor.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The :meth:`predict` method operates using the :func:`numpy.argmax`\n",
      " |  function on the outputs of :meth:`predict_proba`. This means that in\n",
      " |  case the highest predicted probabilities are tied, the classifier will\n",
      " |  predict the tied class with the lowest index in :term:`classes_`.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  .. [1] https://en.wikipedia.org/wiki/Decision_tree_learning\n",
      " |  \n",
      " |  .. [2] L. Breiman, J. Friedman, R. Olshen, and C. Stone, \"Classification\n",
      " |         and Regression Trees\", Wadsworth, Belmont, CA, 1984.\n",
      " |  \n",
      " |  .. [3] T. Hastie, R. Tibshirani and J. Friedman. \"Elements of Statistical\n",
      " |         Learning\", Springer, 2009.\n",
      " |  \n",
      " |  .. [4] L. Breiman, and A. Cutler, \"Random Forests\",\n",
      " |         https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_iris\n",
      " |  >>> from sklearn.model_selection import cross_val_score\n",
      " |  >>> from sklearn.tree import DecisionTreeClassifier\n",
      " |  >>> clf = DecisionTreeClassifier(random_state=0)\n",
      " |  >>> iris = load_iris()\n",
      " |  >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
      " |  ...                             # doctest: +SKIP\n",
      " |  ...\n",
      " |  array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,\n",
      " |          0.93...,  0.93...,  1.     ,  0.93...,  1.      ])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DecisionTreeClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseDecisionTree\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, criterion='gini', splitter='best', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, ccp_alpha=0.0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, check_input=True, X_idx_sorted='deprecated')\n",
      " |      Build a decision tree classifier from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels) as integers or strings.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. Splits are also\n",
      " |          ignored if they would result in any single class carrying a\n",
      " |          negative weight in either child node.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      X_idx_sorted : deprecated, default=\"deprecated\"\n",
      " |          This parameter is deprecated and has no effect.\n",
      " |          It will be removed in 1.1 (renaming of 0.26).\n",
      " |      \n",
      " |          .. deprecated :: 0.24\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : DecisionTreeClassifier\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities of the input samples X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
      " |          The class log-probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X, check_input=True)\n",
      " |      Predict class probabilities of the input samples X.\n",
      " |      \n",
      " |      The predicted class probability is the fraction of samples of the same\n",
      " |      class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      proba : ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs > 1\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  apply(self, X, check_input=True)\n",
      " |      Return the index of the leaf that each sample is predicted as.\n",
      " |      \n",
      " |      .. versionadded:: 0.17\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array-like of shape (n_samples,)\n",
      " |          For each datapoint x in X, return the index of the leaf x\n",
      " |          ends up in. Leaves are numbered within\n",
      " |          ``[0; self.tree_.node_count)``, possibly with gaps in the\n",
      " |          numbering.\n",
      " |  \n",
      " |  cost_complexity_pruning_path(self, X, y, sample_weight=None)\n",
      " |      Compute the pruning path during Minimal Cost-Complexity Pruning.\n",
      " |      \n",
      " |      See :ref:`minimal_cost_complexity_pruning` for details on the pruning\n",
      " |      process.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels) as integers or strings.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. Splits are also\n",
      " |          ignored if they would result in any single class carrying a\n",
      " |          negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ccp_path : :class:`~sklearn.utils.Bunch`\n",
      " |          Dictionary-like object, with the following attributes.\n",
      " |      \n",
      " |          ccp_alphas : ndarray\n",
      " |              Effective alphas of subtree during pruning.\n",
      " |      \n",
      " |          impurities : ndarray\n",
      " |              Sum of the impurities of the subtree leaves for the\n",
      " |              corresponding alpha value in ``ccp_alphas``.\n",
      " |  \n",
      " |  decision_path(self, X, check_input=True)\n",
      " |      Return the decision path in the tree.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator CSR matrix where non zero elements\n",
      " |          indicates that the samples goes through the nodes.\n",
      " |  \n",
      " |  get_depth(self)\n",
      " |      Return the depth of the decision tree.\n",
      " |      \n",
      " |      The depth of a tree is the maximum distance between the root\n",
      " |      and any leaf.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self.tree_.max_depth : int\n",
      " |          The maximum depth of the tree.\n",
      " |  \n",
      " |  get_n_leaves(self)\n",
      " |      Return the number of leaves of the decision tree.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self.tree_.n_leaves : int\n",
      " |          Number of leaves.\n",
      " |  \n",
      " |  predict(self, X, check_input=True)\n",
      " |      Predict class or regression value for X.\n",
      " |      \n",
      " |      For a classification model, the predicted class for each sample in X is\n",
      " |      returned. For a regression model, the predicted value based on X is\n",
      " |      returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted classes, or the predict values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseDecisionTree:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances.\n",
      " |      \n",
      " |      The importance of a feature is computed as the (normalized) total\n",
      " |      reduction of the criterion brought by that feature.\n",
      " |      It is also known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          Normalized total reduction of criteria by feature\n",
      " |          (Gini importance).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DecisionTreeClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa18ef45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74e52a8b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SVC in module sklearn.svm._classes:\n",
      "\n",
      "class SVC(sklearn.svm._base.BaseSVC)\n",
      " |  SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\n",
      " |  \n",
      " |  C-Support Vector Classification.\n",
      " |  \n",
      " |  The implementation is based on libsvm. The fit time scales at least\n",
      " |  quadratically with the number of samples and may be impractical\n",
      " |  beyond tens of thousands of samples. For large datasets\n",
      " |  consider using :class:`~sklearn.svm.LinearSVC` or\n",
      " |  :class:`~sklearn.linear_model.SGDClassifier` instead, possibly after a\n",
      " |  :class:`~sklearn.kernel_approximation.Nystroem` transformer.\n",
      " |  \n",
      " |  The multiclass support is handled according to a one-vs-one scheme.\n",
      " |  \n",
      " |  For details on the precise mathematical formulation of the provided\n",
      " |  kernel functions and how `gamma`, `coef0` and `degree` affect each\n",
      " |  other, see the corresponding section in the narrative documentation:\n",
      " |  :ref:`svm_kernels`.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <svm_classification>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  C : float, default=1.0\n",
      " |      Regularization parameter. The strength of the regularization is\n",
      " |      inversely proportional to C. Must be strictly positive. The penalty\n",
      " |      is a squared l2 penalty.\n",
      " |  \n",
      " |  kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}, default='rbf'\n",
      " |      Specifies the kernel type to be used in the algorithm.\n",
      " |      It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' or\n",
      " |      a callable.\n",
      " |      If none is given, 'rbf' will be used. If a callable is given it is\n",
      " |      used to pre-compute the kernel matrix from data matrices; that matrix\n",
      " |      should be an array of shape ``(n_samples, n_samples)``.\n",
      " |  \n",
      " |  degree : int, default=3\n",
      " |      Degree of the polynomial kernel function ('poly').\n",
      " |      Ignored by all other kernels.\n",
      " |  \n",
      " |  gamma : {'scale', 'auto'} or float, default='scale'\n",
      " |      Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
      " |  \n",
      " |      - if ``gamma='scale'`` (default) is passed then it uses\n",
      " |        1 / (n_features * X.var()) as value of gamma,\n",
      " |      - if 'auto', uses 1 / n_features.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |         The default value of ``gamma`` changed from 'auto' to 'scale'.\n",
      " |  \n",
      " |  coef0 : float, default=0.0\n",
      " |      Independent term in kernel function.\n",
      " |      It is only significant in 'poly' and 'sigmoid'.\n",
      " |  \n",
      " |  shrinking : bool, default=True\n",
      " |      Whether to use the shrinking heuristic.\n",
      " |      See the :ref:`User Guide <shrinking_svm>`.\n",
      " |  \n",
      " |  probability : bool, default=False\n",
      " |      Whether to enable probability estimates. This must be enabled prior\n",
      " |      to calling `fit`, will slow down that method as it internally uses\n",
      " |      5-fold cross-validation, and `predict_proba` may be inconsistent with\n",
      " |      `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\n",
      " |  \n",
      " |  tol : float, default=1e-3\n",
      " |      Tolerance for stopping criterion.\n",
      " |  \n",
      " |  cache_size : float, default=200\n",
      " |      Specify the size of the kernel cache (in MB).\n",
      " |  \n",
      " |  class_weight : dict or 'balanced', default=None\n",
      " |      Set the parameter C of class i to class_weight[i]*C for\n",
      " |      SVC. If not given, all classes are supposed to have\n",
      " |      weight one.\n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |  verbose : bool, default=False\n",
      " |      Enable verbose output. Note that this setting takes advantage of a\n",
      " |      per-process runtime setting in libsvm that, if enabled, may not work\n",
      " |      properly in a multithreaded context.\n",
      " |  \n",
      " |  max_iter : int, default=-1\n",
      " |      Hard limit on iterations within solver, or -1 for no limit.\n",
      " |  \n",
      " |  decision_function_shape : {'ovo', 'ovr'}, default='ovr'\n",
      " |      Whether to return a one-vs-rest ('ovr') decision function of shape\n",
      " |      (n_samples, n_classes) as all other classifiers, or the original\n",
      " |      one-vs-one ('ovo') decision function of libsvm which has shape\n",
      " |      (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one\n",
      " |      ('ovo') is always used as multi-class strategy. The parameter is\n",
      " |      ignored for binary classification.\n",
      " |  \n",
      " |      .. versionchanged:: 0.19\n",
      " |          decision_function_shape is 'ovr' by default.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         *decision_function_shape='ovr'* is recommended.\n",
      " |  \n",
      " |      .. versionchanged:: 0.17\n",
      " |         Deprecated *decision_function_shape='ovo' and None*.\n",
      " |  \n",
      " |  break_ties : bool, default=False\n",
      " |      If true, ``decision_function_shape='ovr'``, and number of classes > 2,\n",
      " |      :term:`predict` will break ties according to the confidence values of\n",
      " |      :term:`decision_function`; otherwise the first class among the tied\n",
      " |      classes is returned. Please note that breaking ties comes at a\n",
      " |      relatively high computational cost compared to a simple predict.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the pseudo random number generation for shuffling the data for\n",
      " |      probability estimates. Ignored when `probability` is False.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  class_weight_ : ndarray of shape (n_classes,)\n",
      " |      Multipliers of parameter C for each class.\n",
      " |      Computed based on the ``class_weight`` parameter.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      The classes labels.\n",
      " |  \n",
      " |  coef_ : ndarray of shape (n_classes * (n_classes - 1) / 2, n_features)\n",
      " |      Weights assigned to the features (coefficients in the primal\n",
      " |      problem). This is only available in the case of a linear kernel.\n",
      " |  \n",
      " |      `coef_` is a readonly property derived from `dual_coef_` and\n",
      " |      `support_vectors_`.\n",
      " |  \n",
      " |  dual_coef_ : ndarray of shape (n_classes -1, n_SV)\n",
      " |      Dual coefficients of the support vector in the decision\n",
      " |      function (see :ref:`sgd_mathematical_formulation`), multiplied by\n",
      " |      their targets.\n",
      " |      For multiclass, coefficient for all 1-vs-1 classifiers.\n",
      " |      The layout of the coefficients in the multiclass case is somewhat\n",
      " |      non-trivial. See the :ref:`multi-class section of the User Guide\n",
      " |      <svm_multi_class>` for details.\n",
      " |  \n",
      " |  fit_status_ : int\n",
      " |      0 if correctly fitted, 1 otherwise (will raise warning)\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n",
      " |      Constants in decision function.\n",
      " |  \n",
      " |  support_ : ndarray of shape (n_SV)\n",
      " |      Indices of support vectors.\n",
      " |  \n",
      " |  support_vectors_ : ndarray of shape (n_SV, n_features)\n",
      " |      Support vectors.\n",
      " |  \n",
      " |  n_support_ : ndarray of shape (n_classes,), dtype=int32\n",
      " |      Number of support vectors for each class.\n",
      " |  \n",
      " |  probA_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n",
      " |  probB_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n",
      " |      If `probability=True`, it corresponds to the parameters learned in\n",
      " |      Platt scaling to produce probability estimates from decision values.\n",
      " |      If `probability=False`, it's an empty array. Platt scaling uses the\n",
      " |      logistic function\n",
      " |      ``1 / (1 + exp(decision_value * probA_ + probB_))``\n",
      " |      where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\n",
      " |      more information on the multiclass case and training procedure see\n",
      " |      section 8 of [1]_.\n",
      " |  \n",
      " |  shape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n",
      " |      Array dimensions of training vector ``X``.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.pipeline import make_pipeline\n",
      " |  >>> from sklearn.preprocessing import StandardScaler\n",
      " |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
      " |  >>> y = np.array([1, 1, 2, 2])\n",
      " |  >>> from sklearn.svm import SVC\n",
      " |  >>> clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
      " |  >>> clf.fit(X, y)\n",
      " |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      " |                  ('svc', SVC(gamma='auto'))])\n",
      " |  \n",
      " |  >>> print(clf.predict([[-0.8, -1]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  SVR : Support Vector Machine for Regression implemented using libsvm.\n",
      " |  \n",
      " |  LinearSVC : Scalable Linear Support Vector Machine for classification\n",
      " |      implemented using liblinear. Check the See Also section of\n",
      " |      LinearSVC for more comparison element.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] `LIBSVM: A Library for Support Vector Machines\n",
      " |      <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n",
      " |  \n",
      " |  .. [2] `Platt, John (1999). \"Probabilistic outputs for support vector\n",
      " |      machines and comparison to regularizedlikelihood methods.\"\n",
      " |      <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639>`_\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SVC\n",
      " |      sklearn.svm._base.BaseSVC\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.svm._base.BaseLibSVM\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.svm._base.BaseSVC:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Evaluates the decision function for the samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X : ndarray of shape (n_samples, n_classes * (n_classes-1) / 2)\n",
      " |          Returns the decision function of the sample for each class\n",
      " |          in the model.\n",
      " |          If decision_function_shape='ovr', the shape is (n_samples,\n",
      " |          n_classes).\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If decision_function_shape='ovo', the function values are proportional\n",
      " |      to the distance of the samples X to the separating hyperplane. If the\n",
      " |      exact distances are required, divide the function values by the norm of\n",
      " |      the weight vector (``coef_``). See also `this question\n",
      " |      <https://stats.stackexchange.com/questions/14876/\n",
      " |      interpreting-distance-from-hyperplane-in-svm>`_ for further details.\n",
      " |      If decision_function_shape='ovr', the decision function is a monotonic\n",
      " |      transformation of ovo decision function.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Perform classification on samples in X.\n",
      " |      \n",
      " |      For an one-class model, +1 or -1 is returned.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples_test, n_samples_train)\n",
      " |          For kernel=\"precomputed\", the expected shape of X is\n",
      " |          (n_samples_test, n_samples_train).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_pred : ndarray of shape (n_samples,)\n",
      " |          Class labels for samples in X.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from sklearn.svm._base.BaseSVC:\n",
      " |  \n",
      " |  predict_log_proba\n",
      " |      Compute log probabilities of possible outcomes for samples in X.\n",
      " |      \n",
      " |      The model need to have probability information computed at training\n",
      " |      time: fit with attribute `probability` set to True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features) or                 (n_samples_test, n_samples_train)\n",
      " |          For kernel=\"precomputed\", the expected shape of X is\n",
      " |          (n_samples_test, n_samples_train).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : ndarray of shape (n_samples, n_classes)\n",
      " |          Returns the log-probabilities of the sample for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute :term:`classes_`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The probability model is created using cross validation, so\n",
      " |      the results can be slightly different than those obtained by\n",
      " |      predict. Also, it will produce meaningless results on very small\n",
      " |      datasets.\n",
      " |  \n",
      " |  predict_proba\n",
      " |      Compute probabilities of possible outcomes for samples in X.\n",
      " |      \n",
      " |      The model need to have probability information computed at training\n",
      " |      time: fit with attribute `probability` set to True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          For kernel=\"precomputed\", the expected shape of X is\n",
      " |          (n_samples_test, n_samples_train).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      T : ndarray of shape (n_samples, n_classes)\n",
      " |          Returns the probability of the sample for each class in\n",
      " |          the model. The columns correspond to the classes in sorted\n",
      " |          order, as they appear in the attribute :term:`classes_`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The probability model is created using cross validation, so\n",
      " |      the results can be slightly different than those obtained by\n",
      " |      predict. Also, it will produce meaningless results on very small\n",
      " |      datasets.\n",
      " |  \n",
      " |  probA_\n",
      " |  \n",
      " |  probB_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.svm._base.BaseLibSVM:\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the SVM model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)                 or (n_samples, n_samples)\n",
      " |          Training vectors, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |          For kernel=\"precomputed\", the expected shape of X is\n",
      " |          (n_samples, n_samples).\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Per-sample weights. Rescale C per sample. Higher weights\n",
      " |          force the classifier to put more emphasis on these points.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If X and y are not C-ordered and contiguous arrays of np.float64 and\n",
      " |      X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n",
      " |      \n",
      " |      If X is a dense array, then the other methods will not support sparse\n",
      " |      matrices as input.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from sklearn.svm._base.BaseLibSVM:\n",
      " |  \n",
      " |  coef_\n",
      " |  \n",
      " |  n_support_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "SVC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1429051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d69bc066",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package sklearn.ensemble in sklearn:\n",
      "\n",
      "NAME\n",
      "    sklearn.ensemble\n",
      "\n",
      "DESCRIPTION\n",
      "    The :mod:`sklearn.ensemble` module includes ensemble-based methods for\n",
      "    classification, regression and anomaly detection.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _bagging\n",
      "    _base\n",
      "    _forest\n",
      "    _gb\n",
      "    _gb_losses\n",
      "    _gradient_boosting\n",
      "    _hist_gradient_boosting (package)\n",
      "    _iforest\n",
      "    _stacking\n",
      "    _voting\n",
      "    _weight_boosting\n",
      "    setup\n",
      "    tests (package)\n",
      "\n",
      "CLASSES\n",
      "    sklearn.base.BaseEstimator(builtins.object)\n",
      "        sklearn.ensemble._base.BaseEnsemble(sklearn.base.MetaEstimatorMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.ClassifierMixin(builtins.object)\n",
      "        sklearn.ensemble._bagging.BaggingClassifier(sklearn.base.ClassifierMixin, sklearn.ensemble._bagging.BaseBagging)\n",
      "        sklearn.ensemble._gb.GradientBoostingClassifier(sklearn.base.ClassifierMixin, sklearn.ensemble._gb.BaseGradientBoosting)\n",
      "        sklearn.ensemble._stacking.StackingClassifier(sklearn.base.ClassifierMixin, sklearn.ensemble._stacking._BaseStacking)\n",
      "        sklearn.ensemble._voting.VotingClassifier(sklearn.base.ClassifierMixin, sklearn.ensemble._voting._BaseVoting)\n",
      "        sklearn.ensemble._weight_boosting.AdaBoostClassifier(sklearn.base.ClassifierMixin, sklearn.ensemble._weight_boosting.BaseWeightBoosting)\n",
      "    sklearn.base.MetaEstimatorMixin(builtins.object)\n",
      "        sklearn.ensemble._base.BaseEnsemble(sklearn.base.MetaEstimatorMixin, sklearn.base.BaseEstimator)\n",
      "    sklearn.base.OutlierMixin(builtins.object)\n",
      "        sklearn.ensemble._iforest.IsolationForest(sklearn.base.OutlierMixin, sklearn.ensemble._bagging.BaseBagging)\n",
      "    sklearn.base.RegressorMixin(builtins.object)\n",
      "        sklearn.ensemble._bagging.BaggingRegressor(sklearn.base.RegressorMixin, sklearn.ensemble._bagging.BaseBagging)\n",
      "        sklearn.ensemble._gb.GradientBoostingRegressor(sklearn.base.RegressorMixin, sklearn.ensemble._gb.BaseGradientBoosting)\n",
      "        sklearn.ensemble._stacking.StackingRegressor(sklearn.base.RegressorMixin, sklearn.ensemble._stacking._BaseStacking)\n",
      "        sklearn.ensemble._voting.VotingRegressor(sklearn.base.RegressorMixin, sklearn.ensemble._voting._BaseVoting)\n",
      "        sklearn.ensemble._weight_boosting.AdaBoostRegressor(sklearn.base.RegressorMixin, sklearn.ensemble._weight_boosting.BaseWeightBoosting)\n",
      "    sklearn.ensemble._bagging.BaseBagging(sklearn.ensemble._base.BaseEnsemble)\n",
      "        sklearn.ensemble._bagging.BaggingClassifier(sklearn.base.ClassifierMixin, sklearn.ensemble._bagging.BaseBagging)\n",
      "        sklearn.ensemble._bagging.BaggingRegressor(sklearn.base.RegressorMixin, sklearn.ensemble._bagging.BaseBagging)\n",
      "        sklearn.ensemble._iforest.IsolationForest(sklearn.base.OutlierMixin, sklearn.ensemble._bagging.BaseBagging)\n",
      "    sklearn.ensemble._forest.BaseForest(sklearn.base.MultiOutputMixin, sklearn.ensemble._base.BaseEnsemble)\n",
      "        sklearn.ensemble._forest.RandomTreesEmbedding\n",
      "    sklearn.ensemble._forest.ForestClassifier(sklearn.base.ClassifierMixin, sklearn.ensemble._forest.BaseForest)\n",
      "        sklearn.ensemble._forest.ExtraTreesClassifier\n",
      "        sklearn.ensemble._forest.RandomForestClassifier\n",
      "    sklearn.ensemble._forest.ForestRegressor(sklearn.base.RegressorMixin, sklearn.ensemble._forest.BaseForest)\n",
      "        sklearn.ensemble._forest.ExtraTreesRegressor\n",
      "        sklearn.ensemble._forest.RandomForestRegressor\n",
      "    sklearn.ensemble._gb.BaseGradientBoosting(sklearn.ensemble._base.BaseEnsemble)\n",
      "        sklearn.ensemble._gb.GradientBoostingClassifier(sklearn.base.ClassifierMixin, sklearn.ensemble._gb.BaseGradientBoosting)\n",
      "        sklearn.ensemble._gb.GradientBoostingRegressor(sklearn.base.RegressorMixin, sklearn.ensemble._gb.BaseGradientBoosting)\n",
      "    sklearn.ensemble._stacking._BaseStacking(sklearn.base.TransformerMixin, sklearn.ensemble._base._BaseHeterogeneousEnsemble)\n",
      "        sklearn.ensemble._stacking.StackingClassifier(sklearn.base.ClassifierMixin, sklearn.ensemble._stacking._BaseStacking)\n",
      "        sklearn.ensemble._stacking.StackingRegressor(sklearn.base.RegressorMixin, sklearn.ensemble._stacking._BaseStacking)\n",
      "    sklearn.ensemble._voting._BaseVoting(sklearn.base.TransformerMixin, sklearn.ensemble._base._BaseHeterogeneousEnsemble)\n",
      "        sklearn.ensemble._voting.VotingClassifier(sklearn.base.ClassifierMixin, sklearn.ensemble._voting._BaseVoting)\n",
      "        sklearn.ensemble._voting.VotingRegressor(sklearn.base.RegressorMixin, sklearn.ensemble._voting._BaseVoting)\n",
      "    sklearn.ensemble._weight_boosting.BaseWeightBoosting(sklearn.ensemble._base.BaseEnsemble)\n",
      "        sklearn.ensemble._weight_boosting.AdaBoostClassifier(sklearn.base.ClassifierMixin, sklearn.ensemble._weight_boosting.BaseWeightBoosting)\n",
      "        sklearn.ensemble._weight_boosting.AdaBoostRegressor(sklearn.base.RegressorMixin, sklearn.ensemble._weight_boosting.BaseWeightBoosting)\n",
      "    \n",
      "    class AdaBoostClassifier(sklearn.base.ClassifierMixin, BaseWeightBoosting)\n",
      "     |  AdaBoostClassifier(base_estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)\n",
      "     |  \n",
      "     |  An AdaBoost classifier.\n",
      "     |  \n",
      "     |  An AdaBoost [1] classifier is a meta-estimator that begins by fitting a\n",
      "     |  classifier on the original dataset and then fits additional copies of the\n",
      "     |  classifier on the same dataset but where the weights of incorrectly\n",
      "     |  classified instances are adjusted such that subsequent classifiers focus\n",
      "     |  more on difficult cases.\n",
      "     |  \n",
      "     |  This class implements the algorithm known as AdaBoost-SAMME [2].\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <adaboost>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.14\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  base_estimator : object, default=None\n",
      "     |      The base estimator from which the boosted ensemble is built.\n",
      "     |      Support for sample weighting is required, as well as proper\n",
      "     |      ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n",
      "     |      the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`\n",
      "     |      initialized with `max_depth=1`.\n",
      "     |  \n",
      "     |  n_estimators : int, default=50\n",
      "     |      The maximum number of estimators at which boosting is terminated.\n",
      "     |      In case of perfect fit, the learning procedure is stopped early.\n",
      "     |  \n",
      "     |  learning_rate : float, default=1.\n",
      "     |      Weight applied to each classifier at each boosting iteration. A higher\n",
      "     |      learning rate increases the contribution of each classifier. There is\n",
      "     |      a trade-off between the `learning_rate` and `n_estimators` parameters.\n",
      "     |  \n",
      "     |  algorithm : {'SAMME', 'SAMME.R'}, default='SAMME.R'\n",
      "     |      If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n",
      "     |      ``base_estimator`` must support calculation of class probabilities.\n",
      "     |      If 'SAMME' then use the SAMME discrete boosting algorithm.\n",
      "     |      The SAMME.R algorithm typically converges faster than SAMME,\n",
      "     |      achieving a lower test error with fewer boosting iterations.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the random seed given at each `base_estimator` at each\n",
      "     |      boosting iteration.\n",
      "     |      Thus, it is only used when `base_estimator` exposes a `random_state`.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  base_estimator_ : estimator\n",
      "     |      The base estimator from which the ensemble is grown.\n",
      "     |  \n",
      "     |  estimators_ : list of classifiers\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The classes labels.\n",
      "     |  \n",
      "     |  n_classes_ : int\n",
      "     |      The number of classes.\n",
      "     |  \n",
      "     |  estimator_weights_ : ndarray of floats\n",
      "     |      Weights for each estimator in the boosted ensemble.\n",
      "     |  \n",
      "     |  estimator_errors_ : ndarray of floats\n",
      "     |      Classification error for each estimator in the boosted\n",
      "     |      ensemble.\n",
      "     |  \n",
      "     |  feature_importances_ : ndarray of shape (n_features,)\n",
      "     |      The impurity-based feature importances if supported by the\n",
      "     |      ``base_estimator`` (when based on decision trees).\n",
      "     |  \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  AdaBoostRegressor : An AdaBoost regressor that begins by fitting a\n",
      "     |      regressor on the original dataset and then fits additional copies of\n",
      "     |      the regressor on the same dataset but where the weights of instances\n",
      "     |      are adjusted according to the error of the current prediction.\n",
      "     |  \n",
      "     |  GradientBoostingClassifier : GB builds an additive model in a forward\n",
      "     |      stage-wise fashion. Regression trees are fit on the negative gradient\n",
      "     |      of the binomial or multinomial deviance loss function. Binary\n",
      "     |      classification is a special case where only a single regression tree is\n",
      "     |      induced.\n",
      "     |  \n",
      "     |  sklearn.tree.DecisionTreeClassifier : A non-parametric supervised learning\n",
      "     |      method used for classification.\n",
      "     |      Creates a model that predicts the value of a target variable by\n",
      "     |      learning simple decision rules inferred from the data features.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n",
      "     |         on-Line Learning and an Application to Boosting\", 1995.\n",
      "     |  \n",
      "     |  .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.ensemble import AdaBoostClassifier\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      "     |  ...                            n_informative=2, n_redundant=0,\n",
      "     |  ...                            random_state=0, shuffle=False)\n",
      "     |  >>> clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  AdaBoostClassifier(n_estimators=100, random_state=0)\n",
      "     |  >>> clf.predict([[0, 0, 0, 0]])\n",
      "     |  array([1])\n",
      "     |  >>> clf.score(X, y)\n",
      "     |  0.983...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AdaBoostClassifier\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      BaseWeightBoosting\n",
      "     |      sklearn.ensemble._base.BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Compute the decision function of ``X``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : ndarray of shape of (n_samples, k)\n",
      "     |          The decision function of the input samples. The order of\n",
      "     |          outputs is the same of that of the :term:`classes_` attribute.\n",
      "     |          Binary classification is a special cases with ``k == 1``,\n",
      "     |          otherwise ``k==n_classes``. For binary classification,\n",
      "     |          values closer to -1 or 1 mean more like the first or second\n",
      "     |          class in ``classes_``, respectively.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a boosted classifier from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target values (class labels).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, the sample weights are initialized to\n",
      "     |          ``1 / n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict classes for X.\n",
      "     |      \n",
      "     |      The predicted class of an input sample is computed as the weighted mean\n",
      "     |      prediction of the classifiers in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          The predicted classes.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class log-probabilities of an input sample is computed as\n",
      "     |      the weighted mean predicted class log-probabilities of the classifiers\n",
      "     |      in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray of shape (n_samples, n_classes)\n",
      "     |          The class probabilities of the input samples. The order of\n",
      "     |          outputs is the same of that of the :term:`classes_` attribute.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class probabilities of an input sample is computed as\n",
      "     |      the weighted mean predicted class probabilities of the classifiers\n",
      "     |      in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray of shape (n_samples, n_classes)\n",
      "     |          The class probabilities of the input samples. The order of\n",
      "     |          outputs is the same of that of the :term:`classes_` attribute.\n",
      "     |  \n",
      "     |  staged_decision_function(self, X)\n",
      "     |      Compute decision function of ``X`` for each boosting iteration.\n",
      "     |      \n",
      "     |      This method allows monitoring (i.e. determine error on testing set)\n",
      "     |      after each boosting iteration.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      score : generator of ndarray of shape (n_samples, k)\n",
      "     |          The decision function of the input samples. The order of\n",
      "     |          outputs is the same of that of the :term:`classes_` attribute.\n",
      "     |          Binary classification is a special cases with ``k == 1``,\n",
      "     |          otherwise ``k==n_classes``. For binary classification,\n",
      "     |          values closer to -1 or 1 mean more like the first or second\n",
      "     |          class in ``classes_``, respectively.\n",
      "     |  \n",
      "     |  staged_predict(self, X)\n",
      "     |      Return staged predictions for X.\n",
      "     |      \n",
      "     |      The predicted class of an input sample is computed as the weighted mean\n",
      "     |      prediction of the classifiers in the ensemble.\n",
      "     |      \n",
      "     |      This generator method yields the ensemble prediction after each\n",
      "     |      iteration of boosting and therefore allows monitoring, such as to\n",
      "     |      determine the prediction on a test set after each boost.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          The input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      y : generator of ndarray of shape (n_samples,)\n",
      "     |          The predicted classes.\n",
      "     |  \n",
      "     |  staged_predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class probabilities of an input sample is computed as\n",
      "     |      the weighted mean predicted class probabilities of the classifiers\n",
      "     |      in the ensemble.\n",
      "     |      \n",
      "     |      This generator method yields the ensemble predicted class probabilities\n",
      "     |      after each iteration of boosting and therefore allows monitoring, such\n",
      "     |      as to determine the predicted class probabilities on a test set after\n",
      "     |      each boost.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      -------\n",
      "     |      p : generator of ndarray of shape (n_samples,)\n",
      "     |          The class probabilities of the input samples. The order of\n",
      "     |          outputs is the same of that of the :term:`classes_` attribute.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseWeightBoosting:\n",
      "     |  \n",
      "     |  staged_score(self, X, y, sample_weight=None)\n",
      "     |      Return staged scores for X, y.\n",
      "     |      \n",
      "     |      This generator method yields the ensemble score after each iteration of\n",
      "     |      boosting and therefore allows monitoring, such as to determine the\n",
      "     |      score on a test set after each boost.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      z : float\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseWeightBoosting:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      The impurity-based feature importances.\n",
      "     |      \n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |      \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : ndarray of shape (n_features,)\n",
      "     |          The feature importances.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class AdaBoostRegressor(sklearn.base.RegressorMixin, BaseWeightBoosting)\n",
      "     |  AdaBoostRegressor(base_estimator=None, *, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)\n",
      "     |  \n",
      "     |  An AdaBoost regressor.\n",
      "     |  \n",
      "     |  An AdaBoost [1] regressor is a meta-estimator that begins by fitting a\n",
      "     |  regressor on the original dataset and then fits additional copies of the\n",
      "     |  regressor on the same dataset but where the weights of instances are\n",
      "     |  adjusted according to the error of the current prediction. As such,\n",
      "     |  subsequent regressors focus more on difficult cases.\n",
      "     |  \n",
      "     |  This class implements the algorithm known as AdaBoost.R2 [2].\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <adaboost>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.14\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  base_estimator : object, default=None\n",
      "     |      The base estimator from which the boosted ensemble is built.\n",
      "     |      If ``None``, then the base estimator is\n",
      "     |      :class:`~sklearn.tree.DecisionTreeRegressor` initialized with\n",
      "     |      `max_depth=3`.\n",
      "     |  \n",
      "     |  n_estimators : int, default=50\n",
      "     |      The maximum number of estimators at which boosting is terminated.\n",
      "     |      In case of perfect fit, the learning procedure is stopped early.\n",
      "     |  \n",
      "     |  learning_rate : float, default=1.\n",
      "     |      Weight applied to each classifier at each boosting iteration. A higher\n",
      "     |      learning rate increases the contribution of each classifier. There is\n",
      "     |      a trade-off between the `learning_rate` and `n_estimators` parameters.\n",
      "     |  \n",
      "     |  loss : {'linear', 'square', 'exponential'}, default='linear'\n",
      "     |      The loss function to use when updating the weights after each\n",
      "     |      boosting iteration.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the random seed given at each `base_estimator` at each\n",
      "     |      boosting iteration.\n",
      "     |      Thus, it is only used when `base_estimator` exposes a `random_state`.\n",
      "     |      In addition, it controls the bootstrap of the weights used to train the\n",
      "     |      `base_estimator` at each boosting iteration.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  base_estimator_ : estimator\n",
      "     |      The base estimator from which the ensemble is grown.\n",
      "     |  \n",
      "     |  estimators_ : list of classifiers\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  estimator_weights_ : ndarray of floats\n",
      "     |      Weights for each estimator in the boosted ensemble.\n",
      "     |  \n",
      "     |  estimator_errors_ : ndarray of floats\n",
      "     |      Regression error for each estimator in the boosted ensemble.\n",
      "     |  \n",
      "     |  feature_importances_ : ndarray of shape (n_features,)\n",
      "     |      The impurity-based feature importances if supported by the\n",
      "     |      ``base_estimator`` (when based on decision trees).\n",
      "     |  \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.ensemble import AdaBoostRegressor\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      "     |  ...                        random_state=0, shuffle=False)\n",
      "     |  >>> regr = AdaBoostRegressor(random_state=0, n_estimators=100)\n",
      "     |  >>> regr.fit(X, y)\n",
      "     |  AdaBoostRegressor(n_estimators=100, random_state=0)\n",
      "     |  >>> regr.predict([[0, 0, 0, 0]])\n",
      "     |  array([4.7972...])\n",
      "     |  >>> regr.score(X, y)\n",
      "     |  0.9771...\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  AdaBoostClassifier, GradientBoostingRegressor,\n",
      "     |  sklearn.tree.DecisionTreeRegressor\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n",
      "     |         on-Line Learning and an Application to Boosting\", 1995.\n",
      "     |  \n",
      "     |  .. [2] H. Drucker, \"Improving Regressors using Boosting Techniques\", 1997.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AdaBoostRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      BaseWeightBoosting\n",
      "     |      sklearn.ensemble._base.BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_estimator=None, *, n_estimators=50, learning_rate=1.0, loss='linear', random_state=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a boosted regressor from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target values (real numbers).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, the sample weights are initialized to\n",
      "     |          1 / n_samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict regression value for X.\n",
      "     |      \n",
      "     |      The predicted regression value of an input sample is computed\n",
      "     |      as the weighted median prediction of the classifiers in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          The predicted regression values.\n",
      "     |  \n",
      "     |  staged_predict(self, X)\n",
      "     |      Return staged predictions for X.\n",
      "     |      \n",
      "     |      The predicted regression value of an input sample is computed\n",
      "     |      as the weighted median prediction of the classifiers in the ensemble.\n",
      "     |      \n",
      "     |      This generator method yields the ensemble prediction after each\n",
      "     |      iteration of boosting and therefore allows monitoring, such as to\n",
      "     |      determine the prediction on a test set after each boost.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      -------\n",
      "     |      y : generator of ndarray of shape (n_samples,)\n",
      "     |          The predicted regression values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseWeightBoosting:\n",
      "     |  \n",
      "     |  staged_score(self, X, y, sample_weight=None)\n",
      "     |      Return staged scores for X, y.\n",
      "     |      \n",
      "     |      This generator method yields the ensemble score after each iteration of\n",
      "     |      boosting and therefore allows monitoring, such as to determine the\n",
      "     |      score on a test set after each boost.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      "     |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Yields\n",
      "     |      ------\n",
      "     |      z : float\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseWeightBoosting:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      The impurity-based feature importances.\n",
      "     |      \n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |      \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : ndarray of shape (n_features,)\n",
      "     |          The feature importances.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class BaggingClassifier(sklearn.base.ClassifierMixin, BaseBagging)\n",
      "     |  BaggingClassifier(base_estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)\n",
      "     |  \n",
      "     |  A Bagging classifier.\n",
      "     |  \n",
      "     |  A Bagging classifier is an ensemble meta-estimator that fits base\n",
      "     |  classifiers each on random subsets of the original dataset and then\n",
      "     |  aggregate their individual predictions (either by voting or by averaging)\n",
      "     |  to form a final prediction. Such a meta-estimator can typically be used as\n",
      "     |  a way to reduce the variance of a black-box estimator (e.g., a decision\n",
      "     |  tree), by introducing randomization into its construction procedure and\n",
      "     |  then making an ensemble out of it.\n",
      "     |  \n",
      "     |  This algorithm encompasses several works from the literature. When random\n",
      "     |  subsets of the dataset are drawn as random subsets of the samples, then\n",
      "     |  this algorithm is known as Pasting [1]_. If samples are drawn with\n",
      "     |  replacement, then the method is known as Bagging [2]_. When random subsets\n",
      "     |  of the dataset are drawn as random subsets of the features, then the method\n",
      "     |  is known as Random Subspaces [3]_. Finally, when base estimators are built\n",
      "     |  on subsets of both samples and features, then the method is known as\n",
      "     |  Random Patches [4]_.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <bagging>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.15\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  base_estimator : object, default=None\n",
      "     |      The base estimator to fit on random subsets of the dataset.\n",
      "     |      If None, then the base estimator is a\n",
      "     |      :class:`~sklearn.tree.DecisionTreeClassifier`.\n",
      "     |  \n",
      "     |  n_estimators : int, default=10\n",
      "     |      The number of base estimators in the ensemble.\n",
      "     |  \n",
      "     |  max_samples : int or float, default=1.0\n",
      "     |      The number of samples to draw from X to train each base estimator (with\n",
      "     |      replacement by default, see `bootstrap` for more details).\n",
      "     |  \n",
      "     |      - If int, then draw `max_samples` samples.\n",
      "     |      - If float, then draw `max_samples * X.shape[0]` samples.\n",
      "     |  \n",
      "     |  max_features : int or float, default=1.0\n",
      "     |      The number of features to draw from X to train each base estimator (\n",
      "     |      without replacement by default, see `bootstrap_features` for more\n",
      "     |      details).\n",
      "     |  \n",
      "     |      - If int, then draw `max_features` features.\n",
      "     |      - If float, then draw `max_features * X.shape[1]` features.\n",
      "     |  \n",
      "     |  bootstrap : bool, default=True\n",
      "     |      Whether samples are drawn with replacement. If False, sampling\n",
      "     |      without replacement is performed.\n",
      "     |  \n",
      "     |  bootstrap_features : bool, default=False\n",
      "     |      Whether features are drawn with replacement.\n",
      "     |  \n",
      "     |  oob_score : bool, default=False\n",
      "     |      Whether to use out-of-bag samples to estimate\n",
      "     |      the generalization error. Only available if bootstrap=True.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit\n",
      "     |      a whole new ensemble. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.17\n",
      "     |         *warm_start* constructor parameter.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel for both :meth:`fit` and\n",
      "     |      :meth:`predict`. ``None`` means 1 unless in a\n",
      "     |      :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
      "     |      processors. See :term:`Glossary <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the random resampling of the original dataset\n",
      "     |      (sample wise and feature wise).\n",
      "     |      If the base estimator accepts a `random_state` attribute, a different\n",
      "     |      seed is generated for each instance in the ensemble.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Controls the verbosity when fitting and predicting.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  base_estimator_ : estimator\n",
      "     |      The base estimator from which the ensemble is grown.\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of features when :meth:`fit` is performed.\n",
      "     |  \n",
      "     |  estimators_ : list of estimators\n",
      "     |      The collection of fitted base estimators.\n",
      "     |  \n",
      "     |  estimators_samples_ : list of arrays\n",
      "     |      The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "     |      estimator. Each subset is defined by an array of the indices selected.\n",
      "     |  \n",
      "     |  estimators_features_ : list of arrays\n",
      "     |      The subset of drawn features for each base estimator.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The classes labels.\n",
      "     |  \n",
      "     |  n_classes_ : int or list\n",
      "     |      The number of classes.\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      "     |      This attribute exists only when ``oob_score`` is True.\n",
      "     |  \n",
      "     |  oob_decision_function_ : ndarray of shape (n_samples, n_classes)\n",
      "     |      Decision function computed with out-of-bag estimate on the training\n",
      "     |      set. If n_estimators is small it might be possible that a data point\n",
      "     |      was never left out during the bootstrap. In this case,\n",
      "     |      `oob_decision_function_` might contain NaN. This attribute exists\n",
      "     |      only when ``oob_score`` is True.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.svm import SVC\n",
      "     |  >>> from sklearn.ensemble import BaggingClassifier\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> X, y = make_classification(n_samples=100, n_features=4,\n",
      "     |  ...                            n_informative=2, n_redundant=0,\n",
      "     |  ...                            random_state=0, shuffle=False)\n",
      "     |  >>> clf = BaggingClassifier(base_estimator=SVC(),\n",
      "     |  ...                         n_estimators=10, random_state=0).fit(X, y)\n",
      "     |  >>> clf.predict([[0, 0, 0, 0]])\n",
      "     |  array([1])\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] L. Breiman, \"Pasting small votes for classification in large\n",
      "     |         databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n",
      "     |  \n",
      "     |  .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n",
      "     |         1996.\n",
      "     |  \n",
      "     |  .. [3] T. Ho, \"The random subspace method for constructing decision\n",
      "     |         forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n",
      "     |         1998.\n",
      "     |  \n",
      "     |  .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n",
      "     |         Learning and Knowledge Discovery in Databases, 346-361, 2012.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BaggingClassifier\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      BaseBagging\n",
      "     |      sklearn.ensemble._base.BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Average of the decision functions of the base classifiers.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : ndarray of shape (n_samples, k)\n",
      "     |          The decision function of the input samples. The columns correspond\n",
      "     |          to the classes in sorted order, as they appear in the attribute\n",
      "     |          ``classes_``. Regression and binary classification are special\n",
      "     |          cases with ``k == 1``, otherwise ``k==n_classes``.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class for X.\n",
      "     |      \n",
      "     |      The predicted class of an input sample is computed as the class with\n",
      "     |      the highest mean predicted probability. If base estimators do not\n",
      "     |      implement a ``predict_proba`` method, then it resorts to voting.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          The predicted classes.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class log-probabilities of an input sample is computed as\n",
      "     |      the log of the mean predicted class probabilities of the base\n",
      "     |      estimators in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray of shape (n_samples, n_classes)\n",
      "     |          The class log-probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class probabilities of an input sample is computed as\n",
      "     |      the mean predicted class probabilities of the base estimators in the\n",
      "     |      ensemble. If base estimators do not implement a ``predict_proba``\n",
      "     |      method, then it resorts to voting and the predicted class probabilities\n",
      "     |      of an input sample represents the proportion of estimators predicting\n",
      "     |      each class.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray of shape (n_samples, n_classes)\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseBagging:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a Bagging ensemble of estimators from the training\n",
      "     |         set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted.\n",
      "     |          Note that this is supported only if the base estimator supports\n",
      "     |          sample weighting.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseBagging:\n",
      "     |  \n",
      "     |  estimators_samples_\n",
      "     |      The subset of drawn samples for each base estimator.\n",
      "     |      \n",
      "     |      Returns a dynamically generated list of indices identifying\n",
      "     |      the samples used for fitting each member of the ensemble, i.e.,\n",
      "     |      the in-bag samples.\n",
      "     |      \n",
      "     |      Note: the list is re-created at each call to the property in order\n",
      "     |      to reduce the object memory footprint by not storing the sampling\n",
      "     |      data. Thus fetching the property may be slower than expected.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class BaggingRegressor(sklearn.base.RegressorMixin, BaseBagging)\n",
      "     |  BaggingRegressor(base_estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)\n",
      "     |  \n",
      "     |  A Bagging regressor.\n",
      "     |  \n",
      "     |  A Bagging regressor is an ensemble meta-estimator that fits base\n",
      "     |  regressors each on random subsets of the original dataset and then\n",
      "     |  aggregate their individual predictions (either by voting or by averaging)\n",
      "     |  to form a final prediction. Such a meta-estimator can typically be used as\n",
      "     |  a way to reduce the variance of a black-box estimator (e.g., a decision\n",
      "     |  tree), by introducing randomization into its construction procedure and\n",
      "     |  then making an ensemble out of it.\n",
      "     |  \n",
      "     |  This algorithm encompasses several works from the literature. When random\n",
      "     |  subsets of the dataset are drawn as random subsets of the samples, then\n",
      "     |  this algorithm is known as Pasting [1]_. If samples are drawn with\n",
      "     |  replacement, then the method is known as Bagging [2]_. When random subsets\n",
      "     |  of the dataset are drawn as random subsets of the features, then the method\n",
      "     |  is known as Random Subspaces [3]_. Finally, when base estimators are built\n",
      "     |  on subsets of both samples and features, then the method is known as\n",
      "     |  Random Patches [4]_.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <bagging>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.15\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  base_estimator : object, default=None\n",
      "     |      The base estimator to fit on random subsets of the dataset.\n",
      "     |      If None, then the base estimator is a\n",
      "     |      :class:`~sklearn.tree.DecisionTreeRegressor`.\n",
      "     |  \n",
      "     |  n_estimators : int, default=10\n",
      "     |      The number of base estimators in the ensemble.\n",
      "     |  \n",
      "     |  max_samples : int or float, default=1.0\n",
      "     |      The number of samples to draw from X to train each base estimator (with\n",
      "     |      replacement by default, see `bootstrap` for more details).\n",
      "     |  \n",
      "     |      - If int, then draw `max_samples` samples.\n",
      "     |      - If float, then draw `max_samples * X.shape[0]` samples.\n",
      "     |  \n",
      "     |  max_features : int or float, default=1.0\n",
      "     |      The number of features to draw from X to train each base estimator (\n",
      "     |      without replacement by default, see `bootstrap_features` for more\n",
      "     |      details).\n",
      "     |  \n",
      "     |      - If int, then draw `max_features` features.\n",
      "     |      - If float, then draw `max_features * X.shape[1]` features.\n",
      "     |  \n",
      "     |  bootstrap : bool, default=True\n",
      "     |      Whether samples are drawn with replacement. If False, sampling\n",
      "     |      without replacement is performed.\n",
      "     |  \n",
      "     |  bootstrap_features : bool, default=False\n",
      "     |      Whether features are drawn with replacement.\n",
      "     |  \n",
      "     |  oob_score : bool, default=False\n",
      "     |      Whether to use out-of-bag samples to estimate\n",
      "     |      the generalization error. Only available if bootstrap=True.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to True, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit\n",
      "     |      a whole new ensemble. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel for both :meth:`fit` and\n",
      "     |      :meth:`predict`. ``None`` means 1 unless in a\n",
      "     |      :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
      "     |      processors. See :term:`Glossary <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the random resampling of the original dataset\n",
      "     |      (sample wise and feature wise).\n",
      "     |      If the base estimator accepts a `random_state` attribute, a different\n",
      "     |      seed is generated for each instance in the ensemble.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Controls the verbosity when fitting and predicting.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  base_estimator_ : estimator\n",
      "     |      The base estimator from which the ensemble is grown.\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of features when :meth:`fit` is performed.\n",
      "     |  \n",
      "     |  estimators_ : list of estimators\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  estimators_samples_ : list of arrays\n",
      "     |      The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "     |      estimator. Each subset is defined by an array of the indices selected.\n",
      "     |  \n",
      "     |  estimators_features_ : list of arrays\n",
      "     |      The subset of drawn features for each base estimator.\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      "     |      This attribute exists only when ``oob_score`` is True.\n",
      "     |  \n",
      "     |  oob_prediction_ : ndarray of shape (n_samples,)\n",
      "     |      Prediction computed with out-of-bag estimate on the training\n",
      "     |      set. If n_estimators is small it might be possible that a data point\n",
      "     |      was never left out during the bootstrap. In this case,\n",
      "     |      `oob_prediction_` might contain NaN. This attribute exists only\n",
      "     |      when ``oob_score`` is True.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.svm import SVR\n",
      "     |  >>> from sklearn.ensemble import BaggingRegressor\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(n_samples=100, n_features=4,\n",
      "     |  ...                        n_informative=2, n_targets=1,\n",
      "     |  ...                        random_state=0, shuffle=False)\n",
      "     |  >>> regr = BaggingRegressor(base_estimator=SVR(),\n",
      "     |  ...                         n_estimators=10, random_state=0).fit(X, y)\n",
      "     |  >>> regr.predict([[0, 0, 0, 0]])\n",
      "     |  array([-2.8720...])\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  .. [1] L. Breiman, \"Pasting small votes for classification in large\n",
      "     |         databases and on-line\", Machine Learning, 36(1), 85-103, 1999.\n",
      "     |  \n",
      "     |  .. [2] L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140,\n",
      "     |         1996.\n",
      "     |  \n",
      "     |  .. [3] T. Ho, \"The random subspace method for constructing decision\n",
      "     |         forests\", Pattern Analysis and Machine Intelligence, 20(8), 832-844,\n",
      "     |         1998.\n",
      "     |  \n",
      "     |  .. [4] G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine\n",
      "     |         Learning and Knowledge Discovery in Databases, 346-361, 2012.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BaggingRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      BaseBagging\n",
      "     |      sklearn.ensemble._base.BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, base_estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict regression target for X.\n",
      "     |      \n",
      "     |      The predicted regression target of an input sample is computed as the\n",
      "     |      mean predicted regression targets of the estimators in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseBagging:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a Bagging ensemble of estimators from the training\n",
      "     |         set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Sparse matrices are accepted only if\n",
      "     |          they are supported by the base estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted.\n",
      "     |          Note that this is supported only if the base estimator supports\n",
      "     |          sample weighting.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseBagging:\n",
      "     |  \n",
      "     |  estimators_samples_\n",
      "     |      The subset of drawn samples for each base estimator.\n",
      "     |      \n",
      "     |      Returns a dynamically generated list of indices identifying\n",
      "     |      the samples used for fitting each member of the ensemble, i.e.,\n",
      "     |      the in-bag samples.\n",
      "     |      \n",
      "     |      Note: the list is re-created at each call to the property in order\n",
      "     |      to reduce the object memory footprint by not storing the sampling\n",
      "     |      data. Thus fetching the property may be slower than expected.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class BaseEnsemble(sklearn.base.MetaEstimatorMixin, sklearn.base.BaseEstimator)\n",
      "     |  BaseEnsemble(base_estimator, *, n_estimators=10, estimator_params=())\n",
      "     |  \n",
      "     |  Base class for all ensemble classes.\n",
      "     |  \n",
      "     |  Warning: This class should not be used directly. Use derived classes\n",
      "     |  instead.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  base_estimator : object\n",
      "     |      The base estimator from which the ensemble is built.\n",
      "     |  \n",
      "     |  n_estimators : int, default=10\n",
      "     |      The number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  estimator_params : list of str, default=tuple()\n",
      "     |      The list of attributes to use as parameters when instantiating a\n",
      "     |      new base estimator. If none are given, default parameters are used.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  base_estimator_ : estimator\n",
      "     |      The base estimator from which the ensemble is grown.\n",
      "     |  \n",
      "     |  estimators_ : list of estimators\n",
      "     |      The collection of fitted base estimators.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __init__(self, base_estimator, *, n_estimators=10, estimator_params=())\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__init__'})\n",
      "     |  \n",
      "     |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MetaEstimatorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class ExtraTreesClassifier(ForestClassifier)\n",
      "     |  ExtraTreesClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      "     |  \n",
      "     |  An extra-trees classifier.\n",
      "     |  \n",
      "     |  This class implements a meta estimator that fits a number of\n",
      "     |  randomized decision trees (a.k.a. extra-trees) on various sub-samples\n",
      "     |  of the dataset and uses averaging to improve the predictive accuracy\n",
      "     |  and control over-fitting.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <forest>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_estimators : int, default=100\n",
      "     |      The number of trees in the forest.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |         The default value of ``n_estimators`` changed from 10 to 100\n",
      "     |         in 0.22.\n",
      "     |  \n",
      "     |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
      "     |      The function to measure the quality of a split. Supported criteria are\n",
      "     |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      "     |  \n",
      "     |  max_depth : int, default=None\n",
      "     |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int or float, default=2\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a fraction and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int or float, default=1\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a fraction and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, default=0.0\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a fraction and\n",
      "     |        `round(max_features * n_features)` features are considered at each\n",
      "     |        split.\n",
      "     |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int, default=None\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, default=0.0\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  min_impurity_split : float, default=None\n",
      "     |      Threshold for early stopping in tree growth. A node will split\n",
      "     |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.19\n",
      "     |         ``min_impurity_split`` has been deprecated in favor of\n",
      "     |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      "     |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      "     |         will be removed in 1.0 (renaming of 0.25).\n",
      "     |         Use ``min_impurity_decrease`` instead.\n",
      "     |  \n",
      "     |  bootstrap : bool, default=False\n",
      "     |      Whether bootstrap samples are used when building trees. If False, the\n",
      "     |      whole dataset is used to build each tree.\n",
      "     |  \n",
      "     |  oob_score : bool, default=False\n",
      "     |      Whether to use out-of-bag samples to estimate the generalization score.\n",
      "     |      Only available if bootstrap=True.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      "     |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      "     |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      "     |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      "     |      <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls 3 sources of randomness:\n",
      "     |  \n",
      "     |      - the bootstrapping of the samples used when building trees\n",
      "     |        (if ``bootstrap=True``)\n",
      "     |      - the sampling of the features to consider when looking for the best\n",
      "     |        split at each node (if ``max_features < n_features``)\n",
      "     |      - the draw of the splits for each of the `max_features`\n",
      "     |  \n",
      "     |      See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Controls the verbosity when fitting and predicting.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "     |      new forest. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one. For\n",
      "     |      multi-output problems, a list of dicts can be provided in the same\n",
      "     |      order as the columns of y.\n",
      "     |  \n",
      "     |      Note that for multioutput (including multilabel) weights should be\n",
      "     |      defined for each class of every column in its own dict. For example,\n",
      "     |      for four-class multilabel classification weights should be\n",
      "     |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      "     |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      "     |      weights are computed based on the bootstrap sample for every tree\n",
      "     |      grown.\n",
      "     |  \n",
      "     |      For multi-output, the weights of each column of y will be multiplied.\n",
      "     |  \n",
      "     |      Note that these weights will be multiplied with sample_weight (passed\n",
      "     |      through the fit method) if sample_weight is specified.\n",
      "     |  \n",
      "     |  ccp_alpha : non-negative float, default=0.0\n",
      "     |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "     |      subtree with the largest cost complexity that is smaller than\n",
      "     |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "     |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  max_samples : int or float, default=None\n",
      "     |      If bootstrap is True, the number of samples to draw from X\n",
      "     |      to train each base estimator.\n",
      "     |  \n",
      "     |      - If None (default), then draw `X.shape[0]` samples.\n",
      "     |      - If int, then draw `max_samples` samples.\n",
      "     |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
      "     |        `max_samples` should be in the interval `(0, 1)`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  base_estimator_ : ExtraTreesClassifier\n",
      "     |      The child estimator template used to create the collection of fitted\n",
      "     |      sub-estimators.\n",
      "     |  \n",
      "     |  estimators_ : list of DecisionTreeClassifier\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
      "     |      The classes labels (single output problem), or a list of arrays of\n",
      "     |      class labels (multi-output problem).\n",
      "     |  \n",
      "     |  n_classes_ : int or list\n",
      "     |      The number of classes (single output problem), or a list containing the\n",
      "     |      number of classes for each output (multi-output problem).\n",
      "     |  \n",
      "     |  feature_importances_ : ndarray of shape (n_features,)\n",
      "     |      The impurity-based feature importances.\n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |  \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of features when ``fit`` is performed.\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      The number of outputs when ``fit`` is performed.\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      "     |      This attribute exists only when ``oob_score`` is True.\n",
      "     |  \n",
      "     |  oob_decision_function_ : ndarray of shape (n_samples, n_classes)\n",
      "     |      Decision function computed with out-of-bag estimate on the training\n",
      "     |      set. If n_estimators is small it might be possible that a data point\n",
      "     |      was never left out during the bootstrap. In this case,\n",
      "     |      `oob_decision_function_` might contain NaN. This attribute exists\n",
      "     |      only when ``oob_score`` is True.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  sklearn.tree.ExtraTreeClassifier : Base classifier for this ensemble.\n",
      "     |  RandomForestClassifier : Ensemble Classifier based on trees with optimal\n",
      "     |      splits.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The default values for the parameters controlling the size of the trees\n",
      "     |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "     |  unpruned trees which can potentially be very large on some data sets. To\n",
      "     |  reduce memory consumption, the complexity and size of the trees should be\n",
      "     |  controlled by setting those parameter values.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
      "     |         trees\", Machine Learning, 63(1), 3-42, 2006.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.ensemble import ExtraTreesClassifier\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> X, y = make_classification(n_features=4, random_state=0)\n",
      "     |  >>> clf = ExtraTreesClassifier(n_estimators=100, random_state=0)\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  ExtraTreesClassifier(random_state=0)\n",
      "     |  >>> clf.predict([[0, 0, 0, 0]])\n",
      "     |  array([1])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ExtraTreesClassifier\n",
      "     |      ForestClassifier\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      BaseForest\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.ensemble._base.BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ForestClassifier:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class for X.\n",
      "     |      \n",
      "     |      The predicted class of an input sample is a vote by the trees in\n",
      "     |      the forest, weighted by their probability estimates. That is,\n",
      "     |      the predicted class is the one with highest mean probability\n",
      "     |      estimate across the trees.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The predicted classes.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class log-probabilities of an input sample is computed as\n",
      "     |      the log of the mean predicted class probabilities of the trees in the\n",
      "     |      forest.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray of shape (n_samples, n_classes), or a list of n_outputs\n",
      "     |          such arrays if n_outputs > 1.\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class probabilities of an input sample are computed as\n",
      "     |      the mean predicted class probabilities of the trees in the forest.\n",
      "     |      The class probability of a single tree is the fraction of samples of\n",
      "     |      the same class in a leaf.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray of shape (n_samples, n_classes), or a list of n_outputs\n",
      "     |          such arrays if n_outputs > 1.\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseForest:\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the forest to X, return leaf indices.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      "     |          For each datapoint x in X and for each tree in the forest,\n",
      "     |          return the index of the leaf x ends up in.\n",
      "     |  \n",
      "     |  decision_path(self, X)\n",
      "     |      Return the decision path in the forest.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      "     |          Return a node indicator matrix where non zero elements indicates\n",
      "     |          that the samples goes through the nodes. The matrix is of CSR\n",
      "     |          format.\n",
      "     |      \n",
      "     |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      "     |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      "     |          gives the indicator value for the i-th estimator.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a forest of trees from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Internally, its dtype will be converted\n",
      "     |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseForest:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      The impurity-based feature importances.\n",
      "     |      \n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |      \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : ndarray of shape (n_features,)\n",
      "     |          The values of this array sum to 1, unless all trees are single node\n",
      "     |          trees consisting of only the root node, in which case it will be an\n",
      "     |          array of zeros.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class ExtraTreesRegressor(ForestRegressor)\n",
      "     |  ExtraTreesRegressor(n_estimators=100, *, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)\n",
      "     |  \n",
      "     |  An extra-trees regressor.\n",
      "     |  \n",
      "     |  This class implements a meta estimator that fits a number of\n",
      "     |  randomized decision trees (a.k.a. extra-trees) on various sub-samples\n",
      "     |  of the dataset and uses averaging to improve the predictive accuracy\n",
      "     |  and control over-fitting.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <forest>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_estimators : int, default=100\n",
      "     |      The number of trees in the forest.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |         The default value of ``n_estimators`` changed from 10 to 100\n",
      "     |         in 0.22.\n",
      "     |  \n",
      "     |  criterion : {\"mse\", \"mae\"}, default=\"mse\"\n",
      "     |      The function to measure the quality of a split. Supported criteria\n",
      "     |      are \"mse\" for the mean squared error, which is equal to variance\n",
      "     |      reduction as feature selection criterion, and \"mae\" for the mean\n",
      "     |      absolute error.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |         Mean Absolute Error (MAE) criterion.\n",
      "     |  \n",
      "     |  max_depth : int, default=None\n",
      "     |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int or float, default=2\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a fraction and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int or float, default=1\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a fraction and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, default=0.0\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a fraction and\n",
      "     |        `round(max_features * n_features)` features are considered at each\n",
      "     |        split.\n",
      "     |      - If \"auto\", then `max_features=n_features`.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int, default=None\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, default=0.0\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  min_impurity_split : float, default=None\n",
      "     |      Threshold for early stopping in tree growth. A node will split\n",
      "     |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.19\n",
      "     |         ``min_impurity_split`` has been deprecated in favor of\n",
      "     |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      "     |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      "     |         will be removed in 1.0 (renaming of 0.25).\n",
      "     |         Use ``min_impurity_decrease`` instead.\n",
      "     |  \n",
      "     |  bootstrap : bool, default=False\n",
      "     |      Whether bootstrap samples are used when building trees. If False, the\n",
      "     |      whole dataset is used to build each tree.\n",
      "     |  \n",
      "     |  oob_score : bool, default=False\n",
      "     |      Whether to use out-of-bag samples to estimate the generalization score.\n",
      "     |      Only available if bootstrap=True.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      "     |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      "     |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      "     |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      "     |      <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls 3 sources of randomness:\n",
      "     |  \n",
      "     |      - the bootstrapping of the samples used when building trees\n",
      "     |        (if ``bootstrap=True``)\n",
      "     |      - the sampling of the features to consider when looking for the best\n",
      "     |        split at each node (if ``max_features < n_features``)\n",
      "     |      - the draw of the splits for each of the `max_features`\n",
      "     |  \n",
      "     |      See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Controls the verbosity when fitting and predicting.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "     |      new forest. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  ccp_alpha : non-negative float, default=0.0\n",
      "     |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "     |      subtree with the largest cost complexity that is smaller than\n",
      "     |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "     |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  max_samples : int or float, default=None\n",
      "     |      If bootstrap is True, the number of samples to draw from X\n",
      "     |      to train each base estimator.\n",
      "     |  \n",
      "     |      - If None (default), then draw `X.shape[0]` samples.\n",
      "     |      - If int, then draw `max_samples` samples.\n",
      "     |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
      "     |        `max_samples` should be in the interval `(0, 1)`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  base_estimator_ : ExtraTreeRegressor\n",
      "     |      The child estimator template used to create the collection of fitted\n",
      "     |      sub-estimators.\n",
      "     |  \n",
      "     |  estimators_ : list of DecisionTreeRegressor\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  feature_importances_ : ndarray of shape (n_features,)\n",
      "     |      The impurity-based feature importances.\n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |  \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of features.\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      The number of outputs.\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      "     |      This attribute exists only when ``oob_score`` is True.\n",
      "     |  \n",
      "     |  oob_prediction_ : ndarray of shape (n_samples,)\n",
      "     |      Prediction computed with out-of-bag estimate on the training set.\n",
      "     |      This attribute exists only when ``oob_score`` is True.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  sklearn.tree.ExtraTreeRegressor : Base estimator for this ensemble.\n",
      "     |  RandomForestRegressor : Ensemble regressor using trees with optimal splits.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The default values for the parameters controlling the size of the trees\n",
      "     |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "     |  unpruned trees which can potentially be very large on some data sets. To\n",
      "     |  reduce memory consumption, the complexity and size of the trees should be\n",
      "     |  controlled by setting those parameter values.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n",
      "     |         Machine Learning, 63(1), 3-42, 2006.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_diabetes\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> from sklearn.ensemble import ExtraTreesRegressor\n",
      "     |  >>> X, y = load_diabetes(return_X_y=True)\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |  ...     X, y, random_state=0)\n",
      "     |  >>> reg = ExtraTreesRegressor(n_estimators=100, random_state=0).fit(\n",
      "     |  ...    X_train, y_train)\n",
      "     |  >>> reg.score(X_test, y_test)\n",
      "     |  0.2708...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ExtraTreesRegressor\n",
      "     |      ForestRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      BaseForest\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.ensemble._base.BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_estimators=100, *, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=False, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ForestRegressor:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict regression target for X.\n",
      "     |      \n",
      "     |      The predicted regression target of an input sample is computed as the\n",
      "     |      mean predicted regression targets of the trees in the forest.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseForest:\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the forest to X, return leaf indices.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      "     |          For each datapoint x in X and for each tree in the forest,\n",
      "     |          return the index of the leaf x ends up in.\n",
      "     |  \n",
      "     |  decision_path(self, X)\n",
      "     |      Return the decision path in the forest.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      "     |          Return a node indicator matrix where non zero elements indicates\n",
      "     |          that the samples goes through the nodes. The matrix is of CSR\n",
      "     |          format.\n",
      "     |      \n",
      "     |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      "     |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      "     |          gives the indicator value for the i-th estimator.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a forest of trees from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Internally, its dtype will be converted\n",
      "     |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseForest:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      The impurity-based feature importances.\n",
      "     |      \n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |      \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : ndarray of shape (n_features,)\n",
      "     |          The values of this array sum to 1, unless all trees are single node\n",
      "     |          trees consisting of only the root node, in which case it will be an\n",
      "     |          array of zeros.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class GradientBoostingClassifier(sklearn.base.ClassifierMixin, BaseGradientBoosting)\n",
      "     |  GradientBoostingClassifier(*, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
      "     |  \n",
      "     |  Gradient Boosting for classification.\n",
      "     |  \n",
      "     |  GB builds an additive model in a\n",
      "     |  forward stage-wise fashion; it allows for the optimization of\n",
      "     |  arbitrary differentiable loss functions. In each stage ``n_classes_``\n",
      "     |  regression trees are fit on the negative gradient of the\n",
      "     |  binomial or multinomial deviance loss function. Binary classification\n",
      "     |  is a special case where only a single regression tree is induced.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <gradient_boosting>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  loss : {'deviance', 'exponential'}, default='deviance'\n",
      "     |      The loss function to be optimized. 'deviance' refers to\n",
      "     |      deviance (= logistic regression) for classification\n",
      "     |      with probabilistic outputs. For loss 'exponential' gradient\n",
      "     |      boosting recovers the AdaBoost algorithm.\n",
      "     |  \n",
      "     |  learning_rate : float, default=0.1\n",
      "     |      Learning rate shrinks the contribution of each tree by `learning_rate`.\n",
      "     |      There is a trade-off between learning_rate and n_estimators.\n",
      "     |  \n",
      "     |  n_estimators : int, default=100\n",
      "     |      The number of boosting stages to perform. Gradient boosting\n",
      "     |      is fairly robust to over-fitting so a large number usually\n",
      "     |      results in better performance.\n",
      "     |  \n",
      "     |  subsample : float, default=1.0\n",
      "     |      The fraction of samples to be used for fitting the individual base\n",
      "     |      learners. If smaller than 1.0 this results in Stochastic Gradient\n",
      "     |      Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
      "     |      Choosing `subsample < 1.0` leads to a reduction of variance\n",
      "     |      and an increase in bias.\n",
      "     |  \n",
      "     |  criterion : {'friedman_mse', 'mse', 'mae'}, default='friedman_mse'\n",
      "     |      The function to measure the quality of a split. Supported criteria\n",
      "     |      are 'friedman_mse' for the mean squared error with improvement\n",
      "     |      score by Friedman, 'mse' for mean squared error, and 'mae' for\n",
      "     |      the mean absolute error. The default value of 'friedman_mse' is\n",
      "     |      generally the best as it can provide a better approximation in\n",
      "     |      some cases.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      .. deprecated:: 0.24\n",
      "     |          `criterion='mae'` is deprecated and will be removed in version\n",
      "     |          1.1 (renaming of 0.26). Use `criterion='friedman_mse'` or `'mse'`\n",
      "     |          instead, as trees should use a least-square criterion in\n",
      "     |          Gradient Boosting.\n",
      "     |  \n",
      "     |  min_samples_split : int or float, default=2\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a fraction and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int or float, default=1\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a fraction and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, default=0.0\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_depth : int, default=3\n",
      "     |      The maximum depth of the individual regression estimators. The maximum\n",
      "     |      depth limits the number of nodes in the tree. Tune this parameter\n",
      "     |      for best performance; the best value depends on the interaction\n",
      "     |      of the input variables.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, default=0.0\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  min_impurity_split : float, default=None\n",
      "     |      Threshold for early stopping in tree growth. A node will split\n",
      "     |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.19\n",
      "     |         ``min_impurity_split`` has been deprecated in favor of\n",
      "     |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      "     |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      "     |         will be removed in 1.0 (renaming of 0.25).\n",
      "     |         Use ``min_impurity_decrease`` instead.\n",
      "     |  \n",
      "     |  init : estimator or 'zero', default=None\n",
      "     |      An estimator object that is used to compute the initial predictions.\n",
      "     |      ``init`` has to provide :meth:`fit` and :meth:`predict_proba`. If\n",
      "     |      'zero', the initial raw predictions are set to zero. By default, a\n",
      "     |      ``DummyEstimator`` predicting the classes priors is used.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the random seed given to each Tree estimator at each\n",
      "     |      boosting iteration.\n",
      "     |      In addition, it controls the random permutation of the features at\n",
      "     |      each split (see Notes for more details).\n",
      "     |      It also controls the random spliting of the training data to obtain a\n",
      "     |      validation set if `n_iter_no_change` is not None.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  max_features : {'auto', 'sqrt', 'log2'}, int or float, default=None\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a fraction and\n",
      "     |        `int(max_features * n_features)` features are considered at each\n",
      "     |        split.\n",
      "     |      - If 'auto', then `max_features=sqrt(n_features)`.\n",
      "     |      - If 'sqrt', then `max_features=sqrt(n_features)`.\n",
      "     |      - If 'log2', then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Choosing `max_features < n_features` leads to a reduction of variance\n",
      "     |      and an increase in bias.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Enable verbose output. If 1 then it prints progress and performance\n",
      "     |      once in a while (the more trees the lower the frequency). If greater\n",
      "     |      than 1 then it prints progress and performance for every tree.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int, default=None\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just erase the\n",
      "     |      previous solution. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  validation_fraction : float, default=0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if ``n_iter_no_change`` is set to an integer.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default=None\n",
      "     |      ``n_iter_no_change`` is used to decide if early stopping will be used\n",
      "     |      to terminate training when validation score is not improving. By\n",
      "     |      default it is set to None to disable early stopping. If set to a\n",
      "     |      number, it will set aside ``validation_fraction`` size of the training\n",
      "     |      data as validation and terminate training when validation score is not\n",
      "     |      improving in all of the previous ``n_iter_no_change`` numbers of\n",
      "     |      iterations. The split is stratified.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Tolerance for the early stopping. When the loss is not improving\n",
      "     |      by at least tol for ``n_iter_no_change`` iterations (if set to a\n",
      "     |      number), the training stops.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  ccp_alpha : non-negative float, default=0.0\n",
      "     |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "     |      subtree with the largest cost complexity that is smaller than\n",
      "     |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "     |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  n_estimators_ : int\n",
      "     |      The number of estimators as selected by early stopping (if\n",
      "     |      ``n_iter_no_change`` is specified). Otherwise it is set to\n",
      "     |      ``n_estimators``.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  feature_importances_ : ndarray of shape (n_features,)\n",
      "     |      The impurity-based feature importances.\n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |  \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |  \n",
      "     |  oob_improvement_ : ndarray of shape (n_estimators,)\n",
      "     |      The improvement in loss (= deviance) on the out-of-bag samples\n",
      "     |      relative to the previous iteration.\n",
      "     |      ``oob_improvement_[0]`` is the improvement in\n",
      "     |      loss of the first stage over the ``init`` estimator.\n",
      "     |      Only available if ``subsample < 1.0``\n",
      "     |  \n",
      "     |  train_score_ : ndarray of shape (n_estimators,)\n",
      "     |      The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n",
      "     |      model at iteration ``i`` on the in-bag sample.\n",
      "     |      If ``subsample == 1`` this is the deviance on the training data.\n",
      "     |  \n",
      "     |  loss_ : LossFunction\n",
      "     |      The concrete ``LossFunction`` object.\n",
      "     |  \n",
      "     |  init_ : estimator\n",
      "     |      The estimator that provides the initial predictions.\n",
      "     |      Set via the ``init`` argument or ``loss.init_estimator``.\n",
      "     |  \n",
      "     |  estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, ``loss_.K``)\n",
      "     |      The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary\n",
      "     |      classification, otherwise n_classes.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      The classes labels.\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of data features.\n",
      "     |  \n",
      "     |  n_classes_ : int\n",
      "     |      The number of classes.\n",
      "     |  \n",
      "     |  max_features_ : int\n",
      "     |      The inferred value of max_features.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  HistGradientBoostingClassifier : Histogram-based Gradient Boosting\n",
      "     |      Classification Tree.\n",
      "     |  sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
      "     |  RandomForestClassifier : A meta-estimator that fits a number of decision\n",
      "     |      tree classifiers on various sub-samples of the dataset and uses\n",
      "     |      averaging to improve the predictive accuracy and control over-fitting.\n",
      "     |  AdaBoostClassifier : A meta-estimator that begins by fitting a classifier\n",
      "     |      on the original dataset and then fits additional copies of the\n",
      "     |      classifier on the same dataset where the weights of incorrectly\n",
      "     |      classified instances are adjusted such that subsequent classifiers\n",
      "     |      focus more on difficult cases.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The features are always randomly permuted at each split. Therefore,\n",
      "     |  the best found split may vary, even with the same training data and\n",
      "     |  ``max_features=n_features``, if the improvement of the criterion is\n",
      "     |  identical for several splits enumerated during the search of the best\n",
      "     |  split. To obtain a deterministic behaviour during fitting,\n",
      "     |  ``random_state`` has to be fixed.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
      "     |  Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
      "     |  \n",
      "     |  J. Friedman, Stochastic Gradient Boosting, 1999\n",
      "     |  \n",
      "     |  T. Hastie, R. Tibshirani and J. Friedman.\n",
      "     |  Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  The following example shows how to fit a gradient boosting classifier with\n",
      "     |  100 decision stumps as weak learners.\n",
      "     |  \n",
      "     |  >>> from sklearn.datasets import make_hastie_10_2\n",
      "     |  >>> from sklearn.ensemble import GradientBoostingClassifier\n",
      "     |  \n",
      "     |  >>> X, y = make_hastie_10_2(random_state=0)\n",
      "     |  >>> X_train, X_test = X[:2000], X[2000:]\n",
      "     |  >>> y_train, y_test = y[:2000], y[2000:]\n",
      "     |  \n",
      "     |  >>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
      "     |  ...     max_depth=1, random_state=0).fit(X_train, y_train)\n",
      "     |  >>> clf.score(X_test, y_test)\n",
      "     |  0.913...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GradientBoostingClassifier\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      BaseGradientBoosting\n",
      "     |      sklearn.ensemble._base.BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Compute the decision function of ``X``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : ndarray of shape (n_samples, n_classes) or (n_samples,)\n",
      "     |          The decision function of the input samples, which corresponds to\n",
      "     |          the raw values predicted from the trees of the ensemble . The\n",
      "     |          order of the classes corresponds to that in the attribute\n",
      "     |          :term:`classes_`. Regression and binary classification produce an\n",
      "     |          array of shape (n_samples,).\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      AttributeError\n",
      "     |          If the ``loss`` does not support probabilities.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray of shape (n_samples, n_classes)\n",
      "     |          The class log-probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Raises\n",
      "     |      ------\n",
      "     |      AttributeError\n",
      "     |          If the ``loss`` does not support probabilities.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray of shape (n_samples, n_classes)\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  staged_decision_function(self, X)\n",
      "     |      Compute decision function of ``X`` for each iteration.\n",
      "     |      \n",
      "     |      This method allows monitoring (i.e. determine error on testing set)\n",
      "     |      after each stage.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : generator of ndarray of shape (n_samples, k)\n",
      "     |          The decision function of the input samples, which corresponds to\n",
      "     |          the raw values predicted from the trees of the ensemble . The\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |          Regression and binary classification are special cases with\n",
      "     |          ``k == 1``, otherwise ``k==n_classes``.\n",
      "     |  \n",
      "     |  staged_predict(self, X)\n",
      "     |      Predict class at each stage for X.\n",
      "     |      \n",
      "     |      This method allows monitoring (i.e. determine error on testing set)\n",
      "     |      after each stage.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : generator of ndarray of shape (n_samples,)\n",
      "     |          The predicted value of the input samples.\n",
      "     |  \n",
      "     |  staged_predict_proba(self, X)\n",
      "     |      Predict class probabilities at each stage for X.\n",
      "     |      \n",
      "     |      This method allows monitoring (i.e. determine error on testing set)\n",
      "     |      after each stage.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : generator of ndarray of shape (n_samples,)\n",
      "     |          The predicted value of the input samples.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseGradientBoosting:\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the ensemble to X, return leaf indices.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.17\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will\n",
      "     |          be converted to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array-like of shape (n_samples, n_estimators, n_classes)\n",
      "     |          For each datapoint x in X and for each tree in the ensemble,\n",
      "     |          return the index of the leaf x ends up in each estimator.\n",
      "     |          In the case of binary classification n_classes is 1.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, monitor=None)\n",
      "     |      Fit the gradient boosting model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values (strings or integers in classification, real numbers\n",
      "     |          in regression)\n",
      "     |          For classification, labels must correspond to classes.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      monitor : callable, default=None\n",
      "     |          The monitor is called after each iteration with the current\n",
      "     |          iteration, a reference to the estimator and the local variables of\n",
      "     |          ``_fit_stages`` as keyword arguments ``callable(i, self,\n",
      "     |          locals())``. If the callable returns ``True`` the fitting procedure\n",
      "     |          is stopped. The monitor can be used for various things such as\n",
      "     |          computing held-out estimates, early stopping, model introspect, and\n",
      "     |          snapshoting.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseGradientBoosting:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      The impurity-based feature importances.\n",
      "     |      \n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |      \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : ndarray of shape (n_features,)\n",
      "     |          The values of this array sum to 1, unless all trees are single node\n",
      "     |          trees consisting of only the root node, in which case it will be an\n",
      "     |          array of zeros.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class GradientBoostingRegressor(sklearn.base.RegressorMixin, BaseGradientBoosting)\n",
      "     |  GradientBoostingRegressor(*, loss='ls', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
      "     |  \n",
      "     |  Gradient Boosting for regression.\n",
      "     |  \n",
      "     |  GB builds an additive model in a forward stage-wise fashion;\n",
      "     |  it allows for the optimization of arbitrary differentiable loss functions.\n",
      "     |  In each stage a regression tree is fit on the negative gradient of the\n",
      "     |  given loss function.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <gradient_boosting>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  loss : {'ls', 'lad', 'huber', 'quantile'}, default='ls'\n",
      "     |      Loss function to be optimized. 'ls' refers to least squares\n",
      "     |      regression. 'lad' (least absolute deviation) is a highly robust\n",
      "     |      loss function solely based on order information of the input\n",
      "     |      variables. 'huber' is a combination of the two. 'quantile'\n",
      "     |      allows quantile regression (use `alpha` to specify the quantile).\n",
      "     |  \n",
      "     |  learning_rate : float, default=0.1\n",
      "     |      Learning rate shrinks the contribution of each tree by `learning_rate`.\n",
      "     |      There is a trade-off between learning_rate and n_estimators.\n",
      "     |  \n",
      "     |  n_estimators : int, default=100\n",
      "     |      The number of boosting stages to perform. Gradient boosting\n",
      "     |      is fairly robust to over-fitting so a large number usually\n",
      "     |      results in better performance.\n",
      "     |  \n",
      "     |  subsample : float, default=1.0\n",
      "     |      The fraction of samples to be used for fitting the individual base\n",
      "     |      learners. If smaller than 1.0 this results in Stochastic Gradient\n",
      "     |      Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
      "     |      Choosing `subsample < 1.0` leads to a reduction of variance\n",
      "     |      and an increase in bias.\n",
      "     |  \n",
      "     |  criterion : {'friedman_mse', 'mse', 'mae'}, default='friedman_mse'\n",
      "     |      The function to measure the quality of a split. Supported criteria\n",
      "     |      are \"friedman_mse\" for the mean squared error with improvement\n",
      "     |      score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n",
      "     |      the mean absolute error. The default value of \"friedman_mse\" is\n",
      "     |      generally the best as it can provide a better approximation in\n",
      "     |      some cases.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      .. deprecated:: 0.24\n",
      "     |          `criterion='mae'` is deprecated and will be removed in version\n",
      "     |          1.1 (renaming of 0.26). The correct way of minimizing the absolute\n",
      "     |          error is to use `loss='lad'` instead.\n",
      "     |  \n",
      "     |  min_samples_split : int or float, default=2\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a fraction and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int or float, default=1\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a fraction and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, default=0.0\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_depth : int, default=3\n",
      "     |      Maximum depth of the individual regression estimators. The maximum\n",
      "     |      depth limits the number of nodes in the tree. Tune this parameter\n",
      "     |      for best performance; the best value depends on the interaction\n",
      "     |      of the input variables.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, default=0.0\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  min_impurity_split : float, default=None\n",
      "     |      Threshold for early stopping in tree growth. A node will split\n",
      "     |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.19\n",
      "     |         ``min_impurity_split`` has been deprecated in favor of\n",
      "     |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      "     |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      "     |         will be removed in 1.0 (renaming of 0.25).\n",
      "     |         Use ``min_impurity_decrease`` instead.\n",
      "     |  \n",
      "     |  init : estimator or 'zero', default=None\n",
      "     |      An estimator object that is used to compute the initial predictions.\n",
      "     |      ``init`` has to provide :term:`fit` and :term:`predict`. If 'zero', the\n",
      "     |      initial raw predictions are set to zero. By default a\n",
      "     |      ``DummyEstimator`` is used, predicting either the average target value\n",
      "     |      (for loss='ls'), or a quantile for the other losses.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the random seed given to each Tree estimator at each\n",
      "     |      boosting iteration.\n",
      "     |      In addition, it controls the random permutation of the features at\n",
      "     |      each split (see Notes for more details).\n",
      "     |      It also controls the random spliting of the training data to obtain a\n",
      "     |      validation set if `n_iter_no_change` is not None.\n",
      "     |      Pass an int for reproducible output across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  max_features : {'auto', 'sqrt', 'log2'}, int or float, default=None\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a fraction and\n",
      "     |        `int(max_features * n_features)` features are considered at each\n",
      "     |        split.\n",
      "     |      - If \"auto\", then `max_features=n_features`.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Choosing `max_features < n_features` leads to a reduction of variance\n",
      "     |      and an increase in bias.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  alpha : float, default=0.9\n",
      "     |      The alpha-quantile of the huber loss function and the quantile\n",
      "     |      loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Enable verbose output. If 1 then it prints progress and performance\n",
      "     |      once in a while (the more trees the lower the frequency). If greater\n",
      "     |      than 1 then it prints progress and performance for every tree.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int, default=None\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just erase the\n",
      "     |      previous solution. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  validation_fraction : float, default=0.1\n",
      "     |      The proportion of training data to set aside as validation set for\n",
      "     |      early stopping. Must be between 0 and 1.\n",
      "     |      Only used if ``n_iter_no_change`` is set to an integer.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  n_iter_no_change : int, default=None\n",
      "     |      ``n_iter_no_change`` is used to decide if early stopping will be used\n",
      "     |      to terminate training when validation score is not improving. By\n",
      "     |      default it is set to None to disable early stopping. If set to a\n",
      "     |      number, it will set aside ``validation_fraction`` size of the training\n",
      "     |      data as validation and terminate training when validation score is not\n",
      "     |      improving in all of the previous ``n_iter_no_change`` numbers of\n",
      "     |      iterations.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  tol : float, default=1e-4\n",
      "     |      Tolerance for the early stopping. When the loss is not improving\n",
      "     |      by at least tol for ``n_iter_no_change`` iterations (if set to a\n",
      "     |      number), the training stops.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  ccp_alpha : non-negative float, default=0.0\n",
      "     |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "     |      subtree with the largest cost complexity that is smaller than\n",
      "     |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "     |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  feature_importances_ : ndarray of shape (n_features,)\n",
      "     |      The impurity-based feature importances.\n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |  \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |  \n",
      "     |  oob_improvement_ : ndarray of shape (n_estimators,)\n",
      "     |      The improvement in loss (= deviance) on the out-of-bag samples\n",
      "     |      relative to the previous iteration.\n",
      "     |      ``oob_improvement_[0]`` is the improvement in\n",
      "     |      loss of the first stage over the ``init`` estimator.\n",
      "     |      Only available if ``subsample < 1.0``\n",
      "     |  \n",
      "     |  train_score_ : ndarray of shape (n_estimators,)\n",
      "     |      The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n",
      "     |      model at iteration ``i`` on the in-bag sample.\n",
      "     |      If ``subsample == 1`` this is the deviance on the training data.\n",
      "     |  \n",
      "     |  loss_ : LossFunction\n",
      "     |      The concrete ``LossFunction`` object.\n",
      "     |  \n",
      "     |  init_ : estimator\n",
      "     |      The estimator that provides the initial predictions.\n",
      "     |      Set via the ``init`` argument or ``loss.init_estimator``.\n",
      "     |  \n",
      "     |  estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, 1)\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  n_classes_ : int\n",
      "     |      The number of classes, set to 1 for regressors.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.24\n",
      "     |          Attribute ``n_classes_`` was deprecated in version 0.24 and\n",
      "     |          will be removed in 1.1 (renaming of 0.26).\n",
      "     |  \n",
      "     |  n_estimators_ : int\n",
      "     |      The number of estimators as selected by early stopping (if\n",
      "     |      ``n_iter_no_change`` is specified). Otherwise it is set to\n",
      "     |      ``n_estimators``.\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of data features.\n",
      "     |  \n",
      "     |  max_features_ : int\n",
      "     |      The inferred value of max_features.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  HistGradientBoostingRegressor : Histogram-based Gradient Boosting\n",
      "     |      Classification Tree.\n",
      "     |  sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n",
      "     |  sklearn.tree.RandomForestRegressor : A random forest regressor.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The features are always randomly permuted at each split. Therefore,\n",
      "     |  the best found split may vary, even with the same training data and\n",
      "     |  ``max_features=n_features``, if the improvement of the criterion is\n",
      "     |  identical for several splits enumerated during the search of the best\n",
      "     |  split. To obtain a deterministic behaviour during fitting,\n",
      "     |  ``random_state`` has to be fixed.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> from sklearn.ensemble import GradientBoostingRegressor\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> X, y = make_regression(random_state=0)\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |  ...     X, y, random_state=0)\n",
      "     |  >>> reg = GradientBoostingRegressor(random_state=0)\n",
      "     |  >>> reg.fit(X_train, y_train)\n",
      "     |  GradientBoostingRegressor(random_state=0)\n",
      "     |  >>> reg.predict(X_test[1:2])\n",
      "     |  array([-61...])\n",
      "     |  >>> reg.score(X_test, y_test)\n",
      "     |  0.4...\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
      "     |  Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
      "     |  \n",
      "     |  J. Friedman, Stochastic Gradient Boosting, 1999\n",
      "     |  \n",
      "     |  T. Hastie, R. Tibshirani and J. Friedman.\n",
      "     |  Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GradientBoostingRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      BaseGradientBoosting\n",
      "     |      sklearn.ensemble._base.BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, loss='ls', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the ensemble to X, return leaf indices.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.17\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will\n",
      "     |          be converted to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array-like of shape (n_samples, n_estimators)\n",
      "     |          For each datapoint x in X and for each tree in the ensemble,\n",
      "     |          return the index of the leaf x ends up in each estimator.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict regression target for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  staged_predict(self, X)\n",
      "     |      Predict regression target at each stage for X.\n",
      "     |      \n",
      "     |      This method allows monitoring (i.e. determine error on testing set)\n",
      "     |      after each stage.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : generator of ndarray of shape (n_samples,)\n",
      "     |          The predicted value of the input samples.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  n_classes_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseGradientBoosting:\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, monitor=None)\n",
      "     |      Fit the gradient boosting model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values (strings or integers in classification, real numbers\n",
      "     |          in regression)\n",
      "     |          For classification, labels must correspond to classes.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      monitor : callable, default=None\n",
      "     |          The monitor is called after each iteration with the current\n",
      "     |          iteration, a reference to the estimator and the local variables of\n",
      "     |          ``_fit_stages`` as keyword arguments ``callable(i, self,\n",
      "     |          locals())``. If the callable returns ``True`` the fitting procedure\n",
      "     |          is stopped. The monitor can be used for various things such as\n",
      "     |          computing held-out estimates, early stopping, model introspect, and\n",
      "     |          snapshoting.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseGradientBoosting:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      The impurity-based feature importances.\n",
      "     |      \n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |      \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : ndarray of shape (n_features,)\n",
      "     |          The values of this array sum to 1, unless all trees are single node\n",
      "     |          trees consisting of only the root node, in which case it will be an\n",
      "     |          array of zeros.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class IsolationForest(sklearn.base.OutlierMixin, sklearn.ensemble._bagging.BaseBagging)\n",
      "     |  IsolationForest(*, n_estimators=100, max_samples='auto', contamination='auto', max_features=1.0, bootstrap=False, n_jobs=None, random_state=None, verbose=0, warm_start=False)\n",
      "     |  \n",
      "     |  Isolation Forest Algorithm.\n",
      "     |  \n",
      "     |  Return the anomaly score of each sample using the IsolationForest algorithm\n",
      "     |  \n",
      "     |  The IsolationForest 'isolates' observations by randomly selecting a feature\n",
      "     |  and then randomly selecting a split value between the maximum and minimum\n",
      "     |  values of the selected feature.\n",
      "     |  \n",
      "     |  Since recursive partitioning can be represented by a tree structure, the\n",
      "     |  number of splittings required to isolate a sample is equivalent to the path\n",
      "     |  length from the root node to the terminating node.\n",
      "     |  \n",
      "     |  This path length, averaged over a forest of such random trees, is a\n",
      "     |  measure of normality and our decision function.\n",
      "     |  \n",
      "     |  Random partitioning produces noticeably shorter paths for anomalies.\n",
      "     |  Hence, when a forest of random trees collectively produce shorter path\n",
      "     |  lengths for particular samples, they are highly likely to be anomalies.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <isolation_forest>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_estimators : int, default=100\n",
      "     |      The number of base estimators in the ensemble.\n",
      "     |  \n",
      "     |  max_samples : \"auto\", int or float, default=\"auto\"\n",
      "     |      The number of samples to draw from X to train each base estimator.\n",
      "     |          - If int, then draw `max_samples` samples.\n",
      "     |          - If float, then draw `max_samples * X.shape[0]` samples.\n",
      "     |          - If \"auto\", then `max_samples=min(256, n_samples)`.\n",
      "     |  \n",
      "     |      If max_samples is larger than the number of samples provided,\n",
      "     |      all samples will be used for all trees (no sampling).\n",
      "     |  \n",
      "     |  contamination : 'auto' or float, default='auto'\n",
      "     |      The amount of contamination of the data set, i.e. the proportion\n",
      "     |      of outliers in the data set. Used when fitting to define the threshold\n",
      "     |      on the scores of the samples.\n",
      "     |  \n",
      "     |          - If 'auto', the threshold is determined as in the\n",
      "     |            original paper.\n",
      "     |          - If float, the contamination should be in the range [0, 0.5].\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |         The default value of ``contamination`` changed from 0.1\n",
      "     |         to ``'auto'``.\n",
      "     |  \n",
      "     |  max_features : int or float, default=1.0\n",
      "     |      The number of features to draw from X to train each base estimator.\n",
      "     |  \n",
      "     |          - If int, then draw `max_features` features.\n",
      "     |          - If float, then draw `max_features * X.shape[1]` features.\n",
      "     |  \n",
      "     |  bootstrap : bool, default=False\n",
      "     |      If True, individual trees are fit on random subsets of the training\n",
      "     |      data sampled with replacement. If False, sampling without replacement\n",
      "     |      is performed.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel for both :meth:`fit` and\n",
      "     |      :meth:`predict`. ``None`` means 1 unless in a\n",
      "     |      :obj:`joblib.parallel_backend` context. ``-1`` means using all\n",
      "     |      processors. See :term:`Glossary <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the pseudo-randomness of the selection of the feature\n",
      "     |      and split values for each branching step and each tree in the forest.\n",
      "     |  \n",
      "     |      Pass an int for reproducible results across multiple function calls.\n",
      "     |      See :term:`Glossary <random_state>`.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Controls the verbosity of the tree building process.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "     |      new forest. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.21\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  base_estimator_ : ExtraTreeRegressor instance\n",
      "     |      The child estimator template used to create the collection of\n",
      "     |      fitted sub-estimators.\n",
      "     |  \n",
      "     |  estimators_ : list of ExtraTreeRegressor instances\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  estimators_features_ : list of ndarray\n",
      "     |      The subset of drawn features for each base estimator.\n",
      "     |  \n",
      "     |  estimators_samples_ : list of ndarray\n",
      "     |      The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      "     |      estimator.\n",
      "     |  \n",
      "     |  max_samples_ : int\n",
      "     |      The actual number of samples.\n",
      "     |  \n",
      "     |  offset_ : float\n",
      "     |      Offset used to define the decision function from the raw scores. We\n",
      "     |      have the relation: ``decision_function = score_samples - offset_``.\n",
      "     |      ``offset_`` is defined as follows. When the contamination parameter is\n",
      "     |      set to \"auto\", the offset is equal to -0.5 as the scores of inliers are\n",
      "     |      close to 0 and the scores of outliers are close to -1. When a\n",
      "     |      contamination parameter different than \"auto\" is provided, the offset\n",
      "     |      is defined in such a way we obtain the expected number of outliers\n",
      "     |      (samples with decision function < 0) in training.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of features when ``fit`` is performed.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The implementation is based on an ensemble of ExtraTreeRegressor. The\n",
      "     |  maximum depth of each tree is set to ``ceil(log_2(n))`` where\n",
      "     |  :math:`n` is the number of samples used to build the tree\n",
      "     |  (see (Liu et al., 2008) for more details).\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"\n",
      "     |         Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.\n",
      "     |  .. [2] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation-based\n",
      "     |         anomaly detection.\" ACM Transactions on Knowledge Discovery from\n",
      "     |         Data (TKDD) 6.1 (2012): 3.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  ----------\n",
      "     |  sklearn.covariance.EllipticEnvelope : An object for detecting outliers in a\n",
      "     |      Gaussian distributed dataset.\n",
      "     |  sklearn.svm.OneClassSVM : Unsupervised Outlier Detection.\n",
      "     |      Estimate the support of a high-dimensional distribution.\n",
      "     |      The implementation is based on libsvm.\n",
      "     |  sklearn.neighbors.LocalOutlierFactor : Unsupervised Outlier Detection\n",
      "     |      using Local Outlier Factor (LOF).\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.ensemble import IsolationForest\n",
      "     |  >>> X = [[-1.1], [0.3], [0.5], [100]]\n",
      "     |  >>> clf = IsolationForest(random_state=0).fit(X)\n",
      "     |  >>> clf.predict([[0.1], [0], [90]])\n",
      "     |  array([ 1,  1, -1])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      IsolationForest\n",
      "     |      sklearn.base.OutlierMixin\n",
      "     |      sklearn.ensemble._bagging.BaseBagging\n",
      "     |      sklearn.ensemble._base.BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, *, n_estimators=100, max_samples='auto', contamination='auto', max_features=1.0, bootstrap=False, n_jobs=None, random_state=None, verbose=0, warm_start=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Average anomaly score of X of the base classifiers.\n",
      "     |      \n",
      "     |      The anomaly score of an input sample is computed as\n",
      "     |      the mean anomaly score of the trees in the forest.\n",
      "     |      \n",
      "     |      The measure of normality of an observation given a tree is the depth\n",
      "     |      of the leaf containing this observation, which is equivalent to\n",
      "     |      the number of splittings required to isolate this point. In case of\n",
      "     |      several observations n_left in the leaf, the average path length of\n",
      "     |      a n_left samples isolation tree is added.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      scores : ndarray of shape (n_samples,)\n",
      "     |          The anomaly score of the input samples.\n",
      "     |          The lower, the more abnormal. Negative scores represent outliers,\n",
      "     |          positive scores represent inliers.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, sample_weight=None)\n",
      "     |      Fit estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Use ``dtype=np.float32`` for maximum\n",
      "     |          efficiency. Sparse matrices are also supported, use sparse\n",
      "     |          ``csc_matrix`` for maximum efficiency.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict if a particular sample is an outlier or not.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, it will be converted to\n",
      "     |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      "     |          to a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      is_inlier : ndarray of shape (n_samples,)\n",
      "     |          For each observation, tells whether or not (+1 or -1) it should\n",
      "     |          be considered as an inlier according to the fitted model.\n",
      "     |  \n",
      "     |  score_samples(self, X)\n",
      "     |      Opposite of the anomaly score defined in the original paper.\n",
      "     |      \n",
      "     |      The anomaly score of an input sample is computed as\n",
      "     |      the mean anomaly score of the trees in the forest.\n",
      "     |      \n",
      "     |      The measure of normality of an observation given a tree is the depth\n",
      "     |      of the leaf containing this observation, which is equivalent to\n",
      "     |      the number of splittings required to isolate this point. In case of\n",
      "     |      several observations n_left in the leaf, the average path length of\n",
      "     |      a n_left samples isolation tree is added.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      scores : ndarray of shape (n_samples,)\n",
      "     |          The anomaly score of the input samples.\n",
      "     |          The lower, the more abnormal.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.OutlierMixin:\n",
      "     |  \n",
      "     |  fit_predict(self, X, y=None)\n",
      "     |      Perform fit on X and returns labels for X.\n",
      "     |      \n",
      "     |      Returns -1 for outliers and 1 for inliers.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix, dataframe} of shape             (n_samples, n_features)\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          1 for inliers, -1 for outliers.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.OutlierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from sklearn.ensemble._bagging.BaseBagging:\n",
      "     |  \n",
      "     |  estimators_samples_\n",
      "     |      The subset of drawn samples for each base estimator.\n",
      "     |      \n",
      "     |      Returns a dynamically generated list of indices identifying\n",
      "     |      the samples used for fitting each member of the ensemble, i.e.,\n",
      "     |      the in-bag samples.\n",
      "     |      \n",
      "     |      Note: the list is re-created at each call to the property in order\n",
      "     |      to reduce the object memory footprint by not storing the sampling\n",
      "     |      data. Thus fetching the property may be slower than expected.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RandomForestClassifier(ForestClassifier)\n",
      "     |  RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      "     |  \n",
      "     |  A random forest classifier.\n",
      "     |  \n",
      "     |  A random forest is a meta estimator that fits a number of decision tree\n",
      "     |  classifiers on various sub-samples of the dataset and uses averaging to\n",
      "     |  improve the predictive accuracy and control over-fitting.\n",
      "     |  The sub-sample size is controlled with the `max_samples` parameter if\n",
      "     |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      "     |  each tree.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <forest>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_estimators : int, default=100\n",
      "     |      The number of trees in the forest.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |         The default value of ``n_estimators`` changed from 10 to 100\n",
      "     |         in 0.22.\n",
      "     |  \n",
      "     |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
      "     |      The function to measure the quality of a split. Supported criteria are\n",
      "     |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      "     |      Note: this parameter is tree-specific.\n",
      "     |  \n",
      "     |  max_depth : int, default=None\n",
      "     |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int or float, default=2\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a fraction and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int or float, default=1\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a fraction and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, default=0.0\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a fraction and\n",
      "     |        `round(max_features * n_features)` features are considered at each\n",
      "     |        split.\n",
      "     |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int, default=None\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, default=0.0\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  min_impurity_split : float, default=None\n",
      "     |      Threshold for early stopping in tree growth. A node will split\n",
      "     |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.19\n",
      "     |         ``min_impurity_split`` has been deprecated in favor of\n",
      "     |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      "     |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      "     |         will be removed in 1.0 (renaming of 0.25).\n",
      "     |         Use ``min_impurity_decrease`` instead.\n",
      "     |  \n",
      "     |  bootstrap : bool, default=True\n",
      "     |      Whether bootstrap samples are used when building trees. If False, the\n",
      "     |      whole dataset is used to build each tree.\n",
      "     |  \n",
      "     |  oob_score : bool, default=False\n",
      "     |      Whether to use out-of-bag samples to estimate the generalization score.\n",
      "     |      Only available if bootstrap=True.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      "     |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      "     |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      "     |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      "     |      <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls both the randomness of the bootstrapping of the samples used\n",
      "     |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
      "     |      features to consider when looking for the best split at each node\n",
      "     |      (if ``max_features < n_features``).\n",
      "     |      See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Controls the verbosity when fitting and predicting.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "     |      new forest. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n",
      "     |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      "     |      If not given, all classes are supposed to have weight one. For\n",
      "     |      multi-output problems, a list of dicts can be provided in the same\n",
      "     |      order as the columns of y.\n",
      "     |  \n",
      "     |      Note that for multioutput (including multilabel) weights should be\n",
      "     |      defined for each class of every column in its own dict. For example,\n",
      "     |      for four-class multilabel classification weights should be\n",
      "     |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      "     |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      "     |  \n",
      "     |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      "     |      weights inversely proportional to class frequencies in the input data\n",
      "     |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      "     |  \n",
      "     |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      "     |      weights are computed based on the bootstrap sample for every tree\n",
      "     |      grown.\n",
      "     |  \n",
      "     |      For multi-output, the weights of each column of y will be multiplied.\n",
      "     |  \n",
      "     |      Note that these weights will be multiplied with sample_weight (passed\n",
      "     |      through the fit method) if sample_weight is specified.\n",
      "     |  \n",
      "     |  ccp_alpha : non-negative float, default=0.0\n",
      "     |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "     |      subtree with the largest cost complexity that is smaller than\n",
      "     |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "     |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  max_samples : int or float, default=None\n",
      "     |      If bootstrap is True, the number of samples to draw from X\n",
      "     |      to train each base estimator.\n",
      "     |  \n",
      "     |      - If None (default), then draw `X.shape[0]` samples.\n",
      "     |      - If int, then draw `max_samples` samples.\n",
      "     |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
      "     |        `max_samples` should be in the interval `(0, 1)`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  base_estimator_ : DecisionTreeClassifier\n",
      "     |      The child estimator template used to create the collection of fitted\n",
      "     |      sub-estimators.\n",
      "     |  \n",
      "     |  estimators_ : list of DecisionTreeClassifier\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
      "     |      The classes labels (single output problem), or a list of arrays of\n",
      "     |      class labels (multi-output problem).\n",
      "     |  \n",
      "     |  n_classes_ : int or list\n",
      "     |      The number of classes (single output problem), or a list containing the\n",
      "     |      number of classes for each output (multi-output problem).\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of features when ``fit`` is performed.\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      The number of outputs when ``fit`` is performed.\n",
      "     |  \n",
      "     |  feature_importances_ : ndarray of shape (n_features,)\n",
      "     |      The impurity-based feature importances.\n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |  \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      "     |      This attribute exists only when ``oob_score`` is True.\n",
      "     |  \n",
      "     |  oob_decision_function_ : ndarray of shape (n_samples, n_classes)\n",
      "     |      Decision function computed with out-of-bag estimate on the training\n",
      "     |      set. If n_estimators is small it might be possible that a data point\n",
      "     |      was never left out during the bootstrap. In this case,\n",
      "     |      `oob_decision_function_` might contain NaN. This attribute exists\n",
      "     |      only when ``oob_score`` is True.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  DecisionTreeClassifier, ExtraTreesClassifier\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The default values for the parameters controlling the size of the trees\n",
      "     |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "     |  unpruned trees which can potentially be very large on some data sets. To\n",
      "     |  reduce memory consumption, the complexity and size of the trees should be\n",
      "     |  controlled by setting those parameter values.\n",
      "     |  \n",
      "     |  The features are always randomly permuted at each split. Therefore,\n",
      "     |  the best found split may vary, even with the same training data,\n",
      "     |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      "     |  of the criterion is identical for several splits enumerated during the\n",
      "     |  search of the best split. To obtain a deterministic behaviour during\n",
      "     |  fitting, ``random_state`` has to be fixed.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      "     |  >>> from sklearn.datasets import make_classification\n",
      "     |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      "     |  ...                            n_informative=2, n_redundant=0,\n",
      "     |  ...                            random_state=0, shuffle=False)\n",
      "     |  >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
      "     |  >>> clf.fit(X, y)\n",
      "     |  RandomForestClassifier(...)\n",
      "     |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      "     |  [1]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RandomForestClassifier\n",
      "     |      ForestClassifier\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      BaseForest\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.ensemble._base.BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ForestClassifier:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class for X.\n",
      "     |      \n",
      "     |      The predicted class of an input sample is a vote by the trees in\n",
      "     |      the forest, weighted by their probability estimates. That is,\n",
      "     |      the predicted class is the one with highest mean probability\n",
      "     |      estimate across the trees.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The predicted classes.\n",
      "     |  \n",
      "     |  predict_log_proba(self, X)\n",
      "     |      Predict class log-probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class log-probabilities of an input sample is computed as\n",
      "     |      the log of the mean predicted class probabilities of the trees in the\n",
      "     |      forest.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray of shape (n_samples, n_classes), or a list of n_outputs\n",
      "     |          such arrays if n_outputs > 1.\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for X.\n",
      "     |      \n",
      "     |      The predicted class probabilities of an input sample are computed as\n",
      "     |      the mean predicted class probabilities of the trees in the forest.\n",
      "     |      The class probability of a single tree is the fraction of samples of\n",
      "     |      the same class in a leaf.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      p : ndarray of shape (n_samples, n_classes), or a list of n_outputs\n",
      "     |          such arrays if n_outputs > 1.\n",
      "     |          The class probabilities of the input samples. The order of the\n",
      "     |          classes corresponds to that in the attribute :term:`classes_`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseForest:\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the forest to X, return leaf indices.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      "     |          For each datapoint x in X and for each tree in the forest,\n",
      "     |          return the index of the leaf x ends up in.\n",
      "     |  \n",
      "     |  decision_path(self, X)\n",
      "     |      Return the decision path in the forest.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      "     |          Return a node indicator matrix where non zero elements indicates\n",
      "     |          that the samples goes through the nodes. The matrix is of CSR\n",
      "     |          format.\n",
      "     |      \n",
      "     |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      "     |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      "     |          gives the indicator value for the i-th estimator.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a forest of trees from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Internally, its dtype will be converted\n",
      "     |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseForest:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      The impurity-based feature importances.\n",
      "     |      \n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |      \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : ndarray of shape (n_features,)\n",
      "     |          The values of this array sum to 1, unless all trees are single node\n",
      "     |          trees consisting of only the root node, in which case it will be an\n",
      "     |          array of zeros.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RandomForestRegressor(ForestRegressor)\n",
      "     |  RandomForestRegressor(n_estimators=100, *, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)\n",
      "     |  \n",
      "     |  A random forest regressor.\n",
      "     |  \n",
      "     |  A random forest is a meta estimator that fits a number of classifying\n",
      "     |  decision trees on various sub-samples of the dataset and uses averaging\n",
      "     |  to improve the predictive accuracy and control over-fitting.\n",
      "     |  The sub-sample size is controlled with the `max_samples` parameter if\n",
      "     |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      "     |  each tree.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <forest>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_estimators : int, default=100\n",
      "     |      The number of trees in the forest.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |         The default value of ``n_estimators`` changed from 10 to 100\n",
      "     |         in 0.22.\n",
      "     |  \n",
      "     |  criterion : {\"mse\", \"mae\"}, default=\"mse\"\n",
      "     |      The function to measure the quality of a split. Supported criteria\n",
      "     |      are \"mse\" for the mean squared error, which is equal to variance\n",
      "     |      reduction as feature selection criterion, and \"mae\" for the mean\n",
      "     |      absolute error.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |         Mean Absolute Error (MAE) criterion.\n",
      "     |  \n",
      "     |  max_depth : int, default=None\n",
      "     |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int or float, default=2\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a fraction and\n",
      "     |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int or float, default=1\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a fraction and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, default=0.0\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
      "     |      The number of features to consider when looking for the best split:\n",
      "     |  \n",
      "     |      - If int, then consider `max_features` features at each split.\n",
      "     |      - If float, then `max_features` is a fraction and\n",
      "     |        `round(max_features * n_features)` features are considered at each\n",
      "     |        split.\n",
      "     |      - If \"auto\", then `max_features=n_features`.\n",
      "     |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "     |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      "     |      - If None, then `max_features=n_features`.\n",
      "     |  \n",
      "     |      Note: the search for a split does not stop until at least one\n",
      "     |      valid partition of the node samples is found, even if it requires to\n",
      "     |      effectively inspect more than ``max_features`` features.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int, default=None\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, default=0.0\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  min_impurity_split : float, default=None\n",
      "     |      Threshold for early stopping in tree growth. A node will split\n",
      "     |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.19\n",
      "     |         ``min_impurity_split`` has been deprecated in favor of\n",
      "     |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      "     |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      "     |         will be removed in 1.0 (renaming of 0.25).\n",
      "     |         Use ``min_impurity_decrease`` instead.\n",
      "     |  \n",
      "     |  bootstrap : bool, default=True\n",
      "     |      Whether bootstrap samples are used when building trees. If False, the\n",
      "     |      whole dataset is used to build each tree.\n",
      "     |  \n",
      "     |  oob_score : bool, default=False\n",
      "     |      Whether to use out-of-bag samples to estimate the generalization score.\n",
      "     |      Only available if bootstrap=True.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      "     |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      "     |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      "     |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      "     |      <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls both the randomness of the bootstrapping of the samples used\n",
      "     |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
      "     |      features to consider when looking for the best split at each node\n",
      "     |      (if ``max_features < n_features``).\n",
      "     |      See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Controls the verbosity when fitting and predicting.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "     |      new forest. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  ccp_alpha : non-negative float, default=0.0\n",
      "     |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "     |      subtree with the largest cost complexity that is smaller than\n",
      "     |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "     |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  max_samples : int or float, default=None\n",
      "     |      If bootstrap is True, the number of samples to draw from X\n",
      "     |      to train each base estimator.\n",
      "     |  \n",
      "     |      - If None (default), then draw `X.shape[0]` samples.\n",
      "     |      - If int, then draw `max_samples` samples.\n",
      "     |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
      "     |        `max_samples` should be in the interval `(0, 1)`.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  base_estimator_ : DecisionTreeRegressor\n",
      "     |      The child estimator template used to create the collection of fitted\n",
      "     |      sub-estimators.\n",
      "     |  \n",
      "     |  estimators_ : list of DecisionTreeRegressor\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  feature_importances_ : ndarray of shape (n_features,)\n",
      "     |      The impurity-based feature importances.\n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |  \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of features when ``fit`` is performed.\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      The number of outputs when ``fit`` is performed.\n",
      "     |  \n",
      "     |  oob_score_ : float\n",
      "     |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      "     |      This attribute exists only when ``oob_score`` is True.\n",
      "     |  \n",
      "     |  oob_prediction_ : ndarray of shape (n_samples,)\n",
      "     |      Prediction computed with out-of-bag estimate on the training set.\n",
      "     |      This attribute exists only when ``oob_score`` is True.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  DecisionTreeRegressor, ExtraTreesRegressor\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  The default values for the parameters controlling the size of the trees\n",
      "     |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "     |  unpruned trees which can potentially be very large on some data sets. To\n",
      "     |  reduce memory consumption, the complexity and size of the trees should be\n",
      "     |  controlled by setting those parameter values.\n",
      "     |  \n",
      "     |  The features are always randomly permuted at each split. Therefore,\n",
      "     |  the best found split may vary, even with the same training data,\n",
      "     |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      "     |  of the criterion is identical for several splits enumerated during the\n",
      "     |  search of the best split. To obtain a deterministic behaviour during\n",
      "     |  fitting, ``random_state`` has to be fixed.\n",
      "     |  \n",
      "     |  The default value ``max_features=\"auto\"`` uses ``n_features``\n",
      "     |  rather than ``n_features / 3``. The latter was originally suggested in\n",
      "     |  [1], whereas the former was more recently justified empirically in [2].\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      "     |  \n",
      "     |  .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
      "     |         trees\", Machine Learning, 63(1), 3-42, 2006.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.ensemble import RandomForestRegressor\n",
      "     |  >>> from sklearn.datasets import make_regression\n",
      "     |  >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      "     |  ...                        random_state=0, shuffle=False)\n",
      "     |  >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
      "     |  >>> regr.fit(X, y)\n",
      "     |  RandomForestRegressor(...)\n",
      "     |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      "     |  [-8.32987858]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RandomForestRegressor\n",
      "     |      ForestRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      BaseForest\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.ensemble._base.BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_estimators=100, *, criterion='mse', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from ForestRegressor:\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict regression target for X.\n",
      "     |      \n",
      "     |      The predicted regression target of an input sample is computed as the\n",
      "     |      mean predicted regression targets of the trees in the forest.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseForest:\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the forest to X, return leaf indices.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      "     |          For each datapoint x in X and for each tree in the forest,\n",
      "     |          return the index of the leaf x ends up in.\n",
      "     |  \n",
      "     |  decision_path(self, X)\n",
      "     |      Return the decision path in the forest.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      "     |          Return a node indicator matrix where non zero elements indicates\n",
      "     |          that the samples goes through the nodes. The matrix is of CSR\n",
      "     |          format.\n",
      "     |      \n",
      "     |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      "     |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      "     |          gives the indicator value for the i-th estimator.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Build a forest of trees from the training set (X, y).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The training input samples. Internally, its dtype will be converted\n",
      "     |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csc_matrix``.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          The target values (class labels in classification, real numbers in\n",
      "     |          regression).\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseForest:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      The impurity-based feature importances.\n",
      "     |      \n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |      \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : ndarray of shape (n_features,)\n",
      "     |          The values of this array sum to 1, unless all trees are single node\n",
      "     |          trees consisting of only the root node, in which case it will be an\n",
      "     |          array of zeros.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class RandomTreesEmbedding(BaseForest)\n",
      "     |  RandomTreesEmbedding(n_estimators=100, *, max_depth=5, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, sparse_output=True, n_jobs=None, random_state=None, verbose=0, warm_start=False)\n",
      "     |  \n",
      "     |  An ensemble of totally random trees.\n",
      "     |  \n",
      "     |  An unsupervised transformation of a dataset to a high-dimensional\n",
      "     |  sparse representation. A datapoint is coded according to which leaf of\n",
      "     |  each tree it is sorted into. Using a one-hot encoding of the leaves,\n",
      "     |  this leads to a binary coding with as many ones as there are trees in\n",
      "     |  the forest.\n",
      "     |  \n",
      "     |  The dimensionality of the resulting representation is\n",
      "     |  ``n_out <= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``,\n",
      "     |  the number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <random_trees_embedding>`.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_estimators : int, default=100\n",
      "     |      Number of trees in the forest.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.22\n",
      "     |         The default value of ``n_estimators`` changed from 10 to 100\n",
      "     |         in 0.22.\n",
      "     |  \n",
      "     |  max_depth : int, default=5\n",
      "     |      The maximum depth of each tree. If None, then nodes are expanded until\n",
      "     |      all leaves are pure or until all leaves contain less than\n",
      "     |      min_samples_split samples.\n",
      "     |  \n",
      "     |  min_samples_split : int or float, default=2\n",
      "     |      The minimum number of samples required to split an internal node:\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_split` as the minimum number.\n",
      "     |      - If float, then `min_samples_split` is a fraction and\n",
      "     |        `ceil(min_samples_split * n_samples)` is the minimum\n",
      "     |        number of samples for each split.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_samples_leaf : int or float, default=1\n",
      "     |      The minimum number of samples required to be at a leaf node.\n",
      "     |      A split point at any depth will only be considered if it leaves at\n",
      "     |      least ``min_samples_leaf`` training samples in each of the left and\n",
      "     |      right branches.  This may have the effect of smoothing the model,\n",
      "     |      especially in regression.\n",
      "     |  \n",
      "     |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "     |      - If float, then `min_samples_leaf` is a fraction and\n",
      "     |        `ceil(min_samples_leaf * n_samples)` is the minimum\n",
      "     |        number of samples for each node.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.18\n",
      "     |         Added float values for fractions.\n",
      "     |  \n",
      "     |  min_weight_fraction_leaf : float, default=0.0\n",
      "     |      The minimum weighted fraction of the sum total of weights (of all\n",
      "     |      the input samples) required to be at a leaf node. Samples have\n",
      "     |      equal weight when sample_weight is not provided.\n",
      "     |  \n",
      "     |  max_leaf_nodes : int, default=None\n",
      "     |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "     |      Best nodes are defined as relative reduction in impurity.\n",
      "     |      If None then unlimited number of leaf nodes.\n",
      "     |  \n",
      "     |  min_impurity_decrease : float, default=0.0\n",
      "     |      A node will be split if this split induces a decrease of the impurity\n",
      "     |      greater than or equal to this value.\n",
      "     |  \n",
      "     |      The weighted impurity decrease equation is the following::\n",
      "     |  \n",
      "     |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "     |                              - N_t_L / N_t * left_impurity)\n",
      "     |  \n",
      "     |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "     |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "     |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "     |  \n",
      "     |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "     |      if ``sample_weight`` is passed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.19\n",
      "     |  \n",
      "     |  min_impurity_split : float, default=None\n",
      "     |      Threshold for early stopping in tree growth. A node will split\n",
      "     |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      "     |  \n",
      "     |      .. deprecated:: 0.19\n",
      "     |         ``min_impurity_split`` has been deprecated in favor of\n",
      "     |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      "     |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      "     |         will be removed in 1.0 (renaming of 0.25).\n",
      "     |         Use ``min_impurity_decrease`` instead.\n",
      "     |  \n",
      "     |  sparse_output : bool, default=True\n",
      "     |      Whether or not to return a sparse CSR matrix, as default behavior,\n",
      "     |      or to return a dense array compatible with dense pipeline operators.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel. :meth:`fit`, :meth:`transform`,\n",
      "     |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      "     |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      "     |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      "     |      <n_jobs>` for more details.\n",
      "     |  \n",
      "     |  random_state : int, RandomState instance or None, default=None\n",
      "     |      Controls the generation of the random `y` used to fit the trees\n",
      "     |      and the draw of the splits for each feature at the trees' nodes.\n",
      "     |      See :term:`Glossary <random_state>` for details.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Controls the verbosity when fitting and predicting.\n",
      "     |  \n",
      "     |  warm_start : bool, default=False\n",
      "     |      When set to ``True``, reuse the solution of the previous call to fit\n",
      "     |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "     |      new forest. See :term:`the Glossary <warm_start>`.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  base_estimator_ : DecisionTreeClassifier instance\n",
      "     |      The child estimator template used to create the collection of fitted\n",
      "     |      sub-estimators.\n",
      "     |  \n",
      "     |  estimators_ : list of DecisionTreeClassifier instances\n",
      "     |      The collection of fitted sub-estimators.\n",
      "     |  \n",
      "     |  feature_importances_ : ndarray of shape (n_features,)\n",
      "     |      The feature importances (the higher, the more important the feature).\n",
      "     |  \n",
      "     |  n_features_ : int\n",
      "     |      The number of features when ``fit`` is performed.\n",
      "     |  \n",
      "     |  n_outputs_ : int\n",
      "     |      The number of outputs when ``fit`` is performed.\n",
      "     |  \n",
      "     |  one_hot_encoder_ : OneHotEncoder instance\n",
      "     |      One-hot encoder used to create the sparse embedding.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\",\n",
      "     |         Machine Learning, 63(1), 3-42, 2006.\n",
      "     |  .. [2] Moosmann, F. and Triggs, B. and Jurie, F.  \"Fast discriminative\n",
      "     |         visual codebooks using randomized clustering forests\"\n",
      "     |         NIPS 2007\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.ensemble import RandomTreesEmbedding\n",
      "     |  >>> X = [[0,0], [1,0], [0,1], [-1,0], [0,-1]]\n",
      "     |  >>> random_trees = RandomTreesEmbedding(\n",
      "     |  ...    n_estimators=5, random_state=0, max_depth=1).fit(X)\n",
      "     |  >>> X_sparse_embedding = random_trees.transform(X)\n",
      "     |  >>> X_sparse_embedding.toarray()\n",
      "     |  array([[0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],\n",
      "     |         [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],\n",
      "     |         [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],\n",
      "     |         [1., 0., 1., 0., 1., 0., 1., 0., 1., 0.],\n",
      "     |         [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.]])\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RandomTreesEmbedding\n",
      "     |      BaseForest\n",
      "     |      sklearn.base.MultiOutputMixin\n",
      "     |      sklearn.ensemble._base.BaseEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_estimators=100, *, max_depth=5, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, sparse_output=True, n_jobs=None, random_state=None, verbose=0, warm_start=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y=None, sample_weight=None)\n",
      "     |      Fit estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Use ``dtype=np.float32`` for maximum\n",
      "     |          efficiency. Sparse matrices are also supported, use sparse\n",
      "     |          ``csc_matrix`` for maximum efficiency.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, sample_weight=None)\n",
      "     |      Fit estimator and transform dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input data used to build forests. Use ``dtype=np.float32`` for\n",
      "     |          maximum efficiency.\n",
      "     |      \n",
      "     |      y : Ignored\n",
      "     |          Not used, present for API consistency by convention.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted. Splits\n",
      "     |          that would create child nodes with net zero or negative weight are\n",
      "     |          ignored while searching for a split in each node. In the case of\n",
      "     |          classification, splits are also ignored if they would result in any\n",
      "     |          single class carrying a negative weight in either child node.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_transformed : sparse matrix of shape (n_samples, n_out)\n",
      "     |          Transformed dataset.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Transform dataset.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Input data to be transformed. Use ``dtype=np.float32`` for maximum\n",
      "     |          efficiency. Sparse matrices are also supported, use sparse\n",
      "     |          ``csr_matrix`` for maximum efficiency.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_transformed : sparse matrix of shape (n_samples, n_out)\n",
      "     |          Transformed dataset.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  criterion = 'mse'\n",
      "     |  \n",
      "     |  max_features = 1\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from BaseForest:\n",
      "     |  \n",
      "     |  apply(self, X)\n",
      "     |      Apply trees in the forest to X, return leaf indices.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      "     |          For each datapoint x in X and for each tree in the forest,\n",
      "     |          return the index of the leaf x ends up in.\n",
      "     |  \n",
      "     |  decision_path(self, X)\n",
      "     |      Return the decision path in the forest.\n",
      "     |      \n",
      "     |      .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples. Internally, its dtype will be converted to\n",
      "     |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      "     |          converted into a sparse ``csr_matrix``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      "     |          Return a node indicator matrix where non zero elements indicates\n",
      "     |          that the samples goes through the nodes. The matrix is of CSR\n",
      "     |          format.\n",
      "     |      \n",
      "     |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      "     |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      "     |          gives the indicator value for the i-th estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from BaseForest:\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      The impurity-based feature importances.\n",
      "     |      \n",
      "     |      The higher, the more important the feature.\n",
      "     |      The importance of a feature is computed as the (normalized)\n",
      "     |      total reduction of the criterion brought by that feature.  It is also\n",
      "     |      known as the Gini importance.\n",
      "     |      \n",
      "     |      Warning: impurity-based feature importances can be misleading for\n",
      "     |      high cardinality features (many unique values). See\n",
      "     |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : ndarray of shape (n_features,)\n",
      "     |          The values of this array sum to 1, unless all trees are single node\n",
      "     |          trees consisting of only the root node, in which case it will be an\n",
      "     |          array of zeros.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |      Return the index'th estimator in the ensemble.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Return iterator over estimators in the ensemble.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of estimators in the ensemble.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      "     |  \n",
      "     |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : dict\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      "     |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      "     |      possible to update each component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : estimator instance\n",
      "     |          Estimator instance.\n",
      "    \n",
      "    class StackingClassifier(sklearn.base.ClassifierMixin, _BaseStacking)\n",
      "     |  StackingClassifier(estimators, final_estimator=None, *, cv=None, stack_method='auto', n_jobs=None, passthrough=False, verbose=0)\n",
      "     |  \n",
      "     |  Stack of estimators with a final classifier.\n",
      "     |  \n",
      "     |  Stacked generalization consists in stacking the output of individual\n",
      "     |  estimator and use a classifier to compute the final prediction. Stacking\n",
      "     |  allows to use the strength of each individual estimator by using their\n",
      "     |  output as input of a final estimator.\n",
      "     |  \n",
      "     |  Note that `estimators_` are fitted on the full `X` while `final_estimator_`\n",
      "     |  is trained using cross-validated predictions of the base estimators using\n",
      "     |  `cross_val_predict`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <stacking>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimators : list of (str, estimator)\n",
      "     |      Base estimators which will be stacked together. Each element of the\n",
      "     |      list is defined as a tuple of string (i.e. name) and an estimator\n",
      "     |      instance. An estimator can be set to 'drop' using `set_params`.\n",
      "     |  \n",
      "     |  final_estimator : estimator, default=None\n",
      "     |      A classifier which will be used to combine the base estimators.\n",
      "     |      The default classifier is a\n",
      "     |      :class:`~sklearn.linear_model.LogisticRegression`.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy used in\n",
      "     |      `cross_val_predict` to train `final_estimator`. Possible inputs for\n",
      "     |      cv are:\n",
      "     |  \n",
      "     |      * None, to use the default 5-fold cross validation,\n",
      "     |      * integer, to specify the number of folds in a (Stratified) KFold,\n",
      "     |      * An object to be used as a cross-validation generator,\n",
      "     |      * An iterable yielding train, test splits.\n",
      "     |  \n",
      "     |      For integer/None inputs, if the estimator is a classifier and y is\n",
      "     |      either binary or multiclass,\n",
      "     |      :class:`~sklearn.model_selection.StratifiedKFold` is used.\n",
      "     |      In all other cases, :class:`~sklearn.model_selection.KFold` is used.\n",
      "     |      These splitters are instantiated with `shuffle=False` so the splits\n",
      "     |      will be the same across calls.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. note::\n",
      "     |         A larger number of split will provide no benefits if the number\n",
      "     |         of training samples is large enough. Indeed, the training time\n",
      "     |         will increase. ``cv`` is not used for model evaluation but for\n",
      "     |         prediction.\n",
      "     |  \n",
      "     |  stack_method : {'auto', 'predict_proba', 'decision_function', 'predict'},             default='auto'\n",
      "     |      Methods called for each base estimator. It can be:\n",
      "     |  \n",
      "     |      * if 'auto', it will try to invoke, for each estimator,\n",
      "     |        `'predict_proba'`, `'decision_function'` or `'predict'` in that\n",
      "     |        order.\n",
      "     |      * otherwise, one of `'predict_proba'`, `'decision_function'` or\n",
      "     |        `'predict'`. If the method is not implemented by the estimator, it\n",
      "     |        will raise an error.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel all `estimators` `fit`.\n",
      "     |      `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n",
      "     |      using all processors. See Glossary for more details.\n",
      "     |  \n",
      "     |  passthrough : bool, default=False\n",
      "     |      When False, only the predictions of estimators will be used as\n",
      "     |      training data for `final_estimator`. When True, the\n",
      "     |      `final_estimator` is trained on the predictions as well as the\n",
      "     |      original training data.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Verbosity level.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  classes_ : ndarray of shape (n_classes,)\n",
      "     |      Class labels.\n",
      "     |  \n",
      "     |  estimators_ : list of estimators\n",
      "     |      The elements of the estimators parameter, having been fitted on the\n",
      "     |      training data. If an estimator has been set to `'drop'`, it\n",
      "     |      will not appear in `estimators_`.\n",
      "     |  \n",
      "     |  named_estimators_ : :class:`~sklearn.utils.Bunch`\n",
      "     |      Attribute to access any fitted sub-estimators by name.\n",
      "     |  \n",
      "     |  final_estimator_ : estimator\n",
      "     |      The classifier which predicts given the output of `estimators_`.\n",
      "     |  \n",
      "     |  stack_method_ : list of str\n",
      "     |      The method used by each base estimator.\n",
      "     |  \n",
      "     |  Notes\n",
      "     |  -----\n",
      "     |  When `predict_proba` is used by each estimator (i.e. most of the time for\n",
      "     |  `stack_method='auto'` or specifically for `stack_method='predict_proba'`),\n",
      "     |  The first column predicted by each estimator will be dropped in the case\n",
      "     |  of a binary classification problem. Indeed, both feature will be perfectly\n",
      "     |  collinear.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2\n",
      "     |     (1992): 241-259.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_iris\n",
      "     |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      "     |  >>> from sklearn.svm import LinearSVC\n",
      "     |  >>> from sklearn.linear_model import LogisticRegression\n",
      "     |  >>> from sklearn.preprocessing import StandardScaler\n",
      "     |  >>> from sklearn.pipeline import make_pipeline\n",
      "     |  >>> from sklearn.ensemble import StackingClassifier\n",
      "     |  >>> X, y = load_iris(return_X_y=True)\n",
      "     |  >>> estimators = [\n",
      "     |  ...     ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n",
      "     |  ...     ('svr', make_pipeline(StandardScaler(),\n",
      "     |  ...                           LinearSVC(random_state=42)))\n",
      "     |  ... ]\n",
      "     |  >>> clf = StackingClassifier(\n",
      "     |  ...     estimators=estimators, final_estimator=LogisticRegression()\n",
      "     |  ... )\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |  ...     X, y, stratify=y, random_state=42\n",
      "     |  ... )\n",
      "     |  >>> clf.fit(X_train, y_train).score(X_test, y_test)\n",
      "     |  0.9...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StackingClassifier\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      _BaseStacking\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.ensemble._base._BaseHeterogeneousEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.utils.metaestimators._BaseComposition\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimators, final_estimator=None, *, cv=None, stack_method='auto', n_jobs=None, passthrough=False, verbose=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  decision_function(self, X)\n",
      "     |      Predict decision function for samples in X using\n",
      "     |      `final_estimator_.decision_function`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      decisions : ndarray of shape (n_samples,), (n_samples, n_classes),             or (n_samples, n_classes * (n_classes-1) / 2)\n",
      "     |          The decision function computed the final estimator.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the estimators.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted.\n",
      "     |          Note that this is supported only if all underlying estimators\n",
      "     |          support sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  predict(self, X, **predict_params)\n",
      "     |      Predict target for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      **predict_params : dict of str -> obj\n",
      "     |          Parameters to the `predict` called by the `final_estimator`. Note\n",
      "     |          that this may be used to return uncertainties from some estimators\n",
      "     |          with `return_std` or `return_cov`. Be aware that it will only\n",
      "     |          accounts for uncertainty in the final estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,) or (n_samples, n_output)\n",
      "     |          Predicted targets.\n",
      "     |  \n",
      "     |  predict_proba(self, X)\n",
      "     |      Predict class probabilities for X using\n",
      "     |      `final_estimator_.predict_proba`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      probabilities : ndarray of shape (n_samples, n_classes) or             list of ndarray of shape (n_output,)\n",
      "     |          The class probabilities of the input samples.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Return class labels or probabilities for X for each estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_preds : ndarray of shape (n_samples, n_estimators) or                 (n_samples, n_classes * n_estimators)\n",
      "     |          Prediction outputs for each estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from _BaseStacking:\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get the parameters of an estimator from the ensemble.\n",
      "     |      \n",
      "     |      Returns the parameters given in the constructor as well as the\n",
      "     |      estimators contained within the `estimators` parameter.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          Setting it to True gets the various estimators and the parameters\n",
      "     |          of the estimators as well.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of an estimator from the ensemble.\n",
      "     |      \n",
      "     |      Valid parameter keys can be listed with `get_params()`. Note that you\n",
      "     |      can directly set the parameters of the estimators contained in\n",
      "     |      `estimators`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : keyword arguments\n",
      "     |          Specific parameters using e.g.\n",
      "     |          `set_params(parameter_name=new_value)`. In addition, to setting the\n",
      "     |          parameters of the estimator, the individual estimator of the\n",
      "     |          estimators can also be set, or can be removed by setting them to\n",
      "     |          'drop'.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
      "     |  \n",
      "     |  named_estimators\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.utils.metaestimators._BaseComposition:\n",
      "     |  \n",
      "     |  __annotations__ = {'steps': typing.List[typing.Any]}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "    \n",
      "    class StackingRegressor(sklearn.base.RegressorMixin, _BaseStacking)\n",
      "     |  StackingRegressor(estimators, final_estimator=None, *, cv=None, n_jobs=None, passthrough=False, verbose=0)\n",
      "     |  \n",
      "     |  Stack of estimators with a final regressor.\n",
      "     |  \n",
      "     |  Stacked generalization consists in stacking the output of individual\n",
      "     |  estimator and use a regressor to compute the final prediction. Stacking\n",
      "     |  allows to use the strength of each individual estimator by using their\n",
      "     |  output as input of a final estimator.\n",
      "     |  \n",
      "     |  Note that `estimators_` are fitted on the full `X` while `final_estimator_`\n",
      "     |  is trained using cross-validated predictions of the base estimators using\n",
      "     |  `cross_val_predict`.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <stacking>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.22\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimators : list of (str, estimator)\n",
      "     |      Base estimators which will be stacked together. Each element of the\n",
      "     |      list is defined as a tuple of string (i.e. name) and an estimator\n",
      "     |      instance. An estimator can be set to 'drop' using `set_params`.\n",
      "     |  \n",
      "     |  final_estimator : estimator, default=None\n",
      "     |      A regressor which will be used to combine the base estimators.\n",
      "     |      The default regressor is a :class:`~sklearn.linear_model.RidgeCV`.\n",
      "     |  \n",
      "     |  cv : int, cross-validation generator or an iterable, default=None\n",
      "     |      Determines the cross-validation splitting strategy used in\n",
      "     |      `cross_val_predict` to train `final_estimator`. Possible inputs for\n",
      "     |      cv are:\n",
      "     |  \n",
      "     |      * None, to use the default 5-fold cross validation,\n",
      "     |      * integer, to specify the number of folds in a (Stratified) KFold,\n",
      "     |      * An object to be used as a cross-validation generator,\n",
      "     |      * An iterable yielding train, test splits.\n",
      "     |  \n",
      "     |      For integer/None inputs, if the estimator is a classifier and y is\n",
      "     |      either binary or multiclass,\n",
      "     |      :class:`~sklearn.model_selection.StratifiedKFold` is used.\n",
      "     |      In all other cases, :class:`~sklearn.model_selection.KFold` is used.\n",
      "     |      These splitters are instantiated with `shuffle=False` so the splits\n",
      "     |      will be the same across calls.\n",
      "     |  \n",
      "     |      Refer :ref:`User Guide <cross_validation>` for the various\n",
      "     |      cross-validation strategies that can be used here.\n",
      "     |  \n",
      "     |      .. note::\n",
      "     |         A larger number of split will provide no benefits if the number\n",
      "     |         of training samples is large enough. Indeed, the training time\n",
      "     |         will increase. ``cv`` is not used for model evaluation but for\n",
      "     |         prediction.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel for `fit` of all `estimators`.\n",
      "     |      `None` means 1 unless in a `joblib.parallel_backend` context. -1 means\n",
      "     |      using all processors. See Glossary for more details.\n",
      "     |  \n",
      "     |  passthrough : bool, default=False\n",
      "     |      When False, only the predictions of estimators will be used as\n",
      "     |      training data for `final_estimator`. When True, the\n",
      "     |      `final_estimator` is trained on the predictions as well as the\n",
      "     |      original training data.\n",
      "     |  \n",
      "     |  verbose : int, default=0\n",
      "     |      Verbosity level.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of estimator\n",
      "     |      The elements of the estimators parameter, having been fitted on the\n",
      "     |      training data. If an estimator has been set to `'drop'`, it\n",
      "     |      will not appear in `estimators_`.\n",
      "     |  \n",
      "     |  named_estimators_ : :class:`~sklearn.utils.Bunch`\n",
      "     |      Attribute to access any fitted sub-estimators by name.\n",
      "     |  \n",
      "     |  \n",
      "     |  final_estimator_ : estimator\n",
      "     |      The regressor to stacked the base estimators fitted.\n",
      "     |  \n",
      "     |  References\n",
      "     |  ----------\n",
      "     |  .. [1] Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2\n",
      "     |     (1992): 241-259.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from sklearn.datasets import load_diabetes\n",
      "     |  >>> from sklearn.linear_model import RidgeCV\n",
      "     |  >>> from sklearn.svm import LinearSVR\n",
      "     |  >>> from sklearn.ensemble import RandomForestRegressor\n",
      "     |  >>> from sklearn.ensemble import StackingRegressor\n",
      "     |  >>> X, y = load_diabetes(return_X_y=True)\n",
      "     |  >>> estimators = [\n",
      "     |  ...     ('lr', RidgeCV()),\n",
      "     |  ...     ('svr', LinearSVR(random_state=42))\n",
      "     |  ... ]\n",
      "     |  >>> reg = StackingRegressor(\n",
      "     |  ...     estimators=estimators,\n",
      "     |  ...     final_estimator=RandomForestRegressor(n_estimators=10,\n",
      "     |  ...                                           random_state=42)\n",
      "     |  ... )\n",
      "     |  >>> from sklearn.model_selection import train_test_split\n",
      "     |  >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "     |  ...     X, y, random_state=42\n",
      "     |  ... )\n",
      "     |  >>> reg.fit(X_train, y_train).score(X_test, y_test)\n",
      "     |  0.3...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StackingRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      _BaseStacking\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.ensemble._base._BaseHeterogeneousEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.utils.metaestimators._BaseComposition\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimators, final_estimator=None, *, cv=None, n_jobs=None, passthrough=False, verbose=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the estimators.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted.\n",
      "     |          Note that this is supported only if all underlying estimators\n",
      "     |          support sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Return the predictions for X for each estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where `n_samples` is the number of samples and\n",
      "     |          `n_features` is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_preds : ndarray of shape (n_samples, n_estimators)\n",
      "     |          Prediction outputs for each estimator.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseStacking:\n",
      "     |  \n",
      "     |  predict(self, X, **predict_params)\n",
      "     |      Predict target for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      **predict_params : dict of str -> obj\n",
      "     |          Parameters to the `predict` called by the `final_estimator`. Note\n",
      "     |          that this may be used to return uncertainties from some estimators\n",
      "     |          with `return_std` or `return_cov`. Be aware that it will only\n",
      "     |          accounts for uncertainty in the final estimator.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y_pred : ndarray of shape (n_samples,) or (n_samples, n_output)\n",
      "     |          Predicted targets.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from _BaseStacking:\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |      Number of features seen during :term:`fit`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.TransformerMixin:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Fit to data, then transform it.\n",
      "     |      \n",
      "     |      Fits transformer to `X` and `y` with optional parameters `fit_params`\n",
      "     |      and returns a transformed version of `X`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Input samples.\n",
      "     |      \n",
      "     |      y :  array-like of shape (n_samples,) or (n_samples, n_outputs),                 default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get the parameters of an estimator from the ensemble.\n",
      "     |      \n",
      "     |      Returns the parameters given in the constructor as well as the\n",
      "     |      estimators contained within the `estimators` parameter.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          Setting it to True gets the various estimators and the parameters\n",
      "     |          of the estimators as well.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of an estimator from the ensemble.\n",
      "     |      \n",
      "     |      Valid parameter keys can be listed with `get_params()`. Note that you\n",
      "     |      can directly set the parameters of the estimators contained in\n",
      "     |      `estimators`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : keyword arguments\n",
      "     |          Specific parameters using e.g.\n",
      "     |          `set_params(parameter_name=new_value)`. In addition, to setting the\n",
      "     |          parameters of the estimator, the individual estimator of the\n",
      "     |          estimators can also be set, or can be removed by setting them to\n",
      "     |          'drop'.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
      "     |  \n",
      "     |  named_estimators\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.utils.metaestimators._BaseComposition:\n",
      "     |  \n",
      "     |  __annotations__ = {'steps': typing.List[typing.Any]}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "    \n",
      "    class VotingClassifier(sklearn.base.ClassifierMixin, _BaseVoting)\n",
      "     |  VotingClassifier(estimators, *, voting='hard', weights=None, n_jobs=None, flatten_transform=True, verbose=False)\n",
      "     |  \n",
      "     |  Soft Voting/Majority Rule classifier for unfitted estimators.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <voting_classifier>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.17\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimators : list of (str, estimator) tuples\n",
      "     |      Invoking the ``fit`` method on the ``VotingClassifier`` will fit clones\n",
      "     |      of those original estimators that will be stored in the class attribute\n",
      "     |      ``self.estimators_``. An estimator can be set to ``'drop'``\n",
      "     |      using ``set_params``.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.21\n",
      "     |          ``'drop'`` is accepted. Using None was deprecated in 0.22 and\n",
      "     |          support was removed in 0.24.\n",
      "     |  \n",
      "     |  voting : {'hard', 'soft'}, default='hard'\n",
      "     |      If 'hard', uses predicted class labels for majority rule voting.\n",
      "     |      Else if 'soft', predicts the class label based on the argmax of\n",
      "     |      the sums of the predicted probabilities, which is recommended for\n",
      "     |      an ensemble of well-calibrated classifiers.\n",
      "     |  \n",
      "     |  weights : array-like of shape (n_classifiers,), default=None\n",
      "     |      Sequence of weights (`float` or `int`) to weight the occurrences of\n",
      "     |      predicted class labels (`hard` voting) or class probabilities\n",
      "     |      before averaging (`soft` voting). Uses uniform weights if `None`.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel for ``fit``.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.18\n",
      "     |  \n",
      "     |  flatten_transform : bool, default=True\n",
      "     |      Affects shape of transform output only when voting='soft'\n",
      "     |      If voting='soft' and flatten_transform=True, transform method returns\n",
      "     |      matrix with shape (n_samples, n_classifiers * n_classes). If\n",
      "     |      flatten_transform=False, it returns\n",
      "     |      (n_classifiers, n_samples, n_classes).\n",
      "     |  \n",
      "     |  verbose : bool, default=False\n",
      "     |      If True, the time elapsed while fitting will be printed as it\n",
      "     |      is completed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of classifiers\n",
      "     |      The collection of fitted sub-estimators as defined in ``estimators``\n",
      "     |      that are not 'drop'.\n",
      "     |  \n",
      "     |  named_estimators_ : :class:`~sklearn.utils.Bunch`\n",
      "     |      Attribute to access any fitted sub-estimators by name.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  classes_ : array-like of shape (n_predictions,)\n",
      "     |      The classes labels.\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  VotingRegressor : Prediction voting regressor.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.linear_model import LogisticRegression\n",
      "     |  >>> from sklearn.naive_bayes import GaussianNB\n",
      "     |  >>> from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
      "     |  >>> clf1 = LogisticRegression(multi_class='multinomial', random_state=1)\n",
      "     |  >>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
      "     |  >>> clf3 = GaussianNB()\n",
      "     |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      "     |  >>> y = np.array([1, 1, 1, 2, 2, 2])\n",
      "     |  >>> eclf1 = VotingClassifier(estimators=[\n",
      "     |  ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\n",
      "     |  >>> eclf1 = eclf1.fit(X, y)\n",
      "     |  >>> print(eclf1.predict(X))\n",
      "     |  [1 1 1 2 2 2]\n",
      "     |  >>> np.array_equal(eclf1.named_estimators_.lr.predict(X),\n",
      "     |  ...                eclf1.named_estimators_['lr'].predict(X))\n",
      "     |  True\n",
      "     |  >>> eclf2 = VotingClassifier(estimators=[\n",
      "     |  ...         ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
      "     |  ...         voting='soft')\n",
      "     |  >>> eclf2 = eclf2.fit(X, y)\n",
      "     |  >>> print(eclf2.predict(X))\n",
      "     |  [1 1 1 2 2 2]\n",
      "     |  >>> eclf3 = VotingClassifier(estimators=[\n",
      "     |  ...        ('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
      "     |  ...        voting='soft', weights=[2,1,1],\n",
      "     |  ...        flatten_transform=True)\n",
      "     |  >>> eclf3 = eclf3.fit(X, y)\n",
      "     |  >>> print(eclf3.predict(X))\n",
      "     |  [1 1 1 2 2 2]\n",
      "     |  >>> print(eclf3.transform(X).shape)\n",
      "     |  (6, 6)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      VotingClassifier\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      _BaseVoting\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.ensemble._base._BaseHeterogeneousEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.utils.metaestimators._BaseComposition\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimators, *, voting='hard', weights=None, n_jobs=None, flatten_transform=True, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the estimators.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted.\n",
      "     |          Note that this is supported only if all underlying estimators\n",
      "     |          support sample weights.\n",
      "     |      \n",
      "     |          .. versionadded:: 0.18\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict class labels for X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      maj : array-like of shape (n_samples,)\n",
      "     |          Predicted class labels.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Return class labels or probabilities for X for each estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      probabilities_or_labels\n",
      "     |          If `voting='soft'` and `flatten_transform=True`:\n",
      "     |              returns ndarray of shape (n_classifiers, n_samples *\n",
      "     |              n_classes), being class probabilities calculated by each\n",
      "     |              classifier.\n",
      "     |          If `voting='soft' and `flatten_transform=False`:\n",
      "     |              ndarray of shape (n_classifiers, n_samples, n_classes)\n",
      "     |          If `voting='hard'`:\n",
      "     |              ndarray of shape (n_samples, n_classifiers), being\n",
      "     |              class labels predicted by each classifier.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  predict_proba\n",
      "     |      Compute probabilities of possible outcomes for samples in X.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      avg : array-like of shape (n_samples, n_classes)\n",
      "     |          Weighted average probability for each class per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True labels for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseVoting:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Return class labels or probabilities for each estimator.\n",
      "     |      \n",
      "     |      Return predictions for X for each estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      "     |          Input samples\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,), default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from _BaseVoting:\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get the parameters of an estimator from the ensemble.\n",
      "     |      \n",
      "     |      Returns the parameters given in the constructor as well as the\n",
      "     |      estimators contained within the `estimators` parameter.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          Setting it to True gets the various estimators and the parameters\n",
      "     |          of the estimators as well.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of an estimator from the ensemble.\n",
      "     |      \n",
      "     |      Valid parameter keys can be listed with `get_params()`. Note that you\n",
      "     |      can directly set the parameters of the estimators contained in\n",
      "     |      `estimators`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : keyword arguments\n",
      "     |          Specific parameters using e.g.\n",
      "     |          `set_params(parameter_name=new_value)`. In addition, to setting the\n",
      "     |          parameters of the estimator, the individual estimator of the\n",
      "     |          estimators can also be set, or can be removed by setting them to\n",
      "     |          'drop'.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
      "     |  \n",
      "     |  named_estimators\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.utils.metaestimators._BaseComposition:\n",
      "     |  \n",
      "     |  __annotations__ = {'steps': typing.List[typing.Any]}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "    \n",
      "    class VotingRegressor(sklearn.base.RegressorMixin, _BaseVoting)\n",
      "     |  VotingRegressor(estimators, *, weights=None, n_jobs=None, verbose=False)\n",
      "     |  \n",
      "     |  Prediction voting regressor for unfitted estimators.\n",
      "     |  \n",
      "     |  A voting regressor is an ensemble meta-estimator that fits several base\n",
      "     |  regressors, each on the whole dataset. Then it averages the individual\n",
      "     |  predictions to form a final prediction.\n",
      "     |  \n",
      "     |  Read more in the :ref:`User Guide <voting_regressor>`.\n",
      "     |  \n",
      "     |  .. versionadded:: 0.21\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  estimators : list of (str, estimator) tuples\n",
      "     |      Invoking the ``fit`` method on the ``VotingRegressor`` will fit clones\n",
      "     |      of those original estimators that will be stored in the class attribute\n",
      "     |      ``self.estimators_``. An estimator can be set to ``'drop'`` using\n",
      "     |      ``set_params``.\n",
      "     |  \n",
      "     |      .. versionchanged:: 0.21\n",
      "     |          ``'drop'`` is accepted. Using None was deprecated in 0.22 and\n",
      "     |          support was removed in 0.24.\n",
      "     |  \n",
      "     |  weights : array-like of shape (n_regressors,), default=None\n",
      "     |      Sequence of weights (`float` or `int`) to weight the occurrences of\n",
      "     |      predicted values before averaging. Uses uniform weights if `None`.\n",
      "     |  \n",
      "     |  n_jobs : int, default=None\n",
      "     |      The number of jobs to run in parallel for ``fit``.\n",
      "     |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "     |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "     |      for more details.\n",
      "     |  \n",
      "     |  verbose : bool, default=False\n",
      "     |      If True, the time elapsed while fitting will be printed as it\n",
      "     |      is completed.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.23\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  estimators_ : list of regressors\n",
      "     |      The collection of fitted sub-estimators as defined in ``estimators``\n",
      "     |      that are not 'drop'.\n",
      "     |  \n",
      "     |  named_estimators_ : Bunch\n",
      "     |      Attribute to access any fitted sub-estimators by name.\n",
      "     |  \n",
      "     |      .. versionadded:: 0.20\n",
      "     |  \n",
      "     |  See Also\n",
      "     |  --------\n",
      "     |  VotingClassifier : Soft Voting/Majority Rule classifier.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> import numpy as np\n",
      "     |  >>> from sklearn.linear_model import LinearRegression\n",
      "     |  >>> from sklearn.ensemble import RandomForestRegressor\n",
      "     |  >>> from sklearn.ensemble import VotingRegressor\n",
      "     |  >>> r1 = LinearRegression()\n",
      "     |  >>> r2 = RandomForestRegressor(n_estimators=10, random_state=1)\n",
      "     |  >>> X = np.array([[1, 1], [2, 4], [3, 9], [4, 16], [5, 25], [6, 36]])\n",
      "     |  >>> y = np.array([2, 6, 12, 20, 30, 42])\n",
      "     |  >>> er = VotingRegressor([('lr', r1), ('rf', r2)])\n",
      "     |  >>> print(er.fit(X, y).predict(X))\n",
      "     |  [ 3.3  5.7 11.8 19.7 28.  40.3]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      VotingRegressor\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      _BaseVoting\n",
      "     |      sklearn.base.TransformerMixin\n",
      "     |      sklearn.ensemble._base._BaseHeterogeneousEnsemble\n",
      "     |      sklearn.base.MetaEstimatorMixin\n",
      "     |      sklearn.utils.metaestimators._BaseComposition\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, estimators, *, weights=None, n_jobs=None, verbose=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None)\n",
      "     |      Fit the estimators.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          Training vectors, where n_samples is the number of samples and\n",
      "     |          n_features is the number of features.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,)\n",
      "     |          Target values.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights. If None, then samples are equally weighted.\n",
      "     |          Note that this is supported only if all underlying estimators\n",
      "     |          support sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Fitted estimator.\n",
      "     |  \n",
      "     |  predict(self, X)\n",
      "     |      Predict regression target for X.\n",
      "     |      \n",
      "     |      The predicted regression target of an input sample is computed as the\n",
      "     |      mean predicted regression targets of the estimators in the ensemble.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      y : ndarray of shape (n_samples,)\n",
      "     |          The predicted values.\n",
      "     |  \n",
      "     |  transform(self, X)\n",
      "     |      Return predictions for X for each estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      "     |          The input samples.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      predictions: ndarray of shape (n_samples, n_classifiers)\n",
      "     |          Values predicted by each regressor.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Return the coefficient of determination :math:`R^2` of the\n",
      "     |      prediction.\n",
      "     |      \n",
      "     |      The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`,\n",
      "     |      where :math:`u` is the residual sum of squares ``((y_true - y_pred)\n",
      "     |      ** 2).sum()`` and :math:`v` is the total sum of squares ``((y_true -\n",
      "     |      y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it\n",
      "     |      can be negative (because the model can be arbitrarily worse). A\n",
      "     |      constant model that always predicts the expected value of `y`,\n",
      "     |      disregarding the input features, would get a :math:`R^2` score of\n",
      "     |      0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like of shape (n_samples, n_features)\n",
      "     |          Test samples. For some estimators this may be a precomputed\n",
      "     |          kernel matrix or a list of generic objects instead with shape\n",
      "     |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      "     |          is the number of samples used in the fitting for the estimator.\n",
      "     |      \n",
      "     |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      "     |          True values for `X`.\n",
      "     |      \n",
      "     |      sample_weight : array-like of shape (n_samples,), default=None\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      "     |      \n",
      "     |      Notes\n",
      "     |      -----\n",
      "     |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      "     |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      "     |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      "     |      This influences the ``score`` method of all the multioutput\n",
      "     |      regressors (except for\n",
      "     |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from _BaseVoting:\n",
      "     |  \n",
      "     |  fit_transform(self, X, y=None, **fit_params)\n",
      "     |      Return class labels or probabilities for each estimator.\n",
      "     |      \n",
      "     |      Return predictions for X for each estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : {array-like, sparse matrix, dataframe} of shape                 (n_samples, n_features)\n",
      "     |          Input samples\n",
      "     |      \n",
      "     |      y : ndarray of shape (n_samples,), default=None\n",
      "     |          Target values (None for unsupervised transformations).\n",
      "     |      \n",
      "     |      **fit_params : dict\n",
      "     |          Additional fit parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_new : ndarray array of shape (n_samples, n_features_new)\n",
      "     |          Transformed array.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from _BaseVoting:\n",
      "     |  \n",
      "     |  n_features_in_\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get the parameters of an estimator from the ensemble.\n",
      "     |      \n",
      "     |      Returns the parameters given in the constructor as well as the\n",
      "     |      estimators contained within the `estimators` parameter.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          Setting it to True gets the various estimators and the parameters\n",
      "     |          of the estimators as well.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of an estimator from the ensemble.\n",
      "     |      \n",
      "     |      Valid parameter keys can be listed with `get_params()`. Note that you\n",
      "     |      can directly set the parameters of the estimators contained in\n",
      "     |      `estimators`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : keyword arguments\n",
      "     |          Specific parameters using e.g.\n",
      "     |          `set_params(parameter_name=new_value)`. In addition, to setting the\n",
      "     |          parameters of the estimator, the individual estimator of the\n",
      "     |          estimators can also be set, or can be removed by setting them to\n",
      "     |          'drop'.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from sklearn.ensemble._base._BaseHeterogeneousEnsemble:\n",
      "     |  \n",
      "     |  named_estimators\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from sklearn.utils.metaestimators._BaseComposition:\n",
      "     |  \n",
      "     |  __annotations__ = {'steps': typing.List[typing.Any]}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "\n",
      "DATA\n",
      "    __all__ = ['BaseEnsemble', 'RandomForestClassifier', 'RandomForestRegr...\n",
      "\n",
      "FILE\n",
      "    /opt/anaconda3/envs/ksunsupervised/lib/python3.9/site-packages/sklearn/ensemble/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ensemble?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdb00f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2650cfe",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class RandomForestClassifier in module sklearn.ensemble._forest:\n",
      "\n",
      "class RandomForestClassifier(ForestClassifier)\n",
      " |  RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      " |  \n",
      " |  A random forest classifier.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of decision tree\n",
      " |  classifiers on various sub-samples of the dataset and uses averaging to\n",
      " |  improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is controlled with the `max_samples` parameter if\n",
      " |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      " |  each tree.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : int, default=100\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |         The default value of ``n_estimators`` changed from 10 to 100\n",
      " |         in 0.22.\n",
      " |  \n",
      " |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
      " |      The function to measure the quality of a split. Supported criteria are\n",
      " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
      " |      Note: this parameter is tree-specific.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `round(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, default=None\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 1.0 (renaming of 0.25).\n",
      " |         Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  bootstrap : bool, default=True\n",
      " |      Whether bootstrap samples are used when building trees. If False, the\n",
      " |      whole dataset is used to build each tree.\n",
      " |  \n",
      " |  oob_score : bool, default=False\n",
      " |      Whether to use out-of-bag samples to estimate the generalization score.\n",
      " |      Only available if bootstrap=True.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      " |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      " |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      " |      <n_jobs>` for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls both the randomness of the bootstrapping of the samples used\n",
      " |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
      " |      features to consider when looking for the best split at each node\n",
      " |      (if ``max_features < n_features``).\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n",
      " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
      " |      If not given, all classes are supposed to have weight one. For\n",
      " |      multi-output problems, a list of dicts can be provided in the same\n",
      " |      order as the columns of y.\n",
      " |  \n",
      " |      Note that for multioutput (including multilabel) weights should be\n",
      " |      defined for each class of every column in its own dict. For example,\n",
      " |      for four-class multilabel classification weights should be\n",
      " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
      " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
      " |      weights are computed based on the bootstrap sample for every tree\n",
      " |      grown.\n",
      " |  \n",
      " |      For multi-output, the weights of each column of y will be multiplied.\n",
      " |  \n",
      " |      Note that these weights will be multiplied with sample_weight (passed\n",
      " |      through the fit method) if sample_weight is specified.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  max_samples : int or float, default=None\n",
      " |      If bootstrap is True, the number of samples to draw from X\n",
      " |      to train each base estimator.\n",
      " |  \n",
      " |      - If None (default), then draw `X.shape[0]` samples.\n",
      " |      - If int, then draw `max_samples` samples.\n",
      " |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
      " |        `max_samples` should be in the interval `(0, 1)`.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  base_estimator_ : DecisionTreeClassifier\n",
      " |      The child estimator template used to create the collection of fitted\n",
      " |      sub-estimators.\n",
      " |  \n",
      " |  estimators_ : list of DecisionTreeClassifier\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
      " |      The classes labels (single output problem), or a list of arrays of\n",
      " |      class labels (multi-output problem).\n",
      " |  \n",
      " |  n_classes_ : int or list\n",
      " |      The number of classes (single output problem), or a list containing the\n",
      " |      number of classes for each output (multi-output problem).\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  oob_decision_function_ : ndarray of shape (n_samples, n_classes)\n",
      " |      Decision function computed with out-of-bag estimate on the training\n",
      " |      set. If n_estimators is small it might be possible that a data point\n",
      " |      was never left out during the bootstrap. In this case,\n",
      " |      `oob_decision_function_` might contain NaN. This attribute exists\n",
      " |      only when ``oob_score`` is True.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  DecisionTreeClassifier, ExtraTreesClassifier\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      " |  ...                            n_informative=2, n_redundant=0,\n",
      " |  ...                            random_state=0, shuffle=False)\n",
      " |  >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  RandomForestClassifier(...)\n",
      " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestClassifier\n",
      " |      ForestClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseForest\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestClassifier:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is a vote by the trees in\n",
      " |      the forest, weighted by their probability estimates. That is,\n",
      " |      the predicted class is the one with highest mean probability\n",
      " |      estimate across the trees.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the log of the mean predicted class probabilities of the trees in the\n",
      " |      forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes), or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample are computed as\n",
      " |      the mean predicted class probabilities of the trees in the forest.\n",
      " |      The class probability of a single tree is the fraction of samples of\n",
      " |      the same class in a leaf.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes), or a list of n_outputs\n",
      " |          such arrays if n_outputs > 1.\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator matrix where non zero elements indicates\n",
      " |          that the samples goes through the nodes. The matrix is of CSR\n",
      " |          format.\n",
      " |      \n",
      " |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RandomForestClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30614e5c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GradientBoostingClassifier in module sklearn.ensemble._gb:\n",
      "\n",
      "class GradientBoostingClassifier(sklearn.base.ClassifierMixin, BaseGradientBoosting)\n",
      " |  GradientBoostingClassifier(*, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
      " |  \n",
      " |  Gradient Boosting for classification.\n",
      " |  \n",
      " |  GB builds an additive model in a\n",
      " |  forward stage-wise fashion; it allows for the optimization of\n",
      " |  arbitrary differentiable loss functions. In each stage ``n_classes_``\n",
      " |  regression trees are fit on the negative gradient of the\n",
      " |  binomial or multinomial deviance loss function. Binary classification\n",
      " |  is a special case where only a single regression tree is induced.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <gradient_boosting>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  loss : {'deviance', 'exponential'}, default='deviance'\n",
      " |      The loss function to be optimized. 'deviance' refers to\n",
      " |      deviance (= logistic regression) for classification\n",
      " |      with probabilistic outputs. For loss 'exponential' gradient\n",
      " |      boosting recovers the AdaBoost algorithm.\n",
      " |  \n",
      " |  learning_rate : float, default=0.1\n",
      " |      Learning rate shrinks the contribution of each tree by `learning_rate`.\n",
      " |      There is a trade-off between learning_rate and n_estimators.\n",
      " |  \n",
      " |  n_estimators : int, default=100\n",
      " |      The number of boosting stages to perform. Gradient boosting\n",
      " |      is fairly robust to over-fitting so a large number usually\n",
      " |      results in better performance.\n",
      " |  \n",
      " |  subsample : float, default=1.0\n",
      " |      The fraction of samples to be used for fitting the individual base\n",
      " |      learners. If smaller than 1.0 this results in Stochastic Gradient\n",
      " |      Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
      " |      Choosing `subsample < 1.0` leads to a reduction of variance\n",
      " |      and an increase in bias.\n",
      " |  \n",
      " |  criterion : {'friedman_mse', 'mse', 'mae'}, default='friedman_mse'\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are 'friedman_mse' for the mean squared error with improvement\n",
      " |      score by Friedman, 'mse' for mean squared error, and 'mae' for\n",
      " |      the mean absolute error. The default value of 'friedman_mse' is\n",
      " |      generally the best as it can provide a better approximation in\n",
      " |      some cases.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |      .. deprecated:: 0.24\n",
      " |          `criterion='mae'` is deprecated and will be removed in version\n",
      " |          1.1 (renaming of 0.26). Use `criterion='friedman_mse'` or `'mse'`\n",
      " |          instead, as trees should use a least-square criterion in\n",
      " |          Gradient Boosting.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_depth : int, default=3\n",
      " |      The maximum depth of the individual regression estimators. The maximum\n",
      " |      depth limits the number of nodes in the tree. Tune this parameter\n",
      " |      for best performance; the best value depends on the interaction\n",
      " |      of the input variables.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  min_impurity_split : float, default=None\n",
      " |      Threshold for early stopping in tree growth. A node will split\n",
      " |      if its impurity is above the threshold, otherwise it is a leaf.\n",
      " |  \n",
      " |      .. deprecated:: 0.19\n",
      " |         ``min_impurity_split`` has been deprecated in favor of\n",
      " |         ``min_impurity_decrease`` in 0.19. The default value of\n",
      " |         ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      " |         will be removed in 1.0 (renaming of 0.25).\n",
      " |         Use ``min_impurity_decrease`` instead.\n",
      " |  \n",
      " |  init : estimator or 'zero', default=None\n",
      " |      An estimator object that is used to compute the initial predictions.\n",
      " |      ``init`` has to provide :meth:`fit` and :meth:`predict_proba`. If\n",
      " |      'zero', the initial raw predictions are set to zero. By default, a\n",
      " |      ``DummyEstimator`` predicting the classes priors is used.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the random seed given to each Tree estimator at each\n",
      " |      boosting iteration.\n",
      " |      In addition, it controls the random permutation of the features at\n",
      " |      each split (see Notes for more details).\n",
      " |      It also controls the random spliting of the training data to obtain a\n",
      " |      validation set if `n_iter_no_change` is not None.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  max_features : {'auto', 'sqrt', 'log2'}, int or float, default=None\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If 'auto', then `max_features=sqrt(n_features)`.\n",
      " |      - If 'sqrt', then `max_features=sqrt(n_features)`.\n",
      " |      - If 'log2', then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Choosing `max_features < n_features` leads to a reduction of variance\n",
      " |      and an increase in bias.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Enable verbose output. If 1 then it prints progress and performance\n",
      " |      once in a while (the more trees the lower the frequency). If greater\n",
      " |      than 1 then it prints progress and performance for every tree.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just erase the\n",
      " |      previous solution. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  validation_fraction : float, default=0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Must be between 0 and 1.\n",
      " |      Only used if ``n_iter_no_change`` is set to an integer.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  n_iter_no_change : int, default=None\n",
      " |      ``n_iter_no_change`` is used to decide if early stopping will be used\n",
      " |      to terminate training when validation score is not improving. By\n",
      " |      default it is set to None to disable early stopping. If set to a\n",
      " |      number, it will set aside ``validation_fraction`` size of the training\n",
      " |      data as validation and terminate training when validation score is not\n",
      " |      improving in all of the previous ``n_iter_no_change`` numbers of\n",
      " |      iterations. The split is stratified.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for the early stopping. When the loss is not improving\n",
      " |      by at least tol for ``n_iter_no_change`` iterations (if set to a\n",
      " |      number), the training stops.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  n_estimators_ : int\n",
      " |      The number of estimators as selected by early stopping (if\n",
      " |      ``n_iter_no_change`` is specified). Otherwise it is set to\n",
      " |      ``n_estimators``.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  oob_improvement_ : ndarray of shape (n_estimators,)\n",
      " |      The improvement in loss (= deviance) on the out-of-bag samples\n",
      " |      relative to the previous iteration.\n",
      " |      ``oob_improvement_[0]`` is the improvement in\n",
      " |      loss of the first stage over the ``init`` estimator.\n",
      " |      Only available if ``subsample < 1.0``\n",
      " |  \n",
      " |  train_score_ : ndarray of shape (n_estimators,)\n",
      " |      The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n",
      " |      model at iteration ``i`` on the in-bag sample.\n",
      " |      If ``subsample == 1`` this is the deviance on the training data.\n",
      " |  \n",
      " |  loss_ : LossFunction\n",
      " |      The concrete ``LossFunction`` object.\n",
      " |  \n",
      " |  init_ : estimator\n",
      " |      The estimator that provides the initial predictions.\n",
      " |      Set via the ``init`` argument or ``loss.init_estimator``.\n",
      " |  \n",
      " |  estimators_ : ndarray of DecisionTreeRegressor of shape (n_estimators, ``loss_.K``)\n",
      " |      The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary\n",
      " |      classification, otherwise n_classes.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      The classes labels.\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of data features.\n",
      " |  \n",
      " |  n_classes_ : int\n",
      " |      The number of classes.\n",
      " |  \n",
      " |  max_features_ : int\n",
      " |      The inferred value of max_features.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  HistGradientBoostingClassifier : Histogram-based Gradient Boosting\n",
      " |      Classification Tree.\n",
      " |  sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
      " |  RandomForestClassifier : A meta-estimator that fits a number of decision\n",
      " |      tree classifiers on various sub-samples of the dataset and uses\n",
      " |      averaging to improve the predictive accuracy and control over-fitting.\n",
      " |  AdaBoostClassifier : A meta-estimator that begins by fitting a classifier\n",
      " |      on the original dataset and then fits additional copies of the\n",
      " |      classifier on the same dataset where the weights of incorrectly\n",
      " |      classified instances are adjusted such that subsequent classifiers\n",
      " |      focus more on difficult cases.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data and\n",
      " |  ``max_features=n_features``, if the improvement of the criterion is\n",
      " |  identical for several splits enumerated during the search of the best\n",
      " |  split. To obtain a deterministic behaviour during fitting,\n",
      " |  ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
      " |  Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
      " |  \n",
      " |  J. Friedman, Stochastic Gradient Boosting, 1999\n",
      " |  \n",
      " |  T. Hastie, R. Tibshirani and J. Friedman.\n",
      " |  Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  The following example shows how to fit a gradient boosting classifier with\n",
      " |  100 decision stumps as weak learners.\n",
      " |  \n",
      " |  >>> from sklearn.datasets import make_hastie_10_2\n",
      " |  >>> from sklearn.ensemble import GradientBoostingClassifier\n",
      " |  \n",
      " |  >>> X, y = make_hastie_10_2(random_state=0)\n",
      " |  >>> X_train, X_test = X[:2000], X[2000:]\n",
      " |  >>> y_train, y_test = y[:2000], y[2000:]\n",
      " |  \n",
      " |  >>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
      " |  ...     max_depth=1, random_state=0).fit(X_train, y_train)\n",
      " |  >>> clf.score(X_test, y_test)\n",
      " |  0.913...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GradientBoostingClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseGradientBoosting\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Compute the decision function of ``X``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : ndarray of shape (n_samples, n_classes) or (n_samples,)\n",
      " |          The decision function of the input samples, which corresponds to\n",
      " |          the raw values predicted from the trees of the ensemble . The\n",
      " |          order of the classes corresponds to that in the attribute\n",
      " |          :term:`classes_`. Regression and binary classification produce an\n",
      " |          array of shape (n_samples,).\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          If the ``loss`` does not support probabilities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes)\n",
      " |          The class log-probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          If the ``loss`` does not support probabilities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes)\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |  \n",
      " |  staged_decision_function(self, X)\n",
      " |      Compute decision function of ``X`` for each iteration.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : generator of ndarray of shape (n_samples, k)\n",
      " |          The decision function of the input samples, which corresponds to\n",
      " |          the raw values predicted from the trees of the ensemble . The\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |          Regression and binary classification are special cases with\n",
      " |          ``k == 1``, otherwise ``k==n_classes``.\n",
      " |  \n",
      " |  staged_predict(self, X)\n",
      " |      Predict class at each stage for X.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : generator of ndarray of shape (n_samples,)\n",
      " |          The predicted value of the input samples.\n",
      " |  \n",
      " |  staged_predict_proba(self, X)\n",
      " |      Predict class probabilities at each stage for X.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : generator of ndarray of shape (n_samples,)\n",
      " |          The predicted value of the input samples.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseGradientBoosting:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the ensemble to X, return leaf indices.\n",
      " |      \n",
      " |      .. versionadded:: 0.17\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will\n",
      " |          be converted to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array-like of shape (n_samples, n_estimators, n_classes)\n",
      " |          For each datapoint x in X and for each tree in the ensemble,\n",
      " |          return the index of the leaf x ends up in each estimator.\n",
      " |          In the case of binary classification n_classes is 1.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, monitor=None)\n",
      " |      Fit the gradient boosting model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values (strings or integers in classification, real numbers\n",
      " |          in regression)\n",
      " |          For classification, labels must correspond to classes.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      monitor : callable, default=None\n",
      " |          The monitor is called after each iteration with the current\n",
      " |          iteration, a reference to the estimator and the local variables of\n",
      " |          ``_fit_stages`` as keyword arguments ``callable(i, self,\n",
      " |          locals())``. If the callable returns ``True`` the fitting procedure\n",
      " |          is stopped. The monitor can be used for various things such as\n",
      " |          computing held-out estimates, early stopping, model introspect, and\n",
      " |          snapshoting.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseGradientBoosting:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GradientBoostingClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a567fd68",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class AdaBoostClassifier in module sklearn.ensemble._weight_boosting:\n",
      "\n",
      "class AdaBoostClassifier(sklearn.base.ClassifierMixin, BaseWeightBoosting)\n",
      " |  AdaBoostClassifier(base_estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)\n",
      " |  \n",
      " |  An AdaBoost classifier.\n",
      " |  \n",
      " |  An AdaBoost [1] classifier is a meta-estimator that begins by fitting a\n",
      " |  classifier on the original dataset and then fits additional copies of the\n",
      " |  classifier on the same dataset but where the weights of incorrectly\n",
      " |  classified instances are adjusted such that subsequent classifiers focus\n",
      " |  more on difficult cases.\n",
      " |  \n",
      " |  This class implements the algorithm known as AdaBoost-SAMME [2].\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <adaboost>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.14\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  base_estimator : object, default=None\n",
      " |      The base estimator from which the boosted ensemble is built.\n",
      " |      Support for sample weighting is required, as well as proper\n",
      " |      ``classes_`` and ``n_classes_`` attributes. If ``None``, then\n",
      " |      the base estimator is :class:`~sklearn.tree.DecisionTreeClassifier`\n",
      " |      initialized with `max_depth=1`.\n",
      " |  \n",
      " |  n_estimators : int, default=50\n",
      " |      The maximum number of estimators at which boosting is terminated.\n",
      " |      In case of perfect fit, the learning procedure is stopped early.\n",
      " |  \n",
      " |  learning_rate : float, default=1.\n",
      " |      Weight applied to each classifier at each boosting iteration. A higher\n",
      " |      learning rate increases the contribution of each classifier. There is\n",
      " |      a trade-off between the `learning_rate` and `n_estimators` parameters.\n",
      " |  \n",
      " |  algorithm : {'SAMME', 'SAMME.R'}, default='SAMME.R'\n",
      " |      If 'SAMME.R' then use the SAMME.R real boosting algorithm.\n",
      " |      ``base_estimator`` must support calculation of class probabilities.\n",
      " |      If 'SAMME' then use the SAMME discrete boosting algorithm.\n",
      " |      The SAMME.R algorithm typically converges faster than SAMME,\n",
      " |      achieving a lower test error with fewer boosting iterations.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the random seed given at each `base_estimator` at each\n",
      " |      boosting iteration.\n",
      " |      Thus, it is only used when `base_estimator` exposes a `random_state`.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  base_estimator_ : estimator\n",
      " |      The base estimator from which the ensemble is grown.\n",
      " |  \n",
      " |  estimators_ : list of classifiers\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      The classes labels.\n",
      " |  \n",
      " |  n_classes_ : int\n",
      " |      The number of classes.\n",
      " |  \n",
      " |  estimator_weights_ : ndarray of floats\n",
      " |      Weights for each estimator in the boosted ensemble.\n",
      " |  \n",
      " |  estimator_errors_ : ndarray of floats\n",
      " |      Classification error for each estimator in the boosted\n",
      " |      ensemble.\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances if supported by the\n",
      " |      ``base_estimator`` (when based on decision trees).\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  AdaBoostRegressor : An AdaBoost regressor that begins by fitting a\n",
      " |      regressor on the original dataset and then fits additional copies of\n",
      " |      the regressor on the same dataset but where the weights of instances\n",
      " |      are adjusted according to the error of the current prediction.\n",
      " |  \n",
      " |  GradientBoostingClassifier : GB builds an additive model in a forward\n",
      " |      stage-wise fashion. Regression trees are fit on the negative gradient\n",
      " |      of the binomial or multinomial deviance loss function. Binary\n",
      " |      classification is a special case where only a single regression tree is\n",
      " |      induced.\n",
      " |  \n",
      " |  sklearn.tree.DecisionTreeClassifier : A non-parametric supervised learning\n",
      " |      method used for classification.\n",
      " |      Creates a model that predicts the value of a target variable by\n",
      " |      learning simple decision rules inferred from the data features.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] Y. Freund, R. Schapire, \"A Decision-Theoretic Generalization of\n",
      " |         on-Line Learning and an Application to Boosting\", 1995.\n",
      " |  \n",
      " |  .. [2] J. Zhu, H. Zou, S. Rosset, T. Hastie, \"Multi-class AdaBoost\", 2009.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import AdaBoostClassifier\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
      " |  ...                            n_informative=2, n_redundant=0,\n",
      " |  ...                            random_state=0, shuffle=False)\n",
      " |  >>> clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  AdaBoostClassifier(n_estimators=100, random_state=0)\n",
      " |  >>> clf.predict([[0, 0, 0, 0]])\n",
      " |  array([1])\n",
      " |  >>> clf.score(X, y)\n",
      " |  0.983...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      AdaBoostClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseWeightBoosting\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, base_estimator=None, *, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Compute the decision function of ``X``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : ndarray of shape of (n_samples, k)\n",
      " |          The decision function of the input samples. The order of\n",
      " |          outputs is the same of that of the :term:`classes_` attribute.\n",
      " |          Binary classification is a special cases with ``k == 1``,\n",
      " |          otherwise ``k==n_classes``. For binary classification,\n",
      " |          values closer to -1 or 1 mean more like the first or second\n",
      " |          class in ``classes_``, respectively.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a boosted classifier from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          The target values (class labels).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, the sample weights are initialized to\n",
      " |          ``1 / n_samples``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict classes for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is computed as the weighted mean\n",
      " |      prediction of the classifiers in the ensemble.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      The predicted class log-probabilities of an input sample is computed as\n",
      " |      the weighted mean predicted class log-probabilities of the classifiers\n",
      " |      in the ensemble.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes)\n",
      " |          The class probabilities of the input samples. The order of\n",
      " |          outputs is the same of that of the :term:`classes_` attribute.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample is computed as\n",
      " |      the weighted mean predicted class probabilities of the classifiers\n",
      " |      in the ensemble.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes)\n",
      " |          The class probabilities of the input samples. The order of\n",
      " |          outputs is the same of that of the :term:`classes_` attribute.\n",
      " |  \n",
      " |  staged_decision_function(self, X)\n",
      " |      Compute decision function of ``X`` for each boosting iteration.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each boosting iteration.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      score : generator of ndarray of shape (n_samples, k)\n",
      " |          The decision function of the input samples. The order of\n",
      " |          outputs is the same of that of the :term:`classes_` attribute.\n",
      " |          Binary classification is a special cases with ``k == 1``,\n",
      " |          otherwise ``k==n_classes``. For binary classification,\n",
      " |          values closer to -1 or 1 mean more like the first or second\n",
      " |          class in ``classes_``, respectively.\n",
      " |  \n",
      " |  staged_predict(self, X)\n",
      " |      Return staged predictions for X.\n",
      " |      \n",
      " |      The predicted class of an input sample is computed as the weighted mean\n",
      " |      prediction of the classifiers in the ensemble.\n",
      " |      \n",
      " |      This generator method yields the ensemble prediction after each\n",
      " |      iteration of boosting and therefore allows monitoring, such as to\n",
      " |      determine the prediction on a test set after each boost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      y : generator of ndarray of shape (n_samples,)\n",
      " |          The predicted classes.\n",
      " |  \n",
      " |  staged_predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      The predicted class probabilities of an input sample is computed as\n",
      " |      the weighted mean predicted class probabilities of the classifiers\n",
      " |      in the ensemble.\n",
      " |      \n",
      " |      This generator method yields the ensemble predicted class probabilities\n",
      " |      after each iteration of boosting and therefore allows monitoring, such\n",
      " |      as to determine the predicted class probabilities on a test set after\n",
      " |      each boost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      Yields\n",
      " |      -------\n",
      " |      p : generator of ndarray of shape (n_samples,)\n",
      " |          The class probabilities of the input samples. The order of\n",
      " |          outputs is the same of that of the :term:`classes_` attribute.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseWeightBoosting:\n",
      " |  \n",
      " |  staged_score(self, X, y, sample_weight=None)\n",
      " |      Return staged scores for X, y.\n",
      " |      \n",
      " |      This generator method yields the ensemble score after each iteration of\n",
      " |      boosting and therefore allows monitoring, such as to determine the\n",
      " |      score on a test set after each boost.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrix can be CSC, CSR, COO,\n",
      " |          DOK, or LIL. COO, DOK, and LIL are converted to CSR.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      z : float\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseWeightBoosting:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The feature importances.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AdaBoostClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7ea7dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ca1f601",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KMeans in module sklearn.cluster._kmeans:\n",
      "\n",
      "class KMeans(sklearn.base.TransformerMixin, sklearn.base.ClusterMixin, sklearn.base.BaseEstimator)\n",
      " |  KMeans(n_clusters=8, *, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='deprecated', verbose=0, random_state=None, copy_x=True, n_jobs='deprecated', algorithm='auto')\n",
      " |  \n",
      " |  K-Means clustering.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <k_means>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  \n",
      " |  n_clusters : int, default=8\n",
      " |      The number of clusters to form as well as the number of\n",
      " |      centroids to generate.\n",
      " |  \n",
      " |  init : {'k-means++', 'random'}, callable or array-like of shape             (n_clusters, n_features), default='k-means++'\n",
      " |      Method for initialization:\n",
      " |  \n",
      " |      'k-means++' : selects initial cluster centers for k-mean\n",
      " |      clustering in a smart way to speed up convergence. See section\n",
      " |      Notes in k_init for more details.\n",
      " |  \n",
      " |      'random': choose `n_clusters` observations (rows) at random from data\n",
      " |      for the initial centroids.\n",
      " |  \n",
      " |      If an array is passed, it should be of shape (n_clusters, n_features)\n",
      " |      and gives the initial centers.\n",
      " |  \n",
      " |      If a callable is passed, it should take arguments X, n_clusters and a\n",
      " |      random state and return an initialization.\n",
      " |  \n",
      " |  n_init : int, default=10\n",
      " |      Number of time the k-means algorithm will be run with different\n",
      " |      centroid seeds. The final results will be the best output of\n",
      " |      n_init consecutive runs in terms of inertia.\n",
      " |  \n",
      " |  max_iter : int, default=300\n",
      " |      Maximum number of iterations of the k-means algorithm for a\n",
      " |      single run.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Relative tolerance with regards to Frobenius norm of the difference\n",
      " |      in the cluster centers of two consecutive iterations to declare\n",
      " |      convergence.\n",
      " |  \n",
      " |  precompute_distances : {'auto', True, False}, default='auto'\n",
      " |      Precompute distances (faster but takes more memory).\n",
      " |  \n",
      " |      'auto' : do not precompute distances if n_samples * n_clusters > 12\n",
      " |      million. This corresponds to about 100MB overhead per job using\n",
      " |      double precision.\n",
      " |  \n",
      " |      True : always precompute distances.\n",
      " |  \n",
      " |      False : never precompute distances.\n",
      " |  \n",
      " |      .. deprecated:: 0.23\n",
      " |          'precompute_distances' was deprecated in version 0.22 and will be\n",
      " |          removed in 1.0 (renaming of 0.25). It has no effect.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Verbosity mode.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Determines random number generation for centroid initialization. Use\n",
      " |      an int to make the randomness deterministic.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  copy_x : bool, default=True\n",
      " |      When pre-computing distances it is more numerically accurate to center\n",
      " |      the data first. If copy_x is True (default), then the original data is\n",
      " |      not modified. If False, the original data is modified, and put back\n",
      " |      before the function returns, but small numerical differences may be\n",
      " |      introduced by subtracting and then adding the data mean. Note that if\n",
      " |      the original data is not C-contiguous, a copy will be made even if\n",
      " |      copy_x is False. If the original data is sparse, but not in CSR format,\n",
      " |      a copy will be made even if copy_x is False.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of OpenMP threads to use for the computation. Parallelism is\n",
      " |      sample-wise on the main cython loop which assigns each sample to its\n",
      " |      closest center.\n",
      " |  \n",
      " |      ``None`` or ``-1`` means using all processors.\n",
      " |  \n",
      " |      .. deprecated:: 0.23\n",
      " |          ``n_jobs`` was deprecated in version 0.23 and will be removed in\n",
      " |          1.0 (renaming of 0.25).\n",
      " |  \n",
      " |  algorithm : {\"auto\", \"full\", \"elkan\"}, default=\"auto\"\n",
      " |      K-means algorithm to use. The classical EM-style algorithm is \"full\".\n",
      " |      The \"elkan\" variation is more efficient on data with well-defined\n",
      " |      clusters, by using the triangle inequality. However it's more memory\n",
      " |      intensive due to the allocation of an extra array of shape\n",
      " |      (n_samples, n_clusters).\n",
      " |  \n",
      " |      For now \"auto\" (kept for backward compatibiliy) chooses \"elkan\" but it\n",
      " |      might change in the future for a better heuristic.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |          Added Elkan algorithm\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  cluster_centers_ : ndarray of shape (n_clusters, n_features)\n",
      " |      Coordinates of cluster centers. If the algorithm stops before fully\n",
      " |      converging (see ``tol`` and ``max_iter``), these will not be\n",
      " |      consistent with ``labels_``.\n",
      " |  \n",
      " |  labels_ : ndarray of shape (n_samples,)\n",
      " |      Labels of each point\n",
      " |  \n",
      " |  inertia_ : float\n",
      " |      Sum of squared distances of samples to their closest cluster center.\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      Number of iterations run.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  MiniBatchKMeans : Alternative online implementation that does incremental\n",
      " |      updates of the centers positions using mini-batches.\n",
      " |      For large scale learning (say n_samples > 10k) MiniBatchKMeans is\n",
      " |      probably much faster than the default batch implementation.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The k-means problem is solved using either Lloyd's or Elkan's algorithm.\n",
      " |  \n",
      " |  The average complexity is given by O(k n T), where n is the number of\n",
      " |  samples and T is the number of iteration.\n",
      " |  \n",
      " |  The worst case complexity is given by O(n^(k+2/p)) with\n",
      " |  n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii,\n",
      " |  'How slow is the k-means method?' SoCG2006)\n",
      " |  \n",
      " |  In practice, the k-means algorithm is very fast (one of the fastest\n",
      " |  clustering algorithms available), but it falls in local minima. That's why\n",
      " |  it can be useful to restart it several times.\n",
      " |  \n",
      " |  If the algorithm stops before fully converging (because of ``tol`` or\n",
      " |  ``max_iter``), ``labels_`` and ``cluster_centers_`` will not be consistent,\n",
      " |  i.e. the ``cluster_centers_`` will not be the means of the points in each\n",
      " |  cluster. Also, the estimator will reassign ``labels_`` after the last\n",
      " |  iteration to make ``labels_`` consistent with ``predict`` on the training\n",
      " |  set.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  \n",
      " |  >>> from sklearn.cluster import KMeans\n",
      " |  >>> import numpy as np\n",
      " |  >>> X = np.array([[1, 2], [1, 4], [1, 0],\n",
      " |  ...               [10, 2], [10, 4], [10, 0]])\n",
      " |  >>> kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
      " |  >>> kmeans.labels_\n",
      " |  array([1, 1, 1, 0, 0, 0], dtype=int32)\n",
      " |  >>> kmeans.predict([[0, 0], [12, 3]])\n",
      " |  array([1, 0], dtype=int32)\n",
      " |  >>> kmeans.cluster_centers_\n",
      " |  array([[10.,  2.],\n",
      " |         [ 1.,  2.]])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KMeans\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.ClusterMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_clusters=8, *, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='deprecated', verbose=0, random_state=None, copy_x=True, n_jobs='deprecated', algorithm='auto')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None, sample_weight=None)\n",
      " |      Compute k-means clustering.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training instances to cluster. It must be noted that the data\n",
      " |          will be converted to C ordering, which will cause a memory\n",
      " |          copy if the given data is not C-contiguous.\n",
      " |          If a sparse matrix is passed, a copy will be made if it's not in\n",
      " |          CSR format.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |          Not used, present here for API consistency by convention.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          The weights for each observation in X. If None, all observations\n",
      " |          are assigned equal weight.\n",
      " |      \n",
      " |          .. versionadded:: 0.20\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  fit_predict(self, X, y=None, sample_weight=None)\n",
      " |      Compute cluster centers and predict cluster index for each sample.\n",
      " |      \n",
      " |      Convenience method; equivalent to calling fit(X) followed by\n",
      " |      predict(X).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          New data to transform.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |          Not used, present here for API consistency by convention.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          The weights for each observation in X. If None, all observations\n",
      " |          are assigned equal weight.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      labels : ndarray of shape (n_samples,)\n",
      " |          Index of the cluster each sample belongs to.\n",
      " |  \n",
      " |  fit_transform(self, X, y=None, sample_weight=None)\n",
      " |      Compute clustering and transform X to cluster-distance space.\n",
      " |      \n",
      " |      Equivalent to fit(X).transform(X), but more efficiently implemented.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          New data to transform.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |          Not used, present here for API consistency by convention.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          The weights for each observation in X. If None, all observations\n",
      " |          are assigned equal weight.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray of shape (n_samples, n_clusters)\n",
      " |          X transformed in the new space.\n",
      " |  \n",
      " |  predict(self, X, sample_weight=None)\n",
      " |      Predict the closest cluster each sample in X belongs to.\n",
      " |      \n",
      " |      In the vector quantization literature, `cluster_centers_` is called\n",
      " |      the code book and each value returned by `predict` is the index of\n",
      " |      the closest code in the code book.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          New data to predict.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          The weights for each observation in X. If None, all observations\n",
      " |          are assigned equal weight.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      labels : ndarray of shape (n_samples,)\n",
      " |          Index of the cluster each sample belongs to.\n",
      " |  \n",
      " |  score(self, X, y=None, sample_weight=None)\n",
      " |      Opposite of the value of X on the K-means objective.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          New data.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |          Not used, present here for API consistency by convention.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          The weights for each observation in X. If None, all observations\n",
      " |          are assigned equal weight.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Opposite of the value of X on the K-means objective.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Transform X to a cluster-distance space.\n",
      " |      \n",
      " |      In the new space, each dimension is the distance to the cluster\n",
      " |      centers. Note that even if X is sparse, the array returned by\n",
      " |      `transform` will typically be dense.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          New data to transform.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray of shape (n_samples, n_clusters)\n",
      " |          X transformed in the new space.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "KMeans?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51bc9a11",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yellowbrick in /opt/anaconda3/envs/ksunsupervised/lib/python3.9/site-packages (1.3.post1)\n",
      "Requirement already satisfied: numpy<1.20,>=1.16.0 in /opt/anaconda3/envs/ksunsupervised/lib/python3.9/site-packages (from yellowbrick) (1.19.5)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /opt/anaconda3/envs/ksunsupervised/lib/python3.9/site-packages (from yellowbrick) (0.24.2)\n",
      "Requirement already satisfied: cycler>=0.10.0 in /opt/anaconda3/envs/ksunsupervised/lib/python3.9/site-packages (from yellowbrick) (0.10.0)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.2 in /opt/anaconda3/envs/ksunsupervised/lib/python3.9/site-packages (from yellowbrick) (3.3.4)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/anaconda3/envs/ksunsupervised/lib/python3.9/site-packages (from yellowbrick) (1.6.2)\n",
      "Requirement already satisfied: six in /opt/anaconda3/envs/ksunsupervised/lib/python3.9/site-packages (from cycler>=0.10.0->yellowbrick) (1.16.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/envs/ksunsupervised/lib/python3.9/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (8.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/anaconda3/envs/ksunsupervised/lib/python3.9/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/envs/ksunsupervised/lib/python3.9/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/anaconda3/envs/ksunsupervised/lib/python3.9/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.8.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/envs/ksunsupervised/lib/python3.9/site-packages (from scikit-learn>=0.20->yellowbrick) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/envs/ksunsupervised/lib/python3.9/site-packages (from scikit-learn>=0.20->yellowbrick) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install yellowbrick\n",
    "#yellow brick is a library for Machine Learning Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0fc3402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import KElbowVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4e4ade9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class KElbowVisualizer in module yellowbrick.cluster.elbow:\n",
      "\n",
      "class KElbowVisualizer(yellowbrick.cluster.base.ClusteringScoreVisualizer)\n",
      " |  KElbowVisualizer(estimator, ax=None, k=10, metric='distortion', timings=True, locate_elbow=True, **kwargs)\n",
      " |  \n",
      " |  The K-Elbow Visualizer implements the \"elbow\" method of selecting the\n",
      " |  optimal number of clusters for K-means clustering. K-means is a simple\n",
      " |  unsupervised machine learning algorithm that groups data into a specified\n",
      " |  number (k) of clusters. Because the user must specify in advance what k to\n",
      " |  choose, the algorithm is somewhat naive -- it assigns all members to k\n",
      " |  clusters even if that is not the right k for the dataset.\n",
      " |  \n",
      " |  The elbow method runs k-means clustering on the dataset for a range of\n",
      " |  values for k (say from 1-10) and then for each value of k computes an\n",
      " |  average score for all clusters. By default, the ``distortion`` score is\n",
      " |  computed, the sum of square distances from each point to its assigned\n",
      " |  center. Other metrics can also be used such as the ``silhouette`` score,\n",
      " |  the mean silhouette coefficient for all samples or the\n",
      " |  ``calinski_harabasz`` score, which computes the ratio of dispersion between\n",
      " |  and within clusters.\n",
      " |  \n",
      " |  When these overall metrics for each model are plotted, it is possible to\n",
      " |  visually determine the best value for k. If the line chart looks like an\n",
      " |  arm, then the \"elbow\" (the point of inflection on the curve) is the best\n",
      " |  value of k. The \"arm\" can be either up or down, but if there is a strong\n",
      " |  inflection point, it is a good indication that the underlying model fits\n",
      " |  best at that point.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  \n",
      " |  estimator : a scikit-learn clusterer\n",
      " |      Should be an instance of an unfitted clusterer, specifically ``KMeans`` or\n",
      " |      ``MiniBatchKMeans``. If it is not a clusterer, an exception is raised.\n",
      " |  \n",
      " |  ax : matplotlib Axes, default: None\n",
      " |      The axes to plot the figure on. If None is passed in the current axes\n",
      " |      will be used (or generated if required).\n",
      " |  \n",
      " |  k : integer, tuple, or iterable\n",
      " |      The k values to compute silhouette scores for. If a single integer\n",
      " |      is specified, then will compute the range (2,k). If a tuple of 2\n",
      " |      integers is specified, then k will be in np.arange(k[0], k[1]).\n",
      " |      Otherwise, specify an iterable of integers to use as values for k.\n",
      " |  \n",
      " |  metric : string, default: ``\"distortion\"``\n",
      " |      Select the scoring metric to evaluate the clusters. The default is the\n",
      " |      mean distortion, defined by the sum of squared distances between each\n",
      " |      observation and its closest centroid. Other metrics include:\n",
      " |  \n",
      " |      - **distortion**: mean sum of squared distances to centers\n",
      " |      - **silhouette**: mean ratio of intra-cluster and nearest-cluster distance\n",
      " |      - **calinski_harabasz**: ratio of within to between cluster dispersion\n",
      " |  \n",
      " |  timings : bool, default: True\n",
      " |      Display the fitting time per k to evaluate the amount of time required\n",
      " |      to train the clustering model.\n",
      " |  \n",
      " |  locate_elbow : bool, default: True\n",
      " |      Automatically find the \"elbow\" or \"knee\" which likely corresponds to the optimal\n",
      " |      value of k using the \"knee point detection algorithm\". The knee point detection\n",
      " |      algorithm finds the point of maximum curvature, which in a well-behaved\n",
      " |      clustering problem also represents the pivot of the elbow curve. The point is\n",
      " |      labeled with a dashed line and annotated with the score and k values.\n",
      " |  \n",
      " |  kwargs : dict\n",
      " |      Keyword arguments that are passed to the base class and may influence\n",
      " |      the visualization as defined in other Visualizers.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  k_scores_ : array of shape (n,) where n is no. of k values\n",
      " |      The silhouette score corresponding to each k value.\n",
      " |  \n",
      " |  k_timers_ : array of shape (n,) where n is no. of k values\n",
      " |      The time taken to fit n KMeans model corresponding to each k value.\n",
      " |  \n",
      " |  elbow_value_ : integer\n",
      " |      The optimal value of k.\n",
      " |  \n",
      " |  elbow_score_ : float\n",
      " |      The silhouette score corresponding to the optimal value of k.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  \n",
      " |  >>> from yellowbrick.cluster import KElbowVisualizer\n",
      " |  >>> from sklearn.cluster import KMeans\n",
      " |  >>> model = KElbowVisualizer(KMeans(), k=10)\n",
      " |  >>> model.fit(X)\n",
      " |  >>> model.show()\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  \n",
      " |  If you get a visualizer that doesn't have an elbow or inflection point,\n",
      " |  then this method may not be working. The elbow method does not work well\n",
      " |  if the data is not very clustered; in this case, you might see a smooth\n",
      " |  curve and the value of k is unclear. Other scoring methods, such as BIC or\n",
      " |  SSE, also can be used to explore if clustering is a correct choice.\n",
      " |  \n",
      " |  For a discussion on the Elbow method, read more at\n",
      " |  `Robert Gove's Block website <https://bl.ocks.org/rpgove/0060ff3b656618e9136b>`_.\n",
      " |  For more on the knee point detection algorithm see the paper `\"Finding a \"kneedle\"\n",
      " |  in a Haystack\" <https://raghavan.usc.edu//papers/kneedle-simplex11.pdf>`_.\n",
      " |  \n",
      " |  .. seealso:: The scikit-learn documentation for the `silhouette_score\n",
      " |      <https://bit.ly/2LYWjYb>`_ and `calinski_harabasz_score\n",
      " |      <https://bit.ly/2ItAgts>`_. The default, ``distortion_score``, is\n",
      " |      implemented in ``yellowbrick.cluster.elbow``.\n",
      " |  \n",
      " |  .. todo:: add parallelization option for performance\n",
      " |  .. todo:: add different metrics for scores and silhouette\n",
      " |  .. todo:: add timing information about how long it's taking\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      KElbowVisualizer\n",
      " |      yellowbrick.cluster.base.ClusteringScoreVisualizer\n",
      " |      yellowbrick.base.ScoreVisualizer\n",
      " |      yellowbrick.base.ModelVisualizer\n",
      " |      yellowbrick.base.Visualizer\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      yellowbrick.utils.wrapper.Wrapper\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, estimator, ax=None, k=10, metric='distortion', timings=True, locate_elbow=True, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  draw(self)\n",
      " |      Draw the elbow curve for the specified scores and values of K.\n",
      " |  \n",
      " |  finalize(self)\n",
      " |      Prepare the figure for rendering by setting the title as well as the\n",
      " |      X and Y axis labels and adding the legend.\n",
      " |  \n",
      " |  fit(self, X, y=None, **kwargs)\n",
      " |      Fits n KMeans models where n is the length of ``self.k_values_``,\n",
      " |      storing the silhouette scores in the ``self.k_scores_`` attribute.\n",
      " |      The \"elbow\" and silhouette score corresponding to it are stored in\n",
      " |      ``self.elbow_value`` and ``self.elbow_score`` respectively.\n",
      " |      This method finishes up by calling draw to create the plot.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from yellowbrick.base.ScoreVisualizer:\n",
      " |  \n",
      " |  score(self, X, y, **kwargs)\n",
      " |      The primary entry point for score visualizers is the score method,\n",
      " |      which makes predictions based on X and scores them relative to y.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float or array-like\n",
      " |          Returns the score of the underlying model, which is model-specific,\n",
      " |          e.g. accuracy for classifiers, R2 for regressors, etc.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from yellowbrick.base.ModelVisualizer:\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      After v0.24 - scikit-learn is able to determine that ``self.estimator`` is\n",
      " |      nested and fetches its params using ``estimator__param``. This functionality is\n",
      " |      pretty cool but it's a pretty big overhaul to change our \"wrapped\" estimator API\n",
      " |      to a \"nested\" estimator API, therefore we override ``get_params`` to flatten out\n",
      " |      the estimator params.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      The latest version of scikit-learn is able to determine that ``self.estimator``\n",
      " |      is nested and sets its params using ``estimator__param``. In order to maintain\n",
      " |      the Yellowbrick \"wrapped\" API, this method finds any params belonging to the\n",
      " |      underlying estimator and sets them directly.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from yellowbrick.base.Visualizer:\n",
      " |  \n",
      " |  poof(self, *args, **kwargs)\n",
      " |      This method is deprecated, please use ``show()`` instead.\n",
      " |  \n",
      " |  set_title(self, title=None)\n",
      " |      Sets the title on the current axes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      title: string, default: None\n",
      " |          Add title to figure or if None leave untitled.\n",
      " |  \n",
      " |  show(self, outpath=None, clear_figure=False, **kwargs)\n",
      " |      Makes the magic happen and a visualizer appear! You can pass in a path to\n",
      " |      save the figure to disk with various backends, or you can call it with no\n",
      " |      arguments to show the figure either in a notebook or in a GUI window that\n",
      " |      pops up on screen.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      outpath: string, default: None\n",
      " |          path or None. Save figure to disk or if None show in window\n",
      " |      \n",
      " |      clear_figure: boolean, default: False\n",
      " |          When True, this flag clears the figure after saving to file or\n",
      " |          showing on screen. This is useful when making consecutive plots.\n",
      " |      \n",
      " |      kwargs: dict\n",
      " |          generic keyword arguments.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Developers of visualizers don't usually override show, as it is\n",
      " |      primarily called by the user to render the visualization.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from yellowbrick.base.Visualizer:\n",
      " |  \n",
      " |  ax\n",
      " |      The matplotlib axes that the visualizer draws upon (can also be a grid\n",
      " |      of multiple axes objects). The visualizer uses :func:`matplotlib.pyplot.gca`\n",
      " |      to create an axes for the user if one has not been specified.\n",
      " |  \n",
      " |  fig\n",
      " |      The matplotlib fig that the visualizer draws upon. The visualizer uses\n",
      " |      the matplotlib method :func:`matplotlib.pyplot.gcf` to create a figure for\n",
      " |      the user if one has not been specified.\n",
      " |  \n",
      " |  size\n",
      " |      Returns the actual size in pixels as set by matplotlib, or\n",
      " |      the user provided size if available.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from yellowbrick.utils.wrapper.Wrapper:\n",
      " |  \n",
      " |  __getattr__(self, attr)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "KElbowVisualizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2a984aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5e971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DBSCAN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e31b9a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e76478",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class PCA in module sklearn.decomposition._pca:\n",
      "\n",
      "class PCA(sklearn.decomposition._base._BasePCA)\n",
      " |  PCA(n_components=None, *, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)\n",
      " |  \n",
      " |  Principal component analysis (PCA).\n",
      " |  \n",
      " |  Linear dimensionality reduction using Singular Value Decomposition of the\n",
      " |  data to project it to a lower dimensional space. The input data is centered\n",
      " |  but not scaled for each feature before applying the SVD.\n",
      " |  \n",
      " |  It uses the LAPACK implementation of the full SVD or a randomized truncated\n",
      " |  SVD by the method of Halko et al. 2009, depending on the shape of the input\n",
      " |  data and the number of components to extract.\n",
      " |  \n",
      " |  It can also use the scipy.sparse.linalg ARPACK implementation of the\n",
      " |  truncated SVD.\n",
      " |  \n",
      " |  Notice that this class does not support sparse input. See\n",
      " |  :class:`TruncatedSVD` for an alternative with sparse data.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <PCA>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_components : int, float or 'mle', default=None\n",
      " |      Number of components to keep.\n",
      " |      if n_components is not set all components are kept::\n",
      " |  \n",
      " |          n_components == min(n_samples, n_features)\n",
      " |  \n",
      " |      If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka's\n",
      " |      MLE is used to guess the dimension. Use of ``n_components == 'mle'``\n",
      " |      will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.\n",
      " |  \n",
      " |      If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select the\n",
      " |      number of components such that the amount of variance that needs to be\n",
      " |      explained is greater than the percentage specified by n_components.\n",
      " |  \n",
      " |      If ``svd_solver == 'arpack'``, the number of components must be\n",
      " |      strictly less than the minimum of n_features and n_samples.\n",
      " |  \n",
      " |      Hence, the None case results in::\n",
      " |  \n",
      " |          n_components == min(n_samples, n_features) - 1\n",
      " |  \n",
      " |  copy : bool, default=True\n",
      " |      If False, data passed to fit are overwritten and running\n",
      " |      fit(X).transform(X) will not yield the expected results,\n",
      " |      use fit_transform(X) instead.\n",
      " |  \n",
      " |  whiten : bool, default=False\n",
      " |      When True (False by default) the `components_` vectors are multiplied\n",
      " |      by the square root of n_samples and then divided by the singular values\n",
      " |      to ensure uncorrelated outputs with unit component-wise variances.\n",
      " |  \n",
      " |      Whitening will remove some information from the transformed signal\n",
      " |      (the relative variance scales of the components) but can sometime\n",
      " |      improve the predictive accuracy of the downstream estimators by\n",
      " |      making their data respect some hard-wired assumptions.\n",
      " |  \n",
      " |  svd_solver : {'auto', 'full', 'arpack', 'randomized'}, default='auto'\n",
      " |      If auto :\n",
      " |          The solver is selected by a default policy based on `X.shape` and\n",
      " |          `n_components`: if the input data is larger than 500x500 and the\n",
      " |          number of components to extract is lower than 80% of the smallest\n",
      " |          dimension of the data, then the more efficient 'randomized'\n",
      " |          method is enabled. Otherwise the exact full SVD is computed and\n",
      " |          optionally truncated afterwards.\n",
      " |      If full :\n",
      " |          run exact full SVD calling the standard LAPACK solver via\n",
      " |          `scipy.linalg.svd` and select the components by postprocessing\n",
      " |      If arpack :\n",
      " |          run SVD truncated to n_components calling ARPACK solver via\n",
      " |          `scipy.sparse.linalg.svds`. It requires strictly\n",
      " |          0 < n_components < min(X.shape)\n",
      " |      If randomized :\n",
      " |          run randomized SVD by the method of Halko et al.\n",
      " |  \n",
      " |      .. versionadded:: 0.18.0\n",
      " |  \n",
      " |  tol : float, default=0.0\n",
      " |      Tolerance for singular values computed by svd_solver == 'arpack'.\n",
      " |      Must be of range [0.0, infinity).\n",
      " |  \n",
      " |      .. versionadded:: 0.18.0\n",
      " |  \n",
      " |  iterated_power : int or 'auto', default='auto'\n",
      " |      Number of iterations for the power method computed by\n",
      " |      svd_solver == 'randomized'.\n",
      " |      Must be of range [0, infinity).\n",
      " |  \n",
      " |      .. versionadded:: 0.18.0\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Used when the 'arpack' or 'randomized' solvers are used. Pass an int\n",
      " |      for reproducible results across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |      .. versionadded:: 0.18.0\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  components_ : ndarray of shape (n_components, n_features)\n",
      " |      Principal axes in feature space, representing the directions of\n",
      " |      maximum variance in the data. The components are sorted by\n",
      " |      ``explained_variance_``.\n",
      " |  \n",
      " |  explained_variance_ : ndarray of shape (n_components,)\n",
      " |      The amount of variance explained by each of the selected components.\n",
      " |      The variance estimation uses `n_samples - 1` degrees of freedom.\n",
      " |  \n",
      " |      Equal to n_components largest eigenvalues\n",
      " |      of the covariance matrix of X.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |  \n",
      " |  explained_variance_ratio_ : ndarray of shape (n_components,)\n",
      " |      Percentage of variance explained by each of the selected components.\n",
      " |  \n",
      " |      If ``n_components`` is not set then all components are stored and the\n",
      " |      sum of the ratios is equal to 1.0.\n",
      " |  \n",
      " |  singular_values_ : ndarray of shape (n_components,)\n",
      " |      The singular values corresponding to each of the selected components.\n",
      " |      The singular values are equal to the 2-norms of the ``n_components``\n",
      " |      variables in the lower-dimensional space.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  mean_ : ndarray of shape (n_features,)\n",
      " |      Per-feature empirical mean, estimated from the training set.\n",
      " |  \n",
      " |      Equal to `X.mean(axis=0)`.\n",
      " |  \n",
      " |  n_components_ : int\n",
      " |      The estimated number of components. When n_components is set\n",
      " |      to 'mle' or a number between 0 and 1 (with svd_solver == 'full') this\n",
      " |      number is estimated from input data. Otherwise it equals the parameter\n",
      " |      n_components, or the lesser value of n_features and n_samples\n",
      " |      if n_components is None.\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      Number of features in the training data.\n",
      " |  \n",
      " |  n_samples_ : int\n",
      " |      Number of samples in the training data.\n",
      " |  \n",
      " |  noise_variance_ : float\n",
      " |      The estimated noise covariance following the Probabilistic PCA model\n",
      " |      from Tipping and Bishop 1999. See \"Pattern Recognition and\n",
      " |      Machine Learning\" by C. Bishop, 12.2.1 p. 574 or\n",
      " |      http://www.miketipping.com/papers/met-mppca.pdf. It is required to\n",
      " |      compute the estimated data covariance and score samples.\n",
      " |  \n",
      " |      Equal to the average of (min(n_features, n_samples) - n_components)\n",
      " |      smallest eigenvalues of the covariance matrix of X.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  KernelPCA : Kernel Principal Component Analysis.\n",
      " |  SparsePCA : Sparse Principal Component Analysis.\n",
      " |  TruncatedSVD : Dimensionality reduction using truncated SVD.\n",
      " |  IncrementalPCA : Incremental Principal Component Analysis.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  For n_components == 'mle', this class uses the method from:\n",
      " |  `Minka, T. P.. \"Automatic choice of dimensionality for PCA\".\n",
      " |  In NIPS, pp. 598-604 <https://tminka.github.io/papers/pca/minka-pca.pdf>`_\n",
      " |  \n",
      " |  Implements the probabilistic PCA model from:\n",
      " |  `Tipping, M. E., and Bishop, C. M. (1999). \"Probabilistic principal\n",
      " |  component analysis\". Journal of the Royal Statistical Society:\n",
      " |  Series B (Statistical Methodology), 61(3), 611-622.\n",
      " |  <http://www.miketipping.com/papers/met-mppca.pdf>`_\n",
      " |  via the score and score_samples methods.\n",
      " |  \n",
      " |  For svd_solver == 'arpack', refer to `scipy.sparse.linalg.svds`.\n",
      " |  \n",
      " |  For svd_solver == 'randomized', see:\n",
      " |  `Halko, N., Martinsson, P. G., and Tropp, J. A. (2011).\n",
      " |  \"Finding structure with randomness: Probabilistic algorithms for\n",
      " |  constructing approximate matrix decompositions\".\n",
      " |  SIAM review, 53(2), 217-288.\n",
      " |  <https://doi.org/10.1137/090771806>`_\n",
      " |  and also\n",
      " |  `Martinsson, P. G., Rokhlin, V., and Tygert, M. (2011).\n",
      " |  \"A randomized algorithm for the decomposition of matrices\".\n",
      " |  Applied and Computational Harmonic Analysis, 30(1), 47-68\n",
      " |  <https://doi.org/10.1016/j.acha.2010.02.003>`_.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.decomposition import PCA\n",
      " |  >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
      " |  >>> pca = PCA(n_components=2)\n",
      " |  >>> pca.fit(X)\n",
      " |  PCA(n_components=2)\n",
      " |  >>> print(pca.explained_variance_ratio_)\n",
      " |  [0.9924... 0.0075...]\n",
      " |  >>> print(pca.singular_values_)\n",
      " |  [6.30061... 0.54980...]\n",
      " |  \n",
      " |  >>> pca = PCA(n_components=2, svd_solver='full')\n",
      " |  >>> pca.fit(X)\n",
      " |  PCA(n_components=2, svd_solver='full')\n",
      " |  >>> print(pca.explained_variance_ratio_)\n",
      " |  [0.9924... 0.00755...]\n",
      " |  >>> print(pca.singular_values_)\n",
      " |  [6.30061... 0.54980...]\n",
      " |  \n",
      " |  >>> pca = PCA(n_components=1, svd_solver='arpack')\n",
      " |  >>> pca.fit(X)\n",
      " |  PCA(n_components=1, svd_solver='arpack')\n",
      " |  >>> print(pca.explained_variance_ratio_)\n",
      " |  [0.99244...]\n",
      " |  >>> print(pca.singular_values_)\n",
      " |  [6.30061...]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PCA\n",
      " |      sklearn.decomposition._base._BasePCA\n",
      " |      sklearn.base.TransformerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_components=None, *, copy=True, whiten=False, svd_solver='auto', tol=0.0, iterated_power='auto', random_state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fit the model with X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training data, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns the instance itself.\n",
      " |  \n",
      " |  fit_transform(self, X, y=None)\n",
      " |      Fit the model with X and apply the dimensionality reduction on X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training data, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray of shape (n_samples, n_components)\n",
      " |          Transformed values.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method returns a Fortran-ordered array. To convert it to a\n",
      " |      C-ordered array, use 'np.ascontiguousarray'.\n",
      " |  \n",
      " |  score(self, X, y=None)\n",
      " |      Return the average log-likelihood of all samples.\n",
      " |      \n",
      " |      See. \"Pattern Recognition and Machine Learning\"\n",
      " |      by C. Bishop, 12.2.1 p. 574\n",
      " |      or http://www.miketipping.com/papers/met-mppca.pdf\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ll : float\n",
      " |          Average log-likelihood of the samples under the current model.\n",
      " |  \n",
      " |  score_samples(self, X)\n",
      " |      Return the log-likelihood of each sample.\n",
      " |      \n",
      " |      See. \"Pattern Recognition and Machine Learning\"\n",
      " |      by C. Bishop, 12.2.1 p. 574\n",
      " |      or http://www.miketipping.com/papers/met-mppca.pdf\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          The data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ll : ndarray of shape (n_samples,)\n",
      " |          Log-likelihood of each sample under the current model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.decomposition._base._BasePCA:\n",
      " |  \n",
      " |  get_covariance(self)\n",
      " |      Compute data covariance with the generative model.\n",
      " |      \n",
      " |      ``cov = components_.T * S**2 * components_ + sigma2 * eye(n_features)``\n",
      " |      where S**2 contains the explained variances, and sigma2 contains the\n",
      " |      noise variances.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      cov : array, shape=(n_features, n_features)\n",
      " |          Estimated covariance of data.\n",
      " |  \n",
      " |  get_precision(self)\n",
      " |      Compute data precision matrix with the generative model.\n",
      " |      \n",
      " |      Equals the inverse of the covariance but computed with\n",
      " |      the matrix inversion lemma for efficiency.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      precision : array, shape=(n_features, n_features)\n",
      " |          Estimated precision of data.\n",
      " |  \n",
      " |  inverse_transform(self, X)\n",
      " |      Transform data back to its original space.\n",
      " |      \n",
      " |      In other words, return an input X_original whose transform would be X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_components)\n",
      " |          New data, where n_samples is the number of samples\n",
      " |          and n_components is the number of components.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_original array-like, shape (n_samples, n_features)\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If whitening is enabled, inverse_transform will compute the\n",
      " |      exact inverse operation, which includes reversing whitening.\n",
      " |  \n",
      " |  transform(self, X)\n",
      " |      Apply dimensionality reduction to X.\n",
      " |      \n",
      " |      X is projected on the first principal components previously extracted\n",
      " |      from a training set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape (n_samples, n_features)\n",
      " |          New data, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : array-like, shape (n_samples, n_components)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.TransformerMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1abe3ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "709667fd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fetch_openml in module sklearn.datasets._openml:\n",
      "\n",
      "fetch_openml(name: Optional[str] = None, *, version: Union[str, int] = 'active', data_id: Optional[int] = None, data_home: Optional[str] = None, target_column: Union[str, List, NoneType] = 'default-target', cache: bool = True, return_X_y: bool = False, as_frame: Union[str, bool] = 'auto')\n",
      "    Fetch dataset from openml by name or dataset id.\n",
      "    \n",
      "    Datasets are uniquely identified by either an integer ID or by a\n",
      "    combination of name and version (i.e. there might be multiple\n",
      "    versions of the 'iris' dataset). Please give either name or data_id\n",
      "    (not both). In case a name is given, a version can also be\n",
      "    provided.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <openml>`.\n",
      "    \n",
      "    .. versionadded:: 0.20\n",
      "    \n",
      "    .. note:: EXPERIMENTAL\n",
      "    \n",
      "        The API is experimental (particularly the return value structure),\n",
      "        and might have small backward-incompatible changes without notice\n",
      "        or warning in future releases.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    name : str, default=None\n",
      "        String identifier of the dataset. Note that OpenML can have multiple\n",
      "        datasets with the same name.\n",
      "    \n",
      "    version : int or 'active', default='active'\n",
      "        Version of the dataset. Can only be provided if also ``name`` is given.\n",
      "        If 'active' the oldest version that's still active is used. Since\n",
      "        there may be more than one active version of a dataset, and those\n",
      "        versions may fundamentally be different from one another, setting an\n",
      "        exact version is highly recommended.\n",
      "    \n",
      "    data_id : int, default=None\n",
      "        OpenML ID of the dataset. The most specific way of retrieving a\n",
      "        dataset. If data_id is not given, name (and potential version) are\n",
      "        used to obtain a dataset.\n",
      "    \n",
      "    data_home : str, default=None\n",
      "        Specify another download and cache folder for the data sets. By default\n",
      "        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "    \n",
      "    target_column : str, list or None, default='default-target'\n",
      "        Specify the column name in the data to use as target. If\n",
      "        'default-target', the standard target column a stored on the server\n",
      "        is used. If ``None``, all columns are returned as data and the\n",
      "        target is ``None``. If list (of strings), all columns with these names\n",
      "        are returned as multi-target (Note: not all scikit-learn classifiers\n",
      "        can handle all types of multi-output combinations)\n",
      "    \n",
      "    cache : bool, default=True\n",
      "        Whether to cache downloaded datasets using joblib.\n",
      "    \n",
      "    return_X_y : bool, default=False\n",
      "        If True, returns ``(data, target)`` instead of a Bunch object. See\n",
      "        below for more information about the `data` and `target` objects.\n",
      "    \n",
      "    as_frame : bool or 'auto', default='auto'\n",
      "        If True, the data is a pandas DataFrame including columns with\n",
      "        appropriate dtypes (numeric, string or categorical). The target is\n",
      "        a pandas DataFrame or Series depending on the number of target_columns.\n",
      "        The Bunch will contain a ``frame`` attribute with the target and the\n",
      "        data. If ``return_X_y`` is True, then ``(data, target)`` will be pandas\n",
      "        DataFrames or Series as describe above.\n",
      "    \n",
      "        If as_frame is 'auto', the data and target will be converted to\n",
      "        DataFrame or Series as if as_frame is set to True, unless the dataset\n",
      "        is stored in sparse format.\n",
      "    \n",
      "        .. versionchanged:: 0.24\n",
      "           The default value of `as_frame` changed from `False` to `'auto'`\n",
      "           in 0.24.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    \n",
      "    data : :class:`~sklearn.utils.Bunch`\n",
      "        Dictionary-like object, with the following attributes.\n",
      "    \n",
      "        data : np.array, scipy.sparse.csr_matrix of floats, or pandas DataFrame\n",
      "            The feature matrix. Categorical features are encoded as ordinals.\n",
      "        target : np.array, pandas Series or DataFrame\n",
      "            The regression target or classification labels, if applicable.\n",
      "            Dtype is float if numeric, and object if categorical. If\n",
      "            ``as_frame`` is True, ``target`` is a pandas object.\n",
      "        DESCR : str\n",
      "            The full description of the dataset\n",
      "        feature_names : list\n",
      "            The names of the dataset columns\n",
      "        target_names: list\n",
      "            The names of the target columns\n",
      "    \n",
      "        .. versionadded:: 0.22\n",
      "    \n",
      "        categories : dict or None\n",
      "            Maps each categorical feature name to a list of values, such\n",
      "            that the value encoded as i is ith in the list. If ``as_frame``\n",
      "            is True, this is None.\n",
      "        details : dict\n",
      "            More metadata from OpenML\n",
      "        frame : pandas DataFrame\n",
      "            Only present when `as_frame=True`. DataFrame with ``data`` and\n",
      "            ``target``.\n",
      "    \n",
      "    (data, target) : tuple if ``return_X_y`` is True\n",
      "    \n",
      "        .. note:: EXPERIMENTAL\n",
      "    \n",
      "            This interface is **experimental** and subsequent releases may\n",
      "            change attributes without notice (although there should only be\n",
      "            minor changes to ``data`` and ``target``).\n",
      "    \n",
      "        Missing values in the 'data' are represented as NaN's. Missing values\n",
      "        in 'target' are represented as NaN's (numerical target) or None\n",
      "        (categorical target)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fetch_openml?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bce3e06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3cb13be",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TSNE in module sklearn.manifold._t_sne:\n",
      "\n",
      "class TSNE(sklearn.base.BaseEstimator)\n",
      " |  TSNE(n_components=2, *, perplexity=30.0, early_exaggeration=12.0, learning_rate=200.0, n_iter=1000, n_iter_without_progress=300, min_grad_norm=1e-07, metric='euclidean', init='random', verbose=0, random_state=None, method='barnes_hut', angle=0.5, n_jobs=None, square_distances='legacy')\n",
      " |  \n",
      " |  t-distributed Stochastic Neighbor Embedding.\n",
      " |  \n",
      " |  t-SNE [1] is a tool to visualize high-dimensional data. It converts\n",
      " |  similarities between data points to joint probabilities and tries\n",
      " |  to minimize the Kullback-Leibler divergence between the joint\n",
      " |  probabilities of the low-dimensional embedding and the\n",
      " |  high-dimensional data. t-SNE has a cost function that is not convex,\n",
      " |  i.e. with different initializations we can get different results.\n",
      " |  \n",
      " |  It is highly recommended to use another dimensionality reduction\n",
      " |  method (e.g. PCA for dense data or TruncatedSVD for sparse data)\n",
      " |  to reduce the number of dimensions to a reasonable amount (e.g. 50)\n",
      " |  if the number of features is very high. This will suppress some\n",
      " |  noise and speed up the computation of pairwise distances between\n",
      " |  samples. For more tips see Laurens van der Maaten's FAQ [2].\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <t_sne>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_components : int, default=2\n",
      " |      Dimension of the embedded space.\n",
      " |  \n",
      " |  perplexity : float, default=30.0\n",
      " |      The perplexity is related to the number of nearest neighbors that\n",
      " |      is used in other manifold learning algorithms. Larger datasets\n",
      " |      usually require a larger perplexity. Consider selecting a value\n",
      " |      between 5 and 50. Different values can result in significantly\n",
      " |      different results.\n",
      " |  \n",
      " |  early_exaggeration : float, default=12.0\n",
      " |      Controls how tight natural clusters in the original space are in\n",
      " |      the embedded space and how much space will be between them. For\n",
      " |      larger values, the space between natural clusters will be larger\n",
      " |      in the embedded space. Again, the choice of this parameter is not\n",
      " |      very critical. If the cost function increases during initial\n",
      " |      optimization, the early exaggeration factor or the learning rate\n",
      " |      might be too high.\n",
      " |  \n",
      " |  learning_rate : float, default=200.0\n",
      " |      The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If\n",
      " |      the learning rate is too high, the data may look like a 'ball' with any\n",
      " |      point approximately equidistant from its nearest neighbours. If the\n",
      " |      learning rate is too low, most points may look compressed in a dense\n",
      " |      cloud with few outliers. If the cost function gets stuck in a bad local\n",
      " |      minimum increasing the learning rate may help.\n",
      " |  \n",
      " |  n_iter : int, default=1000\n",
      " |      Maximum number of iterations for the optimization. Should be at\n",
      " |      least 250.\n",
      " |  \n",
      " |  n_iter_without_progress : int, default=300\n",
      " |      Maximum number of iterations without progress before we abort the\n",
      " |      optimization, used after 250 initial iterations with early\n",
      " |      exaggeration. Note that progress is only checked every 50 iterations so\n",
      " |      this value is rounded to the next multiple of 50.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         parameter *n_iter_without_progress* to control stopping criteria.\n",
      " |  \n",
      " |  min_grad_norm : float, default=1e-7\n",
      " |      If the gradient norm is below this threshold, the optimization will\n",
      " |      be stopped.\n",
      " |  \n",
      " |  metric : str or callable, default='euclidean'\n",
      " |      The metric to use when calculating distance between instances in a\n",
      " |      feature array. If metric is a string, it must be one of the options\n",
      " |      allowed by scipy.spatial.distance.pdist for its metric parameter, or\n",
      " |      a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.\n",
      " |      If metric is \"precomputed\", X is assumed to be a distance matrix.\n",
      " |      Alternatively, if metric is a callable function, it is called on each\n",
      " |      pair of instances (rows) and the resulting value recorded. The callable\n",
      " |      should take two arrays from X as input and return a value indicating\n",
      " |      the distance between them. The default is \"euclidean\" which is\n",
      " |      interpreted as squared euclidean distance.\n",
      " |  \n",
      " |  init : {'random', 'pca'} or ndarray of shape (n_samples, n_components),             default='random'\n",
      " |      Initialization of embedding. Possible options are 'random', 'pca',\n",
      " |      and a numpy array of shape (n_samples, n_components).\n",
      " |      PCA initialization cannot be used with precomputed distances and is\n",
      " |      usually more globally stable than random initialization.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Verbosity level.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Determines the random number generator. Pass an int for reproducible\n",
      " |      results across multiple function calls. Note that different\n",
      " |      initializations might result in different local minima of the cost\n",
      " |      function. See :term: `Glossary <random_state>`.\n",
      " |  \n",
      " |  method : str, default='barnes_hut'\n",
      " |      By default the gradient calculation algorithm uses Barnes-Hut\n",
      " |      approximation running in O(NlogN) time. method='exact'\n",
      " |      will run on the slower, but exact, algorithm in O(N^2) time. The\n",
      " |      exact algorithm should be used when nearest-neighbor errors need\n",
      " |      to be better than 3%. However, the exact method cannot scale to\n",
      " |      millions of examples.\n",
      " |  \n",
      " |      .. versionadded:: 0.17\n",
      " |         Approximate optimization *method* via the Barnes-Hut.\n",
      " |  \n",
      " |  angle : float, default=0.5\n",
      " |      Only used if method='barnes_hut'\n",
      " |      This is the trade-off between speed and accuracy for Barnes-Hut T-SNE.\n",
      " |      'angle' is the angular size (referred to as theta in [3]) of a distant\n",
      " |      node as measured from a point. If this size is below 'angle' then it is\n",
      " |      used as a summary node of all points contained within it.\n",
      " |      This method is not very sensitive to changes in this parameter\n",
      " |      in the range of 0.2 - 0.8. Angle less than 0.2 has quickly increasing\n",
      " |      computation time and angle greater 0.8 has quickly increasing error.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of parallel jobs to run for neighbors search. This parameter\n",
      " |      has no impact when ``metric=\"precomputed\"`` or\n",
      " |      (``metric=\"euclidean\"`` and ``method=\"exact\"``).\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  square_distances : True or 'legacy', default='legacy'\n",
      " |      Whether TSNE should square the distance values. ``'legacy'`` means\n",
      " |      that distance values are squared only when ``metric=\"euclidean\"``.\n",
      " |      ``True`` means that distance values are squared for all metrics.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |         Added to provide backward compatibility during deprecation of\n",
      " |         legacy squaring behavior.\n",
      " |      .. deprecated:: 0.24\n",
      " |         Legacy squaring behavior was deprecated in 0.24. The ``'legacy'``\n",
      " |         value will be removed in 1.1 (renaming of 0.26), at which point the\n",
      " |         default value will change to ``True``.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  embedding_ : array-like of shape (n_samples, n_components)\n",
      " |      Stores the embedding vectors.\n",
      " |  \n",
      " |  kl_divergence_ : float\n",
      " |      Kullback-Leibler divergence after optimization.\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      Number of iterations run.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  \n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.manifold import TSNE\n",
      " |  >>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
      " |  >>> X_embedded = TSNE(n_components=2).fit_transform(X)\n",
      " |  >>> X_embedded.shape\n",
      " |  (4, 2)\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  [1] van der Maaten, L.J.P.; Hinton, G.E. Visualizing High-Dimensional Data\n",
      " |      Using t-SNE. Journal of Machine Learning Research 9:2579-2605, 2008.\n",
      " |  \n",
      " |  [2] van der Maaten, L.J.P. t-Distributed Stochastic Neighbor Embedding\n",
      " |      https://lvdmaaten.github.io/tsne/\n",
      " |  \n",
      " |  [3] L.J.P. van der Maaten. Accelerating t-SNE using Tree-Based Algorithms.\n",
      " |      Journal of Machine Learning Research 15(Oct):3221-3245, 2014.\n",
      " |      https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TSNE\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_components=2, *, perplexity=30.0, early_exaggeration=12.0, learning_rate=200.0, n_iter=1000, n_iter_without_progress=300, min_grad_norm=1e-07, metric='euclidean', init='random', verbose=0, random_state=None, method='barnes_hut', angle=0.5, n_jobs=None, square_distances='legacy')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y=None)\n",
      " |      Fit X into an embedded space.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : ndarray of shape (n_samples, n_features) or (n_samples, n_samples)\n",
      " |          If the metric is 'precomputed' X must be a square distance\n",
      " |          matrix. Otherwise it contains a sample per row. If the method\n",
      " |          is 'exact', X may be a sparse matrix of type 'csr', 'csc'\n",
      " |          or 'coo'. If the method is 'barnes_hut' and the metric is\n",
      " |          'precomputed', X may be a precomputed sparse graph.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |  \n",
      " |  fit_transform(self, X, y=None)\n",
      " |      Fit X into an embedded space and return that transformed\n",
      " |      output.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : ndarray of shape (n_samples, n_features) or (n_samples, n_samples)\n",
      " |          If the metric is 'precomputed' X must be a square distance\n",
      " |          matrix. Otherwise it contains a sample per row. If the method\n",
      " |          is 'exact', X may be a sparse matrix of type 'csr', 'csc'\n",
      " |          or 'coo'. If the method is 'barnes_hut' and the metric is\n",
      " |          'precomputed', X may be a precomputed sparse graph.\n",
      " |      \n",
      " |      y : Ignored\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_new : ndarray of shape (n_samples, n_components)\n",
      " |          Embedding of the training data in low-dimensional space.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TSNE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79ef1f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ce895e8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class DummyClassifier in module sklearn.dummy:\n",
      "\n",
      "class DummyClassifier(sklearn.base.MultiOutputMixin, sklearn.base.ClassifierMixin, sklearn.base.BaseEstimator)\n",
      " |  DummyClassifier(*, strategy='prior', random_state=None, constant=None)\n",
      " |  \n",
      " |  DummyClassifier is a classifier that makes predictions using simple rules.\n",
      " |  \n",
      " |  This classifier is useful as a simple baseline to compare with other\n",
      " |  (real) classifiers. Do not use it for real problems.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <dummy_estimators>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.13\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  strategy : {\"stratified\", \"most_frequent\", \"prior\", \"uniform\",             \"constant\"}, default=\"prior\"\n",
      " |      Strategy to use to generate predictions.\n",
      " |  \n",
      " |      * \"stratified\": generates predictions by respecting the training\n",
      " |        set's class distribution.\n",
      " |      * \"most_frequent\": always predicts the most frequent label in the\n",
      " |        training set.\n",
      " |      * \"prior\": always predicts the class that maximizes the class prior\n",
      " |        (like \"most_frequent\") and ``predict_proba`` returns the class prior.\n",
      " |      * \"uniform\": generates predictions uniformly at random.\n",
      " |      * \"constant\": always predicts a constant label that is provided by\n",
      " |        the user. This is useful for metrics that evaluate a non-majority\n",
      " |        class\n",
      " |  \n",
      " |        .. versionchanged:: 0.24\n",
      " |           The default value of `strategy` has changed to \"prior\" in version\n",
      " |           0.24.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the randomness to generate the predictions when\n",
      " |      ``strategy='stratified'`` or ``strategy='uniform'``.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  constant : int or str or array-like of shape (n_outputs,)\n",
      " |      The explicit constant as predicted by the \"constant\" strategy. This\n",
      " |      parameter is useful only for the \"constant\" strategy.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : ndarray of shape (n_classes,) or list of such arrays\n",
      " |      Class labels for each output.\n",
      " |  \n",
      " |  n_classes_ : int or list of int\n",
      " |      Number of label for each output.\n",
      " |  \n",
      " |  class_prior_ : ndarray of shape (n_classes,) or list of such arrays\n",
      " |      Probability of each class for each output.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      Number of outputs.\n",
      " |  \n",
      " |  sparse_output_ : bool\n",
      " |      True if the array returned from predict is to be in sparse CSC format.\n",
      " |      Is automatically set to True if the input y is passed in sparse format.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.dummy import DummyClassifier\n",
      " |  >>> X = np.array([-1, 1, 1, 1])\n",
      " |  >>> y = np.array([0, 1, 1, 1])\n",
      " |  >>> dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
      " |  >>> dummy_clf.fit(X, y)\n",
      " |  DummyClassifier(strategy='most_frequent')\n",
      " |  >>> dummy_clf.predict(X)\n",
      " |  array([1, 1, 1, 1])\n",
      " |  >>> dummy_clf.score(X, y)\n",
      " |  0.75\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DummyClassifier\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, strategy='prior', random_state=None, constant=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the random classifier.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Training data.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          Target values.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Perform classification on test vectors X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          Predicted target values for X.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Return log probability estimates for the test vectors X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, object with finite length or shape}\n",
      " |          Training data, requires length = n_samples\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      P : ndarray of shape (n_samples, n_classes) or list of such arrays\n",
      " |          Returns the log probability of the sample for each class in\n",
      " |          the model, where classes are ordered arithmetically for each\n",
      " |          output.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Return probability estimates for the test vectors X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      P : ndarray of shape (n_samples, n_classes) or list of such arrays\n",
      " |          Returns the probability of the sample for each class in\n",
      " |          the model, where classes are ordered arithmetically, for each\n",
      " |          output.\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : None or array-like of shape (n_samples, n_features)\n",
      " |          Test samples. Passing None as test samples gives the same result\n",
      " |          as passing real test samples, since DummyClassifier\n",
      " |          operates independently of the sampled observations.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of self.predict(X) wrt. y.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DummyClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73d66728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6455e7bd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function confusion_matrix in module sklearn.metrics._classification:\n",
      "\n",
      "confusion_matrix(y_true, y_pred, *, labels=None, sample_weight=None, normalize=None)\n",
      "    Compute confusion matrix to evaluate the accuracy of a classification.\n",
      "    \n",
      "    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\n",
      "    is equal to the number of observations known to be in group :math:`i` and\n",
      "    predicted to be in group :math:`j`.\n",
      "    \n",
      "    Thus in binary classification, the count of true negatives is\n",
      "    :math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is\n",
      "    :math:`C_{1,1}` and false positives is :math:`C_{0,1}`.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <confusion_matrix>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true : array-like of shape (n_samples,)\n",
      "        Ground truth (correct) target values.\n",
      "    \n",
      "    y_pred : array-like of shape (n_samples,)\n",
      "        Estimated targets as returned by a classifier.\n",
      "    \n",
      "    labels : array-like of shape (n_classes), default=None\n",
      "        List of labels to index the matrix. This may be used to reorder\n",
      "        or select a subset of labels.\n",
      "        If ``None`` is given, those that appear at least once\n",
      "        in ``y_true`` or ``y_pred`` are used in sorted order.\n",
      "    \n",
      "    sample_weight : array-like of shape (n_samples,), default=None\n",
      "        Sample weights.\n",
      "    \n",
      "        .. versionadded:: 0.18\n",
      "    \n",
      "    normalize : {'true', 'pred', 'all'}, default=None\n",
      "        Normalizes confusion matrix over the true (rows), predicted (columns)\n",
      "        conditions or all the population. If None, confusion matrix will not be\n",
      "        normalized.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    C : ndarray of shape (n_classes, n_classes)\n",
      "        Confusion matrix whose i-th row and j-th\n",
      "        column entry indicates the number of\n",
      "        samples with true label being i-th class\n",
      "        and predicted label being j-th class.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    ConfusionMatrixDisplay.from_estimator : Plot the confusion matrix\n",
      "        given an estimator, the data, and the label.\n",
      "    ConfusionMatrixDisplay.from_predictions : Plot the confusion matrix\n",
      "        given the true and predicted labels.\n",
      "    ConfusionMatrixDisplay : Confusion Matrix visualization.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] `Wikipedia entry for the Confusion matrix\n",
      "           <https://en.wikipedia.org/wiki/Confusion_matrix>`_\n",
      "           (Wikipedia and other references may use a different\n",
      "           convention for axes).\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.metrics import confusion_matrix\n",
      "    >>> y_true = [2, 0, 2, 2, 0, 1]\n",
      "    >>> y_pred = [0, 0, 2, 2, 0, 2]\n",
      "    >>> confusion_matrix(y_true, y_pred)\n",
      "    array([[2, 0, 0],\n",
      "           [0, 0, 1],\n",
      "           [1, 0, 2]])\n",
      "    \n",
      "    >>> y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
      "    >>> y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
      "    >>> confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\n",
      "    array([[2, 0, 0],\n",
      "           [0, 0, 1],\n",
      "           [1, 0, 2]])\n",
      "    \n",
      "    In the binary case, we can extract true positives, etc as follows:\n",
      "    \n",
      "    >>> tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
      "    >>> (tn, fp, fn, tp)\n",
      "    (0, 2, 1, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6863f1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0a6433d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31af2982",
   "metadata": {},
   "outputs": [],
   "source": [
    "SimpleImputer?\n",
    "# Imputation transformer for completing missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7be2cb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bc7c70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7d41ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "055c91a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c69b275e",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65b623da",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98cf020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9cfdbbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "226db326",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74ead58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# it avoids displaying warnings related to the cell we run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0c415f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd5e1f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "OneHotEncoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870a810d",
   "metadata": {},
   "source": [
    "Encode categorical features as a one-hot numeric array. It only can be applied to DataFrames with one column, so if I want to use it in a pipeline, I have to use a column transformer with `remainder='passthrough'` so that the pipeline works and does nothing to the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1dac17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058185ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ColumnTransformer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bda583",
   "metadata": {},
   "source": [
    "Applies transformers to columns of an array or pandas DataFrame.\n",
    "\n",
    "This estimator allows different columns or column subsets of the input\n",
    "to be transformed separately and the features generated by each transformer\n",
    "will be concatenated to form a single feature space.\n",
    "This is useful for heterogeneous or columnar data, to combine several\n",
    "feature extraction mechanisms or transformations into a single transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "618074c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ksunsupervised/lib/python3.9/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "from category_encoders import TargetEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "175cded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TargetEncoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eb090b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e759e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "StandardScaler?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4b73f1",
   "metadata": {},
   "source": [
    "Standard Scaler standardizes features by removing the mean and scaling to unit variance. This is very important because if a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75ab0b1",
   "metadata": {},
   "source": [
    "### Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d77ba9",
   "metadata": {},
   "source": [
    "Data structures and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e340b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "greater-benchmark",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package pandas:\n",
      "\n",
      "NAME\n",
      "    pandas\n",
      "\n",
      "DESCRIPTION\n",
      "    pandas - a powerful data analysis and manipulation library for Python\n",
      "    =====================================================================\n",
      "    \n",
      "    **pandas** is a Python package providing fast, flexible, and expressive data\n",
      "    structures designed to make working with \"relational\" or \"labeled\" data both\n",
      "    easy and intuitive. It aims to be the fundamental high-level building block for\n",
      "    doing practical, **real world** data analysis in Python. Additionally, it has\n",
      "    the broader goal of becoming **the most powerful and flexible open source data\n",
      "    analysis / manipulation tool available in any language**. It is already well on\n",
      "    its way toward this goal.\n",
      "    \n",
      "    Main Features\n",
      "    -------------\n",
      "    Here are just a few of the things that pandas does well:\n",
      "    \n",
      "      - Easy handling of missing data in floating point as well as non-floating\n",
      "        point data.\n",
      "      - Size mutability: columns can be inserted and deleted from DataFrame and\n",
      "        higher dimensional objects\n",
      "      - Automatic and explicit data alignment: objects can be explicitly aligned\n",
      "        to a set of labels, or the user can simply ignore the labels and let\n",
      "        `Series`, `DataFrame`, etc. automatically align the data for you in\n",
      "        computations.\n",
      "      - Powerful, flexible group by functionality to perform split-apply-combine\n",
      "        operations on data sets, for both aggregating and transforming data.\n",
      "      - Make it easy to convert ragged, differently-indexed data in other Python\n",
      "        and NumPy data structures into DataFrame objects.\n",
      "      - Intelligent label-based slicing, fancy indexing, and subsetting of large\n",
      "        data sets.\n",
      "      - Intuitive merging and joining data sets.\n",
      "      - Flexible reshaping and pivoting of data sets.\n",
      "      - Hierarchical labeling of axes (possible to have multiple labels per tick).\n",
      "      - Robust IO tools for loading data from flat files (CSV and delimited),\n",
      "        Excel files, databases, and saving/loading data from the ultrafast HDF5\n",
      "        format.\n",
      "      - Time series-specific functionality: date range generation and frequency\n",
      "        conversion, moving window statistics, date shifting and lagging.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _config (package)\n",
      "    _libs (package)\n",
      "    _testing\n",
      "    _typing\n",
      "    _version\n",
      "    api (package)\n",
      "    arrays (package)\n",
      "    compat (package)\n",
      "    conftest\n",
      "    core (package)\n",
      "    errors (package)\n",
      "    io (package)\n",
      "    plotting (package)\n",
      "    testing\n",
      "    tests (package)\n",
      "    tseries (package)\n",
      "    util (package)\n",
      "\n",
      "SUBMODULES\n",
      "    _hashtable\n",
      "    _lib\n",
      "    _tslib\n",
      "    offsets\n",
      "\n",
      "FUNCTIONS\n",
      "    __getattr__(name)\n",
      "        # GH 27101\n",
      "\n",
      "DATA\n",
      "    IndexSlice = <pandas.core.indexing._IndexSlice object>\n",
      "    NA = <NA>\n",
      "    NaT = NaT\n",
      "    __docformat__ = 'restructuredtext'\n",
      "    __git_version__ = '9d598a5e1eee26df95b3910e3f2934890d062caa'\n",
      "    describe_option = <pandas._config.config.CallableDynamicDoc object>\n",
      "    get_option = <pandas._config.config.CallableDynamicDoc object>\n",
      "    options = <pandas._config.config.DictWrapper object>\n",
      "    reset_option = <pandas._config.config.CallableDynamicDoc object>\n",
      "    set_option = <pandas._config.config.CallableDynamicDoc object>\n",
      "\n",
      "VERSION\n",
      "    1.2.1\n",
      "\n",
      "FILE\n",
      "    /opt/anaconda3/envs/python_intro/lib/python3.8/site-packages/pandas/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pd?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-illustration",
   "metadata": {},
   "source": [
    "### MatPlotLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "greater-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "vertical-spyware",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module matplotlib.pyplot in matplotlib:\n",
      "\n",
      "NAME\n",
      "    matplotlib.pyplot\n",
      "\n",
      "DESCRIPTION\n",
      "    `matplotlib.pyplot` is a state-based interface to matplotlib. It provides\n",
      "    a MATLAB-like way of plotting.\n",
      "    \n",
      "    pyplot is mainly intended for interactive plots and simple cases of\n",
      "    programmatic plot generation::\n",
      "    \n",
      "        import numpy as np\n",
      "        import matplotlib.pyplot as plt\n",
      "    \n",
      "        x = np.arange(0, 5, 0.1)\n",
      "        y = np.sin(x)\n",
      "        plt.plot(x, y)\n",
      "    \n",
      "    The object-oriented API is recommended for more complex plots.\n",
      "\n",
      "FUNCTIONS\n",
      "    acorr(x, *, data=None, **kwargs)\n",
      "        Plot the autocorrelation of *x*.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array-like\n",
      "        \n",
      "        detrend : callable, default: `.mlab.detrend_none` (no detrending)\n",
      "            A detrending function applied to *x*.  It must have the\n",
      "            signature ::\n",
      "        \n",
      "                detrend(x: np.ndarray) -> np.ndarray\n",
      "        \n",
      "        normed : bool, default: True\n",
      "            If ``True``, input vectors are normalised to unit length.\n",
      "        \n",
      "        usevlines : bool, default: True\n",
      "            Determines the plot style.\n",
      "        \n",
      "            If ``True``, vertical lines are plotted from 0 to the acorr value\n",
      "            using `.Axes.vlines`. Additionally, a horizontal line is plotted\n",
      "            at y=0 using `.Axes.axhline`.\n",
      "        \n",
      "            If ``False``, markers are plotted at the acorr values using\n",
      "            `.Axes.plot`.\n",
      "        \n",
      "        maxlags : int, default: 10\n",
      "            Number of lags to show. If ``None``, will return all\n",
      "            ``2 * len(x) - 1`` lags.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        lags : array (length ``2*maxlags+1``)\n",
      "            The lag vector.\n",
      "        c : array  (length ``2*maxlags+1``)\n",
      "            The auto correlation vector.\n",
      "        line : `.LineCollection` or `.Line2D`\n",
      "            `.Artist` added to the axes of the correlation:\n",
      "        \n",
      "            - `.LineCollection` if *usevlines* is True.\n",
      "            - `.Line2D` if *usevlines* is False.\n",
      "        b : `.Line2D` or None\n",
      "            Horizontal line at 0 if *usevlines* is True\n",
      "            None *usevlines* is False.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        linestyle : `.Line2D` property, optional\n",
      "            The linestyle for plotting the data points.\n",
      "            Only used if *usevlines* is ``False``.\n",
      "        \n",
      "        marker : str, default: 'o'\n",
      "            The marker for plotting the data points.\n",
      "            Only used if *usevlines* is ``False``.\n",
      "        \n",
      "        **kwargs\n",
      "            Additional parameters are passed to `.Axes.vlines` and\n",
      "            `.Axes.axhline` if *usevlines* is ``True``; otherwise they are\n",
      "            passed to `.Axes.plot`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The cross correlation is performed with `numpy.correlate` with\n",
      "        ``mode = \"full\"``.\n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            the following arguments can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception):\n",
      "            *x*.\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    angle_spectrum(x, Fs=None, Fc=None, window=None, pad_to=None, sides=None, *, data=None, **kwargs)\n",
      "        Plot the angle spectrum.\n",
      "        \n",
      "        Compute the angle spectrum (wrapped phase spectrum) of *x*.\n",
      "        Data is padded to a length of *pad_to* and the windowing function\n",
      "        *window* is applied to the signal.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : 1-D array or sequence\n",
      "            Array or sequence containing the data.\n",
      "        \n",
      "        Fs : float, default: 2\n",
      "            The sampling frequency (samples per time unit).  It is used to calculate\n",
      "            the Fourier frequencies, *freqs*, in cycles per time unit.\n",
      "        \n",
      "        window : callable or ndarray, default: `.window_hanning`\n",
      "            A function or a vector of length *NFFT*.  To create window vectors see\n",
      "            `.window_hanning`, `.window_none`, `numpy.blackman`, `numpy.hamming`,\n",
      "            `numpy.bartlett`, `scipy.signal`, `scipy.signal.get_window`, etc.  If a\n",
      "            function is passed as the argument, it must take a data segment as an\n",
      "            argument and return the windowed version of the segment.\n",
      "        \n",
      "        sides : {'default', 'onesided', 'twosided'}, optional\n",
      "            Which sides of the spectrum to return. 'default' is one-sided for real\n",
      "            data and two-sided for complex data. 'onesided' forces the return of a\n",
      "            one-sided spectrum, while 'twosided' forces two-sided.\n",
      "        \n",
      "        pad_to : int, optional\n",
      "            The number of points to which the data segment is padded when performing\n",
      "            the FFT.  While not increasing the actual resolution of the spectrum (the\n",
      "            minimum distance between resolvable peaks), this can give more points in\n",
      "            the plot, allowing for more detail. This corresponds to the *n* parameter\n",
      "            in the call to fft().  The default is None, which sets *pad_to* equal to\n",
      "            the length of the input signal (i.e. no padding).\n",
      "        \n",
      "        Fc : int, default: 0\n",
      "            The center frequency of *x*, which offsets the x extents of the\n",
      "            plot to reflect the frequency range used when a signal is acquired\n",
      "            and then filtered and downsampled to baseband.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        spectrum : 1-D array\n",
      "            The values for the angle spectrum in radians (real valued).\n",
      "        \n",
      "        freqs : 1-D array\n",
      "            The frequencies corresponding to the elements in *spectrum*.\n",
      "        \n",
      "        line : `~matplotlib.lines.Line2D`\n",
      "            The line created by this function.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            Keyword arguments control the `.Line2D` properties:\n",
      "        \n",
      "            Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa: bool\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            color or c: color\n",
      "            contains: unknown\n",
      "            dash_capstyle: {'butt', 'round', 'projecting'}\n",
      "            dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      "            data: (2, N) array or two 1D arrays\n",
      "            drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      "            figure: `.Figure`\n",
      "            fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      "            gid: str\n",
      "            in_layout: bool\n",
      "            label: object\n",
      "            linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      "            linewidth or lw: float\n",
      "            marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      "            markeredgecolor or mec: color\n",
      "            markeredgewidth or mew: float\n",
      "            markerfacecolor or mfc: color\n",
      "            markerfacecoloralt or mfcalt: color\n",
      "            markersize or ms: float\n",
      "            markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: unknown\n",
      "            pickradius: float\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            solid_capstyle: {'butt', 'round', 'projecting'}\n",
      "            solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            transform: `matplotlib.transforms.Transform`\n",
      "            url: str\n",
      "            visible: bool\n",
      "            xdata: 1D array\n",
      "            ydata: 1D array\n",
      "            zorder: float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        magnitude_spectrum\n",
      "            Plots the magnitudes of the corresponding frequencies.\n",
      "        phase_spectrum\n",
      "            Plots the unwrapped version of this function.\n",
      "        specgram\n",
      "            Can plot the angle spectrum of segments within the signal in a\n",
      "            colormap.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            the following arguments can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception):\n",
      "            *x*.\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    annotate(text, xy, *args, **kwargs)\n",
      "        Annotate the point *xy* with text *text*.\n",
      "        \n",
      "        In the simplest form, the text is placed at *xy*.\n",
      "        \n",
      "        Optionally, the text can be displayed in another position *xytext*.\n",
      "        An arrow pointing from the text to the annotated point *xy* can then\n",
      "        be added by defining *arrowprops*.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        text : str\n",
      "            The text of the annotation.  *s* is a deprecated synonym for this\n",
      "            parameter.\n",
      "        \n",
      "        xy : (float, float)\n",
      "            The point *(x, y)* to annotate. The coordinate system is determined\n",
      "            by *xycoords*.\n",
      "        \n",
      "        xytext : (float, float), default: *xy*\n",
      "            The position *(x, y)* to place the text at. The coordinate system\n",
      "            is determined by *textcoords*.\n",
      "        \n",
      "        xycoords : str or `.Artist` or `.Transform` or callable or (float, float), default: 'data'\n",
      "        \n",
      "            The coordinate system that *xy* is given in. The following types\n",
      "            of values are supported:\n",
      "        \n",
      "            - One of the following strings:\n",
      "        \n",
      "              =================   =============================================\n",
      "              Value               Description\n",
      "              =================   =============================================\n",
      "              'figure points'     Points from the lower left of the figure\n",
      "              'figure pixels'     Pixels from the lower left of the figure\n",
      "              'figure fraction'   Fraction of figure from lower left\n",
      "              'axes points'       Points from lower left corner of axes\n",
      "              'axes pixels'       Pixels from lower left corner of axes\n",
      "              'axes fraction'     Fraction of axes from lower left\n",
      "              'data'              Use the coordinate system of the object being\n",
      "                                  annotated (default)\n",
      "              'polar'             *(theta, r)* if not native 'data' coordinates\n",
      "              =================   =============================================\n",
      "        \n",
      "            - An `.Artist`: *xy* is interpreted as a fraction of the artist's\n",
      "              `~matplotlib.transforms.Bbox`. E.g. *(0, 0)* would be the lower\n",
      "              left corner of the bounding box and *(0.5, 1)* would be the\n",
      "              center top of the bounding box.\n",
      "        \n",
      "            - A `.Transform` to transform *xy* to screen coordinates.\n",
      "        \n",
      "            - A function with one of the following signatures::\n",
      "        \n",
      "                def transform(renderer) -> Bbox\n",
      "                def transform(renderer) -> Transform\n",
      "        \n",
      "              where *renderer* is a `.RendererBase` subclass.\n",
      "        \n",
      "              The result of the function is interpreted like the `.Artist` and\n",
      "              `.Transform` cases above.\n",
      "        \n",
      "            - A tuple *(xcoords, ycoords)* specifying separate coordinate\n",
      "              systems for *x* and *y*. *xcoords* and *ycoords* must each be\n",
      "              of one of the above described types.\n",
      "        \n",
      "            See :ref:`plotting-guide-annotation` for more details.\n",
      "        \n",
      "        textcoords : str or `.Artist` or `.Transform` or callable or (float, float), default: value of *xycoords*\n",
      "            The coordinate system that *xytext* is given in.\n",
      "        \n",
      "            All *xycoords* values are valid as well as the following\n",
      "            strings:\n",
      "        \n",
      "            =================   =========================================\n",
      "            Value               Description\n",
      "            =================   =========================================\n",
      "            'offset points'     Offset (in points) from the *xy* value\n",
      "            'offset pixels'     Offset (in pixels) from the *xy* value\n",
      "            =================   =========================================\n",
      "        \n",
      "        arrowprops : dict, optional\n",
      "            The properties used to draw a `.FancyArrowPatch` arrow between the\n",
      "            positions *xy* and *xytext*.\n",
      "        \n",
      "            If *arrowprops* does not contain the key 'arrowstyle' the\n",
      "            allowed keys are:\n",
      "        \n",
      "            ==========   ======================================================\n",
      "            Key          Description\n",
      "            ==========   ======================================================\n",
      "            width        The width of the arrow in points\n",
      "            headwidth    The width of the base of the arrow head in points\n",
      "            headlength   The length of the arrow head in points\n",
      "            shrink       Fraction of total length to shrink from both ends\n",
      "            ?            Any key to :class:`matplotlib.patches.FancyArrowPatch`\n",
      "            ==========   ======================================================\n",
      "        \n",
      "            If *arrowprops* contains the key 'arrowstyle' the\n",
      "            above keys are forbidden.  The allowed values of\n",
      "            ``'arrowstyle'`` are:\n",
      "        \n",
      "            ============   =============================================\n",
      "            Name           Attrs\n",
      "            ============   =============================================\n",
      "            ``'-'``        None\n",
      "            ``'->'``       head_length=0.4,head_width=0.2\n",
      "            ``'-['``       widthB=1.0,lengthB=0.2,angleB=None\n",
      "            ``'|-|'``      widthA=1.0,widthB=1.0\n",
      "            ``'-|>'``      head_length=0.4,head_width=0.2\n",
      "            ``'<-'``       head_length=0.4,head_width=0.2\n",
      "            ``'<->'``      head_length=0.4,head_width=0.2\n",
      "            ``'<|-'``      head_length=0.4,head_width=0.2\n",
      "            ``'<|-|>'``    head_length=0.4,head_width=0.2\n",
      "            ``'fancy'``    head_length=0.4,head_width=0.4,tail_width=0.4\n",
      "            ``'simple'``   head_length=0.5,head_width=0.5,tail_width=0.2\n",
      "            ``'wedge'``    tail_width=0.3,shrink_factor=0.5\n",
      "            ============   =============================================\n",
      "        \n",
      "            Valid keys for `~matplotlib.patches.FancyArrowPatch` are:\n",
      "        \n",
      "            ===============  ==================================================\n",
      "            Key              Description\n",
      "            ===============  ==================================================\n",
      "            arrowstyle       the arrow style\n",
      "            connectionstyle  the connection style\n",
      "            relpos           default is (0.5, 0.5)\n",
      "            patchA           default is bounding box of the text\n",
      "            patchB           default is None\n",
      "            shrinkA          default is 2 points\n",
      "            shrinkB          default is 2 points\n",
      "            mutation_scale   default is text size (in points)\n",
      "            mutation_aspect  default is 1.\n",
      "            ?                any key for :class:`matplotlib.patches.PathPatch`\n",
      "            ===============  ==================================================\n",
      "        \n",
      "            Defaults to None, i.e. no arrow is drawn.\n",
      "        \n",
      "        annotation_clip : bool or None, default: None\n",
      "            Whether to draw the annotation when the annotation point *xy* is\n",
      "            outside the axes area.\n",
      "        \n",
      "            - If *True*, the annotation will only be drawn when *xy* is\n",
      "              within the axes.\n",
      "            - If *False*, the annotation will always be drawn.\n",
      "            - If *None*, the annotation will only be drawn when *xy* is\n",
      "              within the axes and *xycoords* is 'data'.\n",
      "        \n",
      "        **kwargs\n",
      "            Additional kwargs are passed to `~matplotlib.text.Text`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `.Annotation`\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        :ref:`plotting-guide-annotation`\n",
      "    \n",
      "    arrow(x, y, dx, dy, **kwargs)\n",
      "        Add an arrow to the axes.\n",
      "        \n",
      "        This draws an arrow from ``(x, y)`` to ``(x+dx, y+dy)``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : float\n",
      "            The x and y coordinates of the arrow base.\n",
      "        \n",
      "        dx, dy : float\n",
      "            The length of the arrow along x and y direction.\n",
      "        \n",
      "        width: float, default: 0.001\n",
      "            Width of full arrow tail.\n",
      "        \n",
      "        length_includes_head: bool, default: False\n",
      "            True if head is to be counted in calculating the length.\n",
      "        \n",
      "        head_width: float or None, default: 3*width\n",
      "            Total width of the full arrow head.\n",
      "        \n",
      "        head_length: float or None, default: 1.5*head_width\n",
      "            Length of arrow head.\n",
      "        \n",
      "        shape: ['full', 'left', 'right'], default: 'full'\n",
      "            Draw the left-half, right-half, or full arrow.\n",
      "        \n",
      "        overhang: float, default: 0\n",
      "            Fraction that the arrow is swept back (0 overhang means\n",
      "            triangular shape). Can be negative or greater than one.\n",
      "        \n",
      "        head_starts_at_zero: bool, default: False\n",
      "            If True, the head starts being drawn at coordinate 0\n",
      "            instead of ending at coordinate 0.\n",
      "        \n",
      "        **kwargs\n",
      "            `.Patch` properties:\n",
      "        \n",
      "            Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa: unknown\n",
      "            capstyle: {'butt', 'round', 'projecting'}\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            color: color\n",
      "            contains: unknown\n",
      "            edgecolor or ec: color or None or 'auto'\n",
      "            facecolor or fc: color or None\n",
      "            figure: `.Figure`\n",
      "            fill: bool\n",
      "            gid: str\n",
      "            hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n",
      "            in_layout: bool\n",
      "            joinstyle: {'miter', 'round', 'bevel'}\n",
      "            label: object\n",
      "            linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      "            linewidth or lw: float or None\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: None or bool or callable\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            transform: `.Transform`\n",
      "            url: str\n",
      "            visible: bool\n",
      "            zorder: float\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `.FancyArrow`\n",
      "            The created `.FancyArrow` object.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The resulting arrow is affected by the axes aspect ratio and limits.\n",
      "        This may produce an arrow whose head is not square with its stem. To\n",
      "        create an arrow whose head is square with its stem,\n",
      "        use :meth:`annotate` for example:\n",
      "        \n",
      "        >>> ax.annotate(\"\", xy=(0.5, 0.5), xytext=(0, 0),\n",
      "        ...             arrowprops=dict(arrowstyle=\"->\"))\n",
      "    \n",
      "    autoscale(enable=True, axis='both', tight=None)\n",
      "        Autoscale the axis view to the data (toggle).\n",
      "        \n",
      "        Convenience method for simple axis view autoscaling.\n",
      "        It turns autoscaling on or off, and then,\n",
      "        if autoscaling for either axis is on, it performs\n",
      "        the autoscaling on the specified axis or axes.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        enable : bool or None, default: True\n",
      "            True turns autoscaling on, False turns it off.\n",
      "            None leaves the autoscaling state unchanged.\n",
      "        axis : {'both', 'x', 'y'}, default: 'both'\n",
      "            Which axis to operate on.\n",
      "        tight : bool or None, default: None\n",
      "            If True, first set the margins to zero.  Then, this argument is\n",
      "            forwarded to `autoscale_view` (regardless of its value); see the\n",
      "            description of its behavior there.\n",
      "    \n",
      "    autumn()\n",
      "        Set the colormap to \"autumn\".\n",
      "        \n",
      "        This changes the default colormap as well as the colormap of the current\n",
      "        image if there is one. See ``help(colormaps)`` for more information.\n",
      "    \n",
      "    axes(arg=None, **kwargs)\n",
      "        Add an axes to the current figure and make it the current axes.\n",
      "        \n",
      "        Call signatures::\n",
      "        \n",
      "            plt.axes()\n",
      "            plt.axes(rect, projection=None, polar=False, **kwargs)\n",
      "            plt.axes(ax)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        arg : None or 4-tuple\n",
      "            The exact behavior of this function depends on the type:\n",
      "        \n",
      "            - *None*: A new full window axes is added using\n",
      "              ``subplot(111, **kwargs)``.\n",
      "            - 4-tuple of floats *rect* = ``[left, bottom, width, height]``.\n",
      "              A new axes is added with dimensions *rect* in normalized\n",
      "              (0, 1) units using `~.Figure.add_axes` on the current figure.\n",
      "        \n",
      "        projection : {None, 'aitoff', 'hammer', 'lambert', 'mollweide', 'polar', 'rectilinear', str}, optional\n",
      "            The projection type of the `~.axes.Axes`. *str* is the name of\n",
      "            a custom projection, see `~matplotlib.projections`. The default\n",
      "            None results in a 'rectilinear' projection.\n",
      "        \n",
      "        polar : bool, default: False\n",
      "            If True, equivalent to projection='polar'.\n",
      "        \n",
      "        sharex, sharey : `~.axes.Axes`, optional\n",
      "            Share the x or y `~matplotlib.axis` with sharex and/or sharey.\n",
      "            The axis will have the same limits, ticks, and scale as the axis\n",
      "            of the shared axes.\n",
      "        \n",
      "        label : str\n",
      "            A label for the returned axes.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `~.axes.Axes`, or a subclass of `~.axes.Axes`\n",
      "            The returned axes class depends on the projection used. It is\n",
      "            `~.axes.Axes` if rectilinear projection is used and\n",
      "            `.projections.polar.PolarAxes` if polar projection is used.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            This method also takes the keyword arguments for\n",
      "            the returned axes class. The keyword arguments for the\n",
      "            rectilinear axes class `~.axes.Axes` can be found in\n",
      "            the following table but there might also be other keyword\n",
      "            arguments if another projection is used, see the actual axes\n",
      "            class.\n",
      "        \n",
      "            Properties:\n",
      "            adjustable: {'box', 'datalim'}\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            anchor: 2-tuple of floats or {'C', 'SW', 'S', 'SE', ...}\n",
      "            animated: bool\n",
      "            aspect: {'auto'} or num\n",
      "            autoscale_on: bool\n",
      "            autoscalex_on: bool\n",
      "            autoscaley_on: bool\n",
      "            axes_locator: Callable[[Axes, Renderer], Bbox]\n",
      "            axisbelow: bool or 'line'\n",
      "            box_aspect: None, or a number\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            contains: unknown\n",
      "            facecolor or fc: color\n",
      "            figure: `.Figure`\n",
      "            frame_on: bool\n",
      "            gid: str\n",
      "            in_layout: bool\n",
      "            label: object\n",
      "            navigate: bool\n",
      "            navigate_mode: unknown\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: None or bool or callable\n",
      "            position: [left, bottom, width, height] or `~matplotlib.transforms.Bbox`\n",
      "            prop_cycle: unknown\n",
      "            rasterization_zorder: float or None\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            title: str\n",
      "            transform: `.Transform`\n",
      "            url: str\n",
      "            visible: bool\n",
      "            xbound: unknown\n",
      "            xlabel: str\n",
      "            xlim: (bottom: float, top: float)\n",
      "            xmargin: float greater than -0.5\n",
      "            xscale: {\"linear\", \"log\", \"symlog\", \"logit\", ...}\n",
      "            xticklabels: unknown\n",
      "            xticks: unknown\n",
      "            ybound: unknown\n",
      "            ylabel: str\n",
      "            ylim: (bottom: float, top: float)\n",
      "            ymargin: float greater than -0.5\n",
      "            yscale: {\"linear\", \"log\", \"symlog\", \"logit\", ...}\n",
      "            yticklabels: unknown\n",
      "            yticks: unknown\n",
      "            zorder: float\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If the figure already has a axes with key (*args*,\n",
      "        *kwargs*) then it will simply make that axes current and\n",
      "        return it.  This behavior is deprecated. Meanwhile, if you do\n",
      "        not want this behavior (i.e., you want to force the creation of a\n",
      "        new axes), you must use a unique set of args and kwargs.  The axes\n",
      "        *label* attribute has been exposed for this purpose: if you want\n",
      "        two axes that are otherwise identical to be added to the figure,\n",
      "        make sure you give them unique labels.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        .Figure.add_axes\n",
      "        .pyplot.subplot\n",
      "        .Figure.add_subplot\n",
      "        .Figure.subplots\n",
      "        .pyplot.subplots\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        ::\n",
      "        \n",
      "            # Creating a new full window axes\n",
      "            plt.axes()\n",
      "        \n",
      "            # Creating a new axes with specified dimensions and some kwargs\n",
      "            plt.axes((left, bottom, width, height), facecolor='w')\n",
      "    \n",
      "    axhline(y=0, xmin=0, xmax=1, **kwargs)\n",
      "        Add a horizontal line across the axis.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : float, default: 0\n",
      "            y position in data coordinates of the horizontal line.\n",
      "        \n",
      "        xmin : float, default: 0\n",
      "            Should be between 0 and 1, 0 being the far left of the plot, 1 the\n",
      "            far right of the plot.\n",
      "        \n",
      "        xmax : float, default: 1\n",
      "            Should be between 0 and 1, 0 being the far left of the plot, 1 the\n",
      "            far right of the plot.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `~matplotlib.lines.Line2D`\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            Valid keyword arguments are `.Line2D` properties, with the\n",
      "            exception of 'transform':\n",
      "        \n",
      "            Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa: bool\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            color or c: color\n",
      "            contains: unknown\n",
      "            dash_capstyle: {'butt', 'round', 'projecting'}\n",
      "            dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      "            data: (2, N) array or two 1D arrays\n",
      "            drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      "            figure: `.Figure`\n",
      "            fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      "            gid: str\n",
      "            in_layout: bool\n",
      "            label: object\n",
      "            linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      "            linewidth or lw: float\n",
      "            marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      "            markeredgecolor or mec: color\n",
      "            markeredgewidth or mew: float\n",
      "            markerfacecolor or mfc: color\n",
      "            markerfacecoloralt or mfcalt: color\n",
      "            markersize or ms: float\n",
      "            markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: unknown\n",
      "            pickradius: float\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            solid_capstyle: {'butt', 'round', 'projecting'}\n",
      "            solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            transform: `matplotlib.transforms.Transform`\n",
      "            url: str\n",
      "            visible: bool\n",
      "            xdata: 1D array\n",
      "            ydata: 1D array\n",
      "            zorder: float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        hlines : Add horizontal lines in data coordinates.\n",
      "        axhspan : Add a horizontal span (rectangle) across the axis.\n",
      "        axline : Add a line with an arbitrary slope.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        * draw a thick red hline at 'y' = 0 that spans the xrange::\n",
      "        \n",
      "            >>> axhline(linewidth=4, color='r')\n",
      "        \n",
      "        * draw a default hline at 'y' = 1 that spans the xrange::\n",
      "        \n",
      "            >>> axhline(y=1)\n",
      "        \n",
      "        * draw a default hline at 'y' = .5 that spans the middle half of\n",
      "          the xrange::\n",
      "        \n",
      "            >>> axhline(y=.5, xmin=0.25, xmax=0.75)\n",
      "    \n",
      "    axhspan(ymin, ymax, xmin=0, xmax=1, **kwargs)\n",
      "        Add a horizontal span (rectangle) across the axis.\n",
      "        \n",
      "        The rectangle spans from *ymin* to *ymax* vertically, and, by default,\n",
      "        the whole x-axis horizontally.  The x-span can be set using *xmin*\n",
      "        (default: 0) and *xmax* (default: 1) which are in axis units; e.g.\n",
      "        ``xmin = 0.5`` always refers to the middle of the x-axis regardless of\n",
      "        the limits set by `~.Axes.set_xlim`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        ymin : float\n",
      "            Lower y-coordinate of the span, in data units.\n",
      "        ymax : float\n",
      "            Upper y-coordinate of the span, in data units.\n",
      "        xmin : float, default: 0\n",
      "            Lower x-coordinate of the span, in x-axis (0-1) units.\n",
      "        xmax : float, default: 1\n",
      "            Upper x-coordinate of the span, in x-axis (0-1) units.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `~matplotlib.patches.Polygon`\n",
      "            Horizontal span (rectangle) from (xmin, ymin) to (xmax, ymax).\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs : `~matplotlib.patches.Polygon` properties\n",
      "        \n",
      "        Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa: unknown\n",
      "            capstyle: {'butt', 'round', 'projecting'}\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            color: color\n",
      "            contains: unknown\n",
      "            edgecolor or ec: color or None or 'auto'\n",
      "            facecolor or fc: color or None\n",
      "            figure: `.Figure`\n",
      "            fill: bool\n",
      "            gid: str\n",
      "            hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n",
      "            in_layout: bool\n",
      "            joinstyle: {'miter', 'round', 'bevel'}\n",
      "            label: object\n",
      "            linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      "            linewidth or lw: float or None\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: None or bool or callable\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            transform: `.Transform`\n",
      "            url: str\n",
      "            visible: bool\n",
      "            zorder: float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        axvspan : Add a vertical span across the axes.\n",
      "    \n",
      "    axis(*args, emit=True, **kwargs)\n",
      "        Convenience method to get or set some axis properties.\n",
      "        \n",
      "        Call signatures::\n",
      "        \n",
      "          xmin, xmax, ymin, ymax = axis()\n",
      "          xmin, xmax, ymin, ymax = axis([xmin, xmax, ymin, ymax])\n",
      "          xmin, xmax, ymin, ymax = axis(option)\n",
      "          xmin, xmax, ymin, ymax = axis(**kwargs)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        xmin, xmax, ymin, ymax : float, optional\n",
      "            The axis limits to be set.  This can also be achieved using ::\n",
      "        \n",
      "                ax.set(xlim=(xmin, xmax), ylim=(ymin, ymax))\n",
      "        \n",
      "        option : bool or str\n",
      "            If a bool, turns axis lines and labels on or off. If a string,\n",
      "            possible values are:\n",
      "        \n",
      "            ======== ==========================================================\n",
      "            Value    Description\n",
      "            ======== ==========================================================\n",
      "            'on'     Turn on axis lines and labels. Same as ``True``.\n",
      "            'off'    Turn off axis lines and labels. Same as ``False``.\n",
      "            'equal'  Set equal scaling (i.e., make circles circular) by\n",
      "                     changing axis limits. This is the same as\n",
      "                     ``ax.set_aspect('equal', adjustable='datalim')``.\n",
      "                     Explicit data limits may not be respected in this case.\n",
      "            'scaled' Set equal scaling (i.e., make circles circular) by\n",
      "                     changing dimensions of the plot box. This is the same as\n",
      "                     ``ax.set_aspect('equal', adjustable='box', anchor='C')``.\n",
      "                     Additionally, further autoscaling will be disabled.\n",
      "            'tight'  Set limits just large enough to show all data, then\n",
      "                     disable further autoscaling.\n",
      "            'auto'   Automatic scaling (fill plot box with data).\n",
      "            'image'  'scaled' with axis limits equal to data limits.\n",
      "            'square' Square plot; similar to 'scaled', but initially forcing\n",
      "                     ``xmax-xmin == ymax-ymin``.\n",
      "            ======== ==========================================================\n",
      "        \n",
      "        emit : bool, default: True\n",
      "            Whether observers are notified of the axis limit change.\n",
      "            This option is passed on to `~.Axes.set_xlim` and\n",
      "            `~.Axes.set_ylim`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xmin, xmax, ymin, ymax : float\n",
      "            The axis limits.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        matplotlib.axes.Axes.set_xlim\n",
      "        matplotlib.axes.Axes.set_ylim\n",
      "    \n",
      "    axline(xy1, xy2=None, *, slope=None, **kwargs)\n",
      "        Add an infinitely long straight line.\n",
      "        \n",
      "        The line can be defined either by two points *xy1* and *xy2*, or\n",
      "        by one point *xy1* and a *slope*.\n",
      "        \n",
      "        This draws a straight line \"on the screen\", regardless of the x and y\n",
      "        scales, and is thus also suitable for drawing exponential decays in\n",
      "        semilog plots, power laws in loglog plots, etc. However, *slope*\n",
      "        should only be used with linear scales; It has no clear meaning for\n",
      "        all other scales, and thus the behavior is undefined. Please specify\n",
      "        the line using the points *xy1*, *xy2* for non-linear scales.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        xy1, xy2 : (float, float)\n",
      "            Points for the line to pass through.\n",
      "            Either *xy2* or *slope* has to be given.\n",
      "        slope : float, optional\n",
      "            The slope of the line. Either *xy2* or *slope* has to be given.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `.Line2D`\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            Valid kwargs are `.Line2D` properties, with the exception of\n",
      "            'transform':\n",
      "        \n",
      "            Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa: bool\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            color or c: color\n",
      "            contains: unknown\n",
      "            dash_capstyle: {'butt', 'round', 'projecting'}\n",
      "            dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      "            data: (2, N) array or two 1D arrays\n",
      "            drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      "            figure: `.Figure`\n",
      "            fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      "            gid: str\n",
      "            in_layout: bool\n",
      "            label: object\n",
      "            linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      "            linewidth or lw: float\n",
      "            marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      "            markeredgecolor or mec: color\n",
      "            markeredgewidth or mew: float\n",
      "            markerfacecolor or mfc: color\n",
      "            markerfacecoloralt or mfcalt: color\n",
      "            markersize or ms: float\n",
      "            markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: unknown\n",
      "            pickradius: float\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            solid_capstyle: {'butt', 'round', 'projecting'}\n",
      "            solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            transform: `matplotlib.transforms.Transform`\n",
      "            url: str\n",
      "            visible: bool\n",
      "            xdata: 1D array\n",
      "            ydata: 1D array\n",
      "            zorder: float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        axhline : for horizontal lines\n",
      "        axvline : for vertical lines\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Draw a thick red line passing through (0, 0) and (1, 1)::\n",
      "        \n",
      "            >>> axline((0, 0), (1, 1), linewidth=4, color='r')\n",
      "    \n",
      "    axvline(x=0, ymin=0, ymax=1, **kwargs)\n",
      "        Add a vertical line across the axes.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : float, default: 0\n",
      "            x position in data coordinates of the vertical line.\n",
      "        \n",
      "        ymin : float, default: 0\n",
      "            Should be between 0 and 1, 0 being the bottom of the plot, 1 the\n",
      "            top of the plot.\n",
      "        \n",
      "        ymax : float, default: 1\n",
      "            Should be between 0 and 1, 0 being the bottom of the plot, 1 the\n",
      "            top of the plot.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `~matplotlib.lines.Line2D`\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            Valid keyword arguments are `.Line2D` properties, with the\n",
      "            exception of 'transform':\n",
      "        \n",
      "            Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa: bool\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            color or c: color\n",
      "            contains: unknown\n",
      "            dash_capstyle: {'butt', 'round', 'projecting'}\n",
      "            dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      "            data: (2, N) array or two 1D arrays\n",
      "            drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      "            figure: `.Figure`\n",
      "            fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      "            gid: str\n",
      "            in_layout: bool\n",
      "            label: object\n",
      "            linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      "            linewidth or lw: float\n",
      "            marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      "            markeredgecolor or mec: color\n",
      "            markeredgewidth or mew: float\n",
      "            markerfacecolor or mfc: color\n",
      "            markerfacecoloralt or mfcalt: color\n",
      "            markersize or ms: float\n",
      "            markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: unknown\n",
      "            pickradius: float\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            solid_capstyle: {'butt', 'round', 'projecting'}\n",
      "            solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            transform: `matplotlib.transforms.Transform`\n",
      "            url: str\n",
      "            visible: bool\n",
      "            xdata: 1D array\n",
      "            ydata: 1D array\n",
      "            zorder: float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        vlines : Add vertical lines in data coordinates.\n",
      "        axvspan : Add a vertical span (rectangle) across the axis.\n",
      "        axline : Add a line with an arbitrary slope.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        * draw a thick red vline at *x* = 0 that spans the yrange::\n",
      "        \n",
      "            >>> axvline(linewidth=4, color='r')\n",
      "        \n",
      "        * draw a default vline at *x* = 1 that spans the yrange::\n",
      "        \n",
      "            >>> axvline(x=1)\n",
      "        \n",
      "        * draw a default vline at *x* = .5 that spans the middle half of\n",
      "          the yrange::\n",
      "        \n",
      "            >>> axvline(x=.5, ymin=0.25, ymax=0.75)\n",
      "    \n",
      "    axvspan(xmin, xmax, ymin=0, ymax=1, **kwargs)\n",
      "        Add a vertical span (rectangle) across the axes.\n",
      "        \n",
      "        The rectangle spans from *xmin* to *xmax* horizontally, and, by\n",
      "        default, the whole y-axis vertically.  The y-span can be set using\n",
      "        *ymin* (default: 0) and *ymax* (default: 1) which are in axis units;\n",
      "        e.g. ``ymin = 0.5`` always refers to the middle of the y-axis\n",
      "        regardless of the limits set by `~.Axes.set_ylim`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        xmin : float\n",
      "            Lower x-coordinate of the span, in data units.\n",
      "        xmax : float\n",
      "            Upper x-coordinate of the span, in data units.\n",
      "        ymin : float, default: 0\n",
      "            Lower y-coordinate of the span, in y-axis units (0-1).\n",
      "        ymax : float, default: 1\n",
      "            Upper y-coordinate of the span, in y-axis units (0-1).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `~matplotlib.patches.Polygon`\n",
      "            Vertical span (rectangle) from (xmin, ymin) to (xmax, ymax).\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs : `~matplotlib.patches.Polygon` properties\n",
      "        \n",
      "        %(Polygon)s\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        axhspan : Add a horizontal span across the axes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Draw a vertical, green, translucent rectangle from x = 1.25 to\n",
      "        x = 1.55 that spans the yrange of the axes.\n",
      "        \n",
      "        >>> axvspan(1.25, 1.55, facecolor='g', alpha=0.5)\n",
      "    \n",
      "    bar(x, height, width=0.8, bottom=None, *, align='center', data=None, **kwargs)\n",
      "        Make a bar plot.\n",
      "        \n",
      "        The bars are positioned at *x* with the given *align*\\ment. Their\n",
      "        dimensions are given by *height* and *width*. The vertical baseline\n",
      "        is *bottom* (default 0).\n",
      "        \n",
      "        Many parameters can take either a single value applying to all bars\n",
      "        or a sequence of values, one for each bar.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : float or array-like\n",
      "            The x coordinates of the bars. See also *align* for the\n",
      "            alignment of the bars to the coordinates.\n",
      "        \n",
      "        height : float or array-like\n",
      "            The height(s) of the bars.\n",
      "        \n",
      "        width : float or array-like, default: 0.8\n",
      "            The width(s) of the bars.\n",
      "        \n",
      "        bottom : float or array-like, default: 0\n",
      "            The y coordinate(s) of the bars bases.\n",
      "        \n",
      "        align : {'center', 'edge'}, default: 'center'\n",
      "            Alignment of the bars to the *x* coordinates:\n",
      "        \n",
      "            - 'center': Center the base on the *x* positions.\n",
      "            - 'edge': Align the left edges of the bars with the *x* positions.\n",
      "        \n",
      "            To align the bars on the right edge pass a negative *width* and\n",
      "            ``align='edge'``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `.BarContainer`\n",
      "            Container with all the bars and optionally errorbars.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        color : color or list of color, optional\n",
      "            The colors of the bar faces.\n",
      "        \n",
      "        edgecolor : color or list of color, optional\n",
      "            The colors of the bar edges.\n",
      "        \n",
      "        linewidth : float or array-like, optional\n",
      "            Width of the bar edge(s). If 0, don't draw edges.\n",
      "        \n",
      "        tick_label : str or list of str, optional\n",
      "            The tick labels of the bars.\n",
      "            Default: None (Use default numeric labels.)\n",
      "        \n",
      "        xerr, yerr : float or array-like of shape(N,) or shape(2, N), optional\n",
      "            If not *None*, add horizontal / vertical errorbars to the bar tips.\n",
      "            The values are +/- sizes relative to the data:\n",
      "        \n",
      "            - scalar: symmetric +/- values for all bars\n",
      "            - shape(N,): symmetric +/- values for each bar\n",
      "            - shape(2, N): Separate - and + values for each bar. First row\n",
      "              contains the lower errors, the second row contains the upper\n",
      "              errors.\n",
      "            - *None*: No errorbar. (Default)\n",
      "        \n",
      "            See :doc:`/gallery/statistics/errorbar_features`\n",
      "            for an example on the usage of ``xerr`` and ``yerr``.\n",
      "        \n",
      "        ecolor : color or list of color, default: 'black'\n",
      "            The line color of the errorbars.\n",
      "        \n",
      "        capsize : float, default: :rc:`errorbar.capsize`\n",
      "           The length of the error bar caps in points.\n",
      "        \n",
      "        error_kw : dict, optional\n",
      "            Dictionary of kwargs to be passed to the `~.Axes.errorbar`\n",
      "            method. Values of *ecolor* or *capsize* defined here take\n",
      "            precedence over the independent kwargs.\n",
      "        \n",
      "        log : bool, default: False\n",
      "            If *True*, set the y-axis to be log scale.\n",
      "        \n",
      "        **kwargs : `.Rectangle` properties\n",
      "        \n",
      "        Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa: unknown\n",
      "            capstyle: {'butt', 'round', 'projecting'}\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            color: color\n",
      "            contains: unknown\n",
      "            edgecolor or ec: color or None or 'auto'\n",
      "            facecolor or fc: color or None\n",
      "            figure: `.Figure`\n",
      "            fill: bool\n",
      "            gid: str\n",
      "            hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n",
      "            in_layout: bool\n",
      "            joinstyle: {'miter', 'round', 'bevel'}\n",
      "            label: object\n",
      "            linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      "            linewidth or lw: float or None\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: None or bool or callable\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            transform: `.Transform`\n",
      "            url: str\n",
      "            visible: bool\n",
      "            zorder: float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        barh: Plot a horizontal bar plot.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Stacked bars can be achieved by passing individual *bottom* values per\n",
      "        bar. See :doc:`/gallery/lines_bars_and_markers/bar_stacked`.\n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            every other argument can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception).\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    barbs(*args, data=None, **kw)\n",
      "        Plot a 2D field of barbs.\n",
      "        \n",
      "        Call signature::\n",
      "        \n",
      "          barbs([X, Y], U, V, [C], **kw)\n",
      "        \n",
      "        Where *X*, *Y* define the barb locations, *U*, *V* define the barb\n",
      "        directions, and *C* optionally sets the color.\n",
      "        \n",
      "        All arguments may be 1D or 2D. *U*, *V*, *C* may be masked arrays, but masked\n",
      "        *X*, *Y* are not supported at present.\n",
      "        \n",
      "        Barbs are traditionally used in meteorology as a way to plot the speed\n",
      "        and direction of wind observations, but can technically be used to\n",
      "        plot any two dimensional vector quantity.  As opposed to arrows, which\n",
      "        give vector magnitude by the length of the arrow, the barbs give more\n",
      "        quantitative information about the vector magnitude by putting slanted\n",
      "        lines or a triangle for various increments in magnitude, as show\n",
      "        schematically below::\n",
      "        \n",
      "          :                   /\\    \\\n",
      "          :                  /  \\    \\\n",
      "          :                 /    \\    \\    \\\n",
      "          :                /      \\    \\    \\\n",
      "          :               ------------------------------\n",
      "        \n",
      "        The largest increment is given by a triangle (or \"flag\"). After those\n",
      "        come full lines (barbs). The smallest increment is a half line.  There\n",
      "        is only, of course, ever at most 1 half line.  If the magnitude is\n",
      "        small and only needs a single half-line and no full lines or\n",
      "        triangles, the half-line is offset from the end of the barb so that it\n",
      "        can be easily distinguished from barbs with a single full line.  The\n",
      "        magnitude for the barb shown above would nominally be 65, using the\n",
      "        standard increments of 50, 10, and 5.\n",
      "        \n",
      "        See also https://en.wikipedia.org/wiki/Wind_barb.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X, Y : 1D or 2D array-like, optional\n",
      "            The x and y coordinates of the barb locations. See *pivot* for how the\n",
      "            barbs are drawn to the x, y positions.\n",
      "        \n",
      "            If not given, they will be generated as a uniform integer meshgrid based\n",
      "            on the dimensions of *U* and *V*.\n",
      "        \n",
      "            If *X* and *Y* are 1D but *U*, *V* are 2D, *X*, *Y* are expanded to 2D\n",
      "            using ``X, Y = np.meshgrid(X, Y)``. In this case ``len(X)`` and ``len(Y)``\n",
      "            must match the column and row dimensions of *U* and *V*.\n",
      "        \n",
      "        U, V : 1D or 2D array-like\n",
      "            The x and y components of the barb shaft.\n",
      "        \n",
      "        C : 1D or 2D array-like, optional\n",
      "            Numeric data that defines the barb colors by colormapping via *norm* and\n",
      "            *cmap*.\n",
      "        \n",
      "            This does not support explicit colors. If you want to set colors directly,\n",
      "            use *barbcolor* instead.\n",
      "        \n",
      "        length : float, default: 7\n",
      "            Length of the barb in points; the other parts of the barb\n",
      "            are scaled against this.\n",
      "        \n",
      "        pivot : {'tip', 'middle'} or float, default: 'tip'\n",
      "            The part of the arrow that is anchored to the *X*, *Y* grid. The barb\n",
      "            rotates about this point. This can also be a number, which shifts the\n",
      "            start of the barb that many points away from grid point.\n",
      "        \n",
      "        barbcolor : color or color sequence\n",
      "            The color of all parts of the barb except for the flags.  This parameter\n",
      "            is analogous to the *edgecolor* parameter for polygons, which can be used\n",
      "            instead. However this parameter will override facecolor.\n",
      "        \n",
      "        flagcolor : color or color sequence\n",
      "            The color of any flags on the barb.  This parameter is analogous to the\n",
      "            *facecolor* parameter for polygons, which can be used instead. However,\n",
      "            this parameter will override facecolor.  If this is not set (and *C* has\n",
      "            not either) then *flagcolor* will be set to match *barbcolor* so that the\n",
      "            barb has a uniform color. If *C* has been set, *flagcolor* has no effect.\n",
      "        \n",
      "        sizes : dict, optional\n",
      "            A dictionary of coefficients specifying the ratio of a given\n",
      "            feature to the length of the barb. Only those values one wishes to\n",
      "            override need to be included.  These features include:\n",
      "        \n",
      "            - 'spacing' - space between features (flags, full/half barbs)\n",
      "            - 'height' - height (distance from shaft to top) of a flag or full barb\n",
      "            - 'width' - width of a flag, twice the width of a full barb\n",
      "            - 'emptybarb' - radius of the circle used for low magnitudes\n",
      "        \n",
      "        fill_empty : bool, default: False\n",
      "            Whether the empty barbs (circles) that are drawn should be filled with\n",
      "            the flag color.  If they are not filled, the center is transparent.\n",
      "        \n",
      "        rounding : bool, default: True\n",
      "            Whether the vector magnitude should be rounded when allocating barb\n",
      "            components.  If True, the magnitude is rounded to the nearest multiple\n",
      "            of the half-barb increment.  If False, the magnitude is simply truncated\n",
      "            to the next lowest multiple.\n",
      "        \n",
      "        barb_increments : dict, optional\n",
      "            A dictionary of increments specifying values to associate with\n",
      "            different parts of the barb. Only those values one wishes to\n",
      "            override need to be included.\n",
      "        \n",
      "            - 'half' - half barbs (Default is 5)\n",
      "            - 'full' - full barbs (Default is 10)\n",
      "            - 'flag' - flags (default is 50)\n",
      "        \n",
      "        flip_barb : bool or array-like of bool, default: False\n",
      "            Whether the lines and flags should point opposite to normal.\n",
      "            Normal behavior is for the barbs and lines to point right (comes from wind\n",
      "            barbs having these features point towards low pressure in the Northern\n",
      "            Hemisphere).\n",
      "        \n",
      "            A single value is applied to all barbs. Individual barbs can be flipped by\n",
      "            passing a bool array of the same size as *U* and *V*.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        barbs : `~matplotlib.quiver.Barbs`\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            The barbs can further be customized using `.PolyCollection` keyword\n",
      "            arguments:\n",
      "        \n",
      "            Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa or antialiaseds: bool or list of bools\n",
      "            array: ndarray\n",
      "            capstyle: {'butt', 'round', 'projecting'}\n",
      "            clim: (vmin: float, vmax: float)\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            cmap: `.Colormap` or str or None\n",
      "            color: color or list of rgba tuples\n",
      "            contains: unknown\n",
      "            edgecolor or ec or edgecolors: color or list of colors or 'face'\n",
      "            facecolor or facecolors or fc: color or list of colors\n",
      "            figure: `.Figure`\n",
      "            gid: str\n",
      "            hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n",
      "            in_layout: bool\n",
      "            joinstyle: {'miter', 'round', 'bevel'}\n",
      "            label: object\n",
      "            linestyle or dashes or linestyles or ls: str or tuple or list thereof\n",
      "            linewidth or linewidths or lw: float or list of floats\n",
      "            norm: `.Normalize` or None\n",
      "            offset_position: unknown\n",
      "            offsets: array-like (N, 2) or (2,)\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: None or bool or callable\n",
      "            pickradius: unknown\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            transform: `.Transform`\n",
      "            url: str\n",
      "            urls: list of str or None\n",
      "            visible: bool\n",
      "            zorder: float\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            every other argument can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception).\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    barh(y, width, height=0.8, left=None, *, align='center', **kwargs)\n",
      "        Make a horizontal bar plot.\n",
      "        \n",
      "        The bars are positioned at *y* with the given *align*\\ment. Their\n",
      "        dimensions are given by *width* and *height*. The horizontal baseline\n",
      "        is *left* (default 0).\n",
      "        \n",
      "        Many parameters can take either a single value applying to all bars\n",
      "        or a sequence of values, one for each bar.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : float or array-like\n",
      "            The y coordinates of the bars. See also *align* for the\n",
      "            alignment of the bars to the coordinates.\n",
      "        \n",
      "        width : float or array-like\n",
      "            The width(s) of the bars.\n",
      "        \n",
      "        height : float or array-like, default: 0.8\n",
      "            The heights of the bars.\n",
      "        \n",
      "        left : float or array-like, default: 0\n",
      "            The x coordinates of the left sides of the bars.\n",
      "        \n",
      "        align : {'center', 'edge'}, default: 'center'\n",
      "            Alignment of the base to the *y* coordinates*:\n",
      "        \n",
      "            - 'center': Center the bars on the *y* positions.\n",
      "            - 'edge': Align the bottom edges of the bars with the *y*\n",
      "              positions.\n",
      "        \n",
      "            To align the bars on the top edge pass a negative *height* and\n",
      "            ``align='edge'``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `.BarContainer`\n",
      "            Container with all the bars and optionally errorbars.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        color : color or list of color, optional\n",
      "            The colors of the bar faces.\n",
      "        \n",
      "        edgecolor : color or list of color, optional\n",
      "            The colors of the bar edges.\n",
      "        \n",
      "        linewidth : float or array-like, optional\n",
      "            Width of the bar edge(s). If 0, don't draw edges.\n",
      "        \n",
      "        tick_label : str or list of str, optional\n",
      "            The tick labels of the bars.\n",
      "            Default: None (Use default numeric labels.)\n",
      "        \n",
      "        xerr, yerr : float or array-like of shape(N,) or shape(2, N), optional\n",
      "            If not ``None``, add horizontal / vertical errorbars to the\n",
      "            bar tips. The values are +/- sizes relative to the data:\n",
      "        \n",
      "            - scalar: symmetric +/- values for all bars\n",
      "            - shape(N,): symmetric +/- values for each bar\n",
      "            - shape(2, N): Separate - and + values for each bar. First row\n",
      "              contains the lower errors, the second row contains the upper\n",
      "              errors.\n",
      "            - *None*: No errorbar. (default)\n",
      "        \n",
      "            See :doc:`/gallery/statistics/errorbar_features`\n",
      "            for an example on the usage of ``xerr`` and ``yerr``.\n",
      "        \n",
      "        ecolor : color or list of color, default: 'black'\n",
      "            The line color of the errorbars.\n",
      "        \n",
      "        capsize : float, default: :rc:`errorbar.capsize`\n",
      "           The length of the error bar caps in points.\n",
      "        \n",
      "        error_kw : dict, optional\n",
      "            Dictionary of kwargs to be passed to the `~.Axes.errorbar`\n",
      "            method. Values of *ecolor* or *capsize* defined here take\n",
      "            precedence over the independent kwargs.\n",
      "        \n",
      "        log : bool, default: False\n",
      "            If ``True``, set the x-axis to be log scale.\n",
      "        \n",
      "        **kwargs : `.Rectangle` properties\n",
      "        \n",
      "        Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa: unknown\n",
      "            capstyle: {'butt', 'round', 'projecting'}\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            color: color\n",
      "            contains: unknown\n",
      "            edgecolor or ec: color or None or 'auto'\n",
      "            facecolor or fc: color or None\n",
      "            figure: `.Figure`\n",
      "            fill: bool\n",
      "            gid: str\n",
      "            hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n",
      "            in_layout: bool\n",
      "            joinstyle: {'miter', 'round', 'bevel'}\n",
      "            label: object\n",
      "            linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      "            linewidth or lw: float or None\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: None or bool or callable\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            transform: `.Transform`\n",
      "            url: str\n",
      "            visible: bool\n",
      "            zorder: float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        bar: Plot a vertical bar plot.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Stacked bars can be achieved by passing individual *left* values per\n",
      "        bar. See\n",
      "        :doc:`/gallery/lines_bars_and_markers/horizontal_barchart_distribution`\n",
      "        .\n",
      "    \n",
      "    bone()\n",
      "        Set the colormap to \"bone\".\n",
      "        \n",
      "        This changes the default colormap as well as the colormap of the current\n",
      "        image if there is one. See ``help(colormaps)`` for more information.\n",
      "    \n",
      "    box(on=None)\n",
      "        Turn the axes box on or off on the current axes.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        on : bool or None\n",
      "            The new `~matplotlib.axes.Axes` box state. If ``None``, toggle\n",
      "            the state.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        :meth:`matplotlib.axes.Axes.set_frame_on`\n",
      "        :meth:`matplotlib.axes.Axes.get_frame_on`\n",
      "    \n",
      "    boxplot(x, notch=None, sym=None, vert=None, whis=None, positions=None, widths=None, patch_artist=None, bootstrap=None, usermedians=None, conf_intervals=None, meanline=None, showmeans=None, showcaps=None, showbox=None, showfliers=None, boxprops=None, labels=None, flierprops=None, medianprops=None, meanprops=None, capprops=None, whiskerprops=None, manage_ticks=True, autorange=False, zorder=None, *, data=None)\n",
      "        Make a box and whisker plot.\n",
      "        \n",
      "        Make a box and whisker plot for each column of *x* or each\n",
      "        vector in sequence *x*.  The box extends from the lower to\n",
      "        upper quartile values of the data, with a line at the median.\n",
      "        The whiskers extend from the box to show the range of the\n",
      "        data.  Flier points are those past the end of the whiskers.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : Array or a sequence of vectors.\n",
      "            The input data.\n",
      "        \n",
      "        notch : bool, default: False\n",
      "            Whether to draw a noteched box plot (`True`), or a rectangular box\n",
      "            plot (`False`).  The notches represent the confidence interval (CI)\n",
      "            around the median.  The documentation for *bootstrap* describes how\n",
      "            the locations of the notches are computed.\n",
      "        \n",
      "            .. note::\n",
      "        \n",
      "                In cases where the values of the CI are less than the\n",
      "                lower quartile or greater than the upper quartile, the\n",
      "                notches will extend beyond the box, giving it a\n",
      "                distinctive \"flipped\" appearance. This is expected\n",
      "                behavior and consistent with other statistical\n",
      "                visualization packages.\n",
      "        \n",
      "        sym : str, optional\n",
      "            The default symbol for flier points.  An empty string ('') hides\n",
      "            the fliers.  If `None`, then the fliers default to 'b+'.  More\n",
      "            control is provided by the *flierprops* parameter.\n",
      "        \n",
      "        vert : bool, default: True\n",
      "            If `True`, draws vertical boxes.\n",
      "            If `False`, draw horizontal boxes.\n",
      "        \n",
      "        whis : float or (float, float), default: 1.5\n",
      "            The position of the whiskers.\n",
      "        \n",
      "            If a float, the lower whisker is at the lowest datum above\n",
      "            ``Q1 - whis*(Q3-Q1)``, and the upper whisker at the highest datum\n",
      "            below ``Q3 + whis*(Q3-Q1)``, where Q1 and Q3 are the first and\n",
      "            third quartiles.  The default value of ``whis = 1.5`` corresponds\n",
      "            to Tukey's original definition of boxplots.\n",
      "        \n",
      "            If a pair of floats, they indicate the percentiles at which to\n",
      "            draw the whiskers (e.g., (5, 95)).  In particular, setting this to\n",
      "            (0, 100) results in whiskers covering the whole range of the data.\n",
      "            \"range\" is a deprecated synonym for (0, 100).\n",
      "        \n",
      "            In the edge case where ``Q1 == Q3``, *whis* is automatically set\n",
      "            to (0, 100) (cover the whole range of the data) if *autorange* is\n",
      "            True.\n",
      "        \n",
      "            Beyond the whiskers, data are considered outliers and are plotted\n",
      "            as individual points.\n",
      "        \n",
      "        bootstrap : int, optional\n",
      "            Specifies whether to bootstrap the confidence intervals\n",
      "            around the median for notched boxplots. If *bootstrap* is\n",
      "            None, no bootstrapping is performed, and notches are\n",
      "            calculated using a Gaussian-based asymptotic approximation\n",
      "            (see McGill, R., Tukey, J.W., and Larsen, W.A., 1978, and\n",
      "            Kendall and Stuart, 1967). Otherwise, bootstrap specifies\n",
      "            the number of times to bootstrap the median to determine its\n",
      "            95% confidence intervals. Values between 1000 and 10000 are\n",
      "            recommended.\n",
      "        \n",
      "        usermedians : array-like, optional\n",
      "            A 1D array-like of length ``len(x)``.  Each entry that is not\n",
      "            `None` forces the value of the median for the corresponding\n",
      "            dataset.  For entries that are `None`, the medians are computed\n",
      "            by Matplotlib as normal.\n",
      "        \n",
      "        conf_intervals : array-like, optional\n",
      "            A 2D array-like of shape ``(len(x), 2)``.  Each entry that is not\n",
      "            None forces the location of the corresponding notch (which is\n",
      "            only drawn if *notch* is `True`).  For entries that are `None`,\n",
      "            the notches are computed by the method specified by the other\n",
      "            parameters (e.g., *bootstrap*).\n",
      "        \n",
      "        positions : array-like, optional\n",
      "            Sets the positions of the boxes. The ticks and limits are\n",
      "            automatically set to match the positions. Defaults to\n",
      "            ``range(1, N+1)`` where N is the number of boxes to be drawn.\n",
      "        \n",
      "        widths : float or array-like\n",
      "            Sets the width of each box either with a scalar or a\n",
      "            sequence. The default is 0.5, or ``0.15*(distance between\n",
      "            extreme positions)``, if that is smaller.\n",
      "        \n",
      "        patch_artist : bool, default: False\n",
      "            If `False` produces boxes with the Line2D artist. Otherwise,\n",
      "            boxes and drawn with Patch artists.\n",
      "        \n",
      "        labels : sequence, optional\n",
      "            Labels for each dataset (one per dataset).\n",
      "        \n",
      "        manage_ticks : bool, default: True\n",
      "            If True, the tick locations and labels will be adjusted to match\n",
      "            the boxplot positions.\n",
      "        \n",
      "        autorange : bool, default: False\n",
      "            When `True` and the data are distributed such that the 25th and\n",
      "            75th percentiles are equal, *whis* is set to (0, 100) such\n",
      "            that the whisker ends are at the minimum and maximum of the data.\n",
      "        \n",
      "        meanline : bool, default: False\n",
      "            If `True` (and *showmeans* is `True`), will try to render the\n",
      "            mean as a line spanning the full width of the box according to\n",
      "            *meanprops* (see below).  Not recommended if *shownotches* is also\n",
      "            True.  Otherwise, means will be shown as points.\n",
      "        \n",
      "        zorder : float, default: ``Line2D.zorder = 2``\n",
      "            Sets the zorder of the boxplot.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dict\n",
      "          A dictionary mapping each component of the boxplot to a list\n",
      "          of the `.Line2D` instances created. That dictionary has the\n",
      "          following keys (assuming vertical boxplots):\n",
      "        \n",
      "          - ``boxes``: the main body of the boxplot showing the\n",
      "            quartiles and the median's confidence intervals if\n",
      "            enabled.\n",
      "        \n",
      "          - ``medians``: horizontal lines at the median of each box.\n",
      "        \n",
      "          - ``whiskers``: the vertical lines extending to the most\n",
      "            extreme, non-outlier data points.\n",
      "        \n",
      "          - ``caps``: the horizontal lines at the ends of the\n",
      "            whiskers.\n",
      "        \n",
      "          - ``fliers``: points representing data that extend beyond\n",
      "            the whiskers (fliers).\n",
      "        \n",
      "          - ``means``: points or lines representing the means.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        showcaps : bool, default: True\n",
      "            Show the caps on the ends of whiskers.\n",
      "        showbox : bool, default: True\n",
      "            Show the central box.\n",
      "        showfliers : bool, default: True\n",
      "            Show the outliers beyond the caps.\n",
      "        showmeans : bool, default: False\n",
      "            Show the arithmetic means.\n",
      "        capprops : dict, default: None\n",
      "            The style of the caps.\n",
      "        boxprops : dict, default: None\n",
      "            The style of the box.\n",
      "        whiskerprops : dict, default: None\n",
      "            The style of the whiskers.\n",
      "        flierprops : dict, default: None\n",
      "            The style of the fliers.\n",
      "        medianprops : dict, default: None\n",
      "            The style of the median.\n",
      "        meanprops : dict, default: None\n",
      "            The style of the mean.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            every other argument can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception).\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    broken_barh(xranges, yrange, *, data=None, **kwargs)\n",
      "        Plot a horizontal sequence of rectangles.\n",
      "        \n",
      "        A rectangle is drawn for each element of *xranges*. All rectangles\n",
      "        have the same vertical position and size defined by *yrange*.\n",
      "        \n",
      "        This is a convenience function for instantiating a\n",
      "        `.BrokenBarHCollection`, adding it to the axes and autoscaling the\n",
      "        view.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        xranges : sequence of tuples (*xmin*, *xwidth*)\n",
      "            The x-positions and extends of the rectangles. For each tuple\n",
      "            (*xmin*, *xwidth*) a rectangle is drawn from *xmin* to *xmin* +\n",
      "            *xwidth*.\n",
      "        yrange : (*ymin*, *yheight*)\n",
      "            The y-position and extend for all the rectangles.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `~.collections.BrokenBarHCollection`\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs : `.BrokenBarHCollection` properties\n",
      "        \n",
      "            Each *kwarg* can be either a single argument applying to all\n",
      "            rectangles, e.g.::\n",
      "        \n",
      "                facecolors='black'\n",
      "        \n",
      "            or a sequence of arguments over which is cycled, e.g.::\n",
      "        \n",
      "                facecolors=('black', 'blue')\n",
      "        \n",
      "            would create interleaving black and blue rectangles.\n",
      "        \n",
      "            Supported keywords:\n",
      "        \n",
      "            Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa or antialiaseds: bool or list of bools\n",
      "            array: ndarray\n",
      "            capstyle: {'butt', 'round', 'projecting'}\n",
      "            clim: (vmin: float, vmax: float)\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            cmap: `.Colormap` or str or None\n",
      "            color: color or list of rgba tuples\n",
      "            contains: unknown\n",
      "            edgecolor or ec or edgecolors: color or list of colors or 'face'\n",
      "            facecolor or facecolors or fc: color or list of colors\n",
      "            figure: `.Figure`\n",
      "            gid: str\n",
      "            hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n",
      "            in_layout: bool\n",
      "            joinstyle: {'miter', 'round', 'bevel'}\n",
      "            label: object\n",
      "            linestyle or dashes or linestyles or ls: str or tuple or list thereof\n",
      "            linewidth or linewidths or lw: float or list of floats\n",
      "            norm: `.Normalize` or None\n",
      "            offset_position: unknown\n",
      "            offsets: array-like (N, 2) or (2,)\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: None or bool or callable\n",
      "            pickradius: unknown\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            transform: `.Transform`\n",
      "            url: str\n",
      "            urls: list of str or None\n",
      "            visible: bool\n",
      "            zorder: float\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            every other argument can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception).\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    cla()\n",
      "        Clear the current axes.\n",
      "    \n",
      "    clabel(CS, levels=None, **kwargs)\n",
      "        Label a contour plot.\n",
      "        \n",
      "        Adds labels to line contours in given `.ContourSet`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        CS : `~.ContourSet` instance\n",
      "            Line contours to label.\n",
      "        \n",
      "        levels : array-like, optional\n",
      "            A list of level values, that should be labeled. The list must be\n",
      "            a subset of ``CS.levels``. If not given, all levels are labeled.\n",
      "        \n",
      "        **kwargs\n",
      "            All other parameters are documented in `~.ContourLabeler.clabel`.\n",
      "    \n",
      "    clf()\n",
      "        Clear the current figure.\n",
      "    \n",
      "    clim(vmin=None, vmax=None)\n",
      "        Set the color limits of the current image.\n",
      "        \n",
      "        If either *vmin* or *vmax* is None, the image min/max respectively\n",
      "        will be used for color scaling.\n",
      "        \n",
      "        If you want to set the clim of multiple images, use\n",
      "        `~.ScalarMappable.set_clim` on every image, for example::\n",
      "        \n",
      "          for im in gca().get_images():\n",
      "              im.set_clim(0, 0.5)\n",
      "    \n",
      "    close(fig=None)\n",
      "        Close a figure window.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fig : None or int or str or `.Figure`\n",
      "            The figure to close. There are a number of ways to specify this:\n",
      "        \n",
      "            - *None*: the current figure\n",
      "            - `.Figure`: the given `.Figure` instance\n",
      "            - ``int``: a figure number\n",
      "            - ``str``: a figure name\n",
      "            - 'all': all figures\n",
      "    \n",
      "    cohere(x, y, NFFT=256, Fs=2, Fc=0, detrend=<function detrend_none at 0x113a44a60>, window=<function window_hanning at 0x113a44700>, noverlap=0, pad_to=None, sides='default', scale_by_freq=None, *, data=None, **kwargs)\n",
      "        Plot the coherence between *x* and *y*.\n",
      "        \n",
      "        Plot the coherence between *x* and *y*.  Coherence is the\n",
      "        normalized cross spectral density:\n",
      "        \n",
      "        .. math::\n",
      "        \n",
      "          C_{xy} = \\frac{|P_{xy}|^2}{P_{xx}P_{yy}}\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        Fs : float, default: 2\n",
      "            The sampling frequency (samples per time unit).  It is used to calculate\n",
      "            the Fourier frequencies, *freqs*, in cycles per time unit.\n",
      "        \n",
      "        window : callable or ndarray, default: `.window_hanning`\n",
      "            A function or a vector of length *NFFT*.  To create window vectors see\n",
      "            `.window_hanning`, `.window_none`, `numpy.blackman`, `numpy.hamming`,\n",
      "            `numpy.bartlett`, `scipy.signal`, `scipy.signal.get_window`, etc.  If a\n",
      "            function is passed as the argument, it must take a data segment as an\n",
      "            argument and return the windowed version of the segment.\n",
      "        \n",
      "        sides : {'default', 'onesided', 'twosided'}, optional\n",
      "            Which sides of the spectrum to return. 'default' is one-sided for real\n",
      "            data and two-sided for complex data. 'onesided' forces the return of a\n",
      "            one-sided spectrum, while 'twosided' forces two-sided.\n",
      "        \n",
      "        pad_to : int, optional\n",
      "            The number of points to which the data segment is padded when performing\n",
      "            the FFT.  This can be different from *NFFT*, which specifies the number\n",
      "            of data points used.  While not increasing the actual resolution of the\n",
      "            spectrum (the minimum distance between resolvable peaks), this can give\n",
      "            more points in the plot, allowing for more detail. This corresponds to\n",
      "            the *n* parameter in the call to fft(). The default is None, which sets\n",
      "            *pad_to* equal to *NFFT*\n",
      "        \n",
      "        NFFT : int, default: 256\n",
      "            The number of data points used in each block for the FFT.  A power 2 is\n",
      "            most efficient.  This should *NOT* be used to get zero padding, or the\n",
      "            scaling of the result will be incorrect; use *pad_to* for this instead.\n",
      "        \n",
      "        detrend : {'none', 'mean', 'linear'} or callable, default 'none'\n",
      "            The function applied to each segment before fft-ing, designed to remove\n",
      "            the mean or linear trend.  Unlike in MATLAB, where the *detrend* parameter\n",
      "            is a vector, in Matplotlib is it a function.  The :mod:`~matplotlib.mlab`\n",
      "            module defines `.detrend_none`, `.detrend_mean`, and `.detrend_linear`,\n",
      "            but you can use a custom function as well.  You can also use a string to\n",
      "            choose one of the functions: 'none' calls `.detrend_none`. 'mean' calls\n",
      "            `.detrend_mean`. 'linear' calls `.detrend_linear`.\n",
      "        \n",
      "        scale_by_freq : bool, default: True\n",
      "            Whether the resulting density values should be scaled by the scaling\n",
      "            frequency, which gives density in units of Hz^-1.  This allows for\n",
      "            integration over the returned frequency values.  The default is True for\n",
      "            MATLAB compatibility.\n",
      "        \n",
      "        noverlap : int, default: 0 (no overlap)\n",
      "            The number of points of overlap between blocks.\n",
      "        \n",
      "        Fc : int, default: 0\n",
      "            The center frequency of *x*, which offsets the x extents of the\n",
      "            plot to reflect the frequency range used when a signal is acquired\n",
      "            and then filtered and downsampled to baseband.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Cxy : 1-D array\n",
      "            The coherence vector.\n",
      "        \n",
      "        freqs : 1-D array\n",
      "            The frequencies for the elements in *Cxy*.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            Keyword arguments control the `.Line2D` properties:\n",
      "        \n",
      "            Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa: bool\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            color or c: color\n",
      "            contains: unknown\n",
      "            dash_capstyle: {'butt', 'round', 'projecting'}\n",
      "            dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      "            data: (2, N) array or two 1D arrays\n",
      "            drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      "            figure: `.Figure`\n",
      "            fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      "            gid: str\n",
      "            in_layout: bool\n",
      "            label: object\n",
      "            linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      "            linewidth or lw: float\n",
      "            marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      "            markeredgecolor or mec: color\n",
      "            markeredgewidth or mew: float\n",
      "            markerfacecolor or mfc: color\n",
      "            markerfacecoloralt or mfcalt: color\n",
      "            markersize or ms: float\n",
      "            markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: unknown\n",
      "            pickradius: float\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            solid_capstyle: {'butt', 'round', 'projecting'}\n",
      "            solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            transform: `matplotlib.transforms.Transform`\n",
      "            url: str\n",
      "            visible: bool\n",
      "            xdata: 1D array\n",
      "            ydata: 1D array\n",
      "            zorder: float\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Bendat & Piersol -- Random Data: Analysis and Measurement Procedures,\n",
      "        John Wiley & Sons (1986)\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            the following arguments can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception):\n",
      "            *x*, *y*.\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    colorbar(mappable=None, cax=None, ax=None, **kw)\n",
      "        Add a colorbar to a plot.\n",
      "        \n",
      "        Function signatures for the :mod:`~matplotlib.pyplot` interface; all\n",
      "        but the first are also method signatures for the `~.Figure.colorbar` method::\n",
      "        \n",
      "          colorbar(**kwargs)\n",
      "          colorbar(mappable, **kwargs)\n",
      "          colorbar(mappable, cax=cax, **kwargs)\n",
      "          colorbar(mappable, ax=ax, **kwargs)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        mappable\n",
      "            The `matplotlib.cm.ScalarMappable` (i.e., `~matplotlib.image.AxesImage`,\n",
      "            `~matplotlib.contour.ContourSet`, etc.) described by this colorbar.\n",
      "            This argument is mandatory for the `.Figure.colorbar` method but optional\n",
      "            for the `.pyplot.colorbar` function, which sets the default to the current\n",
      "            image.\n",
      "        \n",
      "            Note that one can create a `.ScalarMappable` \"on-the-fly\" to generate\n",
      "            colorbars not attached to a previously drawn artist, e.g. ::\n",
      "        \n",
      "                fig.colorbar(cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax)\n",
      "        \n",
      "        cax : `~matplotlib.axes.Axes`, optional\n",
      "            Axes into which the colorbar will be drawn.\n",
      "        \n",
      "        ax : `~matplotlib.axes.Axes`, list of Axes, optional\n",
      "            Parent axes from which space for a new colorbar axes will be stolen.\n",
      "            If a list of axes is given they will all be resized to make room for the\n",
      "            colorbar axes.\n",
      "        \n",
      "        use_gridspec : bool, optional\n",
      "            If *cax* is ``None``, a new *cax* is created as an instance of Axes.  If\n",
      "            *ax* is an instance of Subplot and *use_gridspec* is ``True``, *cax* is\n",
      "            created as an instance of Subplot using the :mod:`~.gridspec` module.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        colorbar : `~matplotlib.colorbar.Colorbar`\n",
      "            See also its base class, `~matplotlib.colorbar.ColorbarBase`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Additional keyword arguments are of two kinds:\n",
      "        \n",
      "          axes properties:\n",
      "        \n",
      "            fraction : float, default: 0.15\n",
      "                Fraction of original axes to use for colorbar.\n",
      "            shrink : float, default: 1.0\n",
      "                Fraction by which to multiply the size of the colorbar.\n",
      "            aspect : float, default: 20\n",
      "                Ratio of long to short dimensions.\n",
      "        \n",
      "            pad : float, default: 0.05 if vertical, 0.15 if horizontal\n",
      "                Fraction of original axes between colorbar and new image axes.\n",
      "            anchor : (float, float), optional\n",
      "                The anchor point of the colorbar axes.\n",
      "                Defaults to (0.0, 0.5) if vertical; (0.5, 1.0) if horizontal.\n",
      "            panchor : (float, float), or *False*, optional\n",
      "                The anchor point of the colorbar parent axes. If *False*, the parent\n",
      "                axes' anchor will be unchanged.\n",
      "                Defaults to (1.0, 0.5) if vertical; (0.5, 0.0) if horizontal.\n",
      "        \n",
      "          colorbar properties:\n",
      "        \n",
      "        \n",
      "            ============  ====================================================\n",
      "            Property      Description\n",
      "            ============  ====================================================\n",
      "            *extend*      {'neither', 'both', 'min', 'max'}\n",
      "                          If not 'neither', make pointed end(s) for out-of-\n",
      "                          range values.  These are set for a given colormap\n",
      "                          using the colormap set_under and set_over methods.\n",
      "            *extendfrac*  {*None*, 'auto', length, lengths}\n",
      "                          If set to *None*, both the minimum and maximum\n",
      "                          triangular colorbar extensions with have a length of\n",
      "                          5% of the interior colorbar length (this is the\n",
      "                          default setting). If set to 'auto', makes the\n",
      "                          triangular colorbar extensions the same lengths as\n",
      "                          the interior boxes (when *spacing* is set to\n",
      "                          'uniform') or the same lengths as the respective\n",
      "                          adjacent interior boxes (when *spacing* is set to\n",
      "                          'proportional'). If a scalar, indicates the length\n",
      "                          of both the minimum and maximum triangular colorbar\n",
      "                          extensions as a fraction of the interior colorbar\n",
      "                          length. A two-element sequence of fractions may also\n",
      "                          be given, indicating the lengths of the minimum and\n",
      "                          maximum colorbar extensions respectively as a\n",
      "                          fraction of the interior colorbar length.\n",
      "            *extendrect*  bool\n",
      "                          If *False* the minimum and maximum colorbar extensions\n",
      "                          will be triangular (the default). If *True* the\n",
      "                          extensions will be rectangular.\n",
      "            *spacing*     {'uniform', 'proportional'}\n",
      "                          Uniform spacing gives each discrete color the same\n",
      "                          space; proportional makes the space proportional to\n",
      "                          the data interval.\n",
      "            *ticks*       *None* or list of ticks or Locator\n",
      "                          If None, ticks are determined automatically from the\n",
      "                          input.\n",
      "            *format*      None or str or Formatter\n",
      "                          If None, `~.ticker.ScalarFormatter` is used.\n",
      "                          If a format string is given, e.g., '%.3f', that is used.\n",
      "                          An alternative `~.ticker.Formatter` may be given instead.\n",
      "            *drawedges*   bool\n",
      "                          Whether to draw lines at color boundaries.\n",
      "            *label*       str\n",
      "                          The label on the colorbar's long axis.\n",
      "            ============  ====================================================\n",
      "        \n",
      "            The following will probably be useful only in the context of\n",
      "            indexed colors (that is, when the mappable has norm=NoNorm()),\n",
      "            or other unusual circumstances.\n",
      "        \n",
      "            ============   ===================================================\n",
      "            Property       Description\n",
      "            ============   ===================================================\n",
      "            *boundaries*   None or a sequence\n",
      "            *values*       None or a sequence which must be of length 1 less\n",
      "                           than the sequence of *boundaries*. For each region\n",
      "                           delimited by adjacent entries in *boundaries*, the\n",
      "                           color mapped to the corresponding value in values\n",
      "                           will be used.\n",
      "            ============   ===================================================\n",
      "        \n",
      "        \n",
      "        \n",
      "        If *mappable* is a `~.contour.ContourSet`, its *extend* kwarg is included\n",
      "        automatically.\n",
      "        \n",
      "        The *shrink* kwarg provides a simple way to scale the colorbar with respect\n",
      "        to the axes. Note that if *cax* is specified, it determines the size of the\n",
      "        colorbar and *shrink* and *aspect* kwargs are ignored.\n",
      "        \n",
      "        For more precise control, you can manually specify the positions of\n",
      "        the axes objects in which the mappable and the colorbar are drawn.  In\n",
      "        this case, do not use any of the axes properties kwargs.\n",
      "        \n",
      "        It is known that some vector graphics viewers (svg and pdf) renders white gaps\n",
      "        between segments of the colorbar.  This is due to bugs in the viewers, not\n",
      "        Matplotlib.  As a workaround, the colorbar can be rendered with overlapping\n",
      "        segments::\n",
      "        \n",
      "            cbar = colorbar()\n",
      "            cbar.solids.set_edgecolor(\"face\")\n",
      "            draw()\n",
      "        \n",
      "        However this has negative consequences in other circumstances, e.g. with\n",
      "        semi-transparent images (alpha < 1) and colorbar extensions; therefore, this\n",
      "        workaround is not used by default (see issue #1188).\n",
      "    \n",
      "    colormaps()\n",
      "        Matplotlib provides a number of colormaps, and others can be added using\n",
      "        :func:`~matplotlib.cm.register_cmap`.  This function documents the built-in\n",
      "        colormaps, and will also return a list of all registered colormaps if\n",
      "        called.\n",
      "        \n",
      "        You can set the colormap for an image, pcolor, scatter, etc,\n",
      "        using a keyword argument::\n",
      "        \n",
      "          imshow(X, cmap=cm.hot)\n",
      "        \n",
      "        or using the :func:`set_cmap` function::\n",
      "        \n",
      "          imshow(X)\n",
      "          pyplot.set_cmap('hot')\n",
      "          pyplot.set_cmap('jet')\n",
      "        \n",
      "        In interactive mode, :func:`set_cmap` will update the colormap post-hoc,\n",
      "        allowing you to see which one works best for your data.\n",
      "        \n",
      "        All built-in colormaps can be reversed by appending ``_r``: For instance,\n",
      "        ``gray_r`` is the reverse of ``gray``.\n",
      "        \n",
      "        There are several common color schemes used in visualization:\n",
      "        \n",
      "        Sequential schemes\n",
      "          for unipolar data that progresses from low to high\n",
      "        Diverging schemes\n",
      "          for bipolar data that emphasizes positive or negative deviations from a\n",
      "          central value\n",
      "        Cyclic schemes\n",
      "          for plotting values that wrap around at the endpoints, such as phase\n",
      "          angle, wind direction, or time of day\n",
      "        Qualitative schemes\n",
      "          for nominal data that has no inherent ordering, where color is used\n",
      "          only to distinguish categories\n",
      "        \n",
      "        Matplotlib ships with 4 perceptually uniform color maps which are\n",
      "        the recommended color maps for sequential data:\n",
      "        \n",
      "          =========   ===================================================\n",
      "          Colormap    Description\n",
      "          =========   ===================================================\n",
      "          inferno     perceptually uniform shades of black-red-yellow\n",
      "          magma       perceptually uniform shades of black-red-white\n",
      "          plasma      perceptually uniform shades of blue-red-yellow\n",
      "          viridis     perceptually uniform shades of blue-green-yellow\n",
      "          =========   ===================================================\n",
      "        \n",
      "        The following colormaps are based on the `ColorBrewer\n",
      "        <https://colorbrewer2.org>`_ color specifications and designs developed by\n",
      "        Cynthia Brewer:\n",
      "        \n",
      "        ColorBrewer Diverging (luminance is highest at the midpoint, and\n",
      "        decreases towards differently-colored endpoints):\n",
      "        \n",
      "          ========  ===================================\n",
      "          Colormap  Description\n",
      "          ========  ===================================\n",
      "          BrBG      brown, white, blue-green\n",
      "          PiYG      pink, white, yellow-green\n",
      "          PRGn      purple, white, green\n",
      "          PuOr      orange, white, purple\n",
      "          RdBu      red, white, blue\n",
      "          RdGy      red, white, gray\n",
      "          RdYlBu    red, yellow, blue\n",
      "          RdYlGn    red, yellow, green\n",
      "          Spectral  red, orange, yellow, green, blue\n",
      "          ========  ===================================\n",
      "        \n",
      "        ColorBrewer Sequential (luminance decreases monotonically):\n",
      "        \n",
      "          ========  ====================================\n",
      "          Colormap  Description\n",
      "          ========  ====================================\n",
      "          Blues     white to dark blue\n",
      "          BuGn      white, light blue, dark green\n",
      "          BuPu      white, light blue, dark purple\n",
      "          GnBu      white, light green, dark blue\n",
      "          Greens    white to dark green\n",
      "          Greys     white to black (not linear)\n",
      "          Oranges   white, orange, dark brown\n",
      "          OrRd      white, orange, dark red\n",
      "          PuBu      white, light purple, dark blue\n",
      "          PuBuGn    white, light purple, dark green\n",
      "          PuRd      white, light purple, dark red\n",
      "          Purples   white to dark purple\n",
      "          RdPu      white, pink, dark purple\n",
      "          Reds      white to dark red\n",
      "          YlGn      light yellow, dark green\n",
      "          YlGnBu    light yellow, light green, dark blue\n",
      "          YlOrBr    light yellow, orange, dark brown\n",
      "          YlOrRd    light yellow, orange, dark red\n",
      "          ========  ====================================\n",
      "        \n",
      "        ColorBrewer Qualitative:\n",
      "        \n",
      "        (For plotting nominal data, `.ListedColormap` is used,\n",
      "        not `.LinearSegmentedColormap`.  Different sets of colors are\n",
      "        recommended for different numbers of categories.)\n",
      "        \n",
      "        * Accent\n",
      "        * Dark2\n",
      "        * Paired\n",
      "        * Pastel1\n",
      "        * Pastel2\n",
      "        * Set1\n",
      "        * Set2\n",
      "        * Set3\n",
      "        \n",
      "        A set of colormaps derived from those of the same name provided\n",
      "        with Matlab are also included:\n",
      "        \n",
      "          =========   =======================================================\n",
      "          Colormap    Description\n",
      "          =========   =======================================================\n",
      "          autumn      sequential linearly-increasing shades of red-orange-yellow\n",
      "          bone        sequential increasing black-white color map with\n",
      "                      a tinge of blue, to emulate X-ray film\n",
      "          cool        linearly-decreasing shades of cyan-magenta\n",
      "          copper      sequential increasing shades of black-copper\n",
      "          flag        repetitive red-white-blue-black pattern (not cyclic at\n",
      "                      endpoints)\n",
      "          gray        sequential linearly-increasing black-to-white\n",
      "                      grayscale\n",
      "          hot         sequential black-red-yellow-white, to emulate blackbody\n",
      "                      radiation from an object at increasing temperatures\n",
      "          jet         a spectral map with dark endpoints, blue-cyan-yellow-red;\n",
      "                      based on a fluid-jet simulation by NCSA [#]_\n",
      "          pink        sequential increasing pastel black-pink-white, meant\n",
      "                      for sepia tone colorization of photographs\n",
      "          prism       repetitive red-yellow-green-blue-purple-...-green pattern\n",
      "                      (not cyclic at endpoints)\n",
      "          spring      linearly-increasing shades of magenta-yellow\n",
      "          summer      sequential linearly-increasing shades of green-yellow\n",
      "          winter      linearly-increasing shades of blue-green\n",
      "          =========   =======================================================\n",
      "        \n",
      "        A set of palettes from the `Yorick scientific visualisation\n",
      "        package <https://dhmunro.github.io/yorick-doc/>`_, an evolution of\n",
      "        the GIST package, both by David H. Munro are included:\n",
      "        \n",
      "          ============  =======================================================\n",
      "          Colormap      Description\n",
      "          ============  =======================================================\n",
      "          gist_earth    mapmaker's colors from dark blue deep ocean to green\n",
      "                        lowlands to brown highlands to white mountains\n",
      "          gist_heat     sequential increasing black-red-orange-white, to emulate\n",
      "                        blackbody radiation from an iron bar as it grows hotter\n",
      "          gist_ncar     pseudo-spectral black-blue-green-yellow-red-purple-white\n",
      "                        colormap from National Center for Atmospheric\n",
      "                        Research [#]_\n",
      "          gist_rainbow  runs through the colors in spectral order from red to\n",
      "                        violet at full saturation (like *hsv* but not cyclic)\n",
      "          gist_stern    \"Stern special\" color table from Interactive Data\n",
      "                        Language software\n",
      "          ============  =======================================================\n",
      "        \n",
      "        A set of cyclic color maps:\n",
      "        \n",
      "          ================  =================================================\n",
      "          Colormap          Description\n",
      "          ================  =================================================\n",
      "          hsv               red-yellow-green-cyan-blue-magenta-red, formed by\n",
      "                            changing the hue component in the HSV color space\n",
      "          twilight          perceptually uniform shades of\n",
      "                            white-blue-black-red-white\n",
      "          twilight_shifted  perceptually uniform shades of\n",
      "                            black-blue-white-red-black\n",
      "          ================  =================================================\n",
      "        \n",
      "        Other miscellaneous schemes:\n",
      "        \n",
      "          ============= =======================================================\n",
      "          Colormap      Description\n",
      "          ============= =======================================================\n",
      "          afmhot        sequential black-orange-yellow-white blackbody\n",
      "                        spectrum, commonly used in atomic force microscopy\n",
      "          brg           blue-red-green\n",
      "          bwr           diverging blue-white-red\n",
      "          coolwarm      diverging blue-gray-red, meant to avoid issues with 3D\n",
      "                        shading, color blindness, and ordering of colors [#]_\n",
      "          CMRmap        \"Default colormaps on color images often reproduce to\n",
      "                        confusing grayscale images. The proposed colormap\n",
      "                        maintains an aesthetically pleasing color image that\n",
      "                        automatically reproduces to a monotonic grayscale with\n",
      "                        discrete, quantifiable saturation levels.\" [#]_\n",
      "          cubehelix     Unlike most other color schemes cubehelix was designed\n",
      "                        by D.A. Green to be monotonically increasing in terms\n",
      "                        of perceived brightness. Also, when printed on a black\n",
      "                        and white postscript printer, the scheme results in a\n",
      "                        greyscale with monotonically increasing brightness.\n",
      "                        This color scheme is named cubehelix because the (r, g, b)\n",
      "                        values produced can be visualised as a squashed helix\n",
      "                        around the diagonal in the (r, g, b) color cube.\n",
      "          gnuplot       gnuplot's traditional pm3d scheme\n",
      "                        (black-blue-red-yellow)\n",
      "          gnuplot2      sequential color printable as gray\n",
      "                        (black-blue-violet-yellow-white)\n",
      "          ocean         green-blue-white\n",
      "          rainbow       spectral purple-blue-green-yellow-orange-red colormap\n",
      "                        with diverging luminance\n",
      "          seismic       diverging blue-white-red\n",
      "          nipy_spectral black-purple-blue-green-yellow-red-white spectrum,\n",
      "                        originally from the Neuroimaging in Python project\n",
      "          terrain       mapmaker's colors, blue-green-yellow-brown-white,\n",
      "                        originally from IGOR Pro\n",
      "          turbo         Spectral map (purple-blue-green-yellow-orange-red) with\n",
      "                        a bright center and darker endpoints. A smoother\n",
      "                        alternative to jet.\n",
      "          ============= =======================================================\n",
      "        \n",
      "        The following colormaps are redundant and may be removed in future\n",
      "        versions.  It's recommended to use the names in the descriptions\n",
      "        instead, which produce identical output:\n",
      "        \n",
      "          =========  =======================================================\n",
      "          Colormap   Description\n",
      "          =========  =======================================================\n",
      "          gist_gray  identical to *gray*\n",
      "          gist_yarg  identical to *gray_r*\n",
      "          binary     identical to *gray_r*\n",
      "          =========  =======================================================\n",
      "        \n",
      "        .. rubric:: Footnotes\n",
      "        \n",
      "        .. [#] Rainbow colormaps, ``jet`` in particular, are considered a poor\n",
      "          choice for scientific visualization by many researchers: `Rainbow Color\n",
      "          Map (Still) Considered Harmful\n",
      "          <https://ieeexplore.ieee.org/document/4118486/?arnumber=4118486>`_\n",
      "        \n",
      "        .. [#] Resembles \"BkBlAqGrYeOrReViWh200\" from NCAR Command\n",
      "          Language. See `Color Table Gallery\n",
      "          <https://www.ncl.ucar.edu/Document/Graphics/color_table_gallery.shtml>`_\n",
      "        \n",
      "        .. [#] See `Diverging Color Maps for Scientific Visualization\n",
      "          <http://www.kennethmoreland.com/color-maps/>`_ by Kenneth Moreland.\n",
      "        \n",
      "        .. [#] See `A Color Map for Effective Black-and-White Rendering of\n",
      "          Color-Scale Images\n",
      "          <https://www.mathworks.com/matlabcentral/fileexchange/2662-cmrmap-m>`_\n",
      "          by Carey Rappaport\n",
      "    \n",
      "    connect(s, func)\n",
      "        Bind function *func* to event *s*.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        s : str\n",
      "            One of the following events ids:\n",
      "        \n",
      "            - 'button_press_event'\n",
      "            - 'button_release_event'\n",
      "            - 'draw_event'\n",
      "            - 'key_press_event'\n",
      "            - 'key_release_event'\n",
      "            - 'motion_notify_event'\n",
      "            - 'pick_event'\n",
      "            - 'resize_event'\n",
      "            - 'scroll_event'\n",
      "            - 'figure_enter_event',\n",
      "            - 'figure_leave_event',\n",
      "            - 'axes_enter_event',\n",
      "            - 'axes_leave_event'\n",
      "            - 'close_event'.\n",
      "        \n",
      "        func : callable\n",
      "            The callback function to be executed, which must have the\n",
      "            signature::\n",
      "        \n",
      "                def func(event: Event) -> Any\n",
      "        \n",
      "            For the location events (button and key press/release), if the\n",
      "            mouse is over the axes, the ``inaxes`` attribute of the event will\n",
      "            be set to the `~matplotlib.axes.Axes` the event occurs is over, and\n",
      "            additionally, the variables ``xdata`` and ``ydata`` attributes will\n",
      "            be set to the mouse location in data coordinates.  See `.KeyEvent`\n",
      "            and `.MouseEvent` for more info.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        cid\n",
      "            A connection id that can be used with\n",
      "            `.FigureCanvasBase.mpl_disconnect`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        ::\n",
      "        \n",
      "            def on_press(event):\n",
      "                print('you pressed', event.button, event.xdata, event.ydata)\n",
      "        \n",
      "            cid = canvas.mpl_connect('button_press_event', on_press)\n",
      "    \n",
      "    contour(*args, data=None, **kwargs)\n",
      "        Plot contours.\n",
      "        \n",
      "        Call signature::\n",
      "        \n",
      "            contour([X, Y,] Z, [levels], **kwargs)\n",
      "        \n",
      "        `.contour` and `.contourf` draw contour lines and filled contours,\n",
      "        respectively.  Except as noted, function signatures and return values\n",
      "        are the same for both versions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X, Y : array-like, optional\n",
      "            The coordinates of the values in *Z*.\n",
      "        \n",
      "            *X* and *Y* must both be 2-D with the same shape as *Z* (e.g.\n",
      "            created via `numpy.meshgrid`), or they must both be 1-D such\n",
      "            that ``len(X) == M`` is the number of columns in *Z* and\n",
      "            ``len(Y) == N`` is the number of rows in *Z*.\n",
      "        \n",
      "            If not given, they are assumed to be integer indices, i.e.\n",
      "            ``X = range(M)``, ``Y = range(N)``.\n",
      "        \n",
      "        Z : array-like(N, M)\n",
      "            The height values over which the contour is drawn.\n",
      "        \n",
      "        levels : int or array-like, optional\n",
      "            Determines the number and positions of the contour lines / regions.\n",
      "        \n",
      "            If an int *n*, use `~matplotlib.ticker.MaxNLocator`, which tries\n",
      "            to automatically choose no more than *n+1* \"nice\" contour levels\n",
      "            between *vmin* and *vmax*.\n",
      "        \n",
      "            If array-like, draw contour lines at the specified levels.\n",
      "            The values must be in increasing order.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `~.contour.QuadContourSet`\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        corner_mask : bool, default: :rc:`contour.corner_mask`\n",
      "            Enable/disable corner masking, which only has an effect if *Z* is\n",
      "            a masked array.  If ``False``, any quad touching a masked point is\n",
      "            masked out.  If ``True``, only the triangular corners of quads\n",
      "            nearest those points are always masked out, other triangular\n",
      "            corners comprising three unmasked points are contoured as usual.\n",
      "        \n",
      "        colors : color string or sequence of colors, optional\n",
      "            The colors of the levels, i.e. the lines for `.contour` and the\n",
      "            areas for `.contourf`.\n",
      "        \n",
      "            The sequence is cycled for the levels in ascending order. If the\n",
      "            sequence is shorter than the number of levels, it's repeated.\n",
      "        \n",
      "            As a shortcut, single color strings may be used in place of\n",
      "            one-element lists, i.e. ``'red'`` instead of ``['red']`` to color\n",
      "            all levels with the same color. This shortcut does only work for\n",
      "            color strings, not for other ways of specifying colors.\n",
      "        \n",
      "            By default (value *None*), the colormap specified by *cmap*\n",
      "            will be used.\n",
      "        \n",
      "        alpha : float, default: 1\n",
      "            The alpha blending value, between 0 (transparent) and 1 (opaque).\n",
      "        \n",
      "        cmap : str or `.Colormap`, default: :rc:`image.cmap`\n",
      "            A `.Colormap` instance or registered colormap name. The colormap\n",
      "            maps the level values to colors.\n",
      "        \n",
      "            If both *colors* and *cmap* are given, an error is raised.\n",
      "        \n",
      "        norm : `~matplotlib.colors.Normalize`, optional\n",
      "            If a colormap is used, the `.Normalize` instance scales the level\n",
      "            values to the canonical colormap range [0, 1] for mapping to\n",
      "            colors. If not given, the default linear scaling is used.\n",
      "        \n",
      "        vmin, vmax : float, optional\n",
      "            If not *None*, either or both of these values will be supplied to\n",
      "            the `.Normalize` instance, overriding the default color scaling\n",
      "            based on *levels*.\n",
      "        \n",
      "        origin : {*None*, 'upper', 'lower', 'image'}, default: None\n",
      "            Determines the orientation and exact position of *Z* by specifying\n",
      "            the position of ``Z[0, 0]``.  This is only relevant, if *X*, *Y*\n",
      "            are not given.\n",
      "        \n",
      "            - *None*: ``Z[0, 0]`` is at X=0, Y=0 in the lower left corner.\n",
      "            - 'lower': ``Z[0, 0]`` is at X=0.5, Y=0.5 in the lower left corner.\n",
      "            - 'upper': ``Z[0, 0]`` is at X=N+0.5, Y=0.5 in the upper left\n",
      "              corner.\n",
      "            - 'image': Use the value from :rc:`image.origin`.\n",
      "        \n",
      "        extent : (x0, x1, y0, y1), optional\n",
      "            If *origin* is not *None*, then *extent* is interpreted as in\n",
      "            `.imshow`: it gives the outer pixel boundaries. In this case, the\n",
      "            position of Z[0, 0] is the center of the pixel, not a corner. If\n",
      "            *origin* is *None*, then (*x0*, *y0*) is the position of Z[0, 0],\n",
      "            and (*x1*, *y1*) is the position of Z[-1, -1].\n",
      "        \n",
      "            This argument is ignored if *X* and *Y* are specified in the call\n",
      "            to contour.\n",
      "        \n",
      "        locator : ticker.Locator subclass, optional\n",
      "            The locator is used to determine the contour levels if they\n",
      "            are not given explicitly via *levels*.\n",
      "            Defaults to `~.ticker.MaxNLocator`.\n",
      "        \n",
      "        extend : {'neither', 'both', 'min', 'max'}, default: 'neither'\n",
      "            Determines the ``contourf``-coloring of values that are outside the\n",
      "            *levels* range.\n",
      "        \n",
      "            If 'neither', values outside the *levels* range are not colored.\n",
      "            If 'min', 'max' or 'both', color the values below, above or below\n",
      "            and above the *levels* range.\n",
      "        \n",
      "            Values below ``min(levels)`` and above ``max(levels)`` are mapped\n",
      "            to the under/over values of the `.Colormap`. Note that most\n",
      "            colormaps do not have dedicated colors for these by default, so\n",
      "            that the over and under values are the edge values of the colormap.\n",
      "            You may want to set these values explicitly using\n",
      "            `.Colormap.set_under` and `.Colormap.set_over`.\n",
      "        \n",
      "            .. note::\n",
      "        \n",
      "                An existing `.QuadContourSet` does not get notified if\n",
      "                properties of its colormap are changed. Therefore, an explicit\n",
      "                call `.QuadContourSet.changed()` is needed after modifying the\n",
      "                colormap. The explicit call can be left out, if a colorbar is\n",
      "                assigned to the `.QuadContourSet` because it internally calls\n",
      "                `.QuadContourSet.changed()`.\n",
      "        \n",
      "            Example::\n",
      "        \n",
      "                x = np.arange(1, 10)\n",
      "                y = x.reshape(-1, 1)\n",
      "                h = x * y\n",
      "        \n",
      "                cs = plt.contourf(h, levels=[10, 30, 50],\n",
      "                    colors=['#808080', '#A0A0A0', '#C0C0C0'], extend='both')\n",
      "                cs.cmap.set_over('red')\n",
      "                cs.cmap.set_under('blue')\n",
      "                cs.changed()\n",
      "        \n",
      "        xunits, yunits : registered units, optional\n",
      "            Override axis units by specifying an instance of a\n",
      "            :class:`matplotlib.units.ConversionInterface`.\n",
      "        \n",
      "        antialiased : bool, optional\n",
      "            Enable antialiasing, overriding the defaults.  For\n",
      "            filled contours, the default is *True*.  For line contours,\n",
      "            it is taken from :rc:`lines.antialiased`.\n",
      "        \n",
      "        nchunk : int >= 0, optional\n",
      "            If 0, no subdivision of the domain.  Specify a positive integer to\n",
      "            divide the domain into subdomains of *nchunk* by *nchunk* quads.\n",
      "            Chunking reduces the maximum length of polygons generated by the\n",
      "            contouring algorithm which reduces the rendering workload passed\n",
      "            on to the backend and also requires slightly less RAM.  It can\n",
      "            however introduce rendering artifacts at chunk boundaries depending\n",
      "            on the backend, the *antialiased* flag and value of *alpha*.\n",
      "        \n",
      "        linewidths : float or array-like, default: :rc:`contour.linewidth`\n",
      "            *Only applies to* `.contour`.\n",
      "        \n",
      "            The line width of the contour lines.\n",
      "        \n",
      "            If a number, all levels will be plotted with this linewidth.\n",
      "        \n",
      "            If a sequence, the levels in ascending order will be plotted with\n",
      "            the linewidths in the order specified.\n",
      "        \n",
      "            If None, this falls back to :rc:`lines.linewidth`.\n",
      "        \n",
      "        linestyles : {*None*, 'solid', 'dashed', 'dashdot', 'dotted'}, optional\n",
      "            *Only applies to* `.contour`.\n",
      "        \n",
      "            If *linestyles* is *None*, the default is 'solid' unless the lines\n",
      "            are monochrome.  In that case, negative contours will take their\n",
      "            linestyle from :rc:`contour.negative_linestyle` setting.\n",
      "        \n",
      "            *linestyles* can also be an iterable of the above strings\n",
      "            specifying a set of linestyles to be used. If this\n",
      "            iterable is shorter than the number of contour levels\n",
      "            it will be repeated as necessary.\n",
      "        \n",
      "        hatches : List[str], optional\n",
      "            *Only applies to* `.contourf`.\n",
      "        \n",
      "            A list of cross hatch patterns to use on the filled areas.\n",
      "            If None, no hatching will be added to the contour.\n",
      "            Hatching is supported in the PostScript, PDF, SVG and Agg\n",
      "            backends only.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        1. `.contourf` differs from the MATLAB version in that it does not draw\n",
      "           the polygon edges. To draw edges, add line contours with calls to\n",
      "           `.contour`.\n",
      "        \n",
      "        2. `.contourf` fills intervals that are closed at the top; that is, for\n",
      "           boundaries *z1* and *z2*, the filled region is::\n",
      "        \n",
      "              z1 < Z <= z2\n",
      "        \n",
      "           except for the lowest interval, which is closed on both sides (i.e.\n",
      "           it includes the lowest value).\n",
      "    \n",
      "    contourf(*args, data=None, **kwargs)\n",
      "        Plot contours.\n",
      "        \n",
      "        Call signature::\n",
      "        \n",
      "            contour([X, Y,] Z, [levels], **kwargs)\n",
      "        \n",
      "        `.contour` and `.contourf` draw contour lines and filled contours,\n",
      "        respectively.  Except as noted, function signatures and return values\n",
      "        are the same for both versions.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X, Y : array-like, optional\n",
      "            The coordinates of the values in *Z*.\n",
      "        \n",
      "            *X* and *Y* must both be 2-D with the same shape as *Z* (e.g.\n",
      "            created via `numpy.meshgrid`), or they must both be 1-D such\n",
      "            that ``len(X) == M`` is the number of columns in *Z* and\n",
      "            ``len(Y) == N`` is the number of rows in *Z*.\n",
      "        \n",
      "            If not given, they are assumed to be integer indices, i.e.\n",
      "            ``X = range(M)``, ``Y = range(N)``.\n",
      "        \n",
      "        Z : array-like(N, M)\n",
      "            The height values over which the contour is drawn.\n",
      "        \n",
      "        levels : int or array-like, optional\n",
      "            Determines the number and positions of the contour lines / regions.\n",
      "        \n",
      "            If an int *n*, use `~matplotlib.ticker.MaxNLocator`, which tries\n",
      "            to automatically choose no more than *n+1* \"nice\" contour levels\n",
      "            between *vmin* and *vmax*.\n",
      "        \n",
      "            If array-like, draw contour lines at the specified levels.\n",
      "            The values must be in increasing order.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `~.contour.QuadContourSet`\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        corner_mask : bool, default: :rc:`contour.corner_mask`\n",
      "            Enable/disable corner masking, which only has an effect if *Z* is\n",
      "            a masked array.  If ``False``, any quad touching a masked point is\n",
      "            masked out.  If ``True``, only the triangular corners of quads\n",
      "            nearest those points are always masked out, other triangular\n",
      "            corners comprising three unmasked points are contoured as usual.\n",
      "        \n",
      "        colors : color string or sequence of colors, optional\n",
      "            The colors of the levels, i.e. the lines for `.contour` and the\n",
      "            areas for `.contourf`.\n",
      "        \n",
      "            The sequence is cycled for the levels in ascending order. If the\n",
      "            sequence is shorter than the number of levels, it's repeated.\n",
      "        \n",
      "            As a shortcut, single color strings may be used in place of\n",
      "            one-element lists, i.e. ``'red'`` instead of ``['red']`` to color\n",
      "            all levels with the same color. This shortcut does only work for\n",
      "            color strings, not for other ways of specifying colors.\n",
      "        \n",
      "            By default (value *None*), the colormap specified by *cmap*\n",
      "            will be used.\n",
      "        \n",
      "        alpha : float, default: 1\n",
      "            The alpha blending value, between 0 (transparent) and 1 (opaque).\n",
      "        \n",
      "        cmap : str or `.Colormap`, default: :rc:`image.cmap`\n",
      "            A `.Colormap` instance or registered colormap name. The colormap\n",
      "            maps the level values to colors.\n",
      "        \n",
      "            If both *colors* and *cmap* are given, an error is raised.\n",
      "        \n",
      "        norm : `~matplotlib.colors.Normalize`, optional\n",
      "            If a colormap is used, the `.Normalize` instance scales the level\n",
      "            values to the canonical colormap range [0, 1] for mapping to\n",
      "            colors. If not given, the default linear scaling is used.\n",
      "        \n",
      "        vmin, vmax : float, optional\n",
      "            If not *None*, either or both of these values will be supplied to\n",
      "            the `.Normalize` instance, overriding the default color scaling\n",
      "            based on *levels*.\n",
      "        \n",
      "        origin : {*None*, 'upper', 'lower', 'image'}, default: None\n",
      "            Determines the orientation and exact position of *Z* by specifying\n",
      "            the position of ``Z[0, 0]``.  This is only relevant, if *X*, *Y*\n",
      "            are not given.\n",
      "        \n",
      "            - *None*: ``Z[0, 0]`` is at X=0, Y=0 in the lower left corner.\n",
      "            - 'lower': ``Z[0, 0]`` is at X=0.5, Y=0.5 in the lower left corner.\n",
      "            - 'upper': ``Z[0, 0]`` is at X=N+0.5, Y=0.5 in the upper left\n",
      "              corner.\n",
      "            - 'image': Use the value from :rc:`image.origin`.\n",
      "        \n",
      "        extent : (x0, x1, y0, y1), optional\n",
      "            If *origin* is not *None*, then *extent* is interpreted as in\n",
      "            `.imshow`: it gives the outer pixel boundaries. In this case, the\n",
      "            position of Z[0, 0] is the center of the pixel, not a corner. If\n",
      "            *origin* is *None*, then (*x0*, *y0*) is the position of Z[0, 0],\n",
      "            and (*x1*, *y1*) is the position of Z[-1, -1].\n",
      "        \n",
      "            This argument is ignored if *X* and *Y* are specified in the call\n",
      "            to contour.\n",
      "        \n",
      "        locator : ticker.Locator subclass, optional\n",
      "            The locator is used to determine the contour levels if they\n",
      "            are not given explicitly via *levels*.\n",
      "            Defaults to `~.ticker.MaxNLocator`.\n",
      "        \n",
      "        extend : {'neither', 'both', 'min', 'max'}, default: 'neither'\n",
      "            Determines the ``contourf``-coloring of values that are outside the\n",
      "            *levels* range.\n",
      "        \n",
      "            If 'neither', values outside the *levels* range are not colored.\n",
      "            If 'min', 'max' or 'both', color the values below, above or below\n",
      "            and above the *levels* range.\n",
      "        \n",
      "            Values below ``min(levels)`` and above ``max(levels)`` are mapped\n",
      "            to the under/over values of the `.Colormap`. Note that most\n",
      "            colormaps do not have dedicated colors for these by default, so\n",
      "            that the over and under values are the edge values of the colormap.\n",
      "            You may want to set these values explicitly using\n",
      "            `.Colormap.set_under` and `.Colormap.set_over`.\n",
      "        \n",
      "            .. note::\n",
      "        \n",
      "                An existing `.QuadContourSet` does not get notified if\n",
      "                properties of its colormap are changed. Therefore, an explicit\n",
      "                call `.QuadContourSet.changed()` is needed after modifying the\n",
      "                colormap. The explicit call can be left out, if a colorbar is\n",
      "                assigned to the `.QuadContourSet` because it internally calls\n",
      "                `.QuadContourSet.changed()`.\n",
      "        \n",
      "            Example::\n",
      "        \n",
      "                x = np.arange(1, 10)\n",
      "                y = x.reshape(-1, 1)\n",
      "                h = x * y\n",
      "        \n",
      "                cs = plt.contourf(h, levels=[10, 30, 50],\n",
      "                    colors=['#808080', '#A0A0A0', '#C0C0C0'], extend='both')\n",
      "                cs.cmap.set_over('red')\n",
      "                cs.cmap.set_under('blue')\n",
      "                cs.changed()\n",
      "        \n",
      "        xunits, yunits : registered units, optional\n",
      "            Override axis units by specifying an instance of a\n",
      "            :class:`matplotlib.units.ConversionInterface`.\n",
      "        \n",
      "        antialiased : bool, optional\n",
      "            Enable antialiasing, overriding the defaults.  For\n",
      "            filled contours, the default is *True*.  For line contours,\n",
      "            it is taken from :rc:`lines.antialiased`.\n",
      "        \n",
      "        nchunk : int >= 0, optional\n",
      "            If 0, no subdivision of the domain.  Specify a positive integer to\n",
      "            divide the domain into subdomains of *nchunk* by *nchunk* quads.\n",
      "            Chunking reduces the maximum length of polygons generated by the\n",
      "            contouring algorithm which reduces the rendering workload passed\n",
      "            on to the backend and also requires slightly less RAM.  It can\n",
      "            however introduce rendering artifacts at chunk boundaries depending\n",
      "            on the backend, the *antialiased* flag and value of *alpha*.\n",
      "        \n",
      "        linewidths : float or array-like, default: :rc:`contour.linewidth`\n",
      "            *Only applies to* `.contour`.\n",
      "        \n",
      "            The line width of the contour lines.\n",
      "        \n",
      "            If a number, all levels will be plotted with this linewidth.\n",
      "        \n",
      "            If a sequence, the levels in ascending order will be plotted with\n",
      "            the linewidths in the order specified.\n",
      "        \n",
      "            If None, this falls back to :rc:`lines.linewidth`.\n",
      "        \n",
      "        linestyles : {*None*, 'solid', 'dashed', 'dashdot', 'dotted'}, optional\n",
      "            *Only applies to* `.contour`.\n",
      "        \n",
      "            If *linestyles* is *None*, the default is 'solid' unless the lines\n",
      "            are monochrome.  In that case, negative contours will take their\n",
      "            linestyle from :rc:`contour.negative_linestyle` setting.\n",
      "        \n",
      "            *linestyles* can also be an iterable of the above strings\n",
      "            specifying a set of linestyles to be used. If this\n",
      "            iterable is shorter than the number of contour levels\n",
      "            it will be repeated as necessary.\n",
      "        \n",
      "        hatches : List[str], optional\n",
      "            *Only applies to* `.contourf`.\n",
      "        \n",
      "            A list of cross hatch patterns to use on the filled areas.\n",
      "            If None, no hatching will be added to the contour.\n",
      "            Hatching is supported in the PostScript, PDF, SVG and Agg\n",
      "            backends only.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        1. `.contourf` differs from the MATLAB version in that it does not draw\n",
      "           the polygon edges. To draw edges, add line contours with calls to\n",
      "           `.contour`.\n",
      "        \n",
      "        2. `.contourf` fills intervals that are closed at the top; that is, for\n",
      "           boundaries *z1* and *z2*, the filled region is::\n",
      "        \n",
      "              z1 < Z <= z2\n",
      "        \n",
      "           except for the lowest interval, which is closed on both sides (i.e.\n",
      "           it includes the lowest value).\n",
      "    \n",
      "    cool()\n",
      "        Set the colormap to \"cool\".\n",
      "        \n",
      "        This changes the default colormap as well as the colormap of the current\n",
      "        image if there is one. See ``help(colormaps)`` for more information.\n",
      "    \n",
      "    copper()\n",
      "        Set the colormap to \"copper\".\n",
      "        \n",
      "        This changes the default colormap as well as the colormap of the current\n",
      "        image if there is one. See ``help(colormaps)`` for more information.\n",
      "    \n",
      "    csd(x, y, NFFT=None, Fs=None, Fc=None, detrend=None, window=None, noverlap=None, pad_to=None, sides=None, scale_by_freq=None, return_line=None, *, data=None, **kwargs)\n",
      "        Plot the cross-spectral density.\n",
      "        \n",
      "        The cross spectral density :math:`P_{xy}` by Welch's average\n",
      "        periodogram method.  The vectors *x* and *y* are divided into\n",
      "        *NFFT* length segments.  Each segment is detrended by function\n",
      "        *detrend* and windowed by function *window*.  *noverlap* gives\n",
      "        the length of the overlap between segments.  The product of\n",
      "        the direct FFTs of *x* and *y* are averaged over each segment\n",
      "        to compute :math:`P_{xy}`, with a scaling to correct for power\n",
      "        loss due to windowing.\n",
      "        \n",
      "        If len(*x*) < *NFFT* or len(*y*) < *NFFT*, they will be zero\n",
      "        padded to *NFFT*.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : 1-D arrays or sequences\n",
      "            Arrays or sequences containing the data.\n",
      "        \n",
      "        Fs : float, default: 2\n",
      "            The sampling frequency (samples per time unit).  It is used to calculate\n",
      "            the Fourier frequencies, *freqs*, in cycles per time unit.\n",
      "        \n",
      "        window : callable or ndarray, default: `.window_hanning`\n",
      "            A function or a vector of length *NFFT*.  To create window vectors see\n",
      "            `.window_hanning`, `.window_none`, `numpy.blackman`, `numpy.hamming`,\n",
      "            `numpy.bartlett`, `scipy.signal`, `scipy.signal.get_window`, etc.  If a\n",
      "            function is passed as the argument, it must take a data segment as an\n",
      "            argument and return the windowed version of the segment.\n",
      "        \n",
      "        sides : {'default', 'onesided', 'twosided'}, optional\n",
      "            Which sides of the spectrum to return. 'default' is one-sided for real\n",
      "            data and two-sided for complex data. 'onesided' forces the return of a\n",
      "            one-sided spectrum, while 'twosided' forces two-sided.\n",
      "        \n",
      "        pad_to : int, optional\n",
      "            The number of points to which the data segment is padded when performing\n",
      "            the FFT.  This can be different from *NFFT*, which specifies the number\n",
      "            of data points used.  While not increasing the actual resolution of the\n",
      "            spectrum (the minimum distance between resolvable peaks), this can give\n",
      "            more points in the plot, allowing for more detail. This corresponds to\n",
      "            the *n* parameter in the call to fft(). The default is None, which sets\n",
      "            *pad_to* equal to *NFFT*\n",
      "        \n",
      "        NFFT : int, default: 256\n",
      "            The number of data points used in each block for the FFT.  A power 2 is\n",
      "            most efficient.  This should *NOT* be used to get zero padding, or the\n",
      "            scaling of the result will be incorrect; use *pad_to* for this instead.\n",
      "        \n",
      "        detrend : {'none', 'mean', 'linear'} or callable, default 'none'\n",
      "            The function applied to each segment before fft-ing, designed to remove\n",
      "            the mean or linear trend.  Unlike in MATLAB, where the *detrend* parameter\n",
      "            is a vector, in Matplotlib is it a function.  The :mod:`~matplotlib.mlab`\n",
      "            module defines `.detrend_none`, `.detrend_mean`, and `.detrend_linear`,\n",
      "            but you can use a custom function as well.  You can also use a string to\n",
      "            choose one of the functions: 'none' calls `.detrend_none`. 'mean' calls\n",
      "            `.detrend_mean`. 'linear' calls `.detrend_linear`.\n",
      "        \n",
      "        scale_by_freq : bool, default: True\n",
      "            Whether the resulting density values should be scaled by the scaling\n",
      "            frequency, which gives density in units of Hz^-1.  This allows for\n",
      "            integration over the returned frequency values.  The default is True for\n",
      "            MATLAB compatibility.\n",
      "        \n",
      "        noverlap : int, default: 0 (no overlap)\n",
      "            The number of points of overlap between segments.\n",
      "        \n",
      "        Fc : int, default: 0\n",
      "            The center frequency of *x*, which offsets the x extents of the\n",
      "            plot to reflect the frequency range used when a signal is acquired\n",
      "            and then filtered and downsampled to baseband.\n",
      "        \n",
      "        return_line : bool, default: False\n",
      "            Whether to include the line object plotted in the returned values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Pxy : 1-D array\n",
      "            The values for the cross spectrum :math:`P_{xy}` before scaling\n",
      "            (complex valued).\n",
      "        \n",
      "        freqs : 1-D array\n",
      "            The frequencies corresponding to the elements in *Pxy*.\n",
      "        \n",
      "        line : `~matplotlib.lines.Line2D`\n",
      "            The line created by this function.\n",
      "            Only returned if *return_line* is True.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            Keyword arguments control the `.Line2D` properties:\n",
      "        \n",
      "            Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa: bool\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            color or c: color\n",
      "            contains: unknown\n",
      "            dash_capstyle: {'butt', 'round', 'projecting'}\n",
      "            dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      "            data: (2, N) array or two 1D arrays\n",
      "            drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      "            figure: `.Figure`\n",
      "            fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      "            gid: str\n",
      "            in_layout: bool\n",
      "            label: object\n",
      "            linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      "            linewidth or lw: float\n",
      "            marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      "            markeredgecolor or mec: color\n",
      "            markeredgewidth or mew: float\n",
      "            markerfacecolor or mfc: color\n",
      "            markerfacecoloralt or mfcalt: color\n",
      "            markersize or ms: float\n",
      "            markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: unknown\n",
      "            pickradius: float\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            solid_capstyle: {'butt', 'round', 'projecting'}\n",
      "            solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            transform: `matplotlib.transforms.Transform`\n",
      "            url: str\n",
      "            visible: bool\n",
      "            xdata: 1D array\n",
      "            ydata: 1D array\n",
      "            zorder: float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        psd : is equivalent to setting ``y = x``.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For plotting, the power is plotted as\n",
      "        :math:`10 \\log_{10}(P_{xy})` for decibels, though :math:`P_{xy}` itself\n",
      "        is returned.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Bendat & Piersol -- Random Data: Analysis and Measurement Procedures,\n",
      "        John Wiley & Sons (1986)\n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            the following arguments can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception):\n",
      "            *x*, *y*.\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    delaxes(ax=None)\n",
      "        Remove an `~.axes.Axes` (defaulting to the current axes) from its figure.\n",
      "    \n",
      "    disconnect(cid)\n",
      "        Disconnect the callback with id *cid*.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        ::\n",
      "        \n",
      "            cid = canvas.mpl_connect('button_press_event', on_press)\n",
      "            # ... later\n",
      "            canvas.mpl_disconnect(cid)\n",
      "    \n",
      "    draw()\n",
      "        Redraw the current figure.\n",
      "        \n",
      "        This is used to update a figure that has been altered, but not\n",
      "        automatically re-drawn.  If interactive mode is on (via `.ion()`), this\n",
      "        should be only rarely needed, but there may be ways to modify the state of\n",
      "        a figure without marking it as \"stale\".  Please report these cases as bugs.\n",
      "        \n",
      "        This is equivalent to calling ``fig.canvas.draw_idle()``, where ``fig`` is\n",
      "        the current figure.\n",
      "    \n",
      "    errorbar(x, y, yerr=None, xerr=None, fmt='', ecolor=None, elinewidth=None, capsize=None, barsabove=False, lolims=False, uplims=False, xlolims=False, xuplims=False, errorevery=1, capthick=None, *, data=None, **kwargs)\n",
      "        Plot y versus x as lines and/or markers with attached errorbars.\n",
      "        \n",
      "        *x*, *y* define the data locations, *xerr*, *yerr* define the errorbar\n",
      "        sizes. By default, this draws the data markers/lines as well the\n",
      "        errorbars. Use fmt='none' to draw errorbars without any data markers.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : float or array-like\n",
      "            The data positions.\n",
      "        \n",
      "        xerr, yerr : float or array-like, shape(N,) or shape(2, N), optional\n",
      "            The errorbar sizes:\n",
      "        \n",
      "            - scalar: Symmetric +/- values for all data points.\n",
      "            - shape(N,): Symmetric +/-values for each data point.\n",
      "            - shape(2, N): Separate - and + values for each bar. First row\n",
      "              contains the lower errors, the second row contains the upper\n",
      "              errors.\n",
      "            - *None*: No errorbar.\n",
      "        \n",
      "            Note that all error arrays should have *positive* values.\n",
      "        \n",
      "            See :doc:`/gallery/statistics/errorbar_features`\n",
      "            for an example on the usage of ``xerr`` and ``yerr``.\n",
      "        \n",
      "        fmt : str, default: ''\n",
      "            The format for the data points / data lines. See `.plot` for\n",
      "            details.\n",
      "        \n",
      "            Use 'none' (case insensitive) to plot errorbars without any data\n",
      "            markers.\n",
      "        \n",
      "        ecolor : color, default: None\n",
      "            The color of the errorbar lines.  If None, use the color of the\n",
      "            line connecting the markers.\n",
      "        \n",
      "        elinewidth : float, default: None\n",
      "            The linewidth of the errorbar lines. If None, the linewidth of\n",
      "            the current style is used.\n",
      "        \n",
      "        capsize : float, default: :rc:`errorbar.capsize`\n",
      "            The length of the error bar caps in points.\n",
      "        \n",
      "        capthick : float, default: None\n",
      "            An alias to the keyword argument *markeredgewidth* (a.k.a. *mew*).\n",
      "            This setting is a more sensible name for the property that\n",
      "            controls the thickness of the error bar cap in points. For\n",
      "            backwards compatibility, if *mew* or *markeredgewidth* are given,\n",
      "            then they will over-ride *capthick*. This may change in future\n",
      "            releases.\n",
      "        \n",
      "        barsabove : bool, default: False\n",
      "            If True, will plot the errorbars above the plot\n",
      "            symbols. Default is below.\n",
      "        \n",
      "        lolims, uplims, xlolims, xuplims : bool, default: False\n",
      "            These arguments can be used to indicate that a value gives only\n",
      "            upper/lower limits.  In that case a caret symbol is used to\n",
      "            indicate this. *lims*-arguments may be scalars, or array-likes of\n",
      "            the same length as *xerr* and *yerr*.  To use limits with inverted\n",
      "            axes, `~.Axes.set_xlim` or `~.Axes.set_ylim` must be called before\n",
      "            :meth:`errorbar`.  Note the tricky parameter names: setting e.g.\n",
      "            *lolims* to True means that the y-value is a *lower* limit of the\n",
      "            True value, so, only an *upward*-pointing arrow will be drawn!\n",
      "        \n",
      "        errorevery : int or (int, int), default: 1\n",
      "            draws error bars on a subset of the data. *errorevery* =N draws\n",
      "            error bars on the points (x[::N], y[::N]).\n",
      "            *errorevery* =(start, N) draws error bars on the points\n",
      "            (x[start::N], y[start::N]). e.g. errorevery=(6, 3)\n",
      "            adds error bars to the data at (x[6], x[9], x[12], x[15], ...).\n",
      "            Used to avoid overlapping error bars when two series share x-axis\n",
      "            values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `.ErrorbarContainer`\n",
      "            The container contains:\n",
      "        \n",
      "            - plotline: `.Line2D` instance of x, y plot markers and/or line.\n",
      "            - caplines: A tuple of `.Line2D` instances of the error bar caps.\n",
      "            - barlinecols: A tuple of `.LineCollection` with the horizontal and\n",
      "              vertical error ranges.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            All other keyword arguments are passed on to the `~.Axes.plot` call\n",
      "            drawing the markers. For example, this code makes big red squares\n",
      "            with thick green edges::\n",
      "        \n",
      "                x, y, yerr = rand(3, 10)\n",
      "                errorbar(x, y, yerr, marker='s', mfc='red',\n",
      "                         mec='green', ms=20, mew=4)\n",
      "        \n",
      "            where *mfc*, *mec*, *ms* and *mew* are aliases for the longer\n",
      "            property names, *markerfacecolor*, *markeredgecolor*, *markersize*\n",
      "            and *markeredgewidth*.\n",
      "        \n",
      "            Valid kwargs for the marker properties are `.Line2D` properties:\n",
      "        \n",
      "            Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa: bool\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            color or c: color\n",
      "            contains: unknown\n",
      "            dash_capstyle: {'butt', 'round', 'projecting'}\n",
      "            dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      "            data: (2, N) array or two 1D arrays\n",
      "            drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      "            figure: `.Figure`\n",
      "            fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      "            gid: str\n",
      "            in_layout: bool\n",
      "            label: object\n",
      "            linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      "            linewidth or lw: float\n",
      "            marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      "            markeredgecolor or mec: color\n",
      "            markeredgewidth or mew: float\n",
      "            markerfacecolor or mfc: color\n",
      "            markerfacecoloralt or mfcalt: color\n",
      "            markersize or ms: float\n",
      "            markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: unknown\n",
      "            pickradius: float\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            solid_capstyle: {'butt', 'round', 'projecting'}\n",
      "            solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            transform: `matplotlib.transforms.Transform`\n",
      "            url: str\n",
      "            visible: bool\n",
      "            xdata: 1D array\n",
      "            ydata: 1D array\n",
      "            zorder: float\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            the following arguments can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception):\n",
      "            *x*, *y*, *xerr*, *yerr*.\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    eventplot(positions, orientation='horizontal', lineoffsets=1, linelengths=1, linewidths=None, colors=None, linestyles='solid', *, data=None, **kwargs)\n",
      "        Plot identical parallel lines at the given positions.\n",
      "        \n",
      "        This type of plot is commonly used in neuroscience for representing\n",
      "        neural events, where it is usually called a spike raster, dot raster,\n",
      "        or raster plot.\n",
      "        \n",
      "        However, it is useful in any situation where you wish to show the\n",
      "        timing or position of multiple sets of discrete events, such as the\n",
      "        arrival times of people to a business on each day of the month or the\n",
      "        date of hurricanes each year of the last century.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        positions : array-like or list of array-like\n",
      "            A 1D array-like defines the positions of one sequence of events.\n",
      "        \n",
      "            Multiple groups of events may be passed as a list of array-likes.\n",
      "            Each group can be styled independently by passing lists of values\n",
      "            to *lineoffsets*, *linelengths*, *linewidths*, *colors* and\n",
      "            *linestyles*.\n",
      "        \n",
      "            Note that *positions* can be a 2D array, but in practice different\n",
      "            event groups usually have different counts so that one will use a\n",
      "            list of different-length arrays rather than a 2D array.\n",
      "        \n",
      "        orientation : {'horizontal', 'vertical'}, default: 'horizontal'\n",
      "            The direction of the event sequence:\n",
      "        \n",
      "            - 'horizontal': the events are arranged horizontally.\n",
      "              The indicator lines are vertical.\n",
      "            - 'vertical': the events are arranged vertically.\n",
      "              The indicator lines are horizontal.\n",
      "        \n",
      "        lineoffsets : float or array-like, default: 1\n",
      "            The offset of the center of the lines from the origin, in the\n",
      "            direction orthogonal to *orientation*.\n",
      "        \n",
      "            If *positions* is 2D, this can be a sequence with length matching\n",
      "            the length of *positions*.\n",
      "        \n",
      "        linelengths : float or array-like, default: 1\n",
      "            The total height of the lines (i.e. the lines stretches from\n",
      "            ``lineoffset - linelength/2`` to ``lineoffset + linelength/2``).\n",
      "        \n",
      "            If *positions* is 2D, this can be a sequence with length matching\n",
      "            the length of *positions*.\n",
      "        \n",
      "        linewidths : float or array-like, default: :rc:`lines.linewidth`\n",
      "            The line width(s) of the event lines, in points.\n",
      "        \n",
      "            If *positions* is 2D, this can be a sequence with length matching\n",
      "            the length of *positions*.\n",
      "        \n",
      "        colors : color or list of colors, default: :rc:`lines.color`\n",
      "            The color(s) of the event lines.\n",
      "        \n",
      "            If *positions* is 2D, this can be a sequence with length matching\n",
      "            the length of *positions*.\n",
      "        \n",
      "        linestyles : str or tuple or list of such values, default: 'solid'\n",
      "            Default is 'solid'. Valid strings are ['solid', 'dashed',\n",
      "            'dashdot', 'dotted', '-', '--', '-.', ':']. Dash tuples\n",
      "            should be of the form::\n",
      "        \n",
      "                (offset, onoffseq),\n",
      "        \n",
      "            where *onoffseq* is an even length tuple of on and off ink\n",
      "            in points.\n",
      "        \n",
      "            If *positions* is 2D, this can be a sequence with length matching\n",
      "            the length of *positions*.\n",
      "        \n",
      "        **kwargs\n",
      "            Other keyword arguments are line collection properties.  See\n",
      "            `.LineCollection` for a list of the valid properties.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        list of `.EventCollection`\n",
      "            The `.EventCollection` that were added.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For *linelengths*, *linewidths*, *colors*, and *linestyles*, if only\n",
      "        a single value is given, that value is applied to all lines.  If an\n",
      "        array-like is given, it must have the same length as *positions*, and\n",
      "        each value will be applied to the corresponding row of the array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        .. plot:: gallery/lines_bars_and_markers/eventplot_demo.py\n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            the following arguments can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception):\n",
      "            *positions*, *lineoffsets*, *linelengths*, *linewidths*, *colors*, *linestyles*.\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    figimage(X, xo=0, yo=0, alpha=None, norm=None, cmap=None, vmin=None, vmax=None, origin=None, resize=False, **kwargs)\n",
      "        Add a non-resampled image to the figure.\n",
      "        \n",
      "        The image is attached to the lower or upper left corner depending on\n",
      "        *origin*.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X\n",
      "            The image data. This is an array of one of the following shapes:\n",
      "        \n",
      "            - MxN: luminance (grayscale) values\n",
      "            - MxNx3: RGB values\n",
      "            - MxNx4: RGBA values\n",
      "        \n",
      "        xo, yo : int\n",
      "            The *x*/*y* image offset in pixels.\n",
      "        \n",
      "        alpha : None or float\n",
      "            The alpha blending value.\n",
      "        \n",
      "        norm : `matplotlib.colors.Normalize`\n",
      "            A `.Normalize` instance to map the luminance to the\n",
      "            interval [0, 1].\n",
      "        \n",
      "        cmap : str or `matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n",
      "            The colormap to use.\n",
      "        \n",
      "        vmin, vmax : float\n",
      "            If *norm* is not given, these values set the data limits for the\n",
      "            colormap.\n",
      "        \n",
      "        origin : {'upper', 'lower'}, default: :rc:`image.origin`\n",
      "            Indicates where the [0, 0] index of the array is in the upper left\n",
      "            or lower left corner of the axes.\n",
      "        \n",
      "        resize : bool\n",
      "            If *True*, resize the figure to match the given image size.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `matplotlib.image.FigureImage`\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            Additional kwargs are `.Artist` kwargs passed on to `.FigureImage`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        figimage complements the axes image (`~matplotlib.axes.Axes.imshow`)\n",
      "        which will be resampled to fit the current axes.  If you want\n",
      "        a resampled image to fill the entire figure, you can define an\n",
      "        `~matplotlib.axes.Axes` with extent [0, 0, 1, 1].\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        ::\n",
      "        \n",
      "            f = plt.figure()\n",
      "            nx = int(f.get_figwidth() * f.dpi)\n",
      "            ny = int(f.get_figheight() * f.dpi)\n",
      "            data = np.random.random((ny, nx))\n",
      "            f.figimage(data)\n",
      "            plt.show()\n",
      "    \n",
      "    figlegend(*args, **kwargs)\n",
      "        Place a legend on the figure.\n",
      "        \n",
      "        To make a legend from existing artists on every axes::\n",
      "        \n",
      "          figlegend()\n",
      "        \n",
      "        To make a legend for a list of lines and labels::\n",
      "        \n",
      "          figlegend(\n",
      "              (line1, line2, line3),\n",
      "              ('label1', 'label2', 'label3'),\n",
      "              loc='upper right')\n",
      "        \n",
      "        These can also be specified by keyword::\n",
      "        \n",
      "          figlegend(\n",
      "              handles=(line1, line2, line3),\n",
      "              labels=('label1', 'label2', 'label3'),\n",
      "              loc='upper right')\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        handles : list of `.Artist`, optional\n",
      "            A list of Artists (lines, patches) to be added to the legend.\n",
      "            Use this together with *labels*, if you need full control on what\n",
      "            is shown in the legend and the automatic mechanism described above\n",
      "            is not sufficient.\n",
      "        \n",
      "            The length of handles and labels should be the same in this\n",
      "            case. If they are not, they are truncated to the smaller length.\n",
      "        \n",
      "        labels : list of str, optional\n",
      "            A list of labels to show next to the artists.\n",
      "            Use this together with *handles*, if you need full control on what\n",
      "            is shown in the legend and the automatic mechanism described above\n",
      "            is not sufficient.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `~matplotlib.legend.Legend`\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        \n",
      "        loc : str or pair of floats, default: :rc:`legend.loc` ('best' for axes, 'upper right' for figures)\n",
      "            The location of the legend.\n",
      "        \n",
      "            The strings\n",
      "            ``'upper left', 'upper right', 'lower left', 'lower right'``\n",
      "            place the legend at the corresponding corner of the axes/figure.\n",
      "        \n",
      "            The strings\n",
      "            ``'upper center', 'lower center', 'center left', 'center right'``\n",
      "            place the legend at the center of the corresponding edge of the\n",
      "            axes/figure.\n",
      "        \n",
      "            The string ``'center'`` places the legend at the center of the axes/figure.\n",
      "        \n",
      "            The string ``'best'`` places the legend at the location, among the nine\n",
      "            locations defined so far, with the minimum overlap with other drawn\n",
      "            artists.  This option can be quite slow for plots with large amounts of\n",
      "            data; your plotting speed may benefit from providing a specific location.\n",
      "        \n",
      "            The location can also be a 2-tuple giving the coordinates of the lower-left\n",
      "            corner of the legend in axes coordinates (in which case *bbox_to_anchor*\n",
      "            will be ignored).\n",
      "        \n",
      "            For back-compatibility, ``'center right'`` (but no other location) can also\n",
      "            be spelled ``'right'``, and each \"string\" locations can also be given as a\n",
      "            numeric value:\n",
      "        \n",
      "                ===============   =============\n",
      "                Location String   Location Code\n",
      "                ===============   =============\n",
      "                'best'            0\n",
      "                'upper right'     1\n",
      "                'upper left'      2\n",
      "                'lower left'      3\n",
      "                'lower right'     4\n",
      "                'right'           5\n",
      "                'center left'     6\n",
      "                'center right'    7\n",
      "                'lower center'    8\n",
      "                'upper center'    9\n",
      "                'center'          10\n",
      "                ===============   =============\n",
      "        \n",
      "        bbox_to_anchor : `.BboxBase`, 2-tuple, or 4-tuple of floats\n",
      "            Box that is used to position the legend in conjunction with *loc*.\n",
      "            Defaults to `axes.bbox` (if called as a method to `.Axes.legend`) or\n",
      "            `figure.bbox` (if `.Figure.legend`).  This argument allows arbitrary\n",
      "            placement of the legend.\n",
      "        \n",
      "            Bbox coordinates are interpreted in the coordinate system given by\n",
      "            *bbox_transform*, with the default transform\n",
      "            Axes or Figure coordinates, depending on which ``legend`` is called.\n",
      "        \n",
      "            If a 4-tuple or `.BboxBase` is given, then it specifies the bbox\n",
      "            ``(x, y, width, height)`` that the legend is placed in.\n",
      "            To put the legend in the best location in the bottom right\n",
      "            quadrant of the axes (or figure)::\n",
      "        \n",
      "                loc='best', bbox_to_anchor=(0.5, 0., 0.5, 0.5)\n",
      "        \n",
      "            A 2-tuple ``(x, y)`` places the corner of the legend specified by *loc* at\n",
      "            x, y.  For example, to put the legend's upper right-hand corner in the\n",
      "            center of the axes (or figure) the following keywords can be used::\n",
      "        \n",
      "                loc='upper right', bbox_to_anchor=(0.5, 0.5)\n",
      "        \n",
      "        ncol : int, default: 1\n",
      "            The number of columns that the legend has.\n",
      "        \n",
      "        prop : None or `matplotlib.font_manager.FontProperties` or dict\n",
      "            The font properties of the legend. If None (default), the current\n",
      "            :data:`matplotlib.rcParams` will be used.\n",
      "        \n",
      "        fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n",
      "            The font size of the legend. If the value is numeric the size will be the\n",
      "            absolute font size in points. String values are relative to the current\n",
      "            default font size. This argument is only used if *prop* is not specified.\n",
      "        \n",
      "        labelcolor : str or list\n",
      "            Sets the color of the text in the legend. Can be a valid color string\n",
      "            (for example, 'red'), or a list of color strings. The labelcolor can\n",
      "            also be made to match the color of the line or marker using 'linecolor',\n",
      "            'markerfacecolor' (or 'mfc'), or 'markeredgecolor' (or 'mec').\n",
      "        \n",
      "        numpoints : int, default: :rc:`legend.numpoints`\n",
      "            The number of marker points in the legend when creating a legend\n",
      "            entry for a `.Line2D` (line).\n",
      "        \n",
      "        scatterpoints : int, default: :rc:`legend.scatterpoints`\n",
      "            The number of marker points in the legend when creating\n",
      "            a legend entry for a `.PathCollection` (scatter plot).\n",
      "        \n",
      "        scatteryoffsets : iterable of floats, default: ``[0.375, 0.5, 0.3125]``\n",
      "            The vertical offset (relative to the font size) for the markers\n",
      "            created for a scatter plot legend entry. 0.0 is at the base the\n",
      "            legend text, and 1.0 is at the top. To draw all markers at the\n",
      "            same height, set to ``[0.5]``.\n",
      "        \n",
      "        markerscale : float, default: :rc:`legend.markerscale`\n",
      "            The relative size of legend markers compared with the originally\n",
      "            drawn ones.\n",
      "        \n",
      "        markerfirst : bool, default: True\n",
      "            If *True*, legend marker is placed to the left of the legend label.\n",
      "            If *False*, legend marker is placed to the right of the legend label.\n",
      "        \n",
      "        frameon : bool, default: :rc:`legend.frameon`\n",
      "            Whether the legend should be drawn on a patch (frame).\n",
      "        \n",
      "        fancybox : bool, default: :rc:`legend.fancybox`\n",
      "            Whether round edges should be enabled around the `~.FancyBboxPatch` which\n",
      "            makes up the legend's background.\n",
      "        \n",
      "        shadow : bool, default: :rc:`legend.shadow`\n",
      "            Whether to draw a shadow behind the legend.\n",
      "        \n",
      "        framealpha : float, default: :rc:`legend.framealpha`\n",
      "            The alpha transparency of the legend's background.\n",
      "            If *shadow* is activated and *framealpha* is ``None``, the default value is\n",
      "            ignored.\n",
      "        \n",
      "        facecolor : \"inherit\" or color, default: :rc:`legend.facecolor`\n",
      "            The legend's background color.\n",
      "            If ``\"inherit\"``, use :rc:`axes.facecolor`.\n",
      "        \n",
      "        edgecolor : \"inherit\" or color, default: :rc:`legend.edgecolor`\n",
      "            The legend's background patch edge color.\n",
      "            If ``\"inherit\"``, use take :rc:`axes.edgecolor`.\n",
      "        \n",
      "        mode : {\"expand\", None}\n",
      "            If *mode* is set to ``\"expand\"`` the legend will be horizontally\n",
      "            expanded to fill the axes area (or *bbox_to_anchor* if defines\n",
      "            the legend's size).\n",
      "        \n",
      "        bbox_transform : None or `matplotlib.transforms.Transform`\n",
      "            The transform for the bounding box (*bbox_to_anchor*). For a value\n",
      "            of ``None`` (default) the Axes'\n",
      "            :data:`~matplotlib.axes.Axes.transAxes` transform will be used.\n",
      "        \n",
      "        title : str or None\n",
      "            The legend's title. Default is no title (``None``).\n",
      "        \n",
      "        title_fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}, default: :rc:`legend.title_fontsize`\n",
      "            The font size of the legend's title.\n",
      "        \n",
      "        borderpad : float, default: :rc:`legend.borderpad`\n",
      "            The fractional whitespace inside the legend border, in font-size units.\n",
      "        \n",
      "        labelspacing : float, default: :rc:`legend.labelspacing`\n",
      "            The vertical space between the legend entries, in font-size units.\n",
      "        \n",
      "        handlelength : float, default: :rc:`legend.handlelength`\n",
      "            The length of the legend handles, in font-size units.\n",
      "        \n",
      "        handletextpad : float, default: :rc:`legend.handletextpad`\n",
      "            The pad between the legend handle and text, in font-size units.\n",
      "        \n",
      "        borderaxespad : float, default: :rc:`legend.borderaxespad`\n",
      "            The pad between the axes and legend border, in font-size units.\n",
      "        \n",
      "        columnspacing : float, default: :rc:`legend.columnspacing`\n",
      "            The spacing between columns, in font-size units.\n",
      "        \n",
      "        handler_map : dict or None\n",
      "            The custom dictionary mapping instances or types to a legend\n",
      "            handler. This *handler_map* updates the default handler map\n",
      "            found at `matplotlib.legend.Legend.get_legend_handler_map`.\n",
      "        \n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Some artists are not supported by this function.  See\n",
      "        :doc:`/tutorials/intermediate/legend_guide` for details.\n",
      "    \n",
      "    fignum_exists(num)\n",
      "        Return whether the figure with the given id exists.\n",
      "    \n",
      "    figtext(x, y, s, fontdict=None, **kwargs)\n",
      "        Add text to figure.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : float\n",
      "            The position to place the text. By default, this is in figure\n",
      "            coordinates, floats in [0, 1]. The coordinate system can be changed\n",
      "            using the *transform* keyword.\n",
      "        \n",
      "        s : str\n",
      "            The text string.\n",
      "        \n",
      "        fontdict : dict, optional\n",
      "            A dictionary to override the default text properties. If not given,\n",
      "            the defaults are determined by :rc:`font.*`. Properties passed as\n",
      "            *kwargs* override the corresponding ones given in *fontdict*.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `~.text.Text`\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs : `~matplotlib.text.Text` properties\n",
      "            Other miscellaneous text parameters.\n",
      "        \n",
      "            Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            backgroundcolor: color\n",
      "            bbox: dict with properties for `.patches.FancyBboxPatch`\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            color or c: color\n",
      "            contains: unknown\n",
      "            figure: `.Figure`\n",
      "            fontfamily or family: {FONTNAME, 'serif', 'sans-serif', 'cursive', 'fantasy', 'monospace'}\n",
      "            fontproperties or font or font_properties: `.font_manager.FontProperties` or `str` or `pathlib.Path`\n",
      "            fontsize or size: float or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n",
      "            fontstretch or stretch: {a numeric value in range 0-1000, 'ultra-condensed', 'extra-condensed', 'condensed', 'semi-condensed', 'normal', 'semi-expanded', 'expanded', 'extra-expanded', 'ultra-expanded'}\n",
      "            fontstyle or style: {'normal', 'italic', 'oblique'}\n",
      "            fontvariant or variant: {'normal', 'small-caps'}\n",
      "            fontweight or weight: {a numeric value in range 0-1000, 'ultralight', 'light', 'normal', 'regular', 'book', 'medium', 'roman', 'semibold', 'demibold', 'demi', 'bold', 'heavy', 'extra bold', 'black'}\n",
      "            gid: str\n",
      "            horizontalalignment or ha: {'center', 'right', 'left'}\n",
      "            in_layout: bool\n",
      "            label: object\n",
      "            linespacing: float (multiple of font size)\n",
      "            multialignment or ma: {'left', 'right', 'center'}\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: None or bool or callable\n",
      "            position: (float, float)\n",
      "            rasterized: bool or None\n",
      "            rotation: float or {'vertical', 'horizontal'}\n",
      "            rotation_mode: {None, 'default', 'anchor'}\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            text: object\n",
      "            transform: `.Transform`\n",
      "            url: str\n",
      "            usetex: bool or None\n",
      "            verticalalignment or va: {'center', 'top', 'bottom', 'baseline', 'center_baseline'}\n",
      "            visible: bool\n",
      "            wrap: bool\n",
      "            x: float\n",
      "            y: float\n",
      "            zorder: float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        .Axes.text\n",
      "        .pyplot.text\n",
      "    \n",
      "    figure(num=None, figsize=None, dpi=None, facecolor=None, edgecolor=None, frameon=True, FigureClass=<class 'matplotlib.figure.Figure'>, clear=False, **kwargs)\n",
      "        Create a new figure, or activate an existing figure.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        num : int or str, optional\n",
      "            A unique identifier for the figure.\n",
      "        \n",
      "            If a figure with that identifier already exists, this figure is made\n",
      "            active and returned. An integer refers to the ``Figure.number``\n",
      "            attribute, a string refers to the figure label.\n",
      "        \n",
      "            If there is no figure with the identifier or *num* is not given, a new\n",
      "            figure is created, made active and returned.  If *num* is an int, it\n",
      "            will be used for the ``Figure.number`` attribute, otherwise, an\n",
      "            auto-generated integer value is used (starting at 1 and incremented\n",
      "            for each new figure). If *num* is a string, the figure label and the\n",
      "            window title is set to this value.\n",
      "        \n",
      "        figsize : (float, float), default: :rc:`figure.figsize`\n",
      "            Width, height in inches.\n",
      "        \n",
      "        dpi : float, default: :rc:`figure.dpi`\n",
      "            The resolution of the figure in dots-per-inch.\n",
      "        \n",
      "        facecolor : color, default: :rc:`figure.facecolor`\n",
      "            The background color.\n",
      "        \n",
      "        edgecolor : color, default: :rc:`figure.edgecolor`\n",
      "            The border color.\n",
      "        \n",
      "        frameon : bool, default: True\n",
      "            If False, suppress drawing the figure frame.\n",
      "        \n",
      "        FigureClass : subclass of `~matplotlib.figure.Figure`\n",
      "            Optionally use a custom `.Figure` instance.\n",
      "        \n",
      "        clear : bool, default: False\n",
      "            If True and the figure already exists, then it is cleared.\n",
      "        \n",
      "        tight_layout : bool or dict, default: :rc:`figure.autolayout`\n",
      "            If ``False`` use *subplotpars*. If ``True`` adjust subplot\n",
      "            parameters using `.tight_layout` with default padding.\n",
      "            When providing a dict containing the keys ``pad``, ``w_pad``,\n",
      "            ``h_pad``, and ``rect``, the default `.tight_layout` paddings\n",
      "            will be overridden.\n",
      "        \n",
      "        constrained_layout : bool, default: :rc:`figure.constrained_layout.use`\n",
      "            If ``True`` use constrained layout to adjust positioning of plot\n",
      "            elements.  Like ``tight_layout``, but designed to be more\n",
      "            flexible.  See\n",
      "            :doc:`/tutorials/intermediate/constrainedlayout_guide`\n",
      "            for examples.  (Note: does not work with `add_subplot` or\n",
      "            `~.pyplot.subplot2grid`.)\n",
      "        \n",
      "        \n",
      "        **kwargs :optional\n",
      "            See `~.matplotlib.figure.Figure` for other possible arguments.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `~matplotlib.figure.Figure`\n",
      "            The `.Figure` instance returned will also be passed to\n",
      "            new_figure_manager in the backends, which allows to hook custom\n",
      "            `.Figure` classes into the pyplot interface. Additional kwargs will be\n",
      "            passed to the `.Figure` init function.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If you are creating many figures, make sure you explicitly call\n",
      "        `.pyplot.close` on the figures you are not using, because this will\n",
      "        enable pyplot to properly clean up the memory.\n",
      "        \n",
      "        `~matplotlib.rcParams` defines the default values, which can be modified\n",
      "        in the matplotlibrc file.\n",
      "    \n",
      "    fill(*args, data=None, **kwargs)\n",
      "        Plot filled polygons.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        *args : sequence of x, y, [color]\n",
      "            Each polygon is defined by the lists of *x* and *y* positions of\n",
      "            its nodes, optionally followed by a *color* specifier. See\n",
      "            :mod:`matplotlib.colors` for supported color specifiers. The\n",
      "            standard color cycle is used for polygons without a color\n",
      "            specifier.\n",
      "        \n",
      "            You can plot multiple polygons by providing multiple *x*, *y*,\n",
      "            *[color]* groups.\n",
      "        \n",
      "            For example, each of the following is legal::\n",
      "        \n",
      "                ax.fill(x, y)                    # a polygon with default color\n",
      "                ax.fill(x, y, \"b\")               # a blue polygon\n",
      "                ax.fill(x, y, x2, y2)            # two polygons\n",
      "                ax.fill(x, y, \"b\", x2, y2, \"r\")  # a blue and a red polygon\n",
      "        \n",
      "        data : indexable object, optional\n",
      "            An object with labelled data. If given, provide the label names to\n",
      "            plot in *x* and *y*, e.g.::\n",
      "        \n",
      "                ax.fill(\"time\", \"signal\",\n",
      "                        data={\"time\": [0, 1, 2], \"signal\": [0, 1, 0]})\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        list of `~matplotlib.patches.Polygon`\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs : `~matplotlib.patches.Polygon` properties\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Use :meth:`fill_between` if you would like to fill the region between\n",
      "        two curves.\n",
      "    \n",
      "    fill_between(x, y1, y2=0, where=None, interpolate=False, step=None, *, data=None, **kwargs)\n",
      "        Fill the area between two horizontal curves.\n",
      "        \n",
      "        The curves are defined by the points (*x*, *y1*) and (*x*,\n",
      "        *y2*).  This creates one or multiple polygons describing the filled\n",
      "        area.\n",
      "        \n",
      "        You may exclude some horizontal sections from filling using *where*.\n",
      "        \n",
      "        By default, the edges connect the given points directly.  Use *step*\n",
      "        if the filling should be a step function, i.e. constant in between\n",
      "        *x*.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array (length N)\n",
      "            The x coordinates of the nodes defining the curves.\n",
      "        \n",
      "        y1 : array (length N) or scalar\n",
      "            The y coordinates of the nodes defining the first curve.\n",
      "        \n",
      "        y2 : array (length N) or scalar, default: 0\n",
      "            The y coordinates of the nodes defining the second curve.\n",
      "        \n",
      "        where : array of bool (length N), optional\n",
      "            Define *where* to exclude some horizontal regions from being filled.\n",
      "            The filled regions are defined by the coordinates ``x[where]``.\n",
      "            More precisely, fill between ``x[i]`` and ``x[i+1]`` if\n",
      "            ``where[i] and where[i+1]``.  Note that this definition implies\n",
      "            that an isolated *True* value between two *False* values in *where*\n",
      "            will not result in filling.  Both sides of the *True* position\n",
      "            remain unfilled due to the adjacent *False* values.\n",
      "        \n",
      "        interpolate : bool, default: False\n",
      "            This option is only relevant if *where* is used and the two curves\n",
      "            are crossing each other.\n",
      "        \n",
      "            Semantically, *where* is often used for *y1* > *y2* or\n",
      "            similar.  By default, the nodes of the polygon defining the filled\n",
      "            region will only be placed at the positions in the *x* array.\n",
      "            Such a polygon cannot describe the above semantics close to the\n",
      "            intersection.  The x-sections containing the intersection are\n",
      "            simply clipped.\n",
      "        \n",
      "            Setting *interpolate* to *True* will calculate the actual\n",
      "            intersection point and extend the filled region up to this point.\n",
      "        \n",
      "        step : {'pre', 'post', 'mid'}, optional\n",
      "            Define *step* if the filling should be a step function,\n",
      "            i.e. constant in between *x*.  The value determines where the\n",
      "            step will occur:\n",
      "        \n",
      "            - 'pre': The y value is continued constantly to the left from\n",
      "              every *x* position, i.e. the interval ``(x[i-1], x[i]]`` has the\n",
      "              value ``y[i]``.\n",
      "            - 'post': The y value is continued constantly to the right from\n",
      "              every *x* position, i.e. the interval ``[x[i], x[i+1])`` has the\n",
      "              value ``y[i]``.\n",
      "            - 'mid': Steps occur half-way between the *x* positions.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `.PolyCollection`\n",
      "            A `.PolyCollection` containing the plotted polygons.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            All other keyword arguments are passed on to `.PolyCollection`.\n",
      "            They control the `.Polygon` properties:\n",
      "        \n",
      "            Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa or antialiaseds: bool or list of bools\n",
      "            array: ndarray\n",
      "            capstyle: {'butt', 'round', 'projecting'}\n",
      "            clim: (vmin: float, vmax: float)\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            cmap: `.Colormap` or str or None\n",
      "            color: color or list of rgba tuples\n",
      "            contains: unknown\n",
      "            edgecolor or ec or edgecolors: color or list of colors or 'face'\n",
      "            facecolor or facecolors or fc: color or list of colors\n",
      "            figure: `.Figure`\n",
      "            gid: str\n",
      "            hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n",
      "            in_layout: bool\n",
      "            joinstyle: {'miter', 'round', 'bevel'}\n",
      "            label: object\n",
      "            linestyle or dashes or linestyles or ls: str or tuple or list thereof\n",
      "            linewidth or linewidths or lw: float or list of floats\n",
      "            norm: `.Normalize` or None\n",
      "            offset_position: unknown\n",
      "            offsets: array-like (N, 2) or (2,)\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: None or bool or callable\n",
      "            pickradius: unknown\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            transform: `.Transform`\n",
      "            url: str\n",
      "            urls: list of str or None\n",
      "            visible: bool\n",
      "            zorder: float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        fill_between : Fill between two sets of y-values.\n",
      "        fill_betweenx : Fill between two sets of x-values.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. [notes section required to get data note injection right]\n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            the following arguments can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception):\n",
      "            *x*, *y1*, *y2*, *where*.\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    fill_betweenx(y, x1, x2=0, where=None, step=None, interpolate=False, *, data=None, **kwargs)\n",
      "        Fill the area between two vertical curves.\n",
      "        \n",
      "        The curves are defined by the points (*y*, *x1*) and (*y*,\n",
      "        *x2*).  This creates one or multiple polygons describing the filled\n",
      "        area.\n",
      "        \n",
      "        You may exclude some vertical sections from filling using *where*.\n",
      "        \n",
      "        By default, the edges connect the given points directly.  Use *step*\n",
      "        if the filling should be a step function, i.e. constant in between\n",
      "        *y*.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : array (length N)\n",
      "            The y coordinates of the nodes defining the curves.\n",
      "        \n",
      "        x1 : array (length N) or scalar\n",
      "            The x coordinates of the nodes defining the first curve.\n",
      "        \n",
      "        x2 : array (length N) or scalar, default: 0\n",
      "            The x coordinates of the nodes defining the second curve.\n",
      "        \n",
      "        where : array of bool (length N), optional\n",
      "            Define *where* to exclude some vertical regions from being filled.\n",
      "            The filled regions are defined by the coordinates ``y[where]``.\n",
      "            More precisely, fill between ``y[i]`` and ``y[i+1]`` if\n",
      "            ``where[i] and where[i+1]``.  Note that this definition implies\n",
      "            that an isolated *True* value between two *False* values in *where*\n",
      "            will not result in filling.  Both sides of the *True* position\n",
      "            remain unfilled due to the adjacent *False* values.\n",
      "        \n",
      "        interpolate : bool, default: False\n",
      "            This option is only relevant if *where* is used and the two curves\n",
      "            are crossing each other.\n",
      "        \n",
      "            Semantically, *where* is often used for *x1* > *x2* or\n",
      "            similar.  By default, the nodes of the polygon defining the filled\n",
      "            region will only be placed at the positions in the *y* array.\n",
      "            Such a polygon cannot describe the above semantics close to the\n",
      "            intersection.  The y-sections containing the intersection are\n",
      "            simply clipped.\n",
      "        \n",
      "            Setting *interpolate* to *True* will calculate the actual\n",
      "            intersection point and extend the filled region up to this point.\n",
      "        \n",
      "        step : {'pre', 'post', 'mid'}, optional\n",
      "            Define *step* if the filling should be a step function,\n",
      "            i.e. constant in between *y*.  The value determines where the\n",
      "            step will occur:\n",
      "        \n",
      "            - 'pre': The y value is continued constantly to the left from\n",
      "              every *x* position, i.e. the interval ``(x[i-1], x[i]]`` has the\n",
      "              value ``y[i]``.\n",
      "            - 'post': The y value is continued constantly to the right from\n",
      "              every *x* position, i.e. the interval ``[x[i], x[i+1])`` has the\n",
      "              value ``y[i]``.\n",
      "            - 'mid': Steps occur half-way between the *x* positions.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `.PolyCollection`\n",
      "            A `.PolyCollection` containing the plotted polygons.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            All other keyword arguments are passed on to `.PolyCollection`.\n",
      "            They control the `.Polygon` properties:\n",
      "        \n",
      "            Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa or antialiaseds: bool or list of bools\n",
      "            array: ndarray\n",
      "            capstyle: {'butt', 'round', 'projecting'}\n",
      "            clim: (vmin: float, vmax: float)\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            cmap: `.Colormap` or str or None\n",
      "            color: color or list of rgba tuples\n",
      "            contains: unknown\n",
      "            edgecolor or ec or edgecolors: color or list of colors or 'face'\n",
      "            facecolor or facecolors or fc: color or list of colors\n",
      "            figure: `.Figure`\n",
      "            gid: str\n",
      "            hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n",
      "            in_layout: bool\n",
      "            joinstyle: {'miter', 'round', 'bevel'}\n",
      "            label: object\n",
      "            linestyle or dashes or linestyles or ls: str or tuple or list thereof\n",
      "            linewidth or linewidths or lw: float or list of floats\n",
      "            norm: `.Normalize` or None\n",
      "            offset_position: unknown\n",
      "            offsets: array-like (N, 2) or (2,)\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: None or bool or callable\n",
      "            pickradius: unknown\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            transform: `.Transform`\n",
      "            url: str\n",
      "            urls: list of str or None\n",
      "            visible: bool\n",
      "            zorder: float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        fill_between : Fill between two sets of y-values.\n",
      "        fill_betweenx : Fill between two sets of x-values.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. [notes section required to get data note injection right]\n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            the following arguments can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception):\n",
      "            *y*, *x1*, *x2*, *where*.\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    findobj(o=None, match=None, include_self=True)\n",
      "        Find artist objects.\n",
      "        \n",
      "        Recursively find all `.Artist` instances contained in the artist.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        match\n",
      "            A filter criterion for the matches. This can be\n",
      "        \n",
      "            - *None*: Return all objects contained in artist.\n",
      "            - A function with signature ``def match(artist: Artist) -> bool``.\n",
      "              The result will only contain artists for which the function\n",
      "              returns *True*.\n",
      "            - A class instance: e.g., `.Line2D`. The result will only contain\n",
      "              artists of this class or its subclasses (``isinstance`` check).\n",
      "        \n",
      "        include_self : bool\n",
      "            Include *self* in the list to be checked for a match.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        list of `.Artist`\n",
      "    \n",
      "    flag()\n",
      "        Set the colormap to \"flag\".\n",
      "        \n",
      "        This changes the default colormap as well as the colormap of the current\n",
      "        image if there is one. See ``help(colormaps)`` for more information.\n",
      "    \n",
      "    gca(**kwargs)\n",
      "        Get the current axes, creating one if necessary.\n",
      "        \n",
      "        The following kwargs are supported for ensuring the returned axes\n",
      "        adheres to the given projection etc., and for axes creation if\n",
      "        the active axes does not exist:\n",
      "        \n",
      "        Properties:\n",
      "            adjustable: {'box', 'datalim'}\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            anchor: 2-tuple of floats or {'C', 'SW', 'S', 'SE', ...}\n",
      "            animated: bool\n",
      "            aspect: {'auto'} or num\n",
      "            autoscale_on: bool\n",
      "            autoscalex_on: bool\n",
      "            autoscaley_on: bool\n",
      "            axes_locator: Callable[[Axes, Renderer], Bbox]\n",
      "            axisbelow: bool or 'line'\n",
      "            box_aspect: None, or a number\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            contains: unknown\n",
      "            facecolor or fc: color\n",
      "            figure: `.Figure`\n",
      "            frame_on: bool\n",
      "            gid: str\n",
      "            in_layout: bool\n",
      "            label: object\n",
      "            navigate: bool\n",
      "            navigate_mode: unknown\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: None or bool or callable\n",
      "            position: [left, bottom, width, height] or `~matplotlib.transforms.Bbox`\n",
      "            prop_cycle: unknown\n",
      "            rasterization_zorder: float or None\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            title: str\n",
      "            transform: `.Transform`\n",
      "            url: str\n",
      "            visible: bool\n",
      "            xbound: unknown\n",
      "            xlabel: str\n",
      "            xlim: (bottom: float, top: float)\n",
      "            xmargin: float greater than -0.5\n",
      "            xscale: {\"linear\", \"log\", \"symlog\", \"logit\", ...}\n",
      "            xticklabels: unknown\n",
      "            xticks: unknown\n",
      "            ybound: unknown\n",
      "            ylabel: str\n",
      "            ylim: (bottom: float, top: float)\n",
      "            ymargin: float greater than -0.5\n",
      "            yscale: {\"linear\", \"log\", \"symlog\", \"logit\", ...}\n",
      "            yticklabels: unknown\n",
      "            yticks: unknown\n",
      "            zorder: float\n",
      "    \n",
      "    gcf()\n",
      "        Get the current figure.\n",
      "        \n",
      "        If no current figure exists, a new one is created using\n",
      "        `~.pyplot.figure()`.\n",
      "    \n",
      "    gci()\n",
      "        Get the current colorable artist.\n",
      "        \n",
      "        Specifically, returns the current `.ScalarMappable` instance (`.Image`\n",
      "        created by `imshow` or `figimage`, `.Collection` created by `pcolor` or\n",
      "        `scatter`, etc.), or *None* if no such instance has been defined.\n",
      "        \n",
      "        The current image is an attribute of the current axes, or the nearest\n",
      "        earlier axes in the current figure that contains an image.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Historically, the only colorable artists were images; hence the name\n",
      "        ``gci`` (get current image).\n",
      "    \n",
      "    get(obj, *args, **kwargs)\n",
      "        Return the value of an object's *property*, or print all of them.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        obj : `.Artist`\n",
      "            The queried artist; e.g., a `.Line2D`, a `.Text`, or an `~.axes.Axes`.\n",
      "        \n",
      "        property : str or None, default: None\n",
      "            If *property* is 'somename', this function returns\n",
      "            ``obj.get_somename()``.\n",
      "        \n",
      "            If is is None (or unset), it *prints* all gettable properties from\n",
      "            *obj*.  Many properties have aliases for shorter typing, e.g. 'lw' is\n",
      "            an alias for 'linewidth'.  In the output, aliases and full property\n",
      "            names will be listed as:\n",
      "        \n",
      "              property or alias = value\n",
      "        \n",
      "            e.g.:\n",
      "        \n",
      "              linewidth or lw = 2\n",
      "    \n",
      "    get_current_fig_manager()\n",
      "        Return the figure manager of the current figure.\n",
      "        \n",
      "        The figure manager is a container for the actual backend-depended window\n",
      "        that displays the figure on screen.\n",
      "        \n",
      "        If if no current figure exists, a new one is created an its figure\n",
      "        manager is returned.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `.FigureManagerBase` or backend-dependent subclass thereof\n",
      "    \n",
      "    get_figlabels()\n",
      "        Return a list of existing figure labels.\n",
      "    \n",
      "    get_fignums()\n",
      "        Return a list of existing figure numbers.\n",
      "    \n",
      "    get_plot_commands()\n",
      "        Get a sorted list of all of the plotting commands.\n",
      "    \n",
      "    getp(obj, *args, **kwargs)\n",
      "        Return the value of an object's *property*, or print all of them.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        obj : `.Artist`\n",
      "            The queried artist; e.g., a `.Line2D`, a `.Text`, or an `~.axes.Axes`.\n",
      "        \n",
      "        property : str or None, default: None\n",
      "            If *property* is 'somename', this function returns\n",
      "            ``obj.get_somename()``.\n",
      "        \n",
      "            If is is None (or unset), it *prints* all gettable properties from\n",
      "            *obj*.  Many properties have aliases for shorter typing, e.g. 'lw' is\n",
      "            an alias for 'linewidth'.  In the output, aliases and full property\n",
      "            names will be listed as:\n",
      "        \n",
      "              property or alias = value\n",
      "        \n",
      "            e.g.:\n",
      "        \n",
      "              linewidth or lw = 2\n",
      "    \n",
      "    ginput(n=1, timeout=30, show_clicks=True, mouse_add=<MouseButton.LEFT: 1>, mouse_pop=<MouseButton.RIGHT: 3>, mouse_stop=<MouseButton.MIDDLE: 2>)\n",
      "        Blocking call to interact with a figure.\n",
      "        \n",
      "        Wait until the user clicks *n* times on the figure, and return the\n",
      "        coordinates of each click in a list.\n",
      "        \n",
      "        There are three possible interactions:\n",
      "        \n",
      "        - Add a point.\n",
      "        - Remove the most recently added point.\n",
      "        - Stop the interaction and return the points added so far.\n",
      "        \n",
      "        The actions are assigned to mouse buttons via the arguments\n",
      "        *mouse_add*, *mouse_pop* and *mouse_stop*.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int, default: 1\n",
      "            Number of mouse clicks to accumulate. If negative, accumulate\n",
      "            clicks until the input is terminated manually.\n",
      "        timeout : float, default: 30 seconds\n",
      "            Number of seconds to wait before timing out. If zero or negative\n",
      "            will never timeout.\n",
      "        show_clicks : bool, default: True\n",
      "            If True, show a red cross at the location of each click.\n",
      "        mouse_add : `.MouseButton` or None, default: `.MouseButton.LEFT`\n",
      "            Mouse button used to add points.\n",
      "        mouse_pop : `.MouseButton` or None, default: `.MouseButton.RIGHT`\n",
      "            Mouse button used to remove the most recently added point.\n",
      "        mouse_stop : `.MouseButton` or None, default: `.MouseButton.MIDDLE`\n",
      "            Mouse button used to stop input.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        list of tuples\n",
      "            A list of the clicked (x, y) coordinates.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The keyboard can also be used to select points in case your mouse\n",
      "        does not have one or more of the buttons.  The delete and backspace\n",
      "        keys act like right clicking (i.e., remove last point), the enter key\n",
      "        terminates input and any other key (not already used by the window\n",
      "        manager) selects a point.\n",
      "    \n",
      "    gray()\n",
      "        Set the colormap to \"gray\".\n",
      "        \n",
      "        This changes the default colormap as well as the colormap of the current\n",
      "        image if there is one. See ``help(colormaps)`` for more information.\n",
      "    \n",
      "    grid(b=None, which='major', axis='both', **kwargs)\n",
      "        Configure the grid lines.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        b : bool or None, optional\n",
      "            Whether to show the grid lines. If any *kwargs* are supplied,\n",
      "            it is assumed you want the grid on and *b* will be set to True.\n",
      "        \n",
      "            If *b* is *None* and there are no *kwargs*, this toggles the\n",
      "            visibility of the lines.\n",
      "        \n",
      "        which : {'major', 'minor', 'both'}, optional\n",
      "            The grid lines to apply the changes on.\n",
      "        \n",
      "        axis : {'both', 'x', 'y'}, optional\n",
      "            The axis to apply the changes on.\n",
      "        \n",
      "        **kwargs : `.Line2D` properties\n",
      "            Define the line properties of the grid, e.g.::\n",
      "        \n",
      "                grid(color='r', linestyle='-', linewidth=2)\n",
      "        \n",
      "            Valid keyword arguments are:\n",
      "        \n",
      "            Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa: bool\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            color or c: color\n",
      "            contains: unknown\n",
      "            dash_capstyle: {'butt', 'round', 'projecting'}\n",
      "            dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      "            data: (2, N) array or two 1D arrays\n",
      "            drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      "            figure: `.Figure`\n",
      "            fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      "            gid: str\n",
      "            in_layout: bool\n",
      "            label: object\n",
      "            linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      "            linewidth or lw: float\n",
      "            marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      "            markeredgecolor or mec: color\n",
      "            markeredgewidth or mew: float\n",
      "            markerfacecolor or mfc: color\n",
      "            markerfacecoloralt or mfcalt: color\n",
      "            markersize or ms: float\n",
      "            markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: unknown\n",
      "            pickradius: float\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            solid_capstyle: {'butt', 'round', 'projecting'}\n",
      "            solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            transform: `matplotlib.transforms.Transform`\n",
      "            url: str\n",
      "            visible: bool\n",
      "            xdata: 1D array\n",
      "            ydata: 1D array\n",
      "            zorder: float\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The axis is drawn as a unit, so the effective zorder for drawing the\n",
      "        grid is determined by the zorder of each axis, not by the zorder of the\n",
      "        `.Line2D` objects comprising the grid.  Therefore, to set grid zorder,\n",
      "        use `.set_axisbelow` or, for more control, call the\n",
      "        `~.Artist.set_zorder` method of each axis.\n",
      "    \n",
      "    hexbin(x, y, C=None, gridsize=100, bins=None, xscale='linear', yscale='linear', extent=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, edgecolors='face', reduce_C_function=<function mean at 0x1047790d0>, mincnt=None, marginals=False, *, data=None, **kwargs)\n",
      "        Make a 2D hexagonal binning plot of points *x*, *y*.\n",
      "        \n",
      "        If *C* is *None*, the value of the hexagon is determined by the number\n",
      "        of points in the hexagon. Otherwise, *C* specifies values at the\n",
      "        coordinate (x[i], y[i]). For each hexagon, these values are reduced\n",
      "        using *reduce_C_function*.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array-like\n",
      "            The data positions. *x* and *y* must be of the same length.\n",
      "        \n",
      "        C : array-like, optional\n",
      "            If given, these values are accumulated in the bins. Otherwise,\n",
      "            every point has a value of 1. Must be of the same length as *x*\n",
      "            and *y*.\n",
      "        \n",
      "        gridsize : int or (int, int), default: 100\n",
      "            If a single int, the number of hexagons in the *x*-direction.\n",
      "            The number of hexagons in the *y*-direction is chosen such that\n",
      "            the hexagons are approximately regular.\n",
      "        \n",
      "            Alternatively, if a tuple (*nx*, *ny*), the number of hexagons\n",
      "            in the *x*-direction and the *y*-direction.\n",
      "        \n",
      "        bins : 'log' or int or sequence, default: None\n",
      "            Discretization of the hexagon values.\n",
      "        \n",
      "            - If *None*, no binning is applied; the color of each hexagon\n",
      "              directly corresponds to its count value.\n",
      "            - If 'log', use a logarithmic scale for the color map.\n",
      "              Internally, :math:`log_{10}(i+1)` is used to determine the\n",
      "              hexagon color. This is equivalent to ``norm=LogNorm()``.\n",
      "            - If an integer, divide the counts in the specified number\n",
      "              of bins, and color the hexagons accordingly.\n",
      "            - If a sequence of values, the values of the lower bound of\n",
      "              the bins to be used.\n",
      "        \n",
      "        xscale : {'linear', 'log'}, default: 'linear'\n",
      "            Use a linear or log10 scale on the horizontal axis.\n",
      "        \n",
      "        yscale : {'linear', 'log'}, default: 'linear'\n",
      "            Use a linear or log10 scale on the vertical axis.\n",
      "        \n",
      "        mincnt : int > 0, default: *None*\n",
      "            If not *None*, only display cells with more than *mincnt*\n",
      "            number of points in the cell.\n",
      "        \n",
      "        marginals : bool, default: *False*\n",
      "            If marginals is *True*, plot the marginal density as\n",
      "            colormapped rectangles along the bottom of the x-axis and\n",
      "            left of the y-axis.\n",
      "        \n",
      "        extent : float, default: *None*\n",
      "            The limits of the bins. The default assigns the limits\n",
      "            based on *gridsize*, *x*, *y*, *xscale* and *yscale*.\n",
      "        \n",
      "            If *xscale* or *yscale* is set to 'log', the limits are\n",
      "            expected to be the exponent for a power of 10. E.g. for\n",
      "            x-limits of 1 and 50 in 'linear' scale and y-limits\n",
      "            of 10 and 1000 in 'log' scale, enter (1, 50, 1, 3).\n",
      "        \n",
      "            Order of scalars is (left, right, bottom, top).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `~matplotlib.collections.PolyCollection`\n",
      "            A `.PolyCollection` defining the hexagonal bins.\n",
      "        \n",
      "            - `.PolyCollection.get_offsets` contains a Mx2 array containing\n",
      "              the x, y positions of the M hexagon centers.\n",
      "            - `.PolyCollection.get_array` contains the values of the M\n",
      "              hexagons.\n",
      "        \n",
      "            If *marginals* is *True*, horizontal\n",
      "            bar and vertical bar (both PolyCollections) will be attached\n",
      "            to the return collection as attributes *hbar* and *vbar*.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        cmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n",
      "            The Colormap instance or registered colormap name used to map\n",
      "            the bin values to colors.\n",
      "        \n",
      "        norm : `~matplotlib.colors.Normalize`, optional\n",
      "            The Normalize instance scales the bin values to the canonical\n",
      "            colormap range [0, 1] for mapping to colors. By default, the data\n",
      "            range is mapped to the colorbar range using linear scaling.\n",
      "        \n",
      "        vmin, vmax : float, default: None\n",
      "            The colorbar range. If *None*, suitable min/max values are\n",
      "            automatically chosen by the `~.Normalize` instance (defaults to\n",
      "            the respective min/max values of the bins in case of the default\n",
      "            linear scaling).\n",
      "            It is deprecated to use *vmin*/*vmax* when *norm* is given.\n",
      "        \n",
      "        alpha : float between 0 and 1, optional\n",
      "            The alpha blending value, between 0 (transparent) and 1 (opaque).\n",
      "        \n",
      "        linewidths : float, default: *None*\n",
      "            If *None*, defaults to 1.0.\n",
      "        \n",
      "        edgecolors : {'face', 'none', *None*} or color, default: 'face'\n",
      "            The color of the hexagon edges. Possible values are:\n",
      "        \n",
      "            - 'face': Draw the edges in the same color as the fill color.\n",
      "            - 'none': No edges are drawn. This can sometimes lead to unsightly\n",
      "              unpainted pixels between the hexagons.\n",
      "            - *None*: Draw outlines in the default color.\n",
      "            - An explicit color.\n",
      "        \n",
      "        reduce_C_function : callable, default: `numpy.mean`\n",
      "            The function to aggregate *C* within the bins. It is ignored if\n",
      "            *C* is not given. This must have the signature::\n",
      "        \n",
      "                def reduce_C_function(C: array) -> float\n",
      "        \n",
      "            Commonly used functions are:\n",
      "        \n",
      "            - `numpy.mean`: average of the points\n",
      "            - `numpy.sum`: integral of the point values\n",
      "            - `numpy.max`: value taken from the largest point\n",
      "        \n",
      "        **kwargs : `~matplotlib.collections.PolyCollection` properties\n",
      "            All other keyword arguments are passed on to `.PolyCollection`:\n",
      "        \n",
      "            Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa or antialiaseds: bool or list of bools\n",
      "            array: ndarray\n",
      "            capstyle: {'butt', 'round', 'projecting'}\n",
      "            clim: (vmin: float, vmax: float)\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            cmap: `.Colormap` or str or None\n",
      "            color: color or list of rgba tuples\n",
      "            contains: unknown\n",
      "            edgecolor or ec or edgecolors: color or list of colors or 'face'\n",
      "            facecolor or facecolors or fc: color or list of colors\n",
      "            figure: `.Figure`\n",
      "            gid: str\n",
      "            hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n",
      "            in_layout: bool\n",
      "            joinstyle: {'miter', 'round', 'bevel'}\n",
      "            label: object\n",
      "            linestyle or dashes or linestyles or ls: str or tuple or list thereof\n",
      "            linewidth or linewidths or lw: float or list of floats\n",
      "            norm: `.Normalize` or None\n",
      "            offset_position: unknown\n",
      "            offsets: array-like (N, 2) or (2,)\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: None or bool or callable\n",
      "            pickradius: unknown\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            transform: `.Transform`\n",
      "            url: str\n",
      "            urls: list of str or None\n",
      "            visible: bool\n",
      "            zorder: float\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            the following arguments can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception):\n",
      "            *x*, *y*.\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    hist(x, bins=None, range=None, density=False, weights=None, cumulative=False, bottom=None, histtype='bar', align='mid', orientation='vertical', rwidth=None, log=False, color=None, label=None, stacked=False, *, data=None, **kwargs)\n",
      "        Plot a histogram.\n",
      "        \n",
      "        Compute and draw the histogram of *x*.  The return value is a tuple\n",
      "        (*n*, *bins*, *patches*) or ([*n0*, *n1*, ...], *bins*, [*patches0*,\n",
      "        *patches1*, ...]) if the input contains multiple data.  See the\n",
      "        documentation of the *weights* parameter to draw a histogram of\n",
      "        already-binned data.\n",
      "        \n",
      "        Multiple data can be provided via *x* as a list of datasets\n",
      "        of potentially different length ([*x0*, *x1*, ...]), or as\n",
      "        a 2-D ndarray in which each column is a dataset.  Note that\n",
      "        the ndarray form is transposed relative to the list form.\n",
      "        \n",
      "        Masked arrays are not supported.\n",
      "        \n",
      "        The *bins*, *range*, *weights*, and *density* parameters behave as in\n",
      "        `numpy.histogram`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : (n,) array or sequence of (n,) arrays\n",
      "            Input values, this takes either a single array or a sequence of\n",
      "            arrays which are not required to be of the same length.\n",
      "        \n",
      "        bins : int or sequence or str, default: :rc:`hist.bins`\n",
      "            If *bins* is an integer, it defines the number of equal-width bins\n",
      "            in the range.\n",
      "        \n",
      "            If *bins* is a sequence, it defines the bin edges, including the\n",
      "            left edge of the first bin and the right edge of the last bin;\n",
      "            in this case, bins may be unequally spaced.  All but the last\n",
      "            (righthand-most) bin is half-open.  In other words, if *bins* is::\n",
      "        \n",
      "                [1, 2, 3, 4]\n",
      "        \n",
      "            then the first bin is ``[1, 2)`` (including 1, but excluding 2) and\n",
      "            the second ``[2, 3)``.  The last bin, however, is ``[3, 4]``, which\n",
      "            *includes* 4.\n",
      "        \n",
      "            If *bins* is a string, it is one of the binning strategies\n",
      "            supported by `numpy.histogram_bin_edges`: 'auto', 'fd', 'doane',\n",
      "            'scott', 'stone', 'rice', 'sturges', or 'sqrt'.\n",
      "        \n",
      "        range : tuple or None, default: None\n",
      "            The lower and upper range of the bins. Lower and upper outliers\n",
      "            are ignored. If not provided, *range* is ``(x.min(), x.max())``.\n",
      "            Range has no effect if *bins* is a sequence.\n",
      "        \n",
      "            If *bins* is a sequence or *range* is specified, autoscaling\n",
      "            is based on the specified bin range instead of the\n",
      "            range of x.\n",
      "        \n",
      "        density : bool, default: False\n",
      "            If ``True``, draw and return a probability density: each bin\n",
      "            will display the bin's raw count divided by the total number of\n",
      "            counts *and the bin width*\n",
      "            (``density = counts / (sum(counts) * np.diff(bins))``),\n",
      "            so that the area under the histogram integrates to 1\n",
      "            (``np.sum(density * np.diff(bins)) == 1``).\n",
      "        \n",
      "            If *stacked* is also ``True``, the sum of the histograms is\n",
      "            normalized to 1.\n",
      "        \n",
      "        weights : (n,) array-like or None, default: None\n",
      "            An array of weights, of the same shape as *x*.  Each value in\n",
      "            *x* only contributes its associated weight towards the bin count\n",
      "            (instead of 1).  If *density* is ``True``, the weights are\n",
      "            normalized, so that the integral of the density over the range\n",
      "            remains 1.\n",
      "        \n",
      "            This parameter can be used to draw a histogram of data that has\n",
      "            already been binned, e.g. using `numpy.histogram` (by treating each\n",
      "            bin as a single point with a weight equal to its count) ::\n",
      "        \n",
      "                counts, bins = np.histogram(data)\n",
      "                plt.hist(bins[:-1], bins, weights=counts)\n",
      "        \n",
      "            (or you may alternatively use `~.bar()`).\n",
      "        \n",
      "        cumulative : bool or -1, default: False\n",
      "            If ``True``, then a histogram is computed where each bin gives the\n",
      "            counts in that bin plus all bins for smaller values. The last bin\n",
      "            gives the total number of datapoints.\n",
      "        \n",
      "            If *density* is also ``True`` then the histogram is normalized such\n",
      "            that the last bin equals 1.\n",
      "        \n",
      "            If *cumulative* is a number less than 0 (e.g., -1), the direction\n",
      "            of accumulation is reversed.  In this case, if *density* is also\n",
      "            ``True``, then the histogram is normalized such that the first bin\n",
      "            equals 1.\n",
      "        \n",
      "        bottom : array-like, scalar, or None, default: None\n",
      "            Location of the bottom of each bin, ie. bins are drawn from\n",
      "            ``bottom`` to ``bottom + hist(x, bins)`` If a scalar, the bottom\n",
      "            of each bin is shifted by the same amount. If an array, each bin\n",
      "            is shifted independently and the length of bottom must match the\n",
      "            number of bins. If None, defaults to 0.\n",
      "        \n",
      "        histtype : {'bar', 'barstacked', 'step', 'stepfilled'}, default: 'bar'\n",
      "            The type of histogram to draw.\n",
      "        \n",
      "            - 'bar' is a traditional bar-type histogram.  If multiple data\n",
      "              are given the bars are arranged side by side.\n",
      "            - 'barstacked' is a bar-type histogram where multiple\n",
      "              data are stacked on top of each other.\n",
      "            - 'step' generates a lineplot that is by default unfilled.\n",
      "            - 'stepfilled' generates a lineplot that is by default filled.\n",
      "        \n",
      "        align : {'left', 'mid', 'right'}, default: 'mid'\n",
      "            The horizontal alignment of the histogram bars.\n",
      "        \n",
      "            - 'left': bars are centered on the left bin edges.\n",
      "            - 'mid': bars are centered between the bin edges.\n",
      "            - 'right': bars are centered on the right bin edges.\n",
      "        \n",
      "        orientation : {'vertical', 'horizontal'}, default: 'vertical'\n",
      "            If 'horizontal', `~.Axes.barh` will be used for bar-type histograms\n",
      "            and the *bottom* kwarg will be the left edges.\n",
      "        \n",
      "        rwidth : float or None, default: None\n",
      "            The relative width of the bars as a fraction of the bin width.  If\n",
      "            ``None``, automatically compute the width.\n",
      "        \n",
      "            Ignored if *histtype* is 'step' or 'stepfilled'.\n",
      "        \n",
      "        log : bool, default: False\n",
      "            If ``True``, the histogram axis will be set to a log scale. If\n",
      "            *log* is ``True`` and *x* is a 1D array, empty bins will be\n",
      "            filtered out and only the non-empty ``(n, bins, patches)``\n",
      "            will be returned.\n",
      "        \n",
      "        color : color or array-like of colors or None, default: None\n",
      "            Color or sequence of colors, one per dataset.  Default (``None``)\n",
      "            uses the standard line color sequence.\n",
      "        \n",
      "        label : str or None, default: None\n",
      "            String, or sequence of strings to match multiple datasets.  Bar\n",
      "            charts yield multiple patches per dataset, but only the first gets\n",
      "            the label, so that `~.Axes.legend` will work as expected.\n",
      "        \n",
      "        stacked : bool, default: False\n",
      "            If ``True``, multiple data are stacked on top of each other If\n",
      "            ``False`` multiple data are arranged side by side if histtype is\n",
      "            'bar' or on top of each other if histtype is 'step'\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        n : array or list of arrays\n",
      "            The values of the histogram bins. See *density* and *weights* for a\n",
      "            description of the possible semantics.  If input *x* is an array,\n",
      "            then this is an array of length *nbins*. If input is a sequence of\n",
      "            arrays ``[data1, data2, ...]``, then this is a list of arrays with\n",
      "            the values of the histograms for each of the arrays in the same\n",
      "            order.  The dtype of the array *n* (or of its element arrays) will\n",
      "            always be float even if no weighting or normalization is used.\n",
      "        \n",
      "        bins : array\n",
      "            The edges of the bins. Length nbins + 1 (nbins left edges and right\n",
      "            edge of last bin).  Always a single array even when multiple data\n",
      "            sets are passed in.\n",
      "        \n",
      "        patches : `.BarContainer` or list of a single `.Polygon` or list of such objects\n",
      "            Container of individual artists used to create the histogram\n",
      "            or list of such containers if there are multiple input datasets.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            `~matplotlib.patches.Patch` properties\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        hist2d : 2D histograms\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For large numbers of bins (>1000), 'step' and 'stepfilled' can be\n",
      "        significantly faster than 'bar' and 'barstacked'.\n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            the following arguments can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception):\n",
      "            *x*, *weights*.\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    hist2d(x, y, bins=10, range=None, density=False, weights=None, cmin=None, cmax=None, *, data=None, **kwargs)\n",
      "        Make a 2D histogram plot.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array-like, shape (n, )\n",
      "            Input values\n",
      "        \n",
      "        bins : None or int or [int, int] or array-like or [array, array]\n",
      "        \n",
      "            The bin specification:\n",
      "        \n",
      "            - If int, the number of bins for the two dimensions\n",
      "              (nx=ny=bins).\n",
      "            - If ``[int, int]``, the number of bins in each dimension\n",
      "              (nx, ny = bins).\n",
      "            - If array-like, the bin edges for the two dimensions\n",
      "              (x_edges=y_edges=bins).\n",
      "            - If ``[array, array]``, the bin edges in each dimension\n",
      "              (x_edges, y_edges = bins).\n",
      "        \n",
      "            The default value is 10.\n",
      "        \n",
      "        range : array-like shape(2, 2), optional\n",
      "            The leftmost and rightmost edges of the bins along each dimension\n",
      "            (if not specified explicitly in the bins parameters): ``[[xmin,\n",
      "            xmax], [ymin, ymax]]``. All values outside of this range will be\n",
      "            considered outliers and not tallied in the histogram.\n",
      "        \n",
      "        density : bool, default: False\n",
      "            Normalize histogram.  See the documentation for the *density*\n",
      "            parameter of `~.Axes.hist` for more details.\n",
      "        \n",
      "        weights : array-like, shape (n, ), optional\n",
      "            An array of values w_i weighing each sample (x_i, y_i).\n",
      "        \n",
      "        cmin, cmax : float, default: None\n",
      "            All bins that has count less than *cmin* or more than *cmax* will\n",
      "            not be displayed (set to NaN before passing to imshow) and these\n",
      "            count values in the return value count histogram will also be set\n",
      "            to nan upon return.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        h : 2D array\n",
      "            The bi-dimensional histogram of samples x and y. Values in x are\n",
      "            histogrammed along the first dimension and values in y are\n",
      "            histogrammed along the second dimension.\n",
      "        xedges : 1D array\n",
      "            The bin edges along the x axis.\n",
      "        yedges : 1D array\n",
      "            The bin edges along the y axis.\n",
      "        image : `~.matplotlib.collections.QuadMesh`\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        cmap : Colormap or str, optional\n",
      "            A `.colors.Colormap` instance.  If not set, use rc settings.\n",
      "        \n",
      "        norm : Normalize, optional\n",
      "            A `.colors.Normalize` instance is used to\n",
      "            scale luminance data to ``[0, 1]``. If not set, defaults to\n",
      "            `.colors.Normalize()`.\n",
      "        \n",
      "        vmin/vmax : None or scalar, optional\n",
      "            Arguments passed to the `~.colors.Normalize` instance.\n",
      "        \n",
      "        alpha : ``0 <= scalar <= 1`` or ``None``, optional\n",
      "            The alpha blending value.\n",
      "        \n",
      "        **kwargs\n",
      "            Additional parameters are passed along to the\n",
      "            `~.Axes.pcolormesh` method and `~matplotlib.collections.QuadMesh`\n",
      "            constructor.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        hist : 1D histogram plotting\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        - Currently ``hist2d`` calculates its own axis limits, and any limits\n",
      "          previously set are ignored.\n",
      "        - Rendering the histogram with a logarithmic color scale is\n",
      "          accomplished by passing a `.colors.LogNorm` instance to the *norm*\n",
      "          keyword argument. Likewise, power-law normalization (similar\n",
      "          in effect to gamma correction) can be accomplished with\n",
      "          `.colors.PowerNorm`.\n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            the following arguments can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception):\n",
      "            *x*, *y*, *weights*.\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    hlines(y, xmin, xmax, colors=None, linestyles='solid', label='', *, data=None, **kwargs)\n",
      "        Plot horizontal lines at each *y* from *xmin* to *xmax*.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        y : float or array-like\n",
      "            y-indexes where to plot the lines.\n",
      "        \n",
      "        xmin, xmax : float or array-like\n",
      "            Respective beginning and end of each line. If scalars are\n",
      "            provided, all lines will have same length.\n",
      "        \n",
      "        colors : list of colors, default: :rc:`lines.color`\n",
      "        \n",
      "        linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional\n",
      "        \n",
      "        label : str, default: ''\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `~matplotlib.collections.LineCollection`\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs :  `~matplotlib.collections.LineCollection` properties.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        vlines : vertical lines\n",
      "        axhline: horizontal line across the axes\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            the following arguments can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception):\n",
      "            *y*, *xmin*, *xmax*, *colors*.\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    hot()\n",
      "        Set the colormap to \"hot\".\n",
      "        \n",
      "        This changes the default colormap as well as the colormap of the current\n",
      "        image if there is one. See ``help(colormaps)`` for more information.\n",
      "    \n",
      "    hsv()\n",
      "        Set the colormap to \"hsv\".\n",
      "        \n",
      "        This changes the default colormap as well as the colormap of the current\n",
      "        image if there is one. See ``help(colormaps)`` for more information.\n",
      "    \n",
      "    imread(fname, format=None)\n",
      "        Read an image from a file into an array.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fname : str or file-like\n",
      "            The image file to read: a filename, a URL or a file-like object opened\n",
      "            in read-binary mode.\n",
      "        format : str, optional\n",
      "            The image file format assumed for reading the data. If not\n",
      "            given, the format is deduced from the filename.  If nothing can\n",
      "            be deduced, PNG is tried.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `numpy.array`\n",
      "            The image data. The returned array has shape\n",
      "        \n",
      "            - (M, N) for grayscale images.\n",
      "            - (M, N, 3) for RGB images.\n",
      "            - (M, N, 4) for RGBA images.\n",
      "    \n",
      "    imsave(fname, arr, **kwargs)\n",
      "        Save an array as an image file.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fname : str or path-like or file-like\n",
      "            A path or a file-like object to store the image in.\n",
      "            If *format* is not set, then the output format is inferred from the\n",
      "            extension of *fname*, if any, and from :rc:`savefig.format` otherwise.\n",
      "            If *format* is set, it determines the output format.\n",
      "        arr : array-like\n",
      "            The image data. The shape can be one of\n",
      "            MxN (luminance), MxNx3 (RGB) or MxNx4 (RGBA).\n",
      "        vmin, vmax : float, optional\n",
      "            *vmin* and *vmax* set the color scaling for the image by fixing the\n",
      "            values that map to the colormap color limits. If either *vmin*\n",
      "            or *vmax* is None, that limit is determined from the *arr*\n",
      "            min/max value.\n",
      "        cmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n",
      "            A Colormap instance or registered colormap name. The colormap\n",
      "            maps scalar data to colors. It is ignored for RGB(A) data.\n",
      "        format : str, optional\n",
      "            The file format, e.g. 'png', 'pdf', 'svg', ...  The behavior when this\n",
      "            is unset is documented under *fname*.\n",
      "        origin : {'upper', 'lower'}, default: :rc:`image.origin`\n",
      "            Indicates whether the ``(0, 0)`` index of the array is in the upper\n",
      "            left or lower left corner of the axes.\n",
      "        dpi : float\n",
      "            The DPI to store in the metadata of the file.  This does not affect the\n",
      "            resolution of the output image.  Depending on file format, this may be\n",
      "            rounded to the nearest integer.\n",
      "        metadata : dict, optional\n",
      "            Metadata in the image file.  The supported keys depend on the output\n",
      "            format, see the documentation of the respective backends for more\n",
      "            information.\n",
      "        pil_kwargs : dict, optional\n",
      "            Keyword arguments passed to `PIL.Image.Image.save`.  If the 'pnginfo'\n",
      "            key is present, it completely overrides *metadata*, including the\n",
      "            default 'Software' key.\n",
      "    \n",
      "    imshow(X, cmap=None, norm=None, aspect=None, interpolation=None, alpha=None, vmin=None, vmax=None, origin=None, extent=None, *, filternorm=True, filterrad=4.0, resample=None, url=None, data=None, **kwargs)\n",
      "        Display data as an image, i.e., on a 2D regular raster.\n",
      "        \n",
      "        The input may either be actual RGB(A) data, or 2D scalar data, which\n",
      "        will be rendered as a pseudocolor image. For displaying a grayscale\n",
      "        image set up the color mapping using the parameters\n",
      "        ``cmap='gray', vmin=0, vmax=255``.\n",
      "        \n",
      "        The number of pixels used to render an image is set by the axes size\n",
      "        and the *dpi* of the figure. This can lead to aliasing artifacts when\n",
      "        the image is resampled because the displayed image size will usually\n",
      "        not match the size of *X* (see\n",
      "        :doc:`/gallery/images_contours_and_fields/image_antialiasing`).\n",
      "        The resampling can be controlled via the *interpolation* parameter\n",
      "        and/or :rc:`image.interpolation`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X : array-like or PIL image\n",
      "            The image data. Supported array shapes are:\n",
      "        \n",
      "            - (M, N): an image with scalar data. The values are mapped to\n",
      "              colors using normalization and a colormap. See parameters *norm*,\n",
      "              *cmap*, *vmin*, *vmax*.\n",
      "            - (M, N, 3): an image with RGB values (0-1 float or 0-255 int).\n",
      "            - (M, N, 4): an image with RGBA values (0-1 float or 0-255 int),\n",
      "              i.e. including transparency.\n",
      "        \n",
      "            The first two dimensions (M, N) define the rows and columns of\n",
      "            the image.\n",
      "        \n",
      "            Out-of-range RGB(A) values are clipped.\n",
      "        \n",
      "        cmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n",
      "            The Colormap instance or registered colormap name used to map\n",
      "            scalar data to colors. This parameter is ignored for RGB(A) data.\n",
      "        \n",
      "        norm : `~matplotlib.colors.Normalize`, optional\n",
      "            The `.Normalize` instance used to scale scalar data to the [0, 1]\n",
      "            range before mapping to colors using *cmap*. By default, a linear\n",
      "            scaling mapping the lowest value to 0 and the highest to 1 is used.\n",
      "            This parameter is ignored for RGB(A) data.\n",
      "        \n",
      "        aspect : {'equal', 'auto'} or float, default: :rc:`image.aspect`\n",
      "            The aspect ratio of the axes.  This parameter is particularly\n",
      "            relevant for images since it determines whether data pixels are\n",
      "            square.\n",
      "        \n",
      "            This parameter is a shortcut for explicitly calling\n",
      "            `.Axes.set_aspect`. See there for further details.\n",
      "        \n",
      "            - 'equal': Ensures an aspect ratio of 1. Pixels will be square\n",
      "              (unless pixel sizes are explicitly made non-square in data\n",
      "              coordinates using *extent*).\n",
      "            - 'auto': The axes is kept fixed and the aspect is adjusted so\n",
      "              that the data fit in the axes. In general, this will result in\n",
      "              non-square pixels.\n",
      "        \n",
      "        interpolation : str, default: :rc:`image.interpolation`\n",
      "            The interpolation method used.\n",
      "        \n",
      "            Supported values are 'none', 'antialiased', 'nearest', 'bilinear',\n",
      "            'bicubic', 'spline16', 'spline36', 'hanning', 'hamming', 'hermite',\n",
      "            'kaiser', 'quadric', 'catrom', 'gaussian', 'bessel', 'mitchell',\n",
      "            'sinc', 'lanczos'.\n",
      "        \n",
      "            If *interpolation* is 'none', then no interpolation is performed\n",
      "            on the Agg, ps, pdf and svg backends. Other backends will fall back\n",
      "            to 'nearest'. Note that most SVG renderers perform interpolation at\n",
      "            rendering and that the default interpolation method they implement\n",
      "            may differ.\n",
      "        \n",
      "            If *interpolation* is the default 'antialiased', then 'nearest'\n",
      "            interpolation is used if the image is upsampled by more than a\n",
      "            factor of three (i.e. the number of display pixels is at least\n",
      "            three times the size of the data array).  If the upsampling rate is\n",
      "            smaller than 3, or the image is downsampled, then 'hanning'\n",
      "            interpolation is used to act as an anti-aliasing filter, unless the\n",
      "            image happens to be upsampled by exactly a factor of two or one.\n",
      "        \n",
      "            See\n",
      "            :doc:`/gallery/images_contours_and_fields/interpolation_methods`\n",
      "            for an overview of the supported interpolation methods, and\n",
      "            :doc:`/gallery/images_contours_and_fields/image_antialiasing` for\n",
      "            a discussion of image antialiasing.\n",
      "        \n",
      "            Some interpolation methods require an additional radius parameter,\n",
      "            which can be set by *filterrad*. Additionally, the antigrain image\n",
      "            resize filter is controlled by the parameter *filternorm*.\n",
      "        \n",
      "        alpha : float or array-like, optional\n",
      "            The alpha blending value, between 0 (transparent) and 1 (opaque).\n",
      "            If *alpha* is an array, the alpha blending values are applied pixel\n",
      "            by pixel, and *alpha* must have the same shape as *X*.\n",
      "        \n",
      "        vmin, vmax : float, optional\n",
      "            When using scalar data and no explicit *norm*, *vmin* and *vmax*\n",
      "            define the data range that the colormap covers. By default,\n",
      "            the colormap covers the complete value range of the supplied\n",
      "            data. It is deprecated to use *vmin*/*vmax* when *norm* is given.\n",
      "        \n",
      "        origin : {'upper', 'lower'}, default: :rc:`image.origin`\n",
      "            Place the [0, 0] index of the array in the upper left or lower\n",
      "            left corner of the axes. The convention (the default) 'upper' is\n",
      "            typically used for matrices and images.\n",
      "        \n",
      "            Note that the vertical axes points upward for 'lower'\n",
      "            but downward for 'upper'.\n",
      "        \n",
      "            See the :doc:`/tutorials/intermediate/imshow_extent` tutorial for\n",
      "            examples and a more detailed description.\n",
      "        \n",
      "        extent : floats (left, right, bottom, top), optional\n",
      "            The bounding box in data coordinates that the image will fill.\n",
      "            The image is stretched individually along x and y to fill the box.\n",
      "        \n",
      "            The default extent is determined by the following conditions.\n",
      "            Pixels have unit size in data coordinates. Their centers are on\n",
      "            integer coordinates, and their center coordinates range from 0 to\n",
      "            columns-1 horizontally and from 0 to rows-1 vertically.\n",
      "        \n",
      "            Note that the direction of the vertical axis and thus the default\n",
      "            values for top and bottom depend on *origin*:\n",
      "        \n",
      "            - For ``origin == 'upper'`` the default is\n",
      "              ``(-0.5, numcols-0.5, numrows-0.5, -0.5)``.\n",
      "            - For ``origin == 'lower'`` the default is\n",
      "              ``(-0.5, numcols-0.5, -0.5, numrows-0.5)``.\n",
      "        \n",
      "            See the :doc:`/tutorials/intermediate/imshow_extent` tutorial for\n",
      "            examples and a more detailed description.\n",
      "        \n",
      "        filternorm : bool, default: True\n",
      "            A parameter for the antigrain image resize filter (see the\n",
      "            antigrain documentation).  If *filternorm* is set, the filter\n",
      "            normalizes integer values and corrects the rounding errors. It\n",
      "            doesn't do anything with the source floating point values, it\n",
      "            corrects only integers according to the rule of 1.0 which means\n",
      "            that any sum of pixel weights must be equal to 1.0.  So, the\n",
      "            filter function must produce a graph of the proper shape.\n",
      "        \n",
      "        filterrad : float > 0, default: 4.0\n",
      "            The filter radius for filters that have a radius parameter, i.e.\n",
      "            when interpolation is one of: 'sinc', 'lanczos' or 'blackman'.\n",
      "        \n",
      "        resample : bool, default: :rc:`image.resample`\n",
      "            When *True*, use a full resampling method.  When *False*, only\n",
      "            resample when the output image is larger than the input image.\n",
      "        \n",
      "        url : str, optional\n",
      "            Set the url of the created `.AxesImage`. See `.Artist.set_url`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `~matplotlib.image.AxesImage`\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs : `~matplotlib.artist.Artist` properties\n",
      "            These parameters are passed on to the constructor of the\n",
      "            `.AxesImage` artist.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        matshow : Plot a matrix or an array as an image.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Unless *extent* is used, pixel centers will be located at integer\n",
      "        coordinates. In other words: the origin will coincide with the center\n",
      "        of pixel (0, 0).\n",
      "        \n",
      "        There are two common representations for RGB images with an alpha\n",
      "        channel:\n",
      "        \n",
      "        -   Straight (unassociated) alpha: R, G, and B channels represent the\n",
      "            color of the pixel, disregarding its opacity.\n",
      "        -   Premultiplied (associated) alpha: R, G, and B channels represent\n",
      "            the color of the pixel, adjusted for its opacity by multiplication.\n",
      "        \n",
      "        `~matplotlib.pyplot.imshow` expects RGB images adopting the straight\n",
      "        (unassociated) alpha representation.\n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            every other argument can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception).\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    inferno()\n",
      "        Set the colormap to \"inferno\".\n",
      "        \n",
      "        This changes the default colormap as well as the colormap of the current\n",
      "        image if there is one. See ``help(colormaps)`` for more information.\n",
      "    \n",
      "    install_repl_displayhook()\n",
      "        Install a repl display hook so that any stale figure are automatically\n",
      "        redrawn when control is returned to the repl.\n",
      "        \n",
      "        This works both with IPython and with vanilla python shells.\n",
      "    \n",
      "    ioff()\n",
      "        Turn the interactive mode off.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ion : enable interactive mode\n",
      "        isinteractive : query current state\n",
      "        \n",
      "        show : show windows (and maybe block)\n",
      "        pause : show windows, run GUI event loop, and block for a time\n",
      "    \n",
      "    ion()\n",
      "        Turn the interactive mode on.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ioff : disable interactive mode\n",
      "        isinteractive : query current state\n",
      "        \n",
      "        show : show windows (and maybe block)\n",
      "        pause : show windows, run GUI event loop, and block for a time\n",
      "    \n",
      "    isinteractive()\n",
      "        Return if pyplot is in \"interactive mode\" or not.\n",
      "        \n",
      "        If in interactive mode then:\n",
      "        \n",
      "          - newly created figures will be shown immediately\n",
      "          - figures will automatically redraw on change\n",
      "          - `.pyplot.show` will not block by default\n",
      "        \n",
      "        If not in interactive mode then:\n",
      "        \n",
      "          - newly created figures and changes to figures will\n",
      "            not be reflected until explicitly asked to be\n",
      "          - `.pyplot.show` will block by default\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ion : enable interactive mode\n",
      "        ioff : disable interactive mode\n",
      "        \n",
      "        show : show windows (and maybe block)\n",
      "        pause : show windows, run GUI event loop, and block for a time\n",
      "    \n",
      "    jet()\n",
      "        Set the colormap to \"jet\".\n",
      "        \n",
      "        This changes the default colormap as well as the colormap of the current\n",
      "        image if there is one. See ``help(colormaps)`` for more information.\n",
      "    \n",
      "    legend(*args, **kwargs)\n",
      "        Place a legend on the axes.\n",
      "        \n",
      "        Call signatures::\n",
      "        \n",
      "            legend()\n",
      "            legend(labels)\n",
      "            legend(handles, labels)\n",
      "        \n",
      "        The call signatures correspond to three different ways how to use\n",
      "        this method.\n",
      "        \n",
      "        **1. Automatic detection of elements to be shown in the legend**\n",
      "        \n",
      "        The elements to be added to the legend are automatically determined,\n",
      "        when you do not pass in any extra arguments.\n",
      "        \n",
      "        In this case, the labels are taken from the artist. You can specify\n",
      "        them either at artist creation or by calling the\n",
      "        :meth:`~.Artist.set_label` method on the artist::\n",
      "        \n",
      "            line, = ax.plot([1, 2, 3], label='Inline label')\n",
      "            ax.legend()\n",
      "        \n",
      "        or::\n",
      "        \n",
      "            line, = ax.plot([1, 2, 3])\n",
      "            line.set_label('Label via method')\n",
      "            ax.legend()\n",
      "        \n",
      "        Specific lines can be excluded from the automatic legend element\n",
      "        selection by defining a label starting with an underscore.\n",
      "        This is default for all artists, so calling `.Axes.legend` without\n",
      "        any arguments and without setting the labels manually will result in\n",
      "        no legend being drawn.\n",
      "        \n",
      "        \n",
      "        **2. Labeling existing plot elements**\n",
      "        \n",
      "        To make a legend for lines which already exist on the axes\n",
      "        (via plot for instance), simply call this function with an iterable\n",
      "        of strings, one for each legend item. For example::\n",
      "        \n",
      "            ax.plot([1, 2, 3])\n",
      "            ax.legend(['A simple line'])\n",
      "        \n",
      "        Note: This way of using is discouraged, because the relation between\n",
      "        plot elements and labels is only implicit by their order and can\n",
      "        easily be mixed up.\n",
      "        \n",
      "        \n",
      "        **3. Explicitly defining the elements in the legend**\n",
      "        \n",
      "        For full control of which artists have a legend entry, it is possible\n",
      "        to pass an iterable of legend artists followed by an iterable of\n",
      "        legend labels respectively::\n",
      "        \n",
      "            legend((line1, line2, line3), ('label1', 'label2', 'label3'))\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        handles : sequence of `.Artist`, optional\n",
      "            A list of Artists (lines, patches) to be added to the legend.\n",
      "            Use this together with *labels*, if you need full control on what\n",
      "            is shown in the legend and the automatic mechanism described above\n",
      "            is not sufficient.\n",
      "        \n",
      "            The length of handles and labels should be the same in this\n",
      "            case. If they are not, they are truncated to the smaller length.\n",
      "        \n",
      "        labels : list of str, optional\n",
      "            A list of labels to show next to the artists.\n",
      "            Use this together with *handles*, if you need full control on what\n",
      "            is shown in the legend and the automatic mechanism described above\n",
      "            is not sufficient.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `~matplotlib.legend.Legend`\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        \n",
      "        loc : str or pair of floats, default: :rc:`legend.loc` ('best' for axes, 'upper right' for figures)\n",
      "            The location of the legend.\n",
      "        \n",
      "            The strings\n",
      "            ``'upper left', 'upper right', 'lower left', 'lower right'``\n",
      "            place the legend at the corresponding corner of the axes/figure.\n",
      "        \n",
      "            The strings\n",
      "            ``'upper center', 'lower center', 'center left', 'center right'``\n",
      "            place the legend at the center of the corresponding edge of the\n",
      "            axes/figure.\n",
      "        \n",
      "            The string ``'center'`` places the legend at the center of the axes/figure.\n",
      "        \n",
      "            The string ``'best'`` places the legend at the location, among the nine\n",
      "            locations defined so far, with the minimum overlap with other drawn\n",
      "            artists.  This option can be quite slow for plots with large amounts of\n",
      "            data; your plotting speed may benefit from providing a specific location.\n",
      "        \n",
      "            The location can also be a 2-tuple giving the coordinates of the lower-left\n",
      "            corner of the legend in axes coordinates (in which case *bbox_to_anchor*\n",
      "            will be ignored).\n",
      "        \n",
      "            For back-compatibility, ``'center right'`` (but no other location) can also\n",
      "            be spelled ``'right'``, and each \"string\" locations can also be given as a\n",
      "            numeric value:\n",
      "        \n",
      "                ===============   =============\n",
      "                Location String   Location Code\n",
      "                ===============   =============\n",
      "                'best'            0\n",
      "                'upper right'     1\n",
      "                'upper left'      2\n",
      "                'lower left'      3\n",
      "                'lower right'     4\n",
      "                'right'           5\n",
      "                'center left'     6\n",
      "                'center right'    7\n",
      "                'lower center'    8\n",
      "                'upper center'    9\n",
      "                'center'          10\n",
      "                ===============   =============\n",
      "        \n",
      "        bbox_to_anchor : `.BboxBase`, 2-tuple, or 4-tuple of floats\n",
      "            Box that is used to position the legend in conjunction with *loc*.\n",
      "            Defaults to `axes.bbox` (if called as a method to `.Axes.legend`) or\n",
      "            `figure.bbox` (if `.Figure.legend`).  This argument allows arbitrary\n",
      "            placement of the legend.\n",
      "        \n",
      "            Bbox coordinates are interpreted in the coordinate system given by\n",
      "            *bbox_transform*, with the default transform\n",
      "            Axes or Figure coordinates, depending on which ``legend`` is called.\n",
      "        \n",
      "            If a 4-tuple or `.BboxBase` is given, then it specifies the bbox\n",
      "            ``(x, y, width, height)`` that the legend is placed in.\n",
      "            To put the legend in the best location in the bottom right\n",
      "            quadrant of the axes (or figure)::\n",
      "        \n",
      "                loc='best', bbox_to_anchor=(0.5, 0., 0.5, 0.5)\n",
      "        \n",
      "            A 2-tuple ``(x, y)`` places the corner of the legend specified by *loc* at\n",
      "            x, y.  For example, to put the legend's upper right-hand corner in the\n",
      "            center of the axes (or figure) the following keywords can be used::\n",
      "        \n",
      "                loc='upper right', bbox_to_anchor=(0.5, 0.5)\n",
      "        \n",
      "        ncol : int, default: 1\n",
      "            The number of columns that the legend has.\n",
      "        \n",
      "        prop : None or `matplotlib.font_manager.FontProperties` or dict\n",
      "            The font properties of the legend. If None (default), the current\n",
      "            :data:`matplotlib.rcParams` will be used.\n",
      "        \n",
      "        fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n",
      "            The font size of the legend. If the value is numeric the size will be the\n",
      "            absolute font size in points. String values are relative to the current\n",
      "            default font size. This argument is only used if *prop* is not specified.\n",
      "        \n",
      "        labelcolor : str or list\n",
      "            Sets the color of the text in the legend. Can be a valid color string\n",
      "            (for example, 'red'), or a list of color strings. The labelcolor can\n",
      "            also be made to match the color of the line or marker using 'linecolor',\n",
      "            'markerfacecolor' (or 'mfc'), or 'markeredgecolor' (or 'mec').\n",
      "        \n",
      "        numpoints : int, default: :rc:`legend.numpoints`\n",
      "            The number of marker points in the legend when creating a legend\n",
      "            entry for a `.Line2D` (line).\n",
      "        \n",
      "        scatterpoints : int, default: :rc:`legend.scatterpoints`\n",
      "            The number of marker points in the legend when creating\n",
      "            a legend entry for a `.PathCollection` (scatter plot).\n",
      "        \n",
      "        scatteryoffsets : iterable of floats, default: ``[0.375, 0.5, 0.3125]``\n",
      "            The vertical offset (relative to the font size) for the markers\n",
      "            created for a scatter plot legend entry. 0.0 is at the base the\n",
      "            legend text, and 1.0 is at the top. To draw all markers at the\n",
      "            same height, set to ``[0.5]``.\n",
      "        \n",
      "        markerscale : float, default: :rc:`legend.markerscale`\n",
      "            The relative size of legend markers compared with the originally\n",
      "            drawn ones.\n",
      "        \n",
      "        markerfirst : bool, default: True\n",
      "            If *True*, legend marker is placed to the left of the legend label.\n",
      "            If *False*, legend marker is placed to the right of the legend label.\n",
      "        \n",
      "        frameon : bool, default: :rc:`legend.frameon`\n",
      "            Whether the legend should be drawn on a patch (frame).\n",
      "        \n",
      "        fancybox : bool, default: :rc:`legend.fancybox`\n",
      "            Whether round edges should be enabled around the `~.FancyBboxPatch` which\n",
      "            makes up the legend's background.\n",
      "        \n",
      "        shadow : bool, default: :rc:`legend.shadow`\n",
      "            Whether to draw a shadow behind the legend.\n",
      "        \n",
      "        framealpha : float, default: :rc:`legend.framealpha`\n",
      "            The alpha transparency of the legend's background.\n",
      "            If *shadow* is activated and *framealpha* is ``None``, the default value is\n",
      "            ignored.\n",
      "        \n",
      "        facecolor : \"inherit\" or color, default: :rc:`legend.facecolor`\n",
      "            The legend's background color.\n",
      "            If ``\"inherit\"``, use :rc:`axes.facecolor`.\n",
      "        \n",
      "        edgecolor : \"inherit\" or color, default: :rc:`legend.edgecolor`\n",
      "            The legend's background patch edge color.\n",
      "            If ``\"inherit\"``, use take :rc:`axes.edgecolor`.\n",
      "        \n",
      "        mode : {\"expand\", None}\n",
      "            If *mode* is set to ``\"expand\"`` the legend will be horizontally\n",
      "            expanded to fill the axes area (or *bbox_to_anchor* if defines\n",
      "            the legend's size).\n",
      "        \n",
      "        bbox_transform : None or `matplotlib.transforms.Transform`\n",
      "            The transform for the bounding box (*bbox_to_anchor*). For a value\n",
      "            of ``None`` (default) the Axes'\n",
      "            :data:`~matplotlib.axes.Axes.transAxes` transform will be used.\n",
      "        \n",
      "        title : str or None\n",
      "            The legend's title. Default is no title (``None``).\n",
      "        \n",
      "        title_fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}, default: :rc:`legend.title_fontsize`\n",
      "            The font size of the legend's title.\n",
      "        \n",
      "        borderpad : float, default: :rc:`legend.borderpad`\n",
      "            The fractional whitespace inside the legend border, in font-size units.\n",
      "        \n",
      "        labelspacing : float, default: :rc:`legend.labelspacing`\n",
      "            The vertical space between the legend entries, in font-size units.\n",
      "        \n",
      "        handlelength : float, default: :rc:`legend.handlelength`\n",
      "            The length of the legend handles, in font-size units.\n",
      "        \n",
      "        handletextpad : float, default: :rc:`legend.handletextpad`\n",
      "            The pad between the legend handle and text, in font-size units.\n",
      "        \n",
      "        borderaxespad : float, default: :rc:`legend.borderaxespad`\n",
      "            The pad between the axes and legend border, in font-size units.\n",
      "        \n",
      "        columnspacing : float, default: :rc:`legend.columnspacing`\n",
      "            The spacing between columns, in font-size units.\n",
      "        \n",
      "        handler_map : dict or None\n",
      "            The custom dictionary mapping instances or types to a legend\n",
      "            handler. This *handler_map* updates the default handler map\n",
      "            found at `matplotlib.legend.Legend.get_legend_handler_map`.\n",
      "        \n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Some artists are not supported by this function.  See\n",
      "        :doc:`/tutorials/intermediate/legend_guide` for details.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        .. plot:: gallery/text_labels_and_annotations/legend.py\n",
      "    \n",
      "    locator_params(axis='both', tight=None, **kwargs)\n",
      "        Control behavior of major tick locators.\n",
      "        \n",
      "        Because the locator is involved in autoscaling, `~.Axes.autoscale_view`\n",
      "        is called automatically after the parameters are changed.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        axis : {'both', 'x', 'y'}, default: 'both'\n",
      "            The axis on which to operate.\n",
      "        \n",
      "        tight : bool or None, optional\n",
      "            Parameter passed to `~.Axes.autoscale_view`.\n",
      "            Default is None, for no change.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            Remaining keyword arguments are passed to directly to the\n",
      "            ``set_params()`` method of the locator. Supported keywords depend\n",
      "            on the type of the locator. See for example\n",
      "            `~.ticker.MaxNLocator.set_params` for the `.ticker.MaxNLocator`\n",
      "            used by default for linear axes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        When plotting small subplots, one might want to reduce the maximum\n",
      "        number of ticks and use tight bounds, for example::\n",
      "        \n",
      "            ax.locator_params(tight=True, nbins=4)\n",
      "    \n",
      "    loglog(*args, **kwargs)\n",
      "        Make a plot with log scaling on both the x and y axis.\n",
      "        \n",
      "        Call signatures::\n",
      "        \n",
      "            loglog([x], y, [fmt], data=None, **kwargs)\n",
      "            loglog([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n",
      "        \n",
      "        This is just a thin wrapper around `.plot` which additionally changes\n",
      "        both the x-axis and the y-axis to log scaling. All of the concepts and\n",
      "        parameters of plot can be used here as well.\n",
      "        \n",
      "        The additional parameters *base*, *subs* and *nonpositive* control the\n",
      "        x/y-axis properties. They are just forwarded to `.Axes.set_xscale` and\n",
      "        `.Axes.set_yscale`. To use different properties on the x-axis and the\n",
      "        y-axis, use e.g.\n",
      "        ``ax.set_xscale(\"log\", base=10); ax.set_yscale(\"log\", base=2)``.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        base : float, default: 10\n",
      "            Base of the logarithm.\n",
      "        \n",
      "        subs : sequence, optional\n",
      "            The location of the minor ticks. If *None*, reasonable locations\n",
      "            are automatically chosen depending on the number of decades in the\n",
      "            plot. See `.Axes.set_xscale`/`.Axes.set_yscale` for details.\n",
      "        \n",
      "        nonpositive : {'mask', 'clip'}, default: 'mask'\n",
      "            Non-positive values can be masked as invalid, or clipped to a very\n",
      "            small positive number.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        lines\n",
      "            A list of `.Line2D` objects representing the plotted data.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            All parameters supported by `.plot`.\n",
      "    \n",
      "    magma()\n",
      "        Set the colormap to \"magma\".\n",
      "        \n",
      "        This changes the default colormap as well as the colormap of the current\n",
      "        image if there is one. See ``help(colormaps)`` for more information.\n",
      "    \n",
      "    magnitude_spectrum(x, Fs=None, Fc=None, window=None, pad_to=None, sides=None, scale=None, *, data=None, **kwargs)\n",
      "        Plot the magnitude spectrum.\n",
      "        \n",
      "        Compute the magnitude spectrum of *x*.  Data is padded to a\n",
      "        length of *pad_to* and the windowing function *window* is applied to\n",
      "        the signal.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : 1-D array or sequence\n",
      "            Array or sequence containing the data.\n",
      "        \n",
      "        Fs : float, default: 2\n",
      "            The sampling frequency (samples per time unit).  It is used to calculate\n",
      "            the Fourier frequencies, *freqs*, in cycles per time unit.\n",
      "        \n",
      "        window : callable or ndarray, default: `.window_hanning`\n",
      "            A function or a vector of length *NFFT*.  To create window vectors see\n",
      "            `.window_hanning`, `.window_none`, `numpy.blackman`, `numpy.hamming`,\n",
      "            `numpy.bartlett`, `scipy.signal`, `scipy.signal.get_window`, etc.  If a\n",
      "            function is passed as the argument, it must take a data segment as an\n",
      "            argument and return the windowed version of the segment.\n",
      "        \n",
      "        sides : {'default', 'onesided', 'twosided'}, optional\n",
      "            Which sides of the spectrum to return. 'default' is one-sided for real\n",
      "            data and two-sided for complex data. 'onesided' forces the return of a\n",
      "            one-sided spectrum, while 'twosided' forces two-sided.\n",
      "        \n",
      "        pad_to : int, optional\n",
      "            The number of points to which the data segment is padded when performing\n",
      "            the FFT.  While not increasing the actual resolution of the spectrum (the\n",
      "            minimum distance between resolvable peaks), this can give more points in\n",
      "            the plot, allowing for more detail. This corresponds to the *n* parameter\n",
      "            in the call to fft().  The default is None, which sets *pad_to* equal to\n",
      "            the length of the input signal (i.e. no padding).\n",
      "        \n",
      "        scale : {'default', 'linear', 'dB'}\n",
      "            The scaling of the values in the *spec*.  'linear' is no scaling.\n",
      "            'dB' returns the values in dB scale, i.e., the dB amplitude\n",
      "            (20 * log10). 'default' is 'linear'.\n",
      "        \n",
      "        Fc : int, default: 0\n",
      "            The center frequency of *x*, which offsets the x extents of the\n",
      "            plot to reflect the frequency range used when a signal is acquired\n",
      "            and then filtered and downsampled to baseband.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        spectrum : 1-D array\n",
      "            The values for the magnitude spectrum before scaling (real valued).\n",
      "        \n",
      "        freqs : 1-D array\n",
      "            The frequencies corresponding to the elements in *spectrum*.\n",
      "        \n",
      "        line : `~matplotlib.lines.Line2D`\n",
      "            The line created by this function.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            Keyword arguments control the `.Line2D` properties:\n",
      "        \n",
      "            Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa: bool\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            color or c: color\n",
      "            contains: unknown\n",
      "            dash_capstyle: {'butt', 'round', 'projecting'}\n",
      "            dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      "            data: (2, N) array or two 1D arrays\n",
      "            drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      "            figure: `.Figure`\n",
      "            fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      "            gid: str\n",
      "            in_layout: bool\n",
      "            label: object\n",
      "            linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      "            linewidth or lw: float\n",
      "            marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      "            markeredgecolor or mec: color\n",
      "            markeredgewidth or mew: float\n",
      "            markerfacecolor or mfc: color\n",
      "            markerfacecoloralt or mfcalt: color\n",
      "            markersize or ms: float\n",
      "            markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: unknown\n",
      "            pickradius: float\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            solid_capstyle: {'butt', 'round', 'projecting'}\n",
      "            solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            transform: `matplotlib.transforms.Transform`\n",
      "            url: str\n",
      "            visible: bool\n",
      "            xdata: 1D array\n",
      "            ydata: 1D array\n",
      "            zorder: float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        psd\n",
      "            Plots the power spectral density.\n",
      "        angle_spectrum\n",
      "            Plots the angles of the corresponding frequencies.\n",
      "        phase_spectrum\n",
      "            Plots the phase (unwrapped angle) of the corresponding frequencies.\n",
      "        specgram\n",
      "            Can plot the magnitude spectrum of segments within the signal in a\n",
      "            colormap.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            the following arguments can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception):\n",
      "            *x*.\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    margins(*margins, x=None, y=None, tight=True)\n",
      "        Set or retrieve autoscaling margins.\n",
      "        \n",
      "        The padding added to each limit of the axes is the *margin*\n",
      "        times the data interval. All input parameters must be floats\n",
      "        within the range [0, 1]. Passing both positional and keyword\n",
      "        arguments is invalid and will raise a TypeError. If no\n",
      "        arguments (positional or otherwise) are provided, the current\n",
      "        margins will remain in place and simply be returned.\n",
      "        \n",
      "        Specifying any margin changes only the autoscaling; for example,\n",
      "        if *xmargin* is not None, then *xmargin* times the X data\n",
      "        interval will be added to each end of that interval before\n",
      "        it is used in autoscaling.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        *margins : float, optional\n",
      "            If a single positional argument is provided, it specifies\n",
      "            both margins of the x-axis and y-axis limits. If two\n",
      "            positional arguments are provided, they will be interpreted\n",
      "            as *xmargin*, *ymargin*. If setting the margin on a single\n",
      "            axis is desired, use the keyword arguments described below.\n",
      "        \n",
      "        x, y : float, optional\n",
      "            Specific margin values for the x-axis and y-axis,\n",
      "            respectively. These cannot be used with positional\n",
      "            arguments, but can be used individually to alter on e.g.,\n",
      "            only the y-axis.\n",
      "        \n",
      "        tight : bool or None, default: True\n",
      "            The *tight* parameter is passed to :meth:`autoscale_view`,\n",
      "            which is executed after a margin is changed; the default\n",
      "            here is *True*, on the assumption that when margins are\n",
      "            specified, no additional padding to match tick marks is\n",
      "            usually desired.  Set *tight* to *None* will preserve\n",
      "            the previous setting.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        xmargin, ymargin : float\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If a previously used Axes method such as :meth:`pcolor` has set\n",
      "        :attr:`use_sticky_edges` to `True`, only the limits not set by\n",
      "        the \"sticky artists\" will be modified. To force all of the\n",
      "        margins to be set, set :attr:`use_sticky_edges` to `False`\n",
      "        before calling :meth:`margins`.\n",
      "    \n",
      "    matshow(A, fignum=None, **kwargs)\n",
      "        Display an array as a matrix in a new figure window.\n",
      "        \n",
      "        The origin is set at the upper left hand corner and rows (first\n",
      "        dimension of the array) are displayed horizontally.  The aspect\n",
      "        ratio of the figure window is that of the array, unless this would\n",
      "        make an excessively short or narrow figure.\n",
      "        \n",
      "        Tick labels for the xaxis are placed on top.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        A : array-like(M, N)\n",
      "            The matrix to be displayed.\n",
      "        \n",
      "        fignum : None or int or False\n",
      "            If *None*, create a new figure window with automatic numbering.\n",
      "        \n",
      "            If a nonzero integer, draw into the figure with the given number\n",
      "            (create it if it does not exist).\n",
      "        \n",
      "            If 0, use the current axes (or create one if it does not exist).\n",
      "        \n",
      "            .. note::\n",
      "        \n",
      "               Because of how `.Axes.matshow` tries to set the figure aspect\n",
      "               ratio to be the one of the array, strange things may happen if you\n",
      "               reuse an existing figure.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `~matplotlib.image.AxesImage`\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs : `~matplotlib.axes.Axes.imshow` arguments\n",
      "    \n",
      "    minorticks_off()\n",
      "        Remove minor ticks from the axes.\n",
      "    \n",
      "    minorticks_on()\n",
      "        Display minor ticks on the axes.\n",
      "        \n",
      "        Displaying minor ticks may reduce performance; you may turn them off\n",
      "        using `minorticks_off()` if drawing speed is a problem.\n",
      "    \n",
      "    new_figure_manager(num, *args, **kwargs)\n",
      "        Create a new figure manager instance.\n",
      "    \n",
      "    nipy_spectral()\n",
      "        Set the colormap to \"nipy_spectral\".\n",
      "        \n",
      "        This changes the default colormap as well as the colormap of the current\n",
      "        image if there is one. See ``help(colormaps)`` for more information.\n",
      "    \n",
      "    pause(interval)\n",
      "        Run the GUI event loop for *interval* seconds.\n",
      "        \n",
      "        If there is an active figure, it will be updated and displayed before the\n",
      "        pause, and the GUI event loop (if any) will run during the pause.\n",
      "        \n",
      "        This can be used for crude animation.  For more complex animation use\n",
      "        :mod:`matplotlib.animation`.\n",
      "        \n",
      "        If there is no active figure, sleep for *interval* seconds instead.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        matplotlib.animation : Complex animation\n",
      "        show : show figures and optional block forever\n",
      "    \n",
      "    pcolor(*args, shading=None, alpha=None, norm=None, cmap=None, vmin=None, vmax=None, data=None, **kwargs)\n",
      "        Create a pseudocolor plot with a non-regular rectangular grid.\n",
      "        \n",
      "        Call signature::\n",
      "        \n",
      "            pcolor([X, Y,] C, **kwargs)\n",
      "        \n",
      "        *X* and *Y* can be used to specify the corners of the quadrilaterals.\n",
      "        \n",
      "        .. hint::\n",
      "        \n",
      "            ``pcolor()`` can be very slow for large arrays. In most\n",
      "            cases you should use the similar but much faster\n",
      "            `~.Axes.pcolormesh` instead. See\n",
      "            :ref:`Differences between pcolor() and pcolormesh()\n",
      "            <differences-pcolor-pcolormesh>` for a discussion of the\n",
      "            differences.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        C : array-like\n",
      "            A scalar 2-D array. The values will be color-mapped.\n",
      "        \n",
      "        X, Y : array-like, optional\n",
      "            The coordinates of the corners of quadrilaterals of a pcolormesh::\n",
      "        \n",
      "                (X[i+1, j], Y[i+1, j])       (X[i+1, j+1], Y[i+1, j+1])\n",
      "                                      +-----+\n",
      "                                      |     |\n",
      "                                      +-----+\n",
      "                    (X[i, j], Y[i, j])       (X[i, j+1], Y[i, j+1])\n",
      "        \n",
      "            Note that the column index corresponds to the x-coordinate, and\n",
      "            the row index corresponds to y. For details, see the\n",
      "            :ref:`Notes <axes-pcolormesh-grid-orientation>` section below.\n",
      "        \n",
      "            If ``shading='flat'`` the dimensions of *X* and *Y* should be one\n",
      "            greater than those of *C*, and the quadrilateral is colored due\n",
      "            to the value at ``C[i, j]``.  If *X*, *Y* and *C* have equal\n",
      "            dimensions, a warning will be raised and the last row and column\n",
      "            of *C* will be ignored.\n",
      "        \n",
      "            If ``shading='nearest'``, the dimensions of *X* and *Y* should be\n",
      "            the same as those of *C* (if not, a ValueError will be raised). The\n",
      "            color ``C[i, j]`` will be centered on ``(X[i, j], Y[i, j])``.\n",
      "        \n",
      "            If *X* and/or *Y* are 1-D arrays or column vectors they will be\n",
      "            expanded as needed into the appropriate 2-D arrays, making a\n",
      "            rectangular grid.\n",
      "        \n",
      "        shading : {'flat', 'nearest', 'auto'}, optional\n",
      "            The fill style for the quadrilateral; defaults to 'flat' or\n",
      "            :rc:`pcolor.shading`. Possible values:\n",
      "        \n",
      "            - 'flat': A solid color is used for each quad. The color of the\n",
      "              quad (i, j), (i+1, j), (i, j+1), (i+1, j+1) is given by\n",
      "              ``C[i, j]``. The dimensions of *X* and *Y* should be\n",
      "              one greater than those of *C*; if they are the same as *C*,\n",
      "              then a deprecation warning is raised, and the last row\n",
      "              and column of *C* are dropped.\n",
      "            - 'nearest': Each grid point will have a color centered on it,\n",
      "              extending halfway between the adjacent grid centers.  The\n",
      "              dimensions of *X* and *Y* must be the same as *C*.\n",
      "            - 'auto': Choose 'flat' if dimensions of *X* and *Y* are one\n",
      "              larger than *C*.  Choose 'nearest' if dimensions are the same.\n",
      "        \n",
      "            See :doc:`/gallery/images_contours_and_fields/pcolormesh_grids`\n",
      "            for more description.\n",
      "        \n",
      "        cmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n",
      "            A Colormap instance or registered colormap name. The colormap\n",
      "            maps the *C* values to colors.\n",
      "        \n",
      "        norm : `~matplotlib.colors.Normalize`, optional\n",
      "            The Normalize instance scales the data values to the canonical\n",
      "            colormap range [0, 1] for mapping to colors. By default, the data\n",
      "            range is mapped to the colorbar range using linear scaling.\n",
      "        \n",
      "        vmin, vmax : float, default: None\n",
      "            The colorbar range. If *None*, suitable min/max values are\n",
      "            automatically chosen by the `~.Normalize` instance (defaults to\n",
      "            the respective min/max values of *C* in case of the default linear\n",
      "            scaling).\n",
      "            It is deprecated to use *vmin*/*vmax* when *norm* is given.\n",
      "        \n",
      "        edgecolors : {'none', None, 'face', color, color sequence}, optional\n",
      "            The color of the edges. Defaults to 'none'. Possible values:\n",
      "        \n",
      "            - 'none' or '': No edge.\n",
      "            - *None*: :rc:`patch.edgecolor` will be used. Note that currently\n",
      "              :rc:`patch.force_edgecolor` has to be True for this to work.\n",
      "            - 'face': Use the adjacent face color.\n",
      "            - A color or sequence of colors will set the edge color.\n",
      "        \n",
      "            The singular form *edgecolor* works as an alias.\n",
      "        \n",
      "        alpha : float, default: None\n",
      "            The alpha blending value of the face color, between 0 (transparent)\n",
      "            and 1 (opaque). Note: The edgecolor is currently not affected by\n",
      "            this.\n",
      "        \n",
      "        snap : bool, default: False\n",
      "            Whether to snap the mesh to pixel boundaries.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `matplotlib.collections.Collection`\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        antialiaseds : bool, default: False\n",
      "            The default *antialiaseds* is False if the default\n",
      "            *edgecolors*\\ =\"none\" is used.  This eliminates artificial lines\n",
      "            at patch boundaries, and works regardless of the value of alpha.\n",
      "            If *edgecolors* is not \"none\", then the default *antialiaseds*\n",
      "            is taken from :rc:`patch.antialiased`.\n",
      "            Stroking the edges may be preferred if *alpha* is 1, but will\n",
      "            cause artifacts otherwise.\n",
      "        \n",
      "        **kwargs\n",
      "            Additionally, the following arguments are allowed. They are passed\n",
      "            along to the `~matplotlib.collections.PolyCollection` constructor:\n",
      "        \n",
      "        Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa or antialiaseds: bool or list of bools\n",
      "            array: ndarray\n",
      "            capstyle: {'butt', 'round', 'projecting'}\n",
      "            clim: (vmin: float, vmax: float)\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            cmap: `.Colormap` or str or None\n",
      "            color: color or list of rgba tuples\n",
      "            contains: unknown\n",
      "            edgecolor or ec or edgecolors: color or list of colors or 'face'\n",
      "            facecolor or facecolors or fc: color or list of colors\n",
      "            figure: `.Figure`\n",
      "            gid: str\n",
      "            hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n",
      "            in_layout: bool\n",
      "            joinstyle: {'miter', 'round', 'bevel'}\n",
      "            label: object\n",
      "            linestyle or dashes or linestyles or ls: str or tuple or list thereof\n",
      "            linewidth or linewidths or lw: float or list of floats\n",
      "            norm: `.Normalize` or None\n",
      "            offset_position: unknown\n",
      "            offsets: array-like (N, 2) or (2,)\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: None or bool or callable\n",
      "            pickradius: unknown\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            transform: `.Transform`\n",
      "            url: str\n",
      "            urls: list of str or None\n",
      "            visible: bool\n",
      "            zorder: float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        pcolormesh : for an explanation of the differences between\n",
      "            pcolor and pcolormesh.\n",
      "        imshow : If *X* and *Y* are each equidistant, `~.Axes.imshow` can be a\n",
      "            faster alternative.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        **Masked arrays**\n",
      "        \n",
      "        *X*, *Y* and *C* may be masked arrays. If either ``C[i, j]``, or one\n",
      "        of the vertices surrounding ``C[i, j]`` (*X* or *Y* at\n",
      "        ``[i, j], [i+1, j], [i, j+1], [i+1, j+1]``) is masked, nothing is\n",
      "        plotted.\n",
      "        \n",
      "        .. _axes-pcolor-grid-orientation:\n",
      "        \n",
      "        **Grid orientation**\n",
      "        \n",
      "        The grid orientation follows the standard matrix convention: An array\n",
      "        *C* with shape (nrows, ncolumns) is plotted with the column number as\n",
      "        *X* and the row number as *Y*.\n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            every other argument can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception).\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    pcolormesh(*args, alpha=None, norm=None, cmap=None, vmin=None, vmax=None, shading=None, antialiased=False, data=None, **kwargs)\n",
      "        Create a pseudocolor plot with a non-regular rectangular grid.\n",
      "        \n",
      "        Call signature::\n",
      "        \n",
      "            pcolormesh([X, Y,] C, **kwargs)\n",
      "        \n",
      "        *X* and *Y* can be used to specify the corners of the quadrilaterals.\n",
      "        \n",
      "        .. hint::\n",
      "        \n",
      "           `~.Axes.pcolormesh` is similar to `~.Axes.pcolor`. It is much faster\n",
      "           and preferred in most cases. For a detailed discussion on the\n",
      "           differences see :ref:`Differences between pcolor() and pcolormesh()\n",
      "           <differences-pcolor-pcolormesh>`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        C : array-like\n",
      "            A scalar 2-D array. The values will be color-mapped.\n",
      "        \n",
      "        X, Y : array-like, optional\n",
      "            The coordinates of the corners of quadrilaterals of a pcolormesh::\n",
      "        \n",
      "                (X[i+1, j], Y[i+1, j])       (X[i+1, j+1], Y[i+1, j+1])\n",
      "                                      +-----+\n",
      "                                      |     |\n",
      "                                      +-----+\n",
      "                    (X[i, j], Y[i, j])       (X[i, j+1], Y[i, j+1])\n",
      "        \n",
      "            Note that the column index corresponds to the x-coordinate, and\n",
      "            the row index corresponds to y. For details, see the\n",
      "            :ref:`Notes <axes-pcolormesh-grid-orientation>` section below.\n",
      "        \n",
      "            If ``shading='flat'`` the dimensions of *X* and *Y* should be one\n",
      "            greater than those of *C*, and the quadrilateral is colored due\n",
      "            to the value at ``C[i, j]``.  If *X*, *Y* and *C* have equal\n",
      "            dimensions, a warning will be raised and the last row and column\n",
      "            of *C* will be ignored.\n",
      "        \n",
      "            If ``shading='nearest'`` or ``'gouraud'``, the dimensions of *X*\n",
      "            and *Y* should be the same as those of *C* (if not, a ValueError\n",
      "            will be raised).  For ``'nearest'`` the color ``C[i, j]`` is\n",
      "            centered on ``(X[i, j], Y[i, j])``.  For ``'gouraud'``, a smooth\n",
      "            interpolation is caried out between the quadrilateral corners.\n",
      "        \n",
      "            If *X* and/or *Y* are 1-D arrays or column vectors they will be\n",
      "            expanded as needed into the appropriate 2-D arrays, making a\n",
      "            rectangular grid.\n",
      "        \n",
      "        cmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n",
      "            A Colormap instance or registered colormap name. The colormap\n",
      "            maps the *C* values to colors.\n",
      "        \n",
      "        norm : `~matplotlib.colors.Normalize`, optional\n",
      "            The Normalize instance scales the data values to the canonical\n",
      "            colormap range [0, 1] for mapping to colors. By default, the data\n",
      "            range is mapped to the colorbar range using linear scaling.\n",
      "        \n",
      "        vmin, vmax : float, default: None\n",
      "            The colorbar range. If *None*, suitable min/max values are\n",
      "            automatically chosen by the `~.Normalize` instance (defaults to\n",
      "            the respective min/max values of *C* in case of the default linear\n",
      "            scaling).\n",
      "            It is deprecated to use *vmin*/*vmax* when *norm* is given.\n",
      "        \n",
      "        edgecolors : {'none', None, 'face', color, color sequence}, optional\n",
      "            The color of the edges. Defaults to 'none'. Possible values:\n",
      "        \n",
      "            - 'none' or '': No edge.\n",
      "            - *None*: :rc:`patch.edgecolor` will be used. Note that currently\n",
      "              :rc:`patch.force_edgecolor` has to be True for this to work.\n",
      "            - 'face': Use the adjacent face color.\n",
      "            - A color or sequence of colors will set the edge color.\n",
      "        \n",
      "            The singular form *edgecolor* works as an alias.\n",
      "        \n",
      "        alpha : float, default: None\n",
      "            The alpha blending value, between 0 (transparent) and 1 (opaque).\n",
      "        \n",
      "        shading : {'flat', 'nearest', 'gouraud', 'auto'}, optional\n",
      "            The fill style for the quadrilateral; defaults to\n",
      "            'flat' or :rc:`pcolor.shading`. Possible values:\n",
      "        \n",
      "            - 'flat': A solid color is used for each quad. The color of the\n",
      "              quad (i, j), (i+1, j), (i, j+1), (i+1, j+1) is given by\n",
      "              ``C[i, j]``. The dimensions of *X* and *Y* should be\n",
      "              one greater than those of *C*; if they are the same as *C*,\n",
      "              then a deprecation warning is raised, and the last row\n",
      "              and column of *C* are dropped.\n",
      "            - 'nearest': Each grid point will have a color centered on it,\n",
      "              extending halfway between the adjacent grid centers.  The\n",
      "              dimensions of *X* and *Y* must be the same as *C*.\n",
      "            - 'gouraud': Each quad will be Gouraud shaded: The color of the\n",
      "              corners (i', j') are given by ``C[i', j']``. The color values of\n",
      "              the area in between is interpolated from the corner values.\n",
      "              The dimensions of *X* and *Y* must be the same as *C*. When\n",
      "              Gouraud shading is used, *edgecolors* is ignored.\n",
      "            - 'auto': Choose 'flat' if dimensions of *X* and *Y* are one\n",
      "              larger than *C*.  Choose 'nearest' if dimensions are the same.\n",
      "        \n",
      "            See :doc:`/gallery/images_contours_and_fields/pcolormesh_grids`\n",
      "            for more description.\n",
      "        \n",
      "        snap : bool, default: False\n",
      "            Whether to snap the mesh to pixel boundaries.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `matplotlib.collections.QuadMesh`\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            Additionally, the following arguments are allowed. They are passed\n",
      "            along to the `~matplotlib.collections.QuadMesh` constructor:\n",
      "        \n",
      "        Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa or antialiaseds: bool or list of bools\n",
      "            array: ndarray\n",
      "            capstyle: {'butt', 'round', 'projecting'}\n",
      "            clim: (vmin: float, vmax: float)\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            cmap: `.Colormap` or str or None\n",
      "            color: color or list of rgba tuples\n",
      "            contains: unknown\n",
      "            edgecolor or ec or edgecolors: color or list of colors or 'face'\n",
      "            facecolor or facecolors or fc: color or list of colors\n",
      "            figure: `.Figure`\n",
      "            gid: str\n",
      "            hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n",
      "            in_layout: bool\n",
      "            joinstyle: {'miter', 'round', 'bevel'}\n",
      "            label: object\n",
      "            linestyle or dashes or linestyles or ls: str or tuple or list thereof\n",
      "            linewidth or linewidths or lw: float or list of floats\n",
      "            norm: `.Normalize` or None\n",
      "            offset_position: unknown\n",
      "            offsets: array-like (N, 2) or (2,)\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: None or bool or callable\n",
      "            pickradius: unknown\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            transform: `.Transform`\n",
      "            url: str\n",
      "            urls: list of str or None\n",
      "            visible: bool\n",
      "            zorder: float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        pcolor : An alternative implementation with slightly different\n",
      "            features. For a detailed discussion on the differences see\n",
      "            :ref:`Differences between pcolor() and pcolormesh()\n",
      "            <differences-pcolor-pcolormesh>`.\n",
      "        imshow : If *X* and *Y* are each equidistant, `~.Axes.imshow` can be a\n",
      "            faster alternative.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        **Masked arrays**\n",
      "        \n",
      "        *C* may be a masked array. If ``C[i, j]`` is masked, the corresponding\n",
      "        quadrilateral will be transparent. Masking of *X* and *Y* is not\n",
      "        supported. Use `~.Axes.pcolor` if you need this functionality.\n",
      "        \n",
      "        .. _axes-pcolormesh-grid-orientation:\n",
      "        \n",
      "        **Grid orientation**\n",
      "        \n",
      "        The grid orientation follows the standard matrix convention: An array\n",
      "        *C* with shape (nrows, ncolumns) is plotted with the column number as\n",
      "        *X* and the row number as *Y*.\n",
      "        \n",
      "        .. _differences-pcolor-pcolormesh:\n",
      "        \n",
      "        **Differences between pcolor() and pcolormesh()**\n",
      "        \n",
      "        Both methods are used to create a pseudocolor plot of a 2-D array\n",
      "        using quadrilaterals.\n",
      "        \n",
      "        The main difference lies in the created object and internal data\n",
      "        handling:\n",
      "        While `~.Axes.pcolor` returns a `.PolyCollection`, `~.Axes.pcolormesh`\n",
      "        returns a `.QuadMesh`. The latter is more specialized for the given\n",
      "        purpose and thus is faster. It should almost always be preferred.\n",
      "        \n",
      "        There is also a slight difference in the handling of masked arrays.\n",
      "        Both `~.Axes.pcolor` and `~.Axes.pcolormesh` support masked arrays\n",
      "        for *C*. However, only `~.Axes.pcolor` supports masked arrays for *X*\n",
      "        and *Y*. The reason lies in the internal handling of the masked values.\n",
      "        `~.Axes.pcolor` leaves out the respective polygons from the\n",
      "        PolyCollection. `~.Axes.pcolormesh` sets the facecolor of the masked\n",
      "        elements to transparent. You can see the difference when using\n",
      "        edgecolors. While all edges are drawn irrespective of masking in a\n",
      "        QuadMesh, the edge between two adjacent masked quadrilaterals in\n",
      "        `~.Axes.pcolor` is not drawn as the corresponding polygons do not\n",
      "        exist in the PolyCollection.\n",
      "        \n",
      "        Another difference is the support of Gouraud shading in\n",
      "        `~.Axes.pcolormesh`, which is not available with `~.Axes.pcolor`.\n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            every other argument can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception).\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    phase_spectrum(x, Fs=None, Fc=None, window=None, pad_to=None, sides=None, *, data=None, **kwargs)\n",
      "        Plot the phase spectrum.\n",
      "        \n",
      "        Compute the phase spectrum (unwrapped angle spectrum) of *x*.\n",
      "        Data is padded to a length of *pad_to* and the windowing function\n",
      "        *window* is applied to the signal.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : 1-D array or sequence\n",
      "            Array or sequence containing the data\n",
      "        \n",
      "        Fs : float, default: 2\n",
      "            The sampling frequency (samples per time unit).  It is used to calculate\n",
      "            the Fourier frequencies, *freqs*, in cycles per time unit.\n",
      "        \n",
      "        window : callable or ndarray, default: `.window_hanning`\n",
      "            A function or a vector of length *NFFT*.  To create window vectors see\n",
      "            `.window_hanning`, `.window_none`, `numpy.blackman`, `numpy.hamming`,\n",
      "            `numpy.bartlett`, `scipy.signal`, `scipy.signal.get_window`, etc.  If a\n",
      "            function is passed as the argument, it must take a data segment as an\n",
      "            argument and return the windowed version of the segment.\n",
      "        \n",
      "        sides : {'default', 'onesided', 'twosided'}, optional\n",
      "            Which sides of the spectrum to return. 'default' is one-sided for real\n",
      "            data and two-sided for complex data. 'onesided' forces the return of a\n",
      "            one-sided spectrum, while 'twosided' forces two-sided.\n",
      "        \n",
      "        pad_to : int, optional\n",
      "            The number of points to which the data segment is padded when performing\n",
      "            the FFT.  While not increasing the actual resolution of the spectrum (the\n",
      "            minimum distance between resolvable peaks), this can give more points in\n",
      "            the plot, allowing for more detail. This corresponds to the *n* parameter\n",
      "            in the call to fft().  The default is None, which sets *pad_to* equal to\n",
      "            the length of the input signal (i.e. no padding).\n",
      "        \n",
      "        Fc : int, default: 0\n",
      "            The center frequency of *x*, which offsets the x extents of the\n",
      "            plot to reflect the frequency range used when a signal is acquired\n",
      "            and then filtered and downsampled to baseband.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        spectrum : 1-D array\n",
      "            The values for the phase spectrum in radians (real valued).\n",
      "        \n",
      "        freqs : 1-D array\n",
      "            The frequencies corresponding to the elements in *spectrum*.\n",
      "        \n",
      "        line : `~matplotlib.lines.Line2D`\n",
      "            The line created by this function.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            Keyword arguments control the `.Line2D` properties:\n",
      "        \n",
      "            Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa: bool\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            color or c: color\n",
      "            contains: unknown\n",
      "            dash_capstyle: {'butt', 'round', 'projecting'}\n",
      "            dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      "            data: (2, N) array or two 1D arrays\n",
      "            drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      "            figure: `.Figure`\n",
      "            fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      "            gid: str\n",
      "            in_layout: bool\n",
      "            label: object\n",
      "            linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      "            linewidth or lw: float\n",
      "            marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      "            markeredgecolor or mec: color\n",
      "            markeredgewidth or mew: float\n",
      "            markerfacecolor or mfc: color\n",
      "            markerfacecoloralt or mfcalt: color\n",
      "            markersize or ms: float\n",
      "            markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: unknown\n",
      "            pickradius: float\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            solid_capstyle: {'butt', 'round', 'projecting'}\n",
      "            solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            transform: `matplotlib.transforms.Transform`\n",
      "            url: str\n",
      "            visible: bool\n",
      "            xdata: 1D array\n",
      "            ydata: 1D array\n",
      "            zorder: float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        magnitude_spectrum\n",
      "            Plots the magnitudes of the corresponding frequencies.\n",
      "        angle_spectrum\n",
      "            Plots the wrapped version of this function.\n",
      "        specgram\n",
      "            Can plot the phase spectrum of segments within the signal in a\n",
      "            colormap.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            the following arguments can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception):\n",
      "            *x*.\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    pie(x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, shadow=False, labeldistance=1.1, startangle=0, radius=1, counterclock=True, wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, *, normalize=None, data=None)\n",
      "        Plot a pie chart.\n",
      "        \n",
      "        Make a pie chart of array *x*.  The fractional area of each wedge is\n",
      "        given by ``x/sum(x)``.  If ``sum(x) < 1``, then the values of *x* give\n",
      "        the fractional area directly and the array will not be normalized. The\n",
      "        resulting pie will have an empty wedge of size ``1 - sum(x)``.\n",
      "        \n",
      "        The wedges are plotted counterclockwise, by default starting from the\n",
      "        x-axis.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : 1D array-like\n",
      "            The wedge sizes.\n",
      "        \n",
      "        explode : array-like, default: None\n",
      "            If not *None*, is a ``len(x)`` array which specifies the fraction\n",
      "            of the radius with which to offset each wedge.\n",
      "        \n",
      "        labels : list, default: None\n",
      "            A sequence of strings providing the labels for each wedge\n",
      "        \n",
      "        colors : array-like, default: None\n",
      "            A sequence of colors through which the pie chart will cycle.  If\n",
      "            *None*, will use the colors in the currently active cycle.\n",
      "        \n",
      "        autopct : None or str or callable, default: None\n",
      "            If not *None*, is a string or function used to label the wedges\n",
      "            with their numeric value.  The label will be placed inside the\n",
      "            wedge.  If it is a format string, the label will be ``fmt % pct``.\n",
      "            If it is a function, it will be called.\n",
      "        \n",
      "        pctdistance : float, default: 0.6\n",
      "            The ratio between the center of each pie slice and the start of\n",
      "            the text generated by *autopct*.  Ignored if *autopct* is *None*.\n",
      "        \n",
      "        shadow : bool, default: False\n",
      "            Draw a shadow beneath the pie.\n",
      "        \n",
      "        normalize: None or bool, default: None\n",
      "            When *True*, always make a full pie by normalizing x so that\n",
      "            ``sum(x) == 1``. *False* makes a partial pie if ``sum(x) <= 1``\n",
      "            and raises a `ValueError` for ``sum(x) > 1``.\n",
      "        \n",
      "            When *None*, defaults to *True* if ``sum(x) >= 1`` and *False* if\n",
      "            ``sum(x) < 1``.\n",
      "        \n",
      "            Please note that the previous default value of *None* is now\n",
      "            deprecated, and the default will change to *True* in the next\n",
      "            release. Please pass ``normalize=False`` explicitly if you want to\n",
      "            draw a partial pie.\n",
      "        \n",
      "        labeldistance : float or None, default: 1.1\n",
      "            The radial distance at which the pie labels are drawn.\n",
      "            If set to ``None``, label are not drawn, but are stored for use in\n",
      "            ``legend()``\n",
      "        \n",
      "        startangle : float, default: 0 degrees\n",
      "            The angle by which the start of the pie is rotated,\n",
      "            counterclockwise from the x-axis.\n",
      "        \n",
      "        radius : float, default: 1\n",
      "            The radius of the pie.\n",
      "        \n",
      "        counterclock : bool, default: True\n",
      "            Specify fractions direction, clockwise or counterclockwise.\n",
      "        \n",
      "        wedgeprops : dict, default: None\n",
      "            Dict of arguments passed to the wedge objects making the pie.\n",
      "            For example, you can pass in ``wedgeprops = {'linewidth': 3}``\n",
      "            to set the width of the wedge border lines equal to 3.\n",
      "            For more details, look at the doc/arguments of the wedge object.\n",
      "            By default ``clip_on=False``.\n",
      "        \n",
      "        textprops : dict, default: None\n",
      "            Dict of arguments to pass to the text objects.\n",
      "        \n",
      "        center : (float, float), default: (0, 0)\n",
      "            The coordinates of the center of the chart.\n",
      "        \n",
      "        frame : bool, default: False\n",
      "            Plot axes frame with the chart if true.\n",
      "        \n",
      "        rotatelabels : bool, default: False\n",
      "            Rotate each label to the angle of the corresponding slice if true.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        patches : list\n",
      "            A sequence of `matplotlib.patches.Wedge` instances\n",
      "        \n",
      "        texts : list\n",
      "            A list of the label `.Text` instances.\n",
      "        \n",
      "        autotexts : list\n",
      "            A list of `.Text` instances for the numeric labels. This will only\n",
      "            be returned if the parameter *autopct* is not *None*.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The pie chart will probably look best if the figure and axes are\n",
      "        square, or the Axes aspect is equal.\n",
      "        This method sets the aspect ratio of the axis to \"equal\".\n",
      "        The axes aspect ratio can be controlled with `.Axes.set_aspect`.\n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            the following arguments can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception):\n",
      "            *x*, *explode*, *labels*, *colors*.\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    pink()\n",
      "        Set the colormap to \"pink\".\n",
      "        \n",
      "        This changes the default colormap as well as the colormap of the current\n",
      "        image if there is one. See ``help(colormaps)`` for more information.\n",
      "    \n",
      "    plasma()\n",
      "        Set the colormap to \"plasma\".\n",
      "        \n",
      "        This changes the default colormap as well as the colormap of the current\n",
      "        image if there is one. See ``help(colormaps)`` for more information.\n",
      "    \n",
      "    plot(*args, scalex=True, scaley=True, data=None, **kwargs)\n",
      "        Plot y versus x as lines and/or markers.\n",
      "        \n",
      "        Call signatures::\n",
      "        \n",
      "            plot([x], y, [fmt], *, data=None, **kwargs)\n",
      "            plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n",
      "        \n",
      "        The coordinates of the points or line nodes are given by *x*, *y*.\n",
      "        \n",
      "        The optional parameter *fmt* is a convenient way for defining basic\n",
      "        formatting like color, marker and linestyle. It's a shortcut string\n",
      "        notation described in the *Notes* section below.\n",
      "        \n",
      "        >>> plot(x, y)        # plot x and y using default line style and color\n",
      "        >>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n",
      "        >>> plot(y)           # plot y using x as index array 0..N-1\n",
      "        >>> plot(y, 'r+')     # ditto, but with red plusses\n",
      "        \n",
      "        You can use `.Line2D` properties as keyword arguments for more\n",
      "        control on the appearance. Line properties and *fmt* can be mixed.\n",
      "        The following two calls yield identical results:\n",
      "        \n",
      "        >>> plot(x, y, 'go--', linewidth=2, markersize=12)\n",
      "        >>> plot(x, y, color='green', marker='o', linestyle='dashed',\n",
      "        ...      linewidth=2, markersize=12)\n",
      "        \n",
      "        When conflicting with *fmt*, keyword arguments take precedence.\n",
      "        \n",
      "        \n",
      "        **Plotting labelled data**\n",
      "        \n",
      "        There's a convenient way for plotting objects with labelled data (i.e.\n",
      "        data that can be accessed by index ``obj['y']``). Instead of giving\n",
      "        the data in *x* and *y*, you can provide the object in the *data*\n",
      "        parameter and just give the labels for *x* and *y*::\n",
      "        \n",
      "        >>> plot('xlabel', 'ylabel', data=obj)\n",
      "        \n",
      "        All indexable objects are supported. This could e.g. be a `dict`, a\n",
      "        `pandas.DataFrame` or a structured numpy array.\n",
      "        \n",
      "        \n",
      "        **Plotting multiple sets of data**\n",
      "        \n",
      "        There are various ways to plot multiple sets of data.\n",
      "        \n",
      "        - The most straight forward way is just to call `plot` multiple times.\n",
      "          Example:\n",
      "        \n",
      "          >>> plot(x1, y1, 'bo')\n",
      "          >>> plot(x2, y2, 'go')\n",
      "        \n",
      "        - Alternatively, if your data is already a 2d array, you can pass it\n",
      "          directly to *x*, *y*. A separate data set will be drawn for every\n",
      "          column.\n",
      "        \n",
      "          Example: an array ``a`` where the first column represents the *x*\n",
      "          values and the other columns are the *y* columns::\n",
      "        \n",
      "          >>> plot(a[0], a[1:])\n",
      "        \n",
      "        - The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n",
      "          groups::\n",
      "        \n",
      "          >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n",
      "        \n",
      "          In this case, any additional keyword argument applies to all\n",
      "          datasets. Also this syntax cannot be combined with the *data*\n",
      "          parameter.\n",
      "        \n",
      "        By default, each line is assigned a different style specified by a\n",
      "        'style cycle'. The *fmt* and line property parameters are only\n",
      "        necessary if you want explicit deviations from these defaults.\n",
      "        Alternatively, you can also change the style cycle using\n",
      "        :rc:`axes.prop_cycle`.\n",
      "        \n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array-like or scalar\n",
      "            The horizontal / vertical coordinates of the data points.\n",
      "            *x* values are optional and default to ``range(len(y))``.\n",
      "        \n",
      "            Commonly, these parameters are 1D arrays.\n",
      "        \n",
      "            They can also be scalars, or two-dimensional (in that case, the\n",
      "            columns represent separate data sets).\n",
      "        \n",
      "            These arguments cannot be passed as keywords.\n",
      "        \n",
      "        fmt : str, optional\n",
      "            A format string, e.g. 'ro' for red circles. See the *Notes*\n",
      "            section for a full description of the format strings.\n",
      "        \n",
      "            Format strings are just an abbreviation for quickly setting\n",
      "            basic line properties. All of these and more can also be\n",
      "            controlled by keyword arguments.\n",
      "        \n",
      "            This argument cannot be passed as keyword.\n",
      "        \n",
      "        data : indexable object, optional\n",
      "            An object with labelled data. If given, provide the label names to\n",
      "            plot in *x* and *y*.\n",
      "        \n",
      "            .. note::\n",
      "                Technically there's a slight ambiguity in calls where the\n",
      "                second label is a valid *fmt*. ``plot('n', 'o', data=obj)``\n",
      "                could be ``plt(x, y)`` or ``plt(y, fmt)``. In such cases,\n",
      "                the former interpretation is chosen, but a warning is issued.\n",
      "                You may suppress the warning by adding an empty format string\n",
      "                ``plot('n', 'o', '', data=obj)``.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        list of `.Line2D`\n",
      "            A list of lines representing the plotted data.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        scalex, scaley : bool, default: True\n",
      "            These parameters determine if the view limits are adapted to the\n",
      "            data limits. The values are passed on to `autoscale_view`.\n",
      "        \n",
      "        **kwargs : `.Line2D` properties, optional\n",
      "            *kwargs* are used to specify properties like a line label (for\n",
      "            auto legends), linewidth, antialiasing, marker face color.\n",
      "            Example::\n",
      "        \n",
      "            >>> plot([1, 2, 3], [1, 2, 3], 'go-', label='line 1', linewidth=2)\n",
      "            >>> plot([1, 2, 3], [1, 4, 9], 'rs', label='line 2')\n",
      "        \n",
      "            If you make multiple lines with one plot call, the kwargs\n",
      "            apply to all those lines.\n",
      "        \n",
      "            Here is a list of available `.Line2D` properties:\n",
      "        \n",
      "            Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa: bool\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            color or c: color\n",
      "            contains: unknown\n",
      "            dash_capstyle: {'butt', 'round', 'projecting'}\n",
      "            dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      "            data: (2, N) array or two 1D arrays\n",
      "            drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      "            figure: `.Figure`\n",
      "            fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      "            gid: str\n",
      "            in_layout: bool\n",
      "            label: object\n",
      "            linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      "            linewidth or lw: float\n",
      "            marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      "            markeredgecolor or mec: color\n",
      "            markeredgewidth or mew: float\n",
      "            markerfacecolor or mfc: color\n",
      "            markerfacecoloralt or mfcalt: color\n",
      "            markersize or ms: float\n",
      "            markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: unknown\n",
      "            pickradius: float\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            solid_capstyle: {'butt', 'round', 'projecting'}\n",
      "            solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            transform: `matplotlib.transforms.Transform`\n",
      "            url: str\n",
      "            visible: bool\n",
      "            xdata: 1D array\n",
      "            ydata: 1D array\n",
      "            zorder: float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        scatter : XY scatter plot with markers of varying size and/or color (\n",
      "            sometimes also called bubble chart).\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        **Format Strings**\n",
      "        \n",
      "        A format string consists of a part for color, marker and line::\n",
      "        \n",
      "            fmt = '[marker][line][color]'\n",
      "        \n",
      "        Each of them is optional. If not provided, the value from the style\n",
      "        cycle is used. Exception: If ``line`` is given, but no ``marker``,\n",
      "        the data will be a line without markers.\n",
      "        \n",
      "        Other combinations such as ``[color][marker][line]`` are also\n",
      "        supported, but note that their parsing may be ambiguous.\n",
      "        \n",
      "        **Markers**\n",
      "        \n",
      "        =============    ===============================\n",
      "        character        description\n",
      "        =============    ===============================\n",
      "        ``'.'``          point marker\n",
      "        ``','``          pixel marker\n",
      "        ``'o'``          circle marker\n",
      "        ``'v'``          triangle_down marker\n",
      "        ``'^'``          triangle_up marker\n",
      "        ``'<'``          triangle_left marker\n",
      "        ``'>'``          triangle_right marker\n",
      "        ``'1'``          tri_down marker\n",
      "        ``'2'``          tri_up marker\n",
      "        ``'3'``          tri_left marker\n",
      "        ``'4'``          tri_right marker\n",
      "        ``'s'``          square marker\n",
      "        ``'p'``          pentagon marker\n",
      "        ``'*'``          star marker\n",
      "        ``'h'``          hexagon1 marker\n",
      "        ``'H'``          hexagon2 marker\n",
      "        ``'+'``          plus marker\n",
      "        ``'x'``          x marker\n",
      "        ``'D'``          diamond marker\n",
      "        ``'d'``          thin_diamond marker\n",
      "        ``'|'``          vline marker\n",
      "        ``'_'``          hline marker\n",
      "        =============    ===============================\n",
      "        \n",
      "        **Line Styles**\n",
      "        \n",
      "        =============    ===============================\n",
      "        character        description\n",
      "        =============    ===============================\n",
      "        ``'-'``          solid line style\n",
      "        ``'--'``         dashed line style\n",
      "        ``'-.'``         dash-dot line style\n",
      "        ``':'``          dotted line style\n",
      "        =============    ===============================\n",
      "        \n",
      "        Example format strings::\n",
      "        \n",
      "            'b'    # blue markers with default shape\n",
      "            'or'   # red circles\n",
      "            '-g'   # green solid line\n",
      "            '--'   # dashed line with default color\n",
      "            '^k:'  # black triangle_up markers connected by a dotted line\n",
      "        \n",
      "        **Colors**\n",
      "        \n",
      "        The supported color abbreviations are the single letter codes\n",
      "        \n",
      "        =============    ===============================\n",
      "        character        color\n",
      "        =============    ===============================\n",
      "        ``'b'``          blue\n",
      "        ``'g'``          green\n",
      "        ``'r'``          red\n",
      "        ``'c'``          cyan\n",
      "        ``'m'``          magenta\n",
      "        ``'y'``          yellow\n",
      "        ``'k'``          black\n",
      "        ``'w'``          white\n",
      "        =============    ===============================\n",
      "        \n",
      "        and the ``'CN'`` colors that index into the default property cycle.\n",
      "        \n",
      "        If the color is the only part of the format string, you can\n",
      "        additionally use any  `matplotlib.colors` spec, e.g. full names\n",
      "        (``'green'``) or hex strings (``'#008000'``).\n",
      "    \n",
      "    plot_date(x, y, fmt='o', tz=None, xdate=True, ydate=False, *, data=None, **kwargs)\n",
      "        Plot data that contains dates.\n",
      "        \n",
      "        Similar to `.plot`, this plots *y* vs. *x* as lines or markers.\n",
      "        However, the axis labels are formatted as dates depending on *xdate*\n",
      "        and *ydate*.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array-like\n",
      "            The coordinates of the data points. If *xdate* or *ydate* is\n",
      "            *True*, the respective values *x* or *y* are interpreted as\n",
      "            :ref:`Matplotlib dates <date-format>`.\n",
      "        \n",
      "        fmt : str, optional\n",
      "            The plot format string. For details, see the corresponding\n",
      "            parameter in `.plot`.\n",
      "        \n",
      "        tz : timezone string or `datetime.tzinfo`, default: :rc:`timezone`\n",
      "            The time zone to use in labeling dates.\n",
      "        \n",
      "        xdate : bool, default: True\n",
      "            If *True*, the *x*-axis will be interpreted as Matplotlib dates.\n",
      "        \n",
      "        ydate : bool, default: False\n",
      "            If *True*, the *y*-axis will be interpreted as Matplotlib dates.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        lines\n",
      "            A list of `.Line2D` objects representing the plotted data.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            Keyword arguments control the `.Line2D` properties:\n",
      "        \n",
      "            Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa: bool\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            color or c: color\n",
      "            contains: unknown\n",
      "            dash_capstyle: {'butt', 'round', 'projecting'}\n",
      "            dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      "            data: (2, N) array or two 1D arrays\n",
      "            drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      "            figure: `.Figure`\n",
      "            fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      "            gid: str\n",
      "            in_layout: bool\n",
      "            label: object\n",
      "            linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      "            linewidth or lw: float\n",
      "            marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      "            markeredgecolor or mec: color\n",
      "            markeredgewidth or mew: float\n",
      "            markerfacecolor or mfc: color\n",
      "            markerfacecoloralt or mfcalt: color\n",
      "            markersize or ms: float\n",
      "            markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: unknown\n",
      "            pickradius: float\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            solid_capstyle: {'butt', 'round', 'projecting'}\n",
      "            solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            transform: `matplotlib.transforms.Transform`\n",
      "            url: str\n",
      "            visible: bool\n",
      "            xdata: 1D array\n",
      "            ydata: 1D array\n",
      "            zorder: float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        matplotlib.dates : Helper functions on dates.\n",
      "        matplotlib.dates.date2num : Convert dates to num.\n",
      "        matplotlib.dates.num2date : Convert num to dates.\n",
      "        matplotlib.dates.drange : Create an equally spaced sequence of dates.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        If you are using custom date tickers and formatters, it may be\n",
      "        necessary to set the formatters/locators after the call to\n",
      "        `.plot_date`. `.plot_date` will set the default tick locator to\n",
      "        `.AutoDateLocator` (if the tick locator is not already set to a\n",
      "        `.DateLocator` instance) and the default tick formatter to\n",
      "        `.AutoDateFormatter` (if the tick formatter is not already set to a\n",
      "        `.DateFormatter` instance).\n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            the following arguments can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception):\n",
      "            *x*, *y*.\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    plotting()\n",
      "        ============================ ====================================================================================================================\n",
      "        Function                     Description                                                                                                         \n",
      "        ============================ ====================================================================================================================\n",
      "        `acorr`                      Plot the autocorrelation of *x*.                                                                                    \n",
      "        `angle_spectrum`             Plot the angle spectrum.                                                                                            \n",
      "        `annotate`                   Annotate the point *xy* with text *text*.                                                                           \n",
      "        `arrow`                      Add an arrow to the axes.                                                                                           \n",
      "        `autoscale`                  Autoscale the axis view to the data (toggle).                                                                       \n",
      "        `axes`                       Add an axes to the current figure and make it the current axes.                                                     \n",
      "        `axhline`                    Add a horizontal line across the axis.                                                                              \n",
      "        `axhspan`                    Add a horizontal span (rectangle) across the axis.                                                                  \n",
      "        `axis`                       Convenience method to get or set some axis properties.                                                              \n",
      "        `axline`                     Add an infinitely long straight line.                                                                               \n",
      "        `axvline`                    Add a vertical line across the axes.                                                                                \n",
      "        `axvspan`                    Add a vertical span (rectangle) across the axes.                                                                    \n",
      "        `bar`                        Make a bar plot.                                                                                                    \n",
      "        `barbs`                      Plot a 2D field of barbs.                                                                                           \n",
      "        `barh`                       Make a horizontal bar plot.                                                                                         \n",
      "        `box`                        Turn the axes box on or off on the current axes.                                                                    \n",
      "        `boxplot`                    Make a box and whisker plot.                                                                                        \n",
      "        `broken_barh`                Plot a horizontal sequence of rectangles.                                                                           \n",
      "        `cla`                        Clear the current axes.                                                                                             \n",
      "        `clabel`                     Label a contour plot.                                                                                               \n",
      "        `clf`                        Clear the current figure.                                                                                           \n",
      "        `clim`                       Set the color limits of the current image.                                                                          \n",
      "        `close`                      Close a figure window.                                                                                              \n",
      "        `cohere`                     Plot the coherence between *x* and *y*.                                                                             \n",
      "        `colorbar`                   Add a colorbar to a plot.                                                                                           \n",
      "        `contour`                    Plot contours.                                                                                                      \n",
      "        `contourf`                   Plot contours.                                                                                                      \n",
      "        `csd`                        Plot the cross-spectral density.                                                                                    \n",
      "        `delaxes`                    Remove an `~.axes.Axes` (defaulting to the current axes) from its figure.                                           \n",
      "        `draw`                       Redraw the current figure.                                                                                          \n",
      "        `errorbar`                   Plot y versus x as lines and/or markers with attached errorbars.                                                    \n",
      "        `eventplot`                  Plot identical parallel lines at the given positions.                                                               \n",
      "        `figimage`                   Add a non-resampled image to the figure.                                                                            \n",
      "        `figlegend`                  Place a legend on the figure.                                                                                       \n",
      "        `fignum_exists`              Return whether the figure with the given id exists.                                                                 \n",
      "        `figtext`                    Add text to figure.                                                                                                 \n",
      "        `figure`                     Create a new figure, or activate an existing figure.                                                                \n",
      "        `fill`                       Plot filled polygons.                                                                                               \n",
      "        `fill_between`               Fill the area between two horizontal curves.                                                                        \n",
      "        `fill_betweenx`              Fill the area between two vertical curves.                                                                          \n",
      "        `findobj`                    Find artist objects.                                                                                                \n",
      "        `gca`                        Get the current axes, creating one if necessary.                                                                    \n",
      "        `gcf`                        Get the current figure.                                                                                             \n",
      "        `gci`                        Get the current colorable artist.                                                                                   \n",
      "        `get`                        Return the value of an object's *property*, or print all of them.                                                   \n",
      "        `get_figlabels`              Return a list of existing figure labels.                                                                            \n",
      "        `get_fignums`                Return a list of existing figure numbers.                                                                           \n",
      "        `getp`                       Return the value of an object's *property*, or print all of them.                                                   \n",
      "        `grid`                       Configure the grid lines.                                                                                           \n",
      "        `hexbin`                     Make a 2D hexagonal binning plot of points *x*, *y*.                                                                \n",
      "        `hist`                       Plot a histogram.                                                                                                   \n",
      "        `hist2d`                     Make a 2D histogram plot.                                                                                           \n",
      "        `hlines`                     Plot horizontal lines at each *y* from *xmin* to *xmax*.                                                            \n",
      "        `imread`                     Read an image from a file into an array.                                                                            \n",
      "        `imsave`                     Save an array as an image file.                                                                                     \n",
      "        `imshow`                     Display data as an image, i.e., on a 2D regular raster.                                                             \n",
      "        `install_repl_displayhook`   Install a repl display hook so that any stale figure are automatically redrawn when control is returned to the repl.\n",
      "        `ioff`                       Turn the interactive mode off.                                                                                      \n",
      "        `ion`                        Turn the interactive mode on.                                                                                       \n",
      "        `isinteractive`              Return if pyplot is in \"interactive mode\" or not.                                                                   \n",
      "        `legend`                     Place a legend on the axes.                                                                                         \n",
      "        `locator_params`             Control behavior of major tick locators.                                                                            \n",
      "        `loglog`                     Make a plot with log scaling on both the x and y axis.                                                              \n",
      "        `magnitude_spectrum`         Plot the magnitude spectrum.                                                                                        \n",
      "        `margins`                    Set or retrieve autoscaling margins.                                                                                \n",
      "        `matshow`                    Display an array as a matrix in a new figure window.                                                                \n",
      "        `minorticks_off`             Remove minor ticks from the axes.                                                                                   \n",
      "        `minorticks_on`              Display minor ticks on the axes.                                                                                    \n",
      "        `new_figure_manager`         Create a new figure manager instance.                                                                               \n",
      "        `pause`                      Run the GUI event loop for *interval* seconds.                                                                      \n",
      "        `pcolor`                     Create a pseudocolor plot with a non-regular rectangular grid.                                                      \n",
      "        `pcolormesh`                 Create a pseudocolor plot with a non-regular rectangular grid.                                                      \n",
      "        `phase_spectrum`             Plot the phase spectrum.                                                                                            \n",
      "        `pie`                        Plot a pie chart.                                                                                                   \n",
      "        `plot`                       Plot y versus x as lines and/or markers.                                                                            \n",
      "        `plot_date`                  Plot data that contains dates.                                                                                      \n",
      "        `polar`                      Make a polar plot.                                                                                                  \n",
      "        `psd`                        Plot the power spectral density.                                                                                    \n",
      "        `quiver`                     Plot a 2D field of arrows.                                                                                          \n",
      "        `quiverkey`                  Add a key to a quiver plot.                                                                                         \n",
      "        `rc`                         Set the current `.rcParams`.                                                                                        \n",
      "        `rc_context`                 Return a context manager for temporarily changing rcParams.                                                         \n",
      "        `rcdefaults`                 Restore the `.rcParams` from Matplotlib's internal default style.                                                   \n",
      "        `rgrids`                     Get or set the radial gridlines on the current polar plot.                                                          \n",
      "        `savefig`                    Save the current figure.                                                                                            \n",
      "        `sca`                        Set the current Axes to *ax* and the current Figure to the parent of *ax*.                                          \n",
      "        `scatter`                    A scatter plot of *y* vs.                                                                                           \n",
      "        `sci`                        Set the current image.                                                                                              \n",
      "        `semilogx`                   Make a plot with log scaling on the x axis.                                                                         \n",
      "        `semilogy`                   Make a plot with log scaling on the y axis.                                                                         \n",
      "        `set_cmap`                   Set the default colormap, and applies it to the current image if any.                                               \n",
      "        `setp`                       Set a property on an artist object.                                                                                 \n",
      "        `show`                       Display all open figures.                                                                                           \n",
      "        `specgram`                   Plot a spectrogram.                                                                                                 \n",
      "        `spy`                        Plot the sparsity pattern of a 2D array.                                                                            \n",
      "        `stackplot`                  Draw a stacked area plot.                                                                                           \n",
      "        `stem`                       Create a stem plot.                                                                                                 \n",
      "        `step`                       Make a step plot.                                                                                                   \n",
      "        `streamplot`                 Draw streamlines of a vector flow.                                                                                  \n",
      "        `subplot`                    Add a subplot to the current figure.                                                                                \n",
      "        `subplot2grid`               Create a subplot at a specific location inside a regular grid.                                                      \n",
      "        `subplot_mosaic`             Build a layout of Axes based on ASCII art or nested lists.                                                          \n",
      "        `subplot_tool`               Launch a subplot tool window for a figure.                                                                          \n",
      "        `subplots`                   Create a figure and a set of subplots.                                                                              \n",
      "        `subplots_adjust`            Adjust the subplot layout parameters.                                                                               \n",
      "        `suptitle`                   Add a centered title to the figure.                                                                                 \n",
      "        `switch_backend`             Close all open figures and set the Matplotlib backend.                                                              \n",
      "        `table`                      Add a table to an `~.axes.Axes`.                                                                                    \n",
      "        `text`                       Add text to the axes.                                                                                               \n",
      "        `thetagrids`                 Get or set the theta gridlines on the current polar plot.                                                           \n",
      "        `tick_params`                Change the appearance of ticks, tick labels, and gridlines.                                                         \n",
      "        `ticklabel_format`           Configure the `.ScalarFormatter` used by default for linear axes.                                                   \n",
      "        `tight_layout`               Adjust the padding between and around subplots.                                                                     \n",
      "        `title`                      Set a title for the axes.                                                                                           \n",
      "        `tricontour`                 Draw contour lines on an unstructured triangular grid.                                                              \n",
      "        `tricontourf`                Draw contour regions on an unstructured triangular grid.                                                            \n",
      "        `tripcolor`                  Create a pseudocolor plot of an unstructured triangular grid.                                                       \n",
      "        `triplot`                    Draw a unstructured triangular grid as lines and/or markers.                                                        \n",
      "        `twinx`                      Make and return a second axes that shares the *x*-axis.                                                             \n",
      "        `twiny`                      Make and return a second axes that shares the *y*-axis.                                                             \n",
      "        `uninstall_repl_displayhook` Uninstall the matplotlib display hook.                                                                              \n",
      "        `violinplot`                 Make a violin plot.                                                                                                 \n",
      "        `vlines`                     Plot vertical lines.                                                                                                \n",
      "        `xcorr`                      Plot the cross correlation between *x* and *y*.                                                                     \n",
      "        `xkcd`                       Turn on `xkcd <https://xkcd.com/>`_ sketch-style drawing mode.                                                      \n",
      "        `xlabel`                     Set the label for the x-axis.                                                                                       \n",
      "        `xlim`                       Get or set the x limits of the current axes.                                                                        \n",
      "        `xscale`                     Set the x-axis scale.                                                                                               \n",
      "        `xticks`                     Get or set the current tick locations and labels of the x-axis.                                                     \n",
      "        `ylabel`                     Set the label for the y-axis.                                                                                       \n",
      "        `ylim`                       Get or set the y-limits of the current axes.                                                                        \n",
      "        `yscale`                     Set the y-axis scale.                                                                                               \n",
      "        `yticks`                     Get or set the current tick locations and labels of the y-axis.                                                     \n",
      "        ============================ ====================================================================================================================\n",
      "    \n",
      "    polar(*args, **kwargs)\n",
      "        Make a polar plot.\n",
      "        \n",
      "        call signature::\n",
      "        \n",
      "          polar(theta, r, **kwargs)\n",
      "        \n",
      "        Multiple *theta*, *r* arguments are supported, with format strings, as in\n",
      "        `plot`.\n",
      "    \n",
      "    prism()\n",
      "        Set the colormap to \"prism\".\n",
      "        \n",
      "        This changes the default colormap as well as the colormap of the current\n",
      "        image if there is one. See ``help(colormaps)`` for more information.\n",
      "    \n",
      "    psd(x, NFFT=None, Fs=None, Fc=None, detrend=None, window=None, noverlap=None, pad_to=None, sides=None, scale_by_freq=None, return_line=None, *, data=None, **kwargs)\n",
      "        Plot the power spectral density.\n",
      "        \n",
      "        The power spectral density :math:`P_{xx}` by Welch's average\n",
      "        periodogram method.  The vector *x* is divided into *NFFT* length\n",
      "        segments.  Each segment is detrended by function *detrend* and\n",
      "        windowed by function *window*.  *noverlap* gives the length of\n",
      "        the overlap between segments.  The :math:`|\\mathrm{fft}(i)|^2`\n",
      "        of each segment :math:`i` are averaged to compute :math:`P_{xx}`,\n",
      "        with a scaling to correct for power loss due to windowing.\n",
      "        \n",
      "        If len(*x*) < *NFFT*, it will be zero padded to *NFFT*.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : 1-D array or sequence\n",
      "            Array or sequence containing the data\n",
      "        \n",
      "        Fs : float, default: 2\n",
      "            The sampling frequency (samples per time unit).  It is used to calculate\n",
      "            the Fourier frequencies, *freqs*, in cycles per time unit.\n",
      "        \n",
      "        window : callable or ndarray, default: `.window_hanning`\n",
      "            A function or a vector of length *NFFT*.  To create window vectors see\n",
      "            `.window_hanning`, `.window_none`, `numpy.blackman`, `numpy.hamming`,\n",
      "            `numpy.bartlett`, `scipy.signal`, `scipy.signal.get_window`, etc.  If a\n",
      "            function is passed as the argument, it must take a data segment as an\n",
      "            argument and return the windowed version of the segment.\n",
      "        \n",
      "        sides : {'default', 'onesided', 'twosided'}, optional\n",
      "            Which sides of the spectrum to return. 'default' is one-sided for real\n",
      "            data and two-sided for complex data. 'onesided' forces the return of a\n",
      "            one-sided spectrum, while 'twosided' forces two-sided.\n",
      "        \n",
      "        pad_to : int, optional\n",
      "            The number of points to which the data segment is padded when performing\n",
      "            the FFT.  This can be different from *NFFT*, which specifies the number\n",
      "            of data points used.  While not increasing the actual resolution of the\n",
      "            spectrum (the minimum distance between resolvable peaks), this can give\n",
      "            more points in the plot, allowing for more detail. This corresponds to\n",
      "            the *n* parameter in the call to fft(). The default is None, which sets\n",
      "            *pad_to* equal to *NFFT*\n",
      "        \n",
      "        NFFT : int, default: 256\n",
      "            The number of data points used in each block for the FFT.  A power 2 is\n",
      "            most efficient.  This should *NOT* be used to get zero padding, or the\n",
      "            scaling of the result will be incorrect; use *pad_to* for this instead.\n",
      "        \n",
      "        detrend : {'none', 'mean', 'linear'} or callable, default 'none'\n",
      "            The function applied to each segment before fft-ing, designed to remove\n",
      "            the mean or linear trend.  Unlike in MATLAB, where the *detrend* parameter\n",
      "            is a vector, in Matplotlib is it a function.  The :mod:`~matplotlib.mlab`\n",
      "            module defines `.detrend_none`, `.detrend_mean`, and `.detrend_linear`,\n",
      "            but you can use a custom function as well.  You can also use a string to\n",
      "            choose one of the functions: 'none' calls `.detrend_none`. 'mean' calls\n",
      "            `.detrend_mean`. 'linear' calls `.detrend_linear`.\n",
      "        \n",
      "        scale_by_freq : bool, default: True\n",
      "            Whether the resulting density values should be scaled by the scaling\n",
      "            frequency, which gives density in units of Hz^-1.  This allows for\n",
      "            integration over the returned frequency values.  The default is True for\n",
      "            MATLAB compatibility.\n",
      "        \n",
      "        noverlap : int, default: 0 (no overlap)\n",
      "            The number of points of overlap between segments.\n",
      "        \n",
      "        Fc : int, default: 0\n",
      "            The center frequency of *x*, which offsets the x extents of the\n",
      "            plot to reflect the frequency range used when a signal is acquired\n",
      "            and then filtered and downsampled to baseband.\n",
      "        \n",
      "        return_line : bool, default: False\n",
      "            Whether to include the line object plotted in the returned values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Pxx : 1-D array\n",
      "            The values for the power spectrum :math:`P_{xx}` before scaling\n",
      "            (real valued).\n",
      "        \n",
      "        freqs : 1-D array\n",
      "            The frequencies corresponding to the elements in *Pxx*.\n",
      "        \n",
      "        line : `~matplotlib.lines.Line2D`\n",
      "            The line created by this function.\n",
      "            Only returned if *return_line* is True.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            Keyword arguments control the `.Line2D` properties:\n",
      "        \n",
      "            Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa: bool\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            color or c: color\n",
      "            contains: unknown\n",
      "            dash_capstyle: {'butt', 'round', 'projecting'}\n",
      "            dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      "            data: (2, N) array or two 1D arrays\n",
      "            drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      "            figure: `.Figure`\n",
      "            fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      "            gid: str\n",
      "            in_layout: bool\n",
      "            label: object\n",
      "            linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      "            linewidth or lw: float\n",
      "            marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      "            markeredgecolor or mec: color\n",
      "            markeredgewidth or mew: float\n",
      "            markerfacecolor or mfc: color\n",
      "            markerfacecoloralt or mfcalt: color\n",
      "            markersize or ms: float\n",
      "            markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: unknown\n",
      "            pickradius: float\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            solid_capstyle: {'butt', 'round', 'projecting'}\n",
      "            solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            transform: `matplotlib.transforms.Transform`\n",
      "            url: str\n",
      "            visible: bool\n",
      "            xdata: 1D array\n",
      "            ydata: 1D array\n",
      "            zorder: float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        specgram\n",
      "            Differs in the default overlap; in not returning the mean of the\n",
      "            segment periodograms; in returning the times of the segments; and\n",
      "            in plotting a colormap instead of a line.\n",
      "        magnitude_spectrum\n",
      "            Plots the magnitude spectrum.\n",
      "        csd\n",
      "            Plots the spectral density between two signals.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        For plotting, the power is plotted as\n",
      "        :math:`10\\log_{10}(P_{xx})` for decibels, though *Pxx* itself\n",
      "        is returned.\n",
      "        \n",
      "        References\n",
      "        ----------\n",
      "        Bendat & Piersol -- Random Data: Analysis and Measurement Procedures,\n",
      "        John Wiley & Sons (1986)\n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            the following arguments can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception):\n",
      "            *x*.\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    quiver(*args, data=None, **kw)\n",
      "        Plot a 2D field of arrows.\n",
      "        \n",
      "        Call signature::\n",
      "        \n",
      "          quiver([X, Y], U, V, [C], **kw)\n",
      "        \n",
      "        *X*, *Y* define the arrow locations, *U*, *V* define the arrow directions, and\n",
      "        *C* optionally sets the color.\n",
      "        \n",
      "        **Arrow size**\n",
      "        \n",
      "        The default settings auto-scales the length of the arrows to a reasonable size.\n",
      "        To change this behavior see the *scale* and *scale_units* parameters.\n",
      "        \n",
      "        **Arrow shape**\n",
      "        \n",
      "        The defaults give a slightly swept-back arrow; to make the head a\n",
      "        triangle, make *headaxislength* the same as *headlength*. To make the\n",
      "        arrow more pointed, reduce *headwidth* or increase *headlength* and\n",
      "        *headaxislength*. To make the head smaller relative to the shaft,\n",
      "        scale down all the head parameters. You will probably do best to leave\n",
      "        minshaft alone.\n",
      "        \n",
      "        **Arrow outline**\n",
      "        \n",
      "        *linewidths* and *edgecolors* can be used to customize the arrow\n",
      "        outlines.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        X, Y : 1D or 2D array-like, optional\n",
      "            The x and y coordinates of the arrow locations.\n",
      "        \n",
      "            If not given, they will be generated as a uniform integer meshgrid based\n",
      "            on the dimensions of *U* and *V*.\n",
      "        \n",
      "            If *X* and *Y* are 1D but *U*, *V* are 2D, *X*, *Y* are expanded to 2D\n",
      "            using ``X, Y = np.meshgrid(X, Y)``. In this case ``len(X)`` and ``len(Y)``\n",
      "            must match the column and row dimensions of *U* and *V*.\n",
      "        \n",
      "        U, V : 1D or 2D array-like\n",
      "            The x and y direction components of the arrow vectors.\n",
      "        \n",
      "            They must have the same number of elements, matching the number of arrow\n",
      "            locations. *U* and *V* may be masked. Only locations unmasked in\n",
      "            *U*, *V*, and *C* will be drawn.\n",
      "        \n",
      "        C : 1D or 2D array-like, optional\n",
      "            Numeric data that defines the arrow colors by colormapping via *norm* and\n",
      "            *cmap*.\n",
      "        \n",
      "            This does not support explicit colors. If you want to set colors directly,\n",
      "            use *color* instead.  The size of *C* must match the number of arrow\n",
      "            locations.\n",
      "        \n",
      "        units : {'width', 'height', 'dots', 'inches', 'x', 'y' 'xy'}, default: 'width'\n",
      "            The arrow dimensions (except for *length*) are measured in multiples of\n",
      "            this unit.\n",
      "        \n",
      "            The following values are supported:\n",
      "        \n",
      "            - 'width', 'height': The width or height of the axis.\n",
      "            - 'dots', 'inches': Pixels or inches based on the figure dpi.\n",
      "            - 'x', 'y', 'xy': *X*, *Y* or :math:`\\sqrt{X^2 + Y^2}` in data units.\n",
      "        \n",
      "            The arrows scale differently depending on the units.  For\n",
      "            'x' or 'y', the arrows get larger as one zooms in; for other\n",
      "            units, the arrow size is independent of the zoom state.  For\n",
      "            'width or 'height', the arrow size increases with the width and\n",
      "            height of the axes, respectively, when the window is resized;\n",
      "            for 'dots' or 'inches', resizing does not change the arrows.\n",
      "        \n",
      "        angles : {'uv', 'xy'} or array-like, default: 'uv'\n",
      "            Method for determining the angle of the arrows.\n",
      "        \n",
      "            - 'uv': The arrow axis aspect ratio is 1 so that\n",
      "              if *U* == *V* the orientation of the arrow on the plot is 45 degrees\n",
      "              counter-clockwise from the horizontal axis (positive to the right).\n",
      "        \n",
      "              Use this if the arrows symbolize a quantity that is not based on\n",
      "              *X*, *Y* data coordinates.\n",
      "        \n",
      "            - 'xy': Arrows point from (x, y) to (x+u, y+v).\n",
      "              Use this for plotting a gradient field, for example.\n",
      "        \n",
      "            - Alternatively, arbitrary angles may be specified explicitly as an array\n",
      "              of values in degrees, counter-clockwise from the horizontal axis.\n",
      "        \n",
      "              In this case *U*, *V* is only used to determine the length of the\n",
      "              arrows.\n",
      "        \n",
      "            Note: inverting a data axis will correspondingly invert the\n",
      "            arrows only with ``angles='xy'``.\n",
      "        \n",
      "        scale : float, optional\n",
      "            Number of data units per arrow length unit, e.g., m/s per plot width; a\n",
      "            smaller scale parameter makes the arrow longer. Default is *None*.\n",
      "        \n",
      "            If *None*, a simple autoscaling algorithm is used, based on the average\n",
      "            vector length and the number of vectors. The arrow length unit is given by\n",
      "            the *scale_units* parameter.\n",
      "        \n",
      "        scale_units : {'width', 'height', 'dots', 'inches', 'x', 'y', 'xy'}, optional\n",
      "            If the *scale* kwarg is *None*, the arrow length unit. Default is *None*.\n",
      "        \n",
      "            e.g. *scale_units* is 'inches', *scale* is 2.0, and ``(u, v) = (1, 0)``,\n",
      "            then the vector will be 0.5 inches long.\n",
      "        \n",
      "            If *scale_units* is 'width' or 'height', then the vector will be half the\n",
      "            width/height of the axes.\n",
      "        \n",
      "            If *scale_units* is 'x' then the vector will be 0.5 x-axis\n",
      "            units. To plot vectors in the x-y plane, with u and v having\n",
      "            the same units as x and y, use\n",
      "            ``angles='xy', scale_units='xy', scale=1``.\n",
      "        \n",
      "        width : float, optional\n",
      "            Shaft width in arrow units; default depends on choice of units,\n",
      "            above, and number of vectors; a typical starting value is about\n",
      "            0.005 times the width of the plot.\n",
      "        \n",
      "        headwidth : float, default: 3\n",
      "            Head width as multiple of shaft width.\n",
      "        \n",
      "        headlength : float, default: 5\n",
      "            Head length as multiple of shaft width.\n",
      "        \n",
      "        headaxislength : float, default: 4.5\n",
      "            Head length at shaft intersection.\n",
      "        \n",
      "        minshaft : float, default: 1\n",
      "            Length below which arrow scales, in units of head length. Do not\n",
      "            set this to less than 1, or small arrows will look terrible!\n",
      "        \n",
      "        minlength : float, default: 1\n",
      "            Minimum length as a multiple of shaft width; if an arrow length\n",
      "            is less than this, plot a dot (hexagon) of this diameter instead.\n",
      "        \n",
      "        pivot : {'tail', 'mid', 'middle', 'tip'}, default: 'tail'\n",
      "            The part of the arrow that is anchored to the *X*, *Y* grid. The arrow\n",
      "            rotates about this point.\n",
      "        \n",
      "            'mid' is a synonym for 'middle'.\n",
      "        \n",
      "        color : color or color sequence, optional\n",
      "            Explicit color(s) for the arrows. If *C* has been set, *color* has no\n",
      "            effect.\n",
      "        \n",
      "            This is a synonym for the `~.PolyCollection` *facecolor* parameter.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs : `~matplotlib.collections.PolyCollection` properties, optional\n",
      "            All other keyword arguments are passed on to `.PolyCollection`:\n",
      "        \n",
      "            Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa or antialiaseds: bool or list of bools\n",
      "            array: ndarray\n",
      "            capstyle: {'butt', 'round', 'projecting'}\n",
      "            clim: (vmin: float, vmax: float)\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            cmap: `.Colormap` or str or None\n",
      "            color: color or list of rgba tuples\n",
      "            contains: unknown\n",
      "            edgecolor or ec or edgecolors: color or list of colors or 'face'\n",
      "            facecolor or facecolors or fc: color or list of colors\n",
      "            figure: `.Figure`\n",
      "            gid: str\n",
      "            hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n",
      "            in_layout: bool\n",
      "            joinstyle: {'miter', 'round', 'bevel'}\n",
      "            label: object\n",
      "            linestyle or dashes or linestyles or ls: str or tuple or list thereof\n",
      "            linewidth or linewidths or lw: float or list of floats\n",
      "            norm: `.Normalize` or None\n",
      "            offset_position: unknown\n",
      "            offsets: array-like (N, 2) or (2,)\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: None or bool or callable\n",
      "            pickradius: unknown\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            transform: `.Transform`\n",
      "            url: str\n",
      "            urls: list of str or None\n",
      "            visible: bool\n",
      "            zorder: float\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        .Axes.quiverkey : Add a key to a quiver plot.\n",
      "    \n",
      "    quiverkey(Q, X, Y, U, label, **kw)\n",
      "        Add a key to a quiver plot.\n",
      "        \n",
      "        The positioning of the key depends on *X*, *Y*, *coordinates*, and\n",
      "        *labelpos*.  If *labelpos* is 'N' or 'S', *X*, *Y* give the position of\n",
      "        the middle of the key arrow.  If *labelpos* is 'E', *X*, *Y* positions\n",
      "        the head, and if *labelpos* is 'W', *X*, *Y* positions the tail; in\n",
      "        either of these two cases, *X*, *Y* is somewhere in the middle of the\n",
      "        arrow+label key object.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        Q : `matplotlib.quiver.Quiver`\n",
      "            A `.Quiver` object as returned by a call to `~.Axes.quiver()`.\n",
      "        X, Y : float\n",
      "            The location of the key.\n",
      "        U : float\n",
      "            The length of the key.\n",
      "        label : str\n",
      "            The key label (e.g., length and units of the key).\n",
      "        angle : float, default: 0\n",
      "            The angle of the key arrow, in degrees anti-clockwise from the\n",
      "            x-axis.\n",
      "        coordinates : {'axes', 'figure', 'data', 'inches'}, default: 'axes'\n",
      "            Coordinate system and units for *X*, *Y*: 'axes' and 'figure' are\n",
      "            normalized coordinate systems with (0, 0) in the lower left and\n",
      "            (1, 1) in the upper right; 'data' are the axes data coordinates\n",
      "            (used for the locations of the vectors in the quiver plot itself);\n",
      "            'inches' is position in the figure in inches, with (0, 0) at the\n",
      "            lower left corner.\n",
      "        color : color\n",
      "            Overrides face and edge colors from *Q*.\n",
      "        labelpos : {'N', 'S', 'E', 'W'}\n",
      "            Position the label above, below, to the right, to the left of the\n",
      "            arrow, respectively.\n",
      "        labelsep : float, default: 0.1\n",
      "            Distance in inches between the arrow and the label.\n",
      "        labelcolor : color, default: :rc:`text.color`\n",
      "            Label color.\n",
      "        fontproperties : dict, optional\n",
      "            A dictionary with keyword arguments accepted by the\n",
      "            `~matplotlib.font_manager.FontProperties` initializer:\n",
      "            *family*, *style*, *variant*, *size*, *weight*.\n",
      "        **kwargs\n",
      "            Any additional keyword arguments are used to override vector\n",
      "            properties taken from *Q*.\n",
      "    \n",
      "    rc(group, **kwargs)\n",
      "        Set the current `.rcParams`.  *group* is the grouping for the rc, e.g.,\n",
      "        for ``lines.linewidth`` the group is ``lines``, for\n",
      "        ``axes.facecolor``, the group is ``axes``, and so on.  Group may\n",
      "        also be a list or tuple of group names, e.g., (*xtick*, *ytick*).\n",
      "        *kwargs* is a dictionary attribute name/value pairs, e.g.,::\n",
      "        \n",
      "          rc('lines', linewidth=2, color='r')\n",
      "        \n",
      "        sets the current `.rcParams` and is equivalent to::\n",
      "        \n",
      "          rcParams['lines.linewidth'] = 2\n",
      "          rcParams['lines.color'] = 'r'\n",
      "        \n",
      "        The following aliases are available to save typing for interactive users:\n",
      "        \n",
      "        =====   =================\n",
      "        Alias   Property\n",
      "        =====   =================\n",
      "        'lw'    'linewidth'\n",
      "        'ls'    'linestyle'\n",
      "        'c'     'color'\n",
      "        'fc'    'facecolor'\n",
      "        'ec'    'edgecolor'\n",
      "        'mew'   'markeredgewidth'\n",
      "        'aa'    'antialiased'\n",
      "        =====   =================\n",
      "        \n",
      "        Thus you could abbreviate the above call as::\n",
      "        \n",
      "              rc('lines', lw=2, c='r')\n",
      "        \n",
      "        Note you can use python's kwargs dictionary facility to store\n",
      "        dictionaries of default parameters.  e.g., you can customize the\n",
      "        font rc as follows::\n",
      "        \n",
      "          font = {'family' : 'monospace',\n",
      "                  'weight' : 'bold',\n",
      "                  'size'   : 'larger'}\n",
      "          rc('font', **font)  # pass in the font dict as kwargs\n",
      "        \n",
      "        This enables you to easily switch between several configurations.  Use\n",
      "        ``matplotlib.style.use('default')`` or :func:`~matplotlib.rcdefaults` to\n",
      "        restore the default `.rcParams` after changes.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Similar functionality is available by using the normal dict interface, i.e.\n",
      "        ``rcParams.update({\"lines.linewidth\": 2, ...})`` (but ``rcParams.update``\n",
      "        does not support abbreviations or grouping).\n",
      "    \n",
      "    rc_context(rc=None, fname=None)\n",
      "        Return a context manager for temporarily changing rcParams.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        rc : dict\n",
      "            The rcParams to temporarily set.\n",
      "        fname : str or path-like\n",
      "            A file with Matplotlib rc settings. If both *fname* and *rc* are given,\n",
      "            settings from *rc* take precedence.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        :ref:`customizing-with-matplotlibrc-files`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Passing explicit values via a dict::\n",
      "        \n",
      "            with mpl.rc_context({'interactive': False}):\n",
      "                fig, ax = plt.subplots()\n",
      "                ax.plot(range(3), range(3))\n",
      "                fig.savefig('example.png')\n",
      "                plt.close(fig)\n",
      "        \n",
      "        Loading settings from a file::\n",
      "        \n",
      "             with mpl.rc_context(fname='print.rc'):\n",
      "                 plt.plot(x, y)  # uses 'print.rc'\n",
      "    \n",
      "    rcdefaults()\n",
      "        Restore the `.rcParams` from Matplotlib's internal default style.\n",
      "        \n",
      "        Style-blacklisted `.rcParams` (defined in\n",
      "        `matplotlib.style.core.STYLE_BLACKLIST`) are not updated.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        matplotlib.rc_file_defaults\n",
      "            Restore the `.rcParams` from the rc file originally loaded by\n",
      "            Matplotlib.\n",
      "        matplotlib.style.use\n",
      "            Use a specific style file.  Call ``style.use('default')`` to restore\n",
      "            the default style.\n",
      "    \n",
      "    rgrids(radii=None, labels=None, angle=None, fmt=None, **kwargs)\n",
      "        Get or set the radial gridlines on the current polar plot.\n",
      "        \n",
      "        Call signatures::\n",
      "        \n",
      "         lines, labels = rgrids()\n",
      "         lines, labels = rgrids(radii, labels=None, angle=22.5, fmt=None, **kwargs)\n",
      "        \n",
      "        When called with no arguments, `.rgrids` simply returns the tuple\n",
      "        (*lines*, *labels*). When called with arguments, the labels will\n",
      "        appear at the specified radial distances and angle.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        radii : tuple with floats\n",
      "            The radii for the radial gridlines\n",
      "        \n",
      "        labels : tuple with strings or None\n",
      "            The labels to use at each radial gridline. The\n",
      "            `matplotlib.ticker.ScalarFormatter` will be used if None.\n",
      "        \n",
      "        angle : float\n",
      "            The angular position of the radius labels in degrees.\n",
      "        \n",
      "        fmt : str or None\n",
      "            Format string used in `matplotlib.ticker.FormatStrFormatter`.\n",
      "            For example '%f'.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        lines : list of `.lines.Line2D`\n",
      "            The radial gridlines.\n",
      "        \n",
      "        labels : list of `.text.Text`\n",
      "            The tick labels.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            *kwargs* are optional `~.Text` properties for the labels.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        .pyplot.thetagrids\n",
      "        .projections.polar.PolarAxes.set_rgrids\n",
      "        .Axis.get_gridlines\n",
      "        .Axis.get_ticklabels\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        ::\n",
      "        \n",
      "          # set the locations of the radial gridlines\n",
      "          lines, labels = rgrids( (0.25, 0.5, 1.0) )\n",
      "        \n",
      "          # set the locations and labels of the radial gridlines\n",
      "          lines, labels = rgrids( (0.25, 0.5, 1.0), ('Tom', 'Dick', 'Harry' ))\n",
      "    \n",
      "    savefig(*args, **kwargs)\n",
      "        Save the current figure.\n",
      "        \n",
      "        Call signature::\n",
      "        \n",
      "          savefig(fname, dpi=None, facecolor='w', edgecolor='w',\n",
      "                  orientation='portrait', papertype=None, format=None,\n",
      "                  transparent=False, bbox_inches=None, pad_inches=0.1,\n",
      "                  frameon=None, metadata=None)\n",
      "        \n",
      "        The available output formats depend on the backend being used.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        fname : str or path-like or file-like\n",
      "            A path, or a Python file-like object, or\n",
      "            possibly some backend-dependent object such as\n",
      "            `matplotlib.backends.backend_pdf.PdfPages`.\n",
      "        \n",
      "            If *format* is set, it determines the output format, and the file\n",
      "            is saved as *fname*.  Note that *fname* is used verbatim, and there\n",
      "            is no attempt to make the extension, if any, of *fname* match\n",
      "            *format*, and no extension is appended.\n",
      "        \n",
      "            If *format* is not set, then the format is inferred from the\n",
      "            extension of *fname*, if there is one.  If *format* is not\n",
      "            set and *fname* has no extension, then the file is saved with\n",
      "            :rc:`savefig.format` and the appropriate extension is appended to\n",
      "            *fname*.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        dpi : float or 'figure', default: :rc:`savefig.dpi`\n",
      "            The resolution in dots per inch.  If 'figure', use the figure's\n",
      "            dpi value.\n",
      "        \n",
      "        quality : int, default: :rc:`savefig.jpeg_quality`\n",
      "            Applicable only if *format* is 'jpg' or 'jpeg', ignored otherwise.\n",
      "        \n",
      "            The image quality, on a scale from 1 (worst) to 95 (best).\n",
      "            Values above 95 should be avoided; 100 disables portions of\n",
      "            the JPEG compression algorithm, and results in large files\n",
      "            with hardly any gain in image quality.\n",
      "        \n",
      "            This parameter is deprecated.\n",
      "        \n",
      "        optimize : bool, default: False\n",
      "            Applicable only if *format* is 'jpg' or 'jpeg', ignored otherwise.\n",
      "        \n",
      "            Whether the encoder should make an extra pass over the image\n",
      "            in order to select optimal encoder settings.\n",
      "        \n",
      "            This parameter is deprecated.\n",
      "        \n",
      "        progressive : bool, default: False\n",
      "            Applicable only if *format* is 'jpg' or 'jpeg', ignored otherwise.\n",
      "        \n",
      "            Whether the image should be stored as a progressive JPEG file.\n",
      "        \n",
      "            This parameter is deprecated.\n",
      "        \n",
      "        facecolor : color or 'auto', default: :rc:`savefig.facecolor`\n",
      "            The facecolor of the figure.  If 'auto', use the current figure\n",
      "            facecolor.\n",
      "        \n",
      "        edgecolor : color or 'auto', default: :rc:`savefig.edgecolor`\n",
      "            The edgecolor of the figure.  If 'auto', use the current figure\n",
      "            edgecolor.\n",
      "        \n",
      "        orientation : {'landscape', 'portrait'}\n",
      "            Currently only supported by the postscript backend.\n",
      "        \n",
      "        papertype : str\n",
      "            One of 'letter', 'legal', 'executive', 'ledger', 'a0' through\n",
      "            'a10', 'b0' through 'b10'. Only supported for postscript\n",
      "            output.\n",
      "        \n",
      "        format : str\n",
      "            The file format, e.g. 'png', 'pdf', 'svg', ... The behavior when\n",
      "            this is unset is documented under *fname*.\n",
      "        \n",
      "        transparent : bool\n",
      "            If *True*, the axes patches will all be transparent; the\n",
      "            figure patch will also be transparent unless facecolor\n",
      "            and/or edgecolor are specified via kwargs.\n",
      "            This is useful, for example, for displaying\n",
      "            a plot on top of a colored background on a web page.  The\n",
      "            transparency of these patches will be restored to their\n",
      "            original values upon exit of this function.\n",
      "        \n",
      "        bbox_inches : str or `.Bbox`, default: :rc:`savefig.bbox`\n",
      "            Bounding box in inches: only the given portion of the figure is\n",
      "            saved.  If 'tight', try to figure out the tight bbox of the figure.\n",
      "        \n",
      "        pad_inches : float, default: :rc:`savefig.pad_inches`\n",
      "            Amount of padding around the figure when bbox_inches is 'tight'.\n",
      "        \n",
      "        bbox_extra_artists : list of `~matplotlib.artist.Artist`, optional\n",
      "            A list of extra artists that will be considered when the\n",
      "            tight bbox is calculated.\n",
      "        \n",
      "        backend : str, optional\n",
      "            Use a non-default backend to render the file, e.g. to render a\n",
      "            png file with the \"cairo\" backend rather than the default \"agg\",\n",
      "            or a pdf file with the \"pgf\" backend rather than the default\n",
      "            \"pdf\".  Note that the default backend is normally sufficient.  See\n",
      "            :ref:`the-builtin-backends` for a list of valid backends for each\n",
      "            file format.  Custom backends can be referenced as \"module://...\".\n",
      "        \n",
      "        metadata : dict, optional\n",
      "            Key/value pairs to store in the image metadata. The supported keys\n",
      "            and defaults depend on the image format and backend:\n",
      "        \n",
      "            - 'png' with Agg backend: See the parameter ``metadata`` of\n",
      "              `~.FigureCanvasAgg.print_png`.\n",
      "            - 'pdf' with pdf backend: See the parameter ``metadata`` of\n",
      "              `~.backend_pdf.PdfPages`.\n",
      "            - 'svg' with svg backend: See the parameter ``metadata`` of\n",
      "              `~.FigureCanvasSVG.print_svg`.\n",
      "            - 'eps' and 'ps' with PS backend: Only 'Creator' is supported.\n",
      "        \n",
      "        pil_kwargs : dict, optional\n",
      "            Additional keyword arguments that are passed to\n",
      "            `PIL.Image.Image.save` when saving the figure.\n",
      "    \n",
      "    sca(ax)\n",
      "        Set the current Axes to *ax* and the current Figure to the parent of *ax*.\n",
      "    \n",
      "    scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, verts=<deprecated parameter>, edgecolors=None, *, plotnonfinite=False, data=None, **kwargs)\n",
      "        A scatter plot of *y* vs. *x* with varying marker size and/or color.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : float or array-like, shape (n, )\n",
      "            The data positions.\n",
      "        \n",
      "        s : float or array-like, shape (n, ), optional\n",
      "            The marker size in points**2.\n",
      "            Default is ``rcParams['lines.markersize'] ** 2``.\n",
      "        \n",
      "        c : array-like or list of colors or color, optional\n",
      "            The marker colors. Possible values:\n",
      "        \n",
      "            - A scalar or sequence of n numbers to be mapped to colors using\n",
      "              *cmap* and *norm*.\n",
      "            - A 2-D array in which the rows are RGB or RGBA.\n",
      "            - A sequence of colors of length n.\n",
      "            - A single color format string.\n",
      "        \n",
      "            Note that *c* should not be a single numeric RGB or RGBA sequence\n",
      "            because that is indistinguishable from an array of values to be\n",
      "            colormapped. If you want to specify the same RGB or RGBA value for\n",
      "            all points, use a 2-D array with a single row.  Otherwise, value-\n",
      "            matching will have precedence in case of a size matching with *x*\n",
      "            and *y*.\n",
      "        \n",
      "            If you wish to specify a single color for all points\n",
      "            prefer the *color* keyword argument.\n",
      "        \n",
      "            Defaults to `None`. In that case the marker color is determined\n",
      "            by the value of *color*, *facecolor* or *facecolors*. In case\n",
      "            those are not specified or `None`, the marker color is determined\n",
      "            by the next color of the ``Axes``' current \"shape and fill\" color\n",
      "            cycle. This cycle defaults to :rc:`axes.prop_cycle`.\n",
      "        \n",
      "        marker : `~.markers.MarkerStyle`, default: :rc:`scatter.marker`\n",
      "            The marker style. *marker* can be either an instance of the class\n",
      "            or the text shorthand for a particular marker.\n",
      "            See :mod:`matplotlib.markers` for more information about marker\n",
      "            styles.\n",
      "        \n",
      "        cmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n",
      "            A `.Colormap` instance or registered colormap name. *cmap* is only\n",
      "            used if *c* is an array of floats.\n",
      "        \n",
      "        norm : `~matplotlib.colors.Normalize`, default: None\n",
      "            If *c* is an array of floats, *norm* is used to scale the color\n",
      "            data, *c*, in the range 0 to 1, in order to map into the colormap\n",
      "            *cmap*.\n",
      "            If *None*, use the default `.colors.Normalize`.\n",
      "        \n",
      "        vmin, vmax : float, default: None\n",
      "            *vmin* and *vmax* are used in conjunction with the default norm to\n",
      "            map the color array *c* to the colormap *cmap*. If None, the\n",
      "            respective min and max of the color array is used.\n",
      "            It is deprecated to use *vmin*/*vmax* when *norm* is given.\n",
      "        \n",
      "        alpha : float, default: None\n",
      "            The alpha blending value, between 0 (transparent) and 1 (opaque).\n",
      "        \n",
      "        linewidths : float or array-like, default: :rc:`lines.linewidth`\n",
      "            The linewidth of the marker edges. Note: The default *edgecolors*\n",
      "            is 'face'. You may want to change this as well.\n",
      "        \n",
      "        edgecolors : {'face', 'none', *None*} or color or sequence of color, default: :rc:`scatter.edgecolors`\n",
      "            The edge color of the marker. Possible values:\n",
      "        \n",
      "            - 'face': The edge color will always be the same as the face color.\n",
      "            - 'none': No patch boundary will be drawn.\n",
      "            - A color or sequence of colors.\n",
      "        \n",
      "            For non-filled markers, the *edgecolors* kwarg is ignored and\n",
      "            forced to 'face' internally.\n",
      "        \n",
      "        plotnonfinite : bool, default: False\n",
      "            Set to plot points with nonfinite *c*, in conjunction with\n",
      "            `~matplotlib.colors.Colormap.set_bad`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `~matplotlib.collections.PathCollection`\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs : `~matplotlib.collections.Collection` properties\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        plot : To plot scatter plots when markers are identical in size and\n",
      "            color.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        * The `.plot` function will be faster for scatterplots where markers\n",
      "          don't vary in size or color.\n",
      "        \n",
      "        * Any or all of *x*, *y*, *s*, and *c* may be masked arrays, in which\n",
      "          case all masks will be combined and only unmasked points will be\n",
      "          plotted.\n",
      "        \n",
      "        * Fundamentally, scatter works with 1-D arrays; *x*, *y*, *s*, and *c*\n",
      "          may be input as N-D arrays, but within scatter they will be\n",
      "          flattened. The exception is *c*, which will be flattened only if its\n",
      "          size matches the size of *x* and *y*.\n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            the following arguments can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception):\n",
      "            *x*, *y*, *s*, *linewidths*, *edgecolors*, *c*, *facecolor*, *facecolors*, *color*.\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    sci(im)\n",
      "        Set the current image.\n",
      "        \n",
      "        This image will be the target of colormap functions like\n",
      "        `~.pyplot.viridis`, and other functions such as `~.pyplot.clim`.  The\n",
      "        current image is an attribute of the current axes.\n",
      "    \n",
      "    semilogx(*args, **kwargs)\n",
      "        Make a plot with log scaling on the x axis.\n",
      "        \n",
      "        Call signatures::\n",
      "        \n",
      "            semilogx([x], y, [fmt], data=None, **kwargs)\n",
      "            semilogx([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n",
      "        \n",
      "        This is just a thin wrapper around `.plot` which additionally changes\n",
      "        the x-axis to log scaling. All of the concepts and parameters of plot\n",
      "        can be used here as well.\n",
      "        \n",
      "        The additional parameters *base*, *subs*, and *nonpositive* control the\n",
      "        x-axis properties. They are just forwarded to `.Axes.set_xscale`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        base : float, default: 10\n",
      "            Base of the x logarithm.\n",
      "        \n",
      "        subs : array-like, optional\n",
      "            The location of the minor xticks. If *None*, reasonable locations\n",
      "            are automatically chosen depending on the number of decades in the\n",
      "            plot. See `.Axes.set_xscale` for details.\n",
      "        \n",
      "        nonpositive : {'mask', 'clip'}, default: 'mask'\n",
      "            Non-positive values in x can be masked as invalid, or clipped to a\n",
      "            very small positive number.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        lines\n",
      "            A list of `.Line2D` objects representing the plotted data.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            All parameters supported by `.plot`.\n",
      "    \n",
      "    semilogy(*args, **kwargs)\n",
      "        Make a plot with log scaling on the y axis.\n",
      "        \n",
      "        Call signatures::\n",
      "        \n",
      "            semilogy([x], y, [fmt], data=None, **kwargs)\n",
      "            semilogy([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n",
      "        \n",
      "        This is just a thin wrapper around `.plot` which additionally changes\n",
      "        the y-axis to log scaling. All of the concepts and parameters of plot\n",
      "        can be used here as well.\n",
      "        \n",
      "        The additional parameters *base*, *subs*, and *nonpositive* control the\n",
      "        y-axis properties. They are just forwarded to `.Axes.set_yscale`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        base : float, default: 10\n",
      "            Base of the y logarithm.\n",
      "        \n",
      "        subs : array-like, optional\n",
      "            The location of the minor yticks. If *None*, reasonable locations\n",
      "            are automatically chosen depending on the number of decades in the\n",
      "            plot. See `.Axes.set_yscale` for details.\n",
      "        \n",
      "        nonpositive : {'mask', 'clip'}, default: 'mask'\n",
      "            Non-positive values in y can be masked as invalid, or clipped to a\n",
      "            very small positive number.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        lines\n",
      "            A list of `.Line2D` objects representing the plotted data.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            All parameters supported by `.plot`.\n",
      "    \n",
      "    set_cmap(cmap)\n",
      "        Set the default colormap, and applies it to the current image if any.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cmap : `~matplotlib.colors.Colormap` or str\n",
      "            A colormap instance or the name of a registered colormap.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        colormaps\n",
      "        matplotlib.cm.register_cmap\n",
      "        matplotlib.cm.get_cmap\n",
      "    \n",
      "    setp(obj, *args, **kwargs)\n",
      "        Set a property on an artist object.\n",
      "        \n",
      "        matplotlib supports the use of :func:`setp` (\"set property\") and\n",
      "        :func:`getp` to set and get object properties, as well as to do\n",
      "        introspection on the object.  For example, to set the linestyle of a\n",
      "        line to be dashed, you can do::\n",
      "        \n",
      "          >>> line, = plot([1, 2, 3])\n",
      "          >>> setp(line, linestyle='--')\n",
      "        \n",
      "        If you want to know the valid types of arguments, you can provide\n",
      "        the name of the property you want to set without a value::\n",
      "        \n",
      "          >>> setp(line, 'linestyle')\n",
      "              linestyle: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      "        \n",
      "        If you want to see all the properties that can be set, and their\n",
      "        possible values, you can do::\n",
      "        \n",
      "          >>> setp(line)\n",
      "              ... long output listing omitted\n",
      "        \n",
      "        By default `setp` prints to `sys.stdout`, but this can be modified using\n",
      "        the *file* keyword-only argument::\n",
      "        \n",
      "          >>> with fopen('output.log') as f:\n",
      "          >>>     setp(line, file=f)\n",
      "        \n",
      "        :func:`setp` operates on a single instance or a iterable of\n",
      "        instances. If you are in query mode introspecting the possible\n",
      "        values, only the first instance in the sequence is used. When\n",
      "        actually setting values, all the instances will be set.  e.g.,\n",
      "        suppose you have a list of two lines, the following will make both\n",
      "        lines thicker and red::\n",
      "        \n",
      "          >>> x = arange(0, 1, 0.01)\n",
      "          >>> y1 = sin(2*pi*x)\n",
      "          >>> y2 = sin(4*pi*x)\n",
      "          >>> lines = plot(x, y1, x, y2)\n",
      "          >>> setp(lines, linewidth=2, color='r')\n",
      "        \n",
      "        :func:`setp` works with the MATLAB style string/value pairs or\n",
      "        with python kwargs.  For example, the following are equivalent::\n",
      "        \n",
      "          >>> setp(lines, 'linewidth', 2, 'color', 'r')  # MATLAB style\n",
      "          >>> setp(lines, linewidth=2, color='r')        # python style\n",
      "    \n",
      "    show(close=None, block=None)\n",
      "        Display all open figures.\n",
      "        \n",
      "        In non-interactive mode, *block* defaults to True.  All figures\n",
      "        will display and show will not return until all windows are closed.\n",
      "        If there are no figures, return immediately.\n",
      "        \n",
      "        In interactive mode *block* defaults to False.  This will ensure\n",
      "        that all of the figures are shown and this function immediately returns.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        block : bool, optional\n",
      "        \n",
      "            If `True` block and run the GUI main loop until all windows\n",
      "            are closed.\n",
      "        \n",
      "            If `False` ensure that all windows are displayed and return\n",
      "            immediately.  In this case, you are responsible for ensuring\n",
      "            that the event loop is running to have responsive figures.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        ion : enable interactive mode\n",
      "        ioff : disable interactive mode\n",
      "    \n",
      "    specgram(x, NFFT=None, Fs=None, Fc=None, detrend=None, window=None, noverlap=None, cmap=None, xextent=None, pad_to=None, sides=None, scale_by_freq=None, mode=None, scale=None, vmin=None, vmax=None, *, data=None, **kwargs)\n",
      "        Plot a spectrogram.\n",
      "        \n",
      "        Compute and plot a spectrogram of data in *x*.  Data are split into\n",
      "        *NFFT* length segments and the spectrum of each section is\n",
      "        computed.  The windowing function *window* is applied to each\n",
      "        segment, and the amount of overlap of each segment is\n",
      "        specified with *noverlap*. The spectrogram is plotted as a colormap\n",
      "        (using imshow).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : 1-D array or sequence\n",
      "            Array or sequence containing the data.\n",
      "        \n",
      "        Fs : float, default: 2\n",
      "            The sampling frequency (samples per time unit).  It is used to calculate\n",
      "            the Fourier frequencies, *freqs*, in cycles per time unit.\n",
      "        \n",
      "        window : callable or ndarray, default: `.window_hanning`\n",
      "            A function or a vector of length *NFFT*.  To create window vectors see\n",
      "            `.window_hanning`, `.window_none`, `numpy.blackman`, `numpy.hamming`,\n",
      "            `numpy.bartlett`, `scipy.signal`, `scipy.signal.get_window`, etc.  If a\n",
      "            function is passed as the argument, it must take a data segment as an\n",
      "            argument and return the windowed version of the segment.\n",
      "        \n",
      "        sides : {'default', 'onesided', 'twosided'}, optional\n",
      "            Which sides of the spectrum to return. 'default' is one-sided for real\n",
      "            data and two-sided for complex data. 'onesided' forces the return of a\n",
      "            one-sided spectrum, while 'twosided' forces two-sided.\n",
      "        \n",
      "        pad_to : int, optional\n",
      "            The number of points to which the data segment is padded when performing\n",
      "            the FFT.  This can be different from *NFFT*, which specifies the number\n",
      "            of data points used.  While not increasing the actual resolution of the\n",
      "            spectrum (the minimum distance between resolvable peaks), this can give\n",
      "            more points in the plot, allowing for more detail. This corresponds to\n",
      "            the *n* parameter in the call to fft(). The default is None, which sets\n",
      "            *pad_to* equal to *NFFT*\n",
      "        \n",
      "        NFFT : int, default: 256\n",
      "            The number of data points used in each block for the FFT.  A power 2 is\n",
      "            most efficient.  This should *NOT* be used to get zero padding, or the\n",
      "            scaling of the result will be incorrect; use *pad_to* for this instead.\n",
      "        \n",
      "        detrend : {'none', 'mean', 'linear'} or callable, default 'none'\n",
      "            The function applied to each segment before fft-ing, designed to remove\n",
      "            the mean or linear trend.  Unlike in MATLAB, where the *detrend* parameter\n",
      "            is a vector, in Matplotlib is it a function.  The :mod:`~matplotlib.mlab`\n",
      "            module defines `.detrend_none`, `.detrend_mean`, and `.detrend_linear`,\n",
      "            but you can use a custom function as well.  You can also use a string to\n",
      "            choose one of the functions: 'none' calls `.detrend_none`. 'mean' calls\n",
      "            `.detrend_mean`. 'linear' calls `.detrend_linear`.\n",
      "        \n",
      "        scale_by_freq : bool, default: True\n",
      "            Whether the resulting density values should be scaled by the scaling\n",
      "            frequency, which gives density in units of Hz^-1.  This allows for\n",
      "            integration over the returned frequency values.  The default is True for\n",
      "            MATLAB compatibility.\n",
      "        \n",
      "        mode : {'default', 'psd', 'magnitude', 'angle', 'phase'}\n",
      "            What sort of spectrum to use.  Default is 'psd', which takes the\n",
      "            power spectral density.  'magnitude' returns the magnitude\n",
      "            spectrum.  'angle' returns the phase spectrum without unwrapping.\n",
      "            'phase' returns the phase spectrum with unwrapping.\n",
      "        \n",
      "        noverlap : int\n",
      "            The number of points of overlap between blocks.  The\n",
      "            default value is 128.\n",
      "        \n",
      "        scale : {'default', 'linear', 'dB'}\n",
      "            The scaling of the values in the *spec*.  'linear' is no scaling.\n",
      "            'dB' returns the values in dB scale.  When *mode* is 'psd',\n",
      "            this is dB power (10 * log10).  Otherwise this is dB amplitude\n",
      "            (20 * log10). 'default' is 'dB' if *mode* is 'psd' or\n",
      "            'magnitude' and 'linear' otherwise.  This must be 'linear'\n",
      "            if *mode* is 'angle' or 'phase'.\n",
      "        \n",
      "        Fc : int, default: 0\n",
      "            The center frequency of *x*, which offsets the x extents of the\n",
      "            plot to reflect the frequency range used when a signal is acquired\n",
      "            and then filtered and downsampled to baseband.\n",
      "        \n",
      "        cmap : `.Colormap`, default: :rc:`image.cmap`\n",
      "        \n",
      "        xextent : *None* or (xmin, xmax)\n",
      "            The image extent along the x-axis. The default sets *xmin* to the\n",
      "            left border of the first bin (*spectrum* column) and *xmax* to the\n",
      "            right border of the last bin. Note that for *noverlap>0* the width\n",
      "            of the bins is smaller than those of the segments.\n",
      "        \n",
      "        **kwargs\n",
      "            Additional keyword arguments are passed on to `~.axes.Axes.imshow`\n",
      "            which makes the specgram image.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        spectrum : 2-D array\n",
      "            Columns are the periodograms of successive segments.\n",
      "        \n",
      "        freqs : 1-D array\n",
      "            The frequencies corresponding to the rows in *spectrum*.\n",
      "        \n",
      "        t : 1-D array\n",
      "            The times corresponding to midpoints of segments (i.e., the columns\n",
      "            in *spectrum*).\n",
      "        \n",
      "        im : `.AxesImage`\n",
      "            The image created by imshow containing the spectrogram.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        psd\n",
      "            Differs in the default overlap; in returning the mean of the\n",
      "            segment periodograms; in not returning times; and in generating a\n",
      "            line plot instead of colormap.\n",
      "        magnitude_spectrum\n",
      "            A single spectrum, similar to having a single segment when *mode*\n",
      "            is 'magnitude'. Plots a line instead of a colormap.\n",
      "        angle_spectrum\n",
      "            A single spectrum, similar to having a single segment when *mode*\n",
      "            is 'angle'. Plots a line instead of a colormap.\n",
      "        phase_spectrum\n",
      "            A single spectrum, similar to having a single segment when *mode*\n",
      "            is 'phase'. Plots a line instead of a colormap.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The parameters *detrend* and *scale_by_freq* do only apply when *mode*\n",
      "        is set to 'psd'.\n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            the following arguments can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception):\n",
      "            *x*.\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    spring()\n",
      "        Set the colormap to \"spring\".\n",
      "        \n",
      "        This changes the default colormap as well as the colormap of the current\n",
      "        image if there is one. See ``help(colormaps)`` for more information.\n",
      "    \n",
      "    spy(Z, precision=0, marker=None, markersize=None, aspect='equal', origin='upper', **kwargs)\n",
      "        Plot the sparsity pattern of a 2D array.\n",
      "        \n",
      "        This visualizes the non-zero values of the array.\n",
      "        \n",
      "        Two plotting styles are available: image and marker. Both\n",
      "        are available for full arrays, but only the marker style\n",
      "        works for `scipy.sparse.spmatrix` instances.\n",
      "        \n",
      "        **Image style**\n",
      "        \n",
      "        If *marker* and *markersize* are *None*, `~.Axes.imshow` is used. Any\n",
      "        extra remaining keyword arguments are passed to this method.\n",
      "        \n",
      "        **Marker style**\n",
      "        \n",
      "        If *Z* is a `scipy.sparse.spmatrix` or *marker* or *markersize* are\n",
      "        *None*, a `.Line2D` object will be returned with the value of marker\n",
      "        determining the marker type, and any remaining keyword arguments\n",
      "        passed to `~.Axes.plot`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        Z : array-like (M, N)\n",
      "            The array to be plotted.\n",
      "        \n",
      "        precision : float or 'present', default: 0\n",
      "            If *precision* is 0, any non-zero value will be plotted. Otherwise,\n",
      "            values of :math:`|Z| > precision` will be plotted.\n",
      "        \n",
      "            For `scipy.sparse.spmatrix` instances, you can also\n",
      "            pass 'present'. In this case any value present in the array\n",
      "            will be plotted, even if it is identically zero.\n",
      "        \n",
      "        aspect : {'equal', 'auto', None} or float, default: 'equal'\n",
      "            The aspect ratio of the axes.  This parameter is particularly\n",
      "            relevant for images since it determines whether data pixels are\n",
      "            square.\n",
      "        \n",
      "            This parameter is a shortcut for explicitly calling\n",
      "            `.Axes.set_aspect`. See there for further details.\n",
      "        \n",
      "            - 'equal': Ensures an aspect ratio of 1. Pixels will be square.\n",
      "            - 'auto': The axes is kept fixed and the aspect is adjusted so\n",
      "              that the data fit in the axes. In general, this will result in\n",
      "              non-square pixels.\n",
      "            - *None*: Use :rc:`image.aspect`.\n",
      "        \n",
      "        origin : {'upper', 'lower'}, default: :rc:`image.origin`\n",
      "            Place the [0, 0] index of the array in the upper left or lower left\n",
      "            corner of the axes. The convention 'upper' is typically used for\n",
      "            matrices and images.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `~matplotlib.image.AxesImage` or `.Line2D`\n",
      "            The return type depends on the plotting style (see above).\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            The supported additional parameters depend on the plotting style.\n",
      "        \n",
      "            For the image style, you can pass the following additional\n",
      "            parameters of `~.Axes.imshow`:\n",
      "        \n",
      "            - *cmap*\n",
      "            - *alpha*\n",
      "            - *url*\n",
      "            - any `.Artist` properties (passed on to the `.AxesImage`)\n",
      "        \n",
      "            For the marker style, you can pass any `.Line2D` property except\n",
      "            for *linestyle*:\n",
      "        \n",
      "            Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            antialiased or aa: bool\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            color or c: color\n",
      "            contains: unknown\n",
      "            dash_capstyle: {'butt', 'round', 'projecting'}\n",
      "            dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      "            data: (2, N) array or two 1D arrays\n",
      "            drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      "            figure: `.Figure`\n",
      "            fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      "            gid: str\n",
      "            in_layout: bool\n",
      "            label: object\n",
      "            linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      "            linewidth or lw: float\n",
      "            marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      "            markeredgecolor or mec: color\n",
      "            markeredgewidth or mew: float\n",
      "            markerfacecolor or mfc: color\n",
      "            markerfacecoloralt or mfcalt: color\n",
      "            markersize or ms: float\n",
      "            markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: unknown\n",
      "            pickradius: float\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            solid_capstyle: {'butt', 'round', 'projecting'}\n",
      "            solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      "            transform: `matplotlib.transforms.Transform`\n",
      "            url: str\n",
      "            visible: bool\n",
      "            xdata: 1D array\n",
      "            ydata: 1D array\n",
      "            zorder: float\n",
      "    \n",
      "    stackplot(x, *args, labels=(), colors=None, baseline='zero', data=None, **kwargs)\n",
      "        Draw a stacked area plot.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : 1d array of dimension N\n",
      "        \n",
      "        y : 2d array (dimension MxN), or sequence of 1d arrays (each dimension 1xN)\n",
      "        \n",
      "            The data is assumed to be unstacked. Each of the following\n",
      "            calls is legal::\n",
      "        \n",
      "                stackplot(x, y)               # where y is MxN\n",
      "                stackplot(x, y1, y2, y3, y4)  # where y1, y2, y3, y4, are all 1xNm\n",
      "        \n",
      "        baseline : {'zero', 'sym', 'wiggle', 'weighted_wiggle'}\n",
      "            Method used to calculate the baseline:\n",
      "        \n",
      "            - ``'zero'``: Constant zero baseline, i.e. a simple stacked plot.\n",
      "            - ``'sym'``:  Symmetric around zero and is sometimes called\n",
      "              'ThemeRiver'.\n",
      "            - ``'wiggle'``: Minimizes the sum of the squared slopes.\n",
      "            - ``'weighted_wiggle'``: Does the same but weights to account for\n",
      "              size of each layer. It is also called 'Streamgraph'-layout. More\n",
      "              details can be found at http://leebyron.com/streamgraph/.\n",
      "        \n",
      "        labels : Length N sequence of strings\n",
      "            Labels to assign to each data series.\n",
      "        \n",
      "        colors : Length N sequence of colors\n",
      "            A list or tuple of colors. These will be cycled through and used to\n",
      "            colour the stacked areas.\n",
      "        \n",
      "        **kwargs\n",
      "            All other keyword arguments are passed to `.Axes.fill_between`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        list of `.PolyCollection`\n",
      "            A list of `.PolyCollection` instances, one for each element in the\n",
      "            stacked area plot.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            every other argument can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception).\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    stem(*args, linefmt=None, markerfmt=None, basefmt=None, bottom=0, label=None, use_line_collection=True, data=None)\n",
      "        Create a stem plot.\n",
      "        \n",
      "        A stem plot plots vertical lines at each *x* location from the baseline\n",
      "        to *y*, and places a marker there.\n",
      "        \n",
      "        Call signature::\n",
      "        \n",
      "          stem([x,] y, linefmt=None, markerfmt=None, basefmt=None)\n",
      "        \n",
      "        The x-positions are optional. The formats may be provided either as\n",
      "        positional or as keyword-arguments.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array-like, optional\n",
      "            The x-positions of the stems. Default: (0, 1, ..., len(y) - 1).\n",
      "        \n",
      "        y : array-like\n",
      "            The y-values of the stem heads.\n",
      "        \n",
      "        linefmt : str, optional\n",
      "            A string defining the properties of the vertical lines. Usually,\n",
      "            this will be a color or a color and a linestyle:\n",
      "        \n",
      "            =========  =============\n",
      "            Character  Line Style\n",
      "            =========  =============\n",
      "            ``'-'``    solid line\n",
      "            ``'--'``   dashed line\n",
      "            ``'-.'``   dash-dot line\n",
      "            ``':'``    dotted line\n",
      "            =========  =============\n",
      "        \n",
      "            Default: 'C0-', i.e. solid line with the first color of the color\n",
      "            cycle.\n",
      "        \n",
      "            Note: While it is technically possible to specify valid formats\n",
      "            other than color or color and linestyle (e.g. 'rx' or '-.'), this\n",
      "            is beyond the intention of the method and will most likely not\n",
      "            result in a reasonable plot.\n",
      "        \n",
      "        markerfmt : str, optional\n",
      "            A string defining the properties of the markers at the stem heads.\n",
      "            Default: 'C0o', i.e. filled circles with the first color of the\n",
      "            color cycle.\n",
      "        \n",
      "        basefmt : str, default: 'C3-' ('C2-' in classic mode)\n",
      "            A format string defining the properties of the baseline.\n",
      "        \n",
      "        bottom : float, default: 0\n",
      "            The y-position of the baseline.\n",
      "        \n",
      "        label : str, default: None\n",
      "            The label to use for the stems in legends.\n",
      "        \n",
      "        use_line_collection : bool, default: True\n",
      "            If ``True``, store and plot the stem lines as a\n",
      "            `~.collections.LineCollection` instead of individual lines, which\n",
      "            significantly increases performance.  If ``False``, defaults to the\n",
      "            old behavior of using a list of `.Line2D` objects.  This parameter\n",
      "            may be deprecated in the future.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `.StemContainer`\n",
      "            The container may be treated like a tuple\n",
      "            (*markerline*, *stemlines*, *baseline*)\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. seealso::\n",
      "            The MATLAB function\n",
      "            `stem <https://www.mathworks.com/help/matlab/ref/stem.html>`_\n",
      "            which inspired this method.\n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            every other argument can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception).\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    step(x, y, *args, where='pre', data=None, **kwargs)\n",
      "        Make a step plot.\n",
      "        \n",
      "        Call signatures::\n",
      "        \n",
      "            step(x, y, [fmt], *, data=None, where='pre', **kwargs)\n",
      "            step(x, y, [fmt], x2, y2, [fmt2], ..., *, where='pre', **kwargs)\n",
      "        \n",
      "        This is just a thin wrapper around `.plot` which changes some\n",
      "        formatting options. Most of the concepts and parameters of plot can be\n",
      "        used here as well.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : array-like\n",
      "            1-D sequence of x positions. It is assumed, but not checked, that\n",
      "            it is uniformly increasing.\n",
      "        \n",
      "        y : array-like\n",
      "            1-D sequence of y levels.\n",
      "        \n",
      "        fmt : str, optional\n",
      "            A format string, e.g. 'g' for a green line. See `.plot` for a more\n",
      "            detailed description.\n",
      "        \n",
      "            Note: While full format strings are accepted, it is recommended to\n",
      "            only specify the color. Line styles are currently ignored (use\n",
      "            the keyword argument *linestyle* instead). Markers are accepted\n",
      "            and plotted on the given positions, however, this is a rarely\n",
      "            needed feature for step plots.\n",
      "        \n",
      "        data : indexable object, optional\n",
      "            An object with labelled data. If given, provide the label names to\n",
      "            plot in *x* and *y*.\n",
      "        \n",
      "        where : {'pre', 'post', 'mid'}, default: 'pre'\n",
      "            Define where the steps should be placed:\n",
      "        \n",
      "            - 'pre': The y value is continued constantly to the left from\n",
      "              every *x* position, i.e. the interval ``(x[i-1], x[i]]`` has the\n",
      "              value ``y[i]``.\n",
      "            - 'post': The y value is continued constantly to the right from\n",
      "              every *x* position, i.e. the interval ``[x[i], x[i+1])`` has the\n",
      "              value ``y[i]``.\n",
      "            - 'mid': Steps occur half-way between the *x* positions.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        lines\n",
      "            A list of `.Line2D` objects representing the plotted data.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            Additional parameters are the same as those for `.plot`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        .. [notes section required to get data note injection right]\n",
      "    \n",
      "    streamplot(x, y, u, v, density=1, linewidth=None, color=None, cmap=None, norm=None, arrowsize=1, arrowstyle='-|>', minlength=0.1, transform=None, zorder=None, start_points=None, maxlength=4.0, integration_direction='both', *, data=None)\n",
      "        Draw streamlines of a vector flow.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : 1D arrays\n",
      "            An evenly spaced grid.\n",
      "        u, v : 2D arrays\n",
      "            *x* and *y*-velocities. The number of rows and columns must match\n",
      "            the length of *y* and *x*, respectively.\n",
      "        density : float or (float, float)\n",
      "            Controls the closeness of streamlines. When ``density = 1``, the domain\n",
      "            is divided into a 30x30 grid. *density* linearly scales this grid.\n",
      "            Each cell in the grid can have, at most, one traversing streamline.\n",
      "            For different densities in each direction, use a tuple\n",
      "            (density_x, density_y).\n",
      "        linewidth : float or 2D array\n",
      "            The width of the stream lines. With a 2D array the line width can be\n",
      "            varied across the grid. The array must have the same shape as *u*\n",
      "            and *v*.\n",
      "        color : color or 2D array\n",
      "            The streamline color. If given an array, its values are converted to\n",
      "            colors using *cmap* and *norm*.  The array must have the same shape\n",
      "            as *u* and *v*.\n",
      "        cmap : `~matplotlib.colors.Colormap`\n",
      "            Colormap used to plot streamlines and arrows. This is only used if\n",
      "            *color* is an array.\n",
      "        norm : `~matplotlib.colors.Normalize`\n",
      "            Normalize object used to scale luminance data to 0, 1. If ``None``,\n",
      "            stretch (min, max) to (0, 1). This is only used if *color* is an array.\n",
      "        arrowsize : float\n",
      "            Scaling factor for the arrow size.\n",
      "        arrowstyle : str\n",
      "            Arrow style specification.\n",
      "            See `~matplotlib.patches.FancyArrowPatch`.\n",
      "        minlength : float\n",
      "            Minimum length of streamline in axes coordinates.\n",
      "        start_points : Nx2 array\n",
      "            Coordinates of starting points for the streamlines in data coordinates\n",
      "            (the same coordinates as the *x* and *y* arrays).\n",
      "        zorder : int\n",
      "            The zorder of the stream lines and arrows.\n",
      "            Artists with lower zorder values are drawn first.\n",
      "        maxlength : float\n",
      "            Maximum length of streamline in axes coordinates.\n",
      "        integration_direction : {'forward', 'backward', 'both'}, default: 'both'\n",
      "            Integrate the streamline in forward, backward or both directions.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        StreamplotSet\n",
      "            Container object with attributes\n",
      "        \n",
      "            - ``lines``: `.LineCollection` of streamlines\n",
      "        \n",
      "            - ``arrows``: `.PatchCollection` containing `.FancyArrowPatch`\n",
      "              objects representing the arrows half-way along stream lines.\n",
      "        \n",
      "            This container will probably change in the future to allow changes\n",
      "            to the colormap, alpha, etc. for both lines and arrows, but these\n",
      "            changes should be backward compatible.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            the following arguments can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception):\n",
      "            *x*, *y*, *u*, *v*, *start_points*.\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    subplot(*args, **kwargs)\n",
      "        Add a subplot to the current figure.\n",
      "        \n",
      "        Wrapper of `.Figure.add_subplot` with a difference in behavior\n",
      "        explained in the notes section.\n",
      "        \n",
      "        Call signatures::\n",
      "        \n",
      "           subplot(nrows, ncols, index, **kwargs)\n",
      "           subplot(pos, **kwargs)\n",
      "           subplot(**kwargs)\n",
      "           subplot(ax)\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        *args : int, (int, int, *index*), or `.SubplotSpec`, default: (1, 1, 1)\n",
      "            The position of the subplot described by one of\n",
      "        \n",
      "            - Three integers (*nrows*, *ncols*, *index*). The subplot will take the\n",
      "              *index* position on a grid with *nrows* rows and *ncols* columns.\n",
      "              *index* starts at 1 in the upper left corner and increases to the\n",
      "              right. *index* can also be a two-tuple specifying the (*first*,\n",
      "              *last*) indices (1-based, and including *last*) of the subplot, e.g.,\n",
      "              ``fig.add_subplot(3, 1, (1, 2))`` makes a subplot that spans the\n",
      "              upper 2/3 of the figure.\n",
      "            - A 3-digit integer. The digits are interpreted as if given separately\n",
      "              as three single-digit integers, i.e. ``fig.add_subplot(235)`` is the\n",
      "              same as ``fig.add_subplot(2, 3, 5)``. Note that this can only be used\n",
      "              if there are no more than 9 subplots.\n",
      "            - A `.SubplotSpec`.\n",
      "        \n",
      "        projection : {None, 'aitoff', 'hammer', 'lambert', 'mollweide', 'polar', 'rectilinear', str}, optional\n",
      "            The projection type of the subplot (`~.axes.Axes`). *str* is the name\n",
      "            of a custom projection, see `~matplotlib.projections`. The default\n",
      "            None results in a 'rectilinear' projection.\n",
      "        \n",
      "        polar : bool, default: False\n",
      "            If True, equivalent to projection='polar'.\n",
      "        \n",
      "        sharex, sharey : `~.axes.Axes`, optional\n",
      "            Share the x or y `~matplotlib.axis` with sharex and/or sharey. The\n",
      "            axis will have the same limits, ticks, and scale as the axis of the\n",
      "            shared axes.\n",
      "        \n",
      "        label : str\n",
      "            A label for the returned axes.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `.axes.SubplotBase`, or another subclass of `~.axes.Axes`\n",
      "        \n",
      "            The axes of the subplot. The returned axes base class depends on\n",
      "            the projection used. It is `~.axes.Axes` if rectilinear projection\n",
      "            is used and `.projections.polar.PolarAxes` if polar projection\n",
      "            is used. The returned axes is then a subplot subclass of the\n",
      "            base class.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            This method also takes the keyword arguments for the returned axes\n",
      "            base class; except for the *figure* argument. The keyword arguments\n",
      "            for the rectilinear base class `~.axes.Axes` can be found in\n",
      "            the following table but there might also be other keyword\n",
      "            arguments if another projection is used.\n",
      "        \n",
      "            Properties:\n",
      "            adjustable: {'box', 'datalim'}\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            anchor: 2-tuple of floats or {'C', 'SW', 'S', 'SE', ...}\n",
      "            animated: bool\n",
      "            aspect: {'auto'} or num\n",
      "            autoscale_on: bool\n",
      "            autoscalex_on: bool\n",
      "            autoscaley_on: bool\n",
      "            axes_locator: Callable[[Axes, Renderer], Bbox]\n",
      "            axisbelow: bool or 'line'\n",
      "            box_aspect: None, or a number\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            contains: unknown\n",
      "            facecolor or fc: color\n",
      "            figure: `.Figure`\n",
      "            frame_on: bool\n",
      "            gid: str\n",
      "            in_layout: bool\n",
      "            label: object\n",
      "            navigate: bool\n",
      "            navigate_mode: unknown\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: None or bool or callable\n",
      "            position: [left, bottom, width, height] or `~matplotlib.transforms.Bbox`\n",
      "            prop_cycle: unknown\n",
      "            rasterization_zorder: float or None\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            title: str\n",
      "            transform: `.Transform`\n",
      "            url: str\n",
      "            visible: bool\n",
      "            xbound: unknown\n",
      "            xlabel: str\n",
      "            xlim: (bottom: float, top: float)\n",
      "            xmargin: float greater than -0.5\n",
      "            xscale: {\"linear\", \"log\", \"symlog\", \"logit\", ...}\n",
      "            xticklabels: unknown\n",
      "            xticks: unknown\n",
      "            ybound: unknown\n",
      "            ylabel: str\n",
      "            ylim: (bottom: float, top: float)\n",
      "            ymargin: float greater than -0.5\n",
      "            yscale: {\"linear\", \"log\", \"symlog\", \"logit\", ...}\n",
      "            yticklabels: unknown\n",
      "            yticks: unknown\n",
      "            zorder: float\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Creating a subplot will delete any pre-existing subplot that overlaps\n",
      "        with it beyond sharing a boundary::\n",
      "        \n",
      "            import matplotlib.pyplot as plt\n",
      "            # plot a line, implicitly creating a subplot(111)\n",
      "            plt.plot([1, 2, 3])\n",
      "            # now create a subplot which represents the top plot of a grid\n",
      "            # with 2 rows and 1 column. Since this subplot will overlap the\n",
      "            # first, the plot (and its axes) previously created, will be removed\n",
      "            plt.subplot(211)\n",
      "        \n",
      "        If you do not want this behavior, use the `.Figure.add_subplot` method\n",
      "        or the `.pyplot.axes` function instead.\n",
      "        \n",
      "        If the figure already has a subplot with key (*args*,\n",
      "        *kwargs*) then it will simply make that subplot current and\n",
      "        return it.  This behavior is deprecated. Meanwhile, if you do\n",
      "        not want this behavior (i.e., you want to force the creation of a\n",
      "        new subplot), you must use a unique set of args and kwargs.  The axes\n",
      "        *label* attribute has been exposed for this purpose: if you want\n",
      "        two subplots that are otherwise identical to be added to the figure,\n",
      "        make sure you give them unique labels.\n",
      "        \n",
      "        In rare circumstances, `.add_subplot` may be called with a single\n",
      "        argument, a subplot axes instance already created in the\n",
      "        present figure but not in the figure's list of axes.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        .Figure.add_subplot\n",
      "        .pyplot.subplots\n",
      "        .pyplot.axes\n",
      "        .Figure.subplots\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        ::\n",
      "        \n",
      "            plt.subplot(221)\n",
      "        \n",
      "            # equivalent but more general\n",
      "            ax1=plt.subplot(2, 2, 1)\n",
      "        \n",
      "            # add a subplot with no frame\n",
      "            ax2=plt.subplot(222, frameon=False)\n",
      "        \n",
      "            # add a polar subplot\n",
      "            plt.subplot(223, projection='polar')\n",
      "        \n",
      "            # add a red subplot that shares the x-axis with ax1\n",
      "            plt.subplot(224, sharex=ax1, facecolor='red')\n",
      "        \n",
      "            # delete ax2 from the figure\n",
      "            plt.delaxes(ax2)\n",
      "        \n",
      "            # add ax2 to the figure again\n",
      "            plt.subplot(ax2)\n",
      "    \n",
      "    subplot2grid(shape, loc, rowspan=1, colspan=1, fig=None, **kwargs)\n",
      "        Create a subplot at a specific location inside a regular grid.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        shape : (int, int)\n",
      "            Number of rows and of columns of the grid in which to place axis.\n",
      "        loc : (int, int)\n",
      "            Row number and column number of the axis location within the grid.\n",
      "        rowspan : int, default: 1\n",
      "            Number of rows for the axis to span to the right.\n",
      "        colspan : int, default: 1\n",
      "            Number of columns for the axis to span downwards.\n",
      "        fig : `.Figure`, optional\n",
      "            Figure to place the subplot in. Defaults to the current figure.\n",
      "        **kwargs\n",
      "            Additional keyword arguments are handed to `~.Figure.add_subplot`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `.axes.SubplotBase`, or another subclass of `~.axes.Axes`\n",
      "        \n",
      "            The axes of the subplot.  The returned axes base class depends on the\n",
      "            projection used.  It is `~.axes.Axes` if rectilinear projection is used\n",
      "            and `.projections.polar.PolarAxes` if polar projection is used.  The\n",
      "            returned axes is then a subplot subclass of the base class.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The following call ::\n",
      "        \n",
      "            ax = subplot2grid((nrows, ncols), (row, col), rowspan, colspan)\n",
      "        \n",
      "        is identical to ::\n",
      "        \n",
      "            fig = gcf()\n",
      "            gs = fig.add_gridspec(nrows, ncols)\n",
      "            ax = fig.add_subplot(gs[row:row+rowspan, col:col+colspan])\n",
      "    \n",
      "    subplot_mosaic(layout, *, subplot_kw=None, gridspec_kw=None, empty_sentinel='.', **fig_kw)\n",
      "        Build a layout of Axes based on ASCII art or nested lists.\n",
      "        \n",
      "        This is a helper function to build complex GridSpec layouts visually.\n",
      "        \n",
      "        .. note ::\n",
      "        \n",
      "           This API is provisional and may be revised in the future based on\n",
      "           early user feedback.\n",
      "        \n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        layout : list of list of {hashable or nested} or str\n",
      "        \n",
      "            A visual layout of how you want your Axes to be arranged\n",
      "            labeled as strings.  For example ::\n",
      "        \n",
      "               x = [['A panel', 'A panel', 'edge'],\n",
      "                    ['C panel', '.',       'edge']]\n",
      "        \n",
      "            Produces 4 axes:\n",
      "        \n",
      "            - 'A panel' which is 1 row high and spans the first two columns\n",
      "            - 'edge' which is 2 rows high and is on the right edge\n",
      "            - 'C panel' which in 1 row and 1 column wide in the bottom left\n",
      "            - a blank space 1 row and 1 column wide in the bottom center\n",
      "        \n",
      "            Any of the entries in the layout can be a list of lists\n",
      "            of the same form to create nested layouts.\n",
      "        \n",
      "            If input is a str, then it must be of the form ::\n",
      "        \n",
      "              '''\n",
      "              AAE\n",
      "              C.E\n",
      "              '''\n",
      "        \n",
      "            where each character is a column and each line is a row.\n",
      "            This only allows only single character Axes labels and does\n",
      "            not allow nesting but is very terse.\n",
      "        \n",
      "        subplot_kw : dict, optional\n",
      "            Dictionary with keywords passed to the `.Figure.add_subplot` call\n",
      "            used to create each subplot.\n",
      "        \n",
      "        gridspec_kw : dict, optional\n",
      "            Dictionary with keywords passed to the `.GridSpec` constructor used\n",
      "            to create the grid the subplots are placed on.\n",
      "        \n",
      "        empty_sentinel : object, optional\n",
      "            Entry in the layout to mean \"leave this space empty\".  Defaults\n",
      "            to ``'.'``. Note, if *layout* is a string, it is processed via\n",
      "            `inspect.cleandoc` to remove leading white space, which may\n",
      "            interfere with using white-space as the empty sentinel.\n",
      "        \n",
      "        **fig_kw\n",
      "            All additional keyword arguments are passed to the\n",
      "            `.pyplot.figure` call.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fig : `~.figure.Figure`\n",
      "           The new figure\n",
      "        \n",
      "        dict[label, Axes]\n",
      "           A dictionary mapping the labels to the Axes objects.\n",
      "    \n",
      "    subplot_tool(targetfig=None)\n",
      "        Launch a subplot tool window for a figure.\n",
      "        \n",
      "        A :class:`matplotlib.widgets.SubplotTool` instance is returned.\n",
      "    \n",
      "    subplots(nrows=1, ncols=1, *, sharex=False, sharey=False, squeeze=True, subplot_kw=None, gridspec_kw=None, **fig_kw)\n",
      "        Create a figure and a set of subplots.\n",
      "        \n",
      "        This utility wrapper makes it convenient to create common layouts of\n",
      "        subplots, including the enclosing figure object, in a single call.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        nrows, ncols : int, default: 1\n",
      "            Number of rows/columns of the subplot grid.\n",
      "        \n",
      "        sharex, sharey : bool or {'none', 'all', 'row', 'col'}, default: False\n",
      "            Controls sharing of properties among x (*sharex*) or y (*sharey*)\n",
      "            axes:\n",
      "        \n",
      "            - True or 'all': x- or y-axis will be shared among all subplots.\n",
      "            - False or 'none': each subplot x- or y-axis will be independent.\n",
      "            - 'row': each subplot row will share an x- or y-axis.\n",
      "            - 'col': each subplot column will share an x- or y-axis.\n",
      "        \n",
      "            When subplots have a shared x-axis along a column, only the x tick\n",
      "            labels of the bottom subplot are created. Similarly, when subplots\n",
      "            have a shared y-axis along a row, only the y tick labels of the first\n",
      "            column subplot are created. To later turn other subplots' ticklabels\n",
      "            on, use `~matplotlib.axes.Axes.tick_params`.\n",
      "        \n",
      "        squeeze : bool, default: True\n",
      "            - If True, extra dimensions are squeezed out from the returned\n",
      "              array of `~matplotlib.axes.Axes`:\n",
      "        \n",
      "              - if only one subplot is constructed (nrows=ncols=1), the\n",
      "                resulting single Axes object is returned as a scalar.\n",
      "              - for Nx1 or 1xM subplots, the returned object is a 1D numpy\n",
      "                object array of Axes objects.\n",
      "              - for NxM, subplots with N>1 and M>1 are returned as a 2D array.\n",
      "        \n",
      "            - If False, no squeezing at all is done: the returned Axes object is\n",
      "              always a 2D array containing Axes instances, even if it ends up\n",
      "              being 1x1.\n",
      "        \n",
      "        subplot_kw : dict, optional\n",
      "            Dict with keywords passed to the\n",
      "            `~matplotlib.figure.Figure.add_subplot` call used to create each\n",
      "            subplot.\n",
      "        \n",
      "        gridspec_kw : dict, optional\n",
      "            Dict with keywords passed to the `~matplotlib.gridspec.GridSpec`\n",
      "            constructor used to create the grid the subplots are placed on.\n",
      "        \n",
      "        **fig_kw\n",
      "            All additional keyword arguments are passed to the\n",
      "            `.pyplot.figure` call.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        fig : `~.figure.Figure`\n",
      "        \n",
      "        ax : `.axes.Axes` or array of Axes\n",
      "            *ax* can be either a single `~matplotlib.axes.Axes` object or an\n",
      "            array of Axes objects if more than one subplot was created.  The\n",
      "            dimensions of the resulting array can be controlled with the squeeze\n",
      "            keyword, see above.\n",
      "        \n",
      "            Typical idioms for handling the return value are::\n",
      "        \n",
      "                # using the variable ax for single a Axes\n",
      "                fig, ax = plt.subplots()\n",
      "        \n",
      "                # using the variable axs for multiple Axes\n",
      "                fig, axs = plt.subplots(2, 2)\n",
      "        \n",
      "                # using tuple unpacking for multiple Axes\n",
      "                fig, (ax1, ax2) = plt.subplot(1, 2)\n",
      "                fig, ((ax1, ax2), (ax3, ax4)) = plt.subplot(2, 2)\n",
      "        \n",
      "            The names ``ax`` and pluralized ``axs`` are preferred over ``axes``\n",
      "            because for the latter it's not clear if it refers to a single\n",
      "            `~.axes.Axes` instance or a collection of these.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        .pyplot.figure\n",
      "        .pyplot.subplot\n",
      "        .pyplot.axes\n",
      "        .Figure.subplots\n",
      "        .Figure.add_subplot\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        ::\n",
      "        \n",
      "            # First create some toy data:\n",
      "            x = np.linspace(0, 2*np.pi, 400)\n",
      "            y = np.sin(x**2)\n",
      "        \n",
      "            # Create just a figure and only one subplot\n",
      "            fig, ax = plt.subplots()\n",
      "            ax.plot(x, y)\n",
      "            ax.set_title('Simple plot')\n",
      "        \n",
      "            # Create two subplots and unpack the output array immediately\n",
      "            f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
      "            ax1.plot(x, y)\n",
      "            ax1.set_title('Sharing Y axis')\n",
      "            ax2.scatter(x, y)\n",
      "        \n",
      "            # Create four polar axes and access them through the returned array\n",
      "            fig, axs = plt.subplots(2, 2, subplot_kw=dict(polar=True))\n",
      "            axs[0, 0].plot(x, y)\n",
      "            axs[1, 1].scatter(x, y)\n",
      "        \n",
      "            # Share a X axis with each column of subplots\n",
      "            plt.subplots(2, 2, sharex='col')\n",
      "        \n",
      "            # Share a Y axis with each row of subplots\n",
      "            plt.subplots(2, 2, sharey='row')\n",
      "        \n",
      "            # Share both X and Y axes with all subplots\n",
      "            plt.subplots(2, 2, sharex='all', sharey='all')\n",
      "        \n",
      "            # Note that this is the same as\n",
      "            plt.subplots(2, 2, sharex=True, sharey=True)\n",
      "        \n",
      "            # Create figure number 10 with a single subplot\n",
      "            # and clears it if it already exists.\n",
      "            fig, ax = plt.subplots(num=10, clear=True)\n",
      "    \n",
      "    subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=None)\n",
      "        Adjust the subplot layout parameters.\n",
      "        \n",
      "        Unset parameters are left unmodified; initial values are given by\n",
      "        :rc:`figure.subplot.[name]`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        left : float, optional\n",
      "            The position of the left edge of the subplots,\n",
      "            as a fraction of the figure width.\n",
      "        right : float, optional\n",
      "            The position of the right edge of the subplots,\n",
      "            as a fraction of the figure width.\n",
      "        bottom : float, optional\n",
      "            The position of the bottom edge of the subplots,\n",
      "            as a fraction of the figure height.\n",
      "        top : float, optional\n",
      "            The position of the top edge of the subplots,\n",
      "            as a fraction of the figure height.\n",
      "        wspace : float, optional\n",
      "            The width of the padding between subplots,\n",
      "            as a fraction of the average axes width.\n",
      "        hspace : float, optional\n",
      "            The height of the padding between subplots,\n",
      "            as a fraction of the average axes height.\n",
      "    \n",
      "    summer()\n",
      "        Set the colormap to \"summer\".\n",
      "        \n",
      "        This changes the default colormap as well as the colormap of the current\n",
      "        image if there is one. See ``help(colormaps)`` for more information.\n",
      "    \n",
      "    suptitle(t, **kwargs)\n",
      "        Add a centered title to the figure.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        t : str\n",
      "            The title text.\n",
      "        \n",
      "        x : float, default 0.5\n",
      "            The x location of the text in figure coordinates.\n",
      "        \n",
      "        y : float, default 0.98\n",
      "            The y location of the text in figure coordinates.\n",
      "        \n",
      "        horizontalalignment, ha : {'center', 'left', right'}, default: 'center'\n",
      "            The horizontal alignment of the text relative to (*x*, *y*).\n",
      "        \n",
      "        verticalalignment, va : {'top', 'center', 'bottom', 'baseline'}, default: 'top'\n",
      "            The vertical alignment of the text relative to (*x*, *y*).\n",
      "        \n",
      "        fontsize, size : default: :rc:`figure.titlesize`\n",
      "            The font size of the text. See `.Text.set_size` for possible\n",
      "            values.\n",
      "        \n",
      "        fontweight, weight : default: :rc:`figure.titleweight`\n",
      "            The font weight of the text. See `.Text.set_weight` for possible\n",
      "            values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        text\n",
      "            The `.Text` instance of the title.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        fontproperties : None or dict, optional\n",
      "            A dict of font properties. If *fontproperties* is given the\n",
      "            default values for font size and weight are taken from the\n",
      "            `.FontProperties` defaults. :rc:`figure.titlesize` and\n",
      "            :rc:`figure.titleweight` are ignored in this case.\n",
      "        \n",
      "        **kwargs\n",
      "            Additional kwargs are `matplotlib.text.Text` properties.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> fig.suptitle('This is the figure title', fontsize=12)\n",
      "    \n",
      "    switch_backend(newbackend)\n",
      "        Close all open figures and set the Matplotlib backend.\n",
      "        \n",
      "        The argument is case-insensitive.  Switching to an interactive backend is\n",
      "        possible only if no event loop for another interactive backend has started.\n",
      "        Switching to and from non-interactive backends is always possible.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        newbackend : str\n",
      "            The name of the backend to use.\n",
      "    \n",
      "    table(cellText=None, cellColours=None, cellLoc='right', colWidths=None, rowLabels=None, rowColours=None, rowLoc='left', colLabels=None, colColours=None, colLoc='center', loc='bottom', bbox=None, edges='closed', **kwargs)\n",
      "        Add a table to an `~.axes.Axes`.\n",
      "        \n",
      "        At least one of *cellText* or *cellColours* must be specified. These\n",
      "        parameters must be 2D lists, in which the outer lists define the rows and\n",
      "        the inner list define the column values per row. Each row must have the\n",
      "        same number of elements.\n",
      "        \n",
      "        The table can optionally have row and column headers, which are configured\n",
      "        using *rowLabels*, *rowColours*, *rowLoc* and *colLabels*, *colColours*,\n",
      "        *colLoc* respectively.\n",
      "        \n",
      "        For finer grained control over tables, use the `.Table` class and add it to\n",
      "        the axes with `.Axes.add_table`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cellText : 2D list of str, optional\n",
      "            The texts to place into the table cells.\n",
      "        \n",
      "            *Note*: Line breaks in the strings are currently not accounted for and\n",
      "            will result in the text exceeding the cell boundaries.\n",
      "        \n",
      "        cellColours : 2D list of colors, optional\n",
      "            The background colors of the cells.\n",
      "        \n",
      "        cellLoc : {'left', 'center', 'right'}, default: 'right'\n",
      "            The alignment of the text within the cells.\n",
      "        \n",
      "        colWidths : list of float, optional\n",
      "            The column widths in units of the axes. If not given, all columns will\n",
      "            have a width of *1 / ncols*.\n",
      "        \n",
      "        rowLabels : list of str, optional\n",
      "            The text of the row header cells.\n",
      "        \n",
      "        rowColours : list of colors, optional\n",
      "            The colors of the row header cells.\n",
      "        \n",
      "        rowLoc : {'left', 'center', 'right'}, default: 'left'\n",
      "            The text alignment of the row header cells.\n",
      "        \n",
      "        colLabels : list of str, optional\n",
      "            The text of the column header cells.\n",
      "        \n",
      "        colColours : list of colors, optional\n",
      "            The colors of the column header cells.\n",
      "        \n",
      "        colLoc : {'left', 'center', 'right'}, default: 'left'\n",
      "            The text alignment of the column header cells.\n",
      "        \n",
      "        loc : str, optional\n",
      "            The position of the cell with respect to *ax*. This must be one of\n",
      "            the `~.Table.codes`.\n",
      "        \n",
      "        bbox : `.Bbox`, optional\n",
      "            A bounding box to draw the table into. If this is not *None*, this\n",
      "            overrides *loc*.\n",
      "        \n",
      "        edges : substring of 'BRTL' or {'open', 'closed', 'horizontal', 'vertical'}\n",
      "            The cell edges to be drawn with a line. See also\n",
      "            `~.Cell.visible_edges`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `~matplotlib.table.Table`\n",
      "            The created table.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            `.Table` properties.\n",
      "        \n",
      "        Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            contains: unknown\n",
      "            figure: `.Figure`\n",
      "            fontsize: float\n",
      "            gid: str\n",
      "            in_layout: bool\n",
      "            label: object\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: None or bool or callable\n",
      "            rasterized: bool or None\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            transform: `.Transform`\n",
      "            url: str\n",
      "            visible: bool\n",
      "            zorder: float\n",
      "    \n",
      "    text(x, y, s, fontdict=None, **kwargs)\n",
      "        Add text to the axes.\n",
      "        \n",
      "        Add the text *s* to the axes at location *x*, *y* in data coordinates.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : float\n",
      "            The position to place the text. By default, this is in data\n",
      "            coordinates. The coordinate system can be changed using the\n",
      "            *transform* parameter.\n",
      "        \n",
      "        s : str\n",
      "            The text.\n",
      "        \n",
      "        fontdict : dict, default: None\n",
      "            A dictionary to override the default text properties. If fontdict\n",
      "            is None, the defaults are determined by `.rcParams`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `.Text`\n",
      "            The created `.Text` instance.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs : `~matplotlib.text.Text` properties.\n",
      "            Other miscellaneous text parameters.\n",
      "        \n",
      "            Properties:\n",
      "            agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      "            alpha: float or None\n",
      "            animated: bool\n",
      "            backgroundcolor: color\n",
      "            bbox: dict with properties for `.patches.FancyBboxPatch`\n",
      "            clip_box: `.Bbox`\n",
      "            clip_on: bool\n",
      "            clip_path: Patch or (Path, Transform) or None\n",
      "            color or c: color\n",
      "            contains: unknown\n",
      "            figure: `.Figure`\n",
      "            fontfamily or family: {FONTNAME, 'serif', 'sans-serif', 'cursive', 'fantasy', 'monospace'}\n",
      "            fontproperties or font or font_properties: `.font_manager.FontProperties` or `str` or `pathlib.Path`\n",
      "            fontsize or size: float or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n",
      "            fontstretch or stretch: {a numeric value in range 0-1000, 'ultra-condensed', 'extra-condensed', 'condensed', 'semi-condensed', 'normal', 'semi-expanded', 'expanded', 'extra-expanded', 'ultra-expanded'}\n",
      "            fontstyle or style: {'normal', 'italic', 'oblique'}\n",
      "            fontvariant or variant: {'normal', 'small-caps'}\n",
      "            fontweight or weight: {a numeric value in range 0-1000, 'ultralight', 'light', 'normal', 'regular', 'book', 'medium', 'roman', 'semibold', 'demibold', 'demi', 'bold', 'heavy', 'extra bold', 'black'}\n",
      "            gid: str\n",
      "            horizontalalignment or ha: {'center', 'right', 'left'}\n",
      "            in_layout: bool\n",
      "            label: object\n",
      "            linespacing: float (multiple of font size)\n",
      "            multialignment or ma: {'left', 'right', 'center'}\n",
      "            path_effects: `.AbstractPathEffect`\n",
      "            picker: None or bool or callable\n",
      "            position: (float, float)\n",
      "            rasterized: bool or None\n",
      "            rotation: float or {'vertical', 'horizontal'}\n",
      "            rotation_mode: {None, 'default', 'anchor'}\n",
      "            sketch_params: (scale: float, length: float, randomness: float)\n",
      "            snap: bool or None\n",
      "            text: object\n",
      "            transform: `.Transform`\n",
      "            url: str\n",
      "            usetex: bool or None\n",
      "            verticalalignment or va: {'center', 'top', 'bottom', 'baseline', 'center_baseline'}\n",
      "            visible: bool\n",
      "            wrap: bool\n",
      "            x: float\n",
      "            y: float\n",
      "            zorder: float\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Individual keyword arguments can be used to override any given\n",
      "        parameter::\n",
      "        \n",
      "            >>> text(x, y, s, fontsize=12)\n",
      "        \n",
      "        The default transform specifies that text is in data coords,\n",
      "        alternatively, you can specify text in axis coords ((0, 0) is\n",
      "        lower-left and (1, 1) is upper-right).  The example below places\n",
      "        text in the center of the axes::\n",
      "        \n",
      "            >>> text(0.5, 0.5, 'matplotlib', horizontalalignment='center',\n",
      "            ...      verticalalignment='center', transform=ax.transAxes)\n",
      "        \n",
      "        You can put a rectangular box around the text instance (e.g., to\n",
      "        set a background color) by using the keyword *bbox*.  *bbox* is\n",
      "        a dictionary of `~matplotlib.patches.Rectangle`\n",
      "        properties.  For example::\n",
      "        \n",
      "            >>> text(x, y, s, bbox=dict(facecolor='red', alpha=0.5))\n",
      "    \n",
      "    thetagrids(angles=None, labels=None, fmt=None, **kwargs)\n",
      "        Get or set the theta gridlines on the current polar plot.\n",
      "        \n",
      "        Call signatures::\n",
      "        \n",
      "         lines, labels = thetagrids()\n",
      "         lines, labels = thetagrids(angles, labels=None, fmt=None, **kwargs)\n",
      "        \n",
      "        When called with no arguments, `.thetagrids` simply returns the tuple\n",
      "        (*lines*, *labels*). When called with arguments, the labels will\n",
      "        appear at the specified angles.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        angles : tuple with floats, degrees\n",
      "            The angles of the theta gridlines.\n",
      "        \n",
      "        labels : tuple with strings or None\n",
      "            The labels to use at each radial gridline. The\n",
      "            `.projections.polar.ThetaFormatter` will be used if None.\n",
      "        \n",
      "        fmt : str or None\n",
      "            Format string used in `matplotlib.ticker.FormatStrFormatter`.\n",
      "            For example '%f'. Note that the angle in radians will be used.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        lines : list of `.lines.Line2D`\n",
      "            The theta gridlines.\n",
      "        \n",
      "        labels : list of `.text.Text`\n",
      "            The tick labels.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs\n",
      "            *kwargs* are optional `~.Text` properties for the labels.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        .pyplot.rgrids\n",
      "        .projections.polar.PolarAxes.set_thetagrids\n",
      "        .Axis.get_gridlines\n",
      "        .Axis.get_ticklabels\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        ::\n",
      "        \n",
      "          # set the locations of the angular gridlines\n",
      "          lines, labels = thetagrids(range(45, 360, 90))\n",
      "        \n",
      "          # set the locations and labels of the angular gridlines\n",
      "          lines, labels = thetagrids(range(45, 360, 90), ('NE', 'NW', 'SW', 'SE'))\n",
      "    \n",
      "    tick_params(axis='both', **kwargs)\n",
      "        Change the appearance of ticks, tick labels, and gridlines.\n",
      "        \n",
      "        Tick properties that are not explicitly set using the keyword\n",
      "        arguments remain unchanged unless *reset* is True.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        axis : {'x', 'y', 'both'}, default: 'both'\n",
      "            The axis to which the parameters are applied.\n",
      "        which : {'major', 'minor', 'both'}, default: 'major'\n",
      "            The group of ticks to which the parameters are applied.\n",
      "        reset : bool, default: False\n",
      "            Whether to reset the ticks to defaults before updating them.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        direction : {'in', 'out', 'inout'}\n",
      "            Puts ticks inside the axes, outside the axes, or both.\n",
      "        length : float\n",
      "            Tick length in points.\n",
      "        width : float\n",
      "            Tick width in points.\n",
      "        color : color\n",
      "            Tick color.\n",
      "        pad : float\n",
      "            Distance in points between tick and label.\n",
      "        labelsize : float or str\n",
      "            Tick label font size in points or as a string (e.g., 'large').\n",
      "        labelcolor : color\n",
      "            Tick label color.\n",
      "        colors : color\n",
      "            Tick color and label color.\n",
      "        zorder : float\n",
      "            Tick and label zorder.\n",
      "        bottom, top, left, right : bool\n",
      "            Whether to draw the respective ticks.\n",
      "        labelbottom, labeltop, labelleft, labelright : bool\n",
      "            Whether to draw the respective tick labels.\n",
      "        labelrotation : float\n",
      "            Tick label rotation\n",
      "        grid_color : color\n",
      "            Gridline color.\n",
      "        grid_alpha : float\n",
      "            Transparency of gridlines: 0 (transparent) to 1 (opaque).\n",
      "        grid_linewidth : float\n",
      "            Width of gridlines in points.\n",
      "        grid_linestyle : str\n",
      "            Any valid `.Line2D` line style spec.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        ::\n",
      "        \n",
      "            ax.tick_params(direction='out', length=6, width=2, colors='r',\n",
      "                           grid_color='r', grid_alpha=0.5)\n",
      "        \n",
      "        This will make all major ticks be red, pointing out of the box,\n",
      "        and with dimensions 6 points by 2 points.  Tick labels will\n",
      "        also be red.  Gridlines will be red and translucent.\n",
      "    \n",
      "    ticklabel_format(*, axis='both', style='', scilimits=None, useOffset=None, useLocale=None, useMathText=None)\n",
      "        Configure the `.ScalarFormatter` used by default for linear axes.\n",
      "        \n",
      "        If a parameter is not set, the corresponding property of the formatter\n",
      "        is left unchanged.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        axis : {'x', 'y', 'both'}, default: 'both'\n",
      "            The axes to configure.  Only major ticks are affected.\n",
      "        \n",
      "        style : {'sci', 'scientific', 'plain'}\n",
      "            Whether to use scientific notation.\n",
      "            The formatter default is to use scientific notation.\n",
      "        \n",
      "        scilimits : pair of ints (m, n)\n",
      "            Scientific notation is used only for numbers outside the range\n",
      "            10\\ :sup:`m` to 10\\ :sup:`n` (and only if the formatter is\n",
      "            configured to use scientific notation at all).  Use (0, 0) to\n",
      "            include all numbers.  Use (m, m) where m != 0 to fix the order of\n",
      "            magnitude to 10\\ :sup:`m`.\n",
      "            The formatter default is :rc:`axes.formatter.limits`.\n",
      "        \n",
      "        useOffset : bool or float\n",
      "            If True, the offset is calculated as needed.\n",
      "            If False, no offset is used.\n",
      "            If a numeric value, it sets the offset.\n",
      "            The formatter default is :rc:`axes.formatter.useoffset`.\n",
      "        \n",
      "        useLocale : bool\n",
      "            Whether to format the number using the current locale or using the\n",
      "            C (English) locale.  This affects e.g. the decimal separator.  The\n",
      "            formatter default is :rc:`axes.formatter.use_locale`.\n",
      "        \n",
      "        useMathText : bool\n",
      "            Render the offset and scientific notation in mathtext.\n",
      "            The formatter default is :rc:`axes.formatter.use_mathtext`.\n",
      "        \n",
      "        Raises\n",
      "        ------\n",
      "        AttributeError\n",
      "            If the current formatter is not a `.ScalarFormatter`.\n",
      "    \n",
      "    tight_layout(*, pad=1.08, h_pad=None, w_pad=None, rect=None)\n",
      "        Adjust the padding between and around subplots.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        pad : float, default: 1.08\n",
      "            Padding between the figure edge and the edges of subplots,\n",
      "            as a fraction of the font size.\n",
      "        h_pad, w_pad : float, default: *pad*\n",
      "            Padding (height/width) between edges of adjacent subplots,\n",
      "            as a fraction of the font size.\n",
      "        rect : tuple (left, bottom, right, top), default: (0, 0, 1, 1)\n",
      "            A rectangle in normalized figure coordinates into which the whole\n",
      "            subplots area (including labels) will fit.\n",
      "    \n",
      "    title(label, fontdict=None, loc=None, pad=None, *, y=None, **kwargs)\n",
      "        Set a title for the axes.\n",
      "        \n",
      "        Set one of the three available axes titles. The available titles\n",
      "        are positioned above the axes in the center, flush with the left\n",
      "        edge, and flush with the right edge.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        label : str\n",
      "            Text to use for the title\n",
      "        \n",
      "        fontdict : dict\n",
      "            A dictionary controlling the appearance of the title text,\n",
      "            the default *fontdict* is::\n",
      "        \n",
      "               {'fontsize': rcParams['axes.titlesize'],\n",
      "                'fontweight': rcParams['axes.titleweight'],\n",
      "                'color': rcParams['axes.titlecolor'],\n",
      "                'verticalalignment': 'baseline',\n",
      "                'horizontalalignment': loc}\n",
      "        \n",
      "        loc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n",
      "            Which title to set.\n",
      "        \n",
      "        y : float, default: :rc:`axes.titley`\n",
      "            Vertical axes loation for the title (1.0 is the top).  If\n",
      "            None (the default), y is determined automatically to avoid\n",
      "            decorators on the axes.\n",
      "        \n",
      "        pad : float, default: :rc:`axes.titlepad`\n",
      "            The offset of the title from the top of the axes, in points.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `.Text`\n",
      "            The matplotlib text instance representing the title\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs : `.Text` properties\n",
      "            Other keyword arguments are text properties, see `.Text` for a list\n",
      "            of valid text properties.\n",
      "    \n",
      "    tricontour(*args, **kwargs)\n",
      "        Draw contour lines on an unstructured triangular grid.\n",
      "        \n",
      "        The triangulation can be specified in one of two ways; either ::\n",
      "        \n",
      "            tricontour(triangulation, ...)\n",
      "        \n",
      "        where *triangulation* is a `.Triangulation` object, or ::\n",
      "        \n",
      "            tricontour(x, y, ...)\n",
      "            tricontour(x, y, triangles, ...)\n",
      "            tricontour(x, y, triangles=triangles, ...)\n",
      "            tricontour(x, y, mask=mask, ...)\n",
      "            tricontour(x, y, triangles, mask=mask, ...)\n",
      "        \n",
      "        in which case a `.Triangulation` object will be created.  See that class'\n",
      "        docstring for an explanation of these cases.\n",
      "        \n",
      "        The remaining arguments may be::\n",
      "        \n",
      "            tricontour(..., Z)\n",
      "        \n",
      "        where *Z* is the array of values to contour, one per point in the\n",
      "        triangulation.  The level values are chosen automatically.\n",
      "        \n",
      "        ::\n",
      "        \n",
      "            tricontour(..., Z, levels)\n",
      "        \n",
      "        contour up to *levels+1* automatically chosen contour levels (*levels*\n",
      "        intervals).\n",
      "        \n",
      "        ::\n",
      "        \n",
      "            tricontour(..., Z, levels)\n",
      "        \n",
      "        draw contour lines at the values specified in sequence *levels*, which must\n",
      "        be in increasing order.\n",
      "        \n",
      "        ::\n",
      "        \n",
      "            tricontour(Z, **kwargs)\n",
      "        \n",
      "        Use keyword arguments to control colors, linewidth, origin, cmap ... see below\n",
      "        for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        triangulation : `.Triangulation`, optional\n",
      "            The unstructured triangular grid.\n",
      "        \n",
      "            If specified, then *x*, *y*, *triangles*, and *mask* are not accepted.\n",
      "        \n",
      "        x, y : array-like, optional\n",
      "            The coordinates of the values in *Z*.\n",
      "        \n",
      "        triangles : int array-like of shape (ntri, 3), optional\n",
      "            For each triangle, the indices of the three points that make up the\n",
      "            triangle, ordered in an anticlockwise manner.  If not specified, the\n",
      "            Delaunay triangulation is calculated.\n",
      "        \n",
      "        mask : bool array-like of shape (ntri), optional\n",
      "            Which triangles are masked out.\n",
      "        \n",
      "        Z : array-like(N, M)\n",
      "            The height values over which the contour is drawn.\n",
      "        \n",
      "        levels : int or array-like, optional\n",
      "            Determines the number and positions of the contour lines / regions.\n",
      "        \n",
      "            If an int *n*, use `~matplotlib.ticker.MaxNLocator`, which tries to\n",
      "            automatically choose no more than *n+1* \"nice\" contour levels between\n",
      "            *vmin* and *vmax*.\n",
      "        \n",
      "            If array-like, draw contour lines at the specified levels.  The values must\n",
      "            be in increasing order.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `~matplotlib.tri.TriContourSet`\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        colors : color string or sequence of colors, optional\n",
      "            The colors of the levels, i.e., the contour lines.\n",
      "        \n",
      "            The sequence is cycled for the levels in ascending order. If the sequence\n",
      "            is shorter than the number of levels, it's repeated.\n",
      "        \n",
      "            As a shortcut, single color strings may be used in place of one-element\n",
      "            lists, i.e. ``'red'`` instead of ``['red']`` to color all levels with the\n",
      "            same color. This shortcut does only work for color strings, not for other\n",
      "            ways of specifying colors.\n",
      "        \n",
      "            By default (value *None*), the colormap specified by *cmap* will be used.\n",
      "        \n",
      "        alpha : float, default: 1\n",
      "            The alpha blending value, between 0 (transparent) and 1 (opaque).\n",
      "        \n",
      "        cmap : str or `.Colormap`, default: :rc:`image.cmap`\n",
      "            A `.Colormap` instance or registered colormap name. The colormap maps the\n",
      "            level values to colors.\n",
      "        \n",
      "            If both *colors* and *cmap* are given, an error is raised.\n",
      "        \n",
      "        norm : `~matplotlib.colors.Normalize`, optional\n",
      "            If a colormap is used, the `.Normalize` instance scales the level values to\n",
      "            the canonical colormap range [0, 1] for mapping to colors. If not given,\n",
      "            the default linear scaling is used.\n",
      "        \n",
      "        origin : {*None*, 'upper', 'lower', 'image'}, default: None\n",
      "            Determines the orientation and exact position of *Z* by specifying the\n",
      "            position of ``Z[0, 0]``.  This is only relevant, if *X*, *Y* are not given.\n",
      "        \n",
      "            - *None*: ``Z[0, 0]`` is at X=0, Y=0 in the lower left corner.\n",
      "            - 'lower': ``Z[0, 0]`` is at X=0.5, Y=0.5 in the lower left corner.\n",
      "            - 'upper': ``Z[0, 0]`` is at X=N+0.5, Y=0.5 in the upper left corner.\n",
      "            - 'image': Use the value from :rc:`image.origin`.\n",
      "        \n",
      "        extent : (x0, x1, y0, y1), optional\n",
      "            If *origin* is not *None*, then *extent* is interpreted as in `.imshow`: it\n",
      "            gives the outer pixel boundaries. In this case, the position of Z[0, 0] is\n",
      "            the center of the pixel, not a corner. If *origin* is *None*, then\n",
      "            (*x0*, *y0*) is the position of Z[0, 0], and (*x1*, *y1*) is the position\n",
      "            of Z[-1, -1].\n",
      "        \n",
      "            This argument is ignored if *X* and *Y* are specified in the call to\n",
      "            contour.\n",
      "        \n",
      "        locator : ticker.Locator subclass, optional\n",
      "            The locator is used to determine the contour levels if they are not given\n",
      "            explicitly via *levels*.\n",
      "            Defaults to `~.ticker.MaxNLocator`.\n",
      "        \n",
      "        extend : {'neither', 'both', 'min', 'max'}, default: 'neither'\n",
      "            Determines the ``tricontour``-coloring of values that are outside the\n",
      "            *levels* range.\n",
      "        \n",
      "            If 'neither', values outside the *levels* range are not colored.  If 'min',\n",
      "            'max' or 'both', color the values below, above or below and above the\n",
      "            *levels* range.\n",
      "        \n",
      "            Values below ``min(levels)`` and above ``max(levels)`` are mapped to the\n",
      "            under/over values of the `.Colormap`. Note that most colormaps do not have\n",
      "            dedicated colors for these by default, so that the over and under values\n",
      "            are the edge values of the colormap.  You may want to set these values\n",
      "            explicitly using `.Colormap.set_under` and `.Colormap.set_over`.\n",
      "        \n",
      "            .. note::\n",
      "        \n",
      "                An existing `.TriContourSet` does not get notified if properties of its\n",
      "                colormap are changed. Therefore, an explicit call to\n",
      "                `.ContourSet.changed()` is needed after modifying the colormap. The\n",
      "                explicit call can be left out, if a colorbar is assigned to the\n",
      "                `.TriContourSet` because it internally calls `.ContourSet.changed()`.\n",
      "        \n",
      "        xunits, yunits : registered units, optional\n",
      "            Override axis units by specifying an instance of a\n",
      "            :class:`matplotlib.units.ConversionInterface`.\n",
      "        \n",
      "        linewidths : float or array-like, default: :rc:`contour.linewidth`\n",
      "            The line width of the contour lines.\n",
      "        \n",
      "            If a number, all levels will be plotted with this linewidth.\n",
      "        \n",
      "            If a sequence, the levels in ascending order will be plotted with\n",
      "            the linewidths in the order specified.\n",
      "        \n",
      "            If None, this falls back to :rc:`lines.linewidth`.\n",
      "        \n",
      "        linestyles : {*None*, 'solid', 'dashed', 'dashdot', 'dotted'}, optional\n",
      "            If *linestyles* is *None*, the default is 'solid' unless the lines are\n",
      "            monochrome.  In that case, negative contours will take their linestyle\n",
      "            from :rc:`contour.negative_linestyle` setting.\n",
      "        \n",
      "            *linestyles* can also be an iterable of the above strings specifying a\n",
      "            set of linestyles to be used. If this iterable is shorter than the\n",
      "            number of contour levels it will be repeated as necessary.\n",
      "    \n",
      "    tricontourf(*args, **kwargs)\n",
      "        Draw contour regions on an unstructured triangular grid.\n",
      "        \n",
      "        The triangulation can be specified in one of two ways; either ::\n",
      "        \n",
      "            tricontourf(triangulation, ...)\n",
      "        \n",
      "        where *triangulation* is a `.Triangulation` object, or ::\n",
      "        \n",
      "            tricontourf(x, y, ...)\n",
      "            tricontourf(x, y, triangles, ...)\n",
      "            tricontourf(x, y, triangles=triangles, ...)\n",
      "            tricontourf(x, y, mask=mask, ...)\n",
      "            tricontourf(x, y, triangles, mask=mask, ...)\n",
      "        \n",
      "        in which case a `.Triangulation` object will be created.  See that class'\n",
      "        docstring for an explanation of these cases.\n",
      "        \n",
      "        The remaining arguments may be::\n",
      "        \n",
      "            tricontourf(..., Z)\n",
      "        \n",
      "        where *Z* is the array of values to contour, one per point in the\n",
      "        triangulation.  The level values are chosen automatically.\n",
      "        \n",
      "        ::\n",
      "        \n",
      "            tricontourf(..., Z, levels)\n",
      "        \n",
      "        contour up to *levels+1* automatically chosen contour levels (*levels*\n",
      "        intervals).\n",
      "        \n",
      "        ::\n",
      "        \n",
      "            tricontourf(..., Z, levels)\n",
      "        \n",
      "        draw contour regions at the values specified in sequence *levels*, which must\n",
      "        be in increasing order.\n",
      "        \n",
      "        ::\n",
      "        \n",
      "            tricontourf(Z, **kwargs)\n",
      "        \n",
      "        Use keyword arguments to control colors, linewidth, origin, cmap ... see below\n",
      "        for more details.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        triangulation : `.Triangulation`, optional\n",
      "            The unstructured triangular grid.\n",
      "        \n",
      "            If specified, then *x*, *y*, *triangles*, and *mask* are not accepted.\n",
      "        \n",
      "        x, y : array-like, optional\n",
      "            The coordinates of the values in *Z*.\n",
      "        \n",
      "        triangles : int array-like of shape (ntri, 3), optional\n",
      "            For each triangle, the indices of the three points that make up the\n",
      "            triangle, ordered in an anticlockwise manner.  If not specified, the\n",
      "            Delaunay triangulation is calculated.\n",
      "        \n",
      "        mask : bool array-like of shape (ntri), optional\n",
      "            Which triangles are masked out.\n",
      "        \n",
      "        Z : array-like(N, M)\n",
      "            The height values over which the contour is drawn.\n",
      "        \n",
      "        levels : int or array-like, optional\n",
      "            Determines the number and positions of the contour lines / regions.\n",
      "        \n",
      "            If an int *n*, use `~matplotlib.ticker.MaxNLocator`, which tries to\n",
      "            automatically choose no more than *n+1* \"nice\" contour levels between\n",
      "            *vmin* and *vmax*.\n",
      "        \n",
      "            If array-like, draw contour lines at the specified levels.  The values must\n",
      "            be in increasing order.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `~matplotlib.tri.TriContourSet`\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        colors : color string or sequence of colors, optional\n",
      "            The colors of the levels, i.e., the contour regions.\n",
      "        \n",
      "            The sequence is cycled for the levels in ascending order. If the sequence\n",
      "            is shorter than the number of levels, it's repeated.\n",
      "        \n",
      "            As a shortcut, single color strings may be used in place of one-element\n",
      "            lists, i.e. ``'red'`` instead of ``['red']`` to color all levels with the\n",
      "            same color. This shortcut does only work for color strings, not for other\n",
      "            ways of specifying colors.\n",
      "        \n",
      "            By default (value *None*), the colormap specified by *cmap* will be used.\n",
      "        \n",
      "        alpha : float, default: 1\n",
      "            The alpha blending value, between 0 (transparent) and 1 (opaque).\n",
      "        \n",
      "        cmap : str or `.Colormap`, default: :rc:`image.cmap`\n",
      "            A `.Colormap` instance or registered colormap name. The colormap maps the\n",
      "            level values to colors.\n",
      "        \n",
      "            If both *colors* and *cmap* are given, an error is raised.\n",
      "        \n",
      "        norm : `~matplotlib.colors.Normalize`, optional\n",
      "            If a colormap is used, the `.Normalize` instance scales the level values to\n",
      "            the canonical colormap range [0, 1] for mapping to colors. If not given,\n",
      "            the default linear scaling is used.\n",
      "        \n",
      "        origin : {*None*, 'upper', 'lower', 'image'}, default: None\n",
      "            Determines the orientation and exact position of *Z* by specifying the\n",
      "            position of ``Z[0, 0]``.  This is only relevant, if *X*, *Y* are not given.\n",
      "        \n",
      "            - *None*: ``Z[0, 0]`` is at X=0, Y=0 in the lower left corner.\n",
      "            - 'lower': ``Z[0, 0]`` is at X=0.5, Y=0.5 in the lower left corner.\n",
      "            - 'upper': ``Z[0, 0]`` is at X=N+0.5, Y=0.5 in the upper left corner.\n",
      "            - 'image': Use the value from :rc:`image.origin`.\n",
      "        \n",
      "        extent : (x0, x1, y0, y1), optional\n",
      "            If *origin* is not *None*, then *extent* is interpreted as in `.imshow`: it\n",
      "            gives the outer pixel boundaries. In this case, the position of Z[0, 0] is\n",
      "            the center of the pixel, not a corner. If *origin* is *None*, then\n",
      "            (*x0*, *y0*) is the position of Z[0, 0], and (*x1*, *y1*) is the position\n",
      "            of Z[-1, -1].\n",
      "        \n",
      "            This argument is ignored if *X* and *Y* are specified in the call to\n",
      "            contour.\n",
      "        \n",
      "        locator : ticker.Locator subclass, optional\n",
      "            The locator is used to determine the contour levels if they are not given\n",
      "            explicitly via *levels*.\n",
      "            Defaults to `~.ticker.MaxNLocator`.\n",
      "        \n",
      "        extend : {'neither', 'both', 'min', 'max'}, default: 'neither'\n",
      "            Determines the ``tricontourf``-coloring of values that are outside the\n",
      "            *levels* range.\n",
      "        \n",
      "            If 'neither', values outside the *levels* range are not colored.  If 'min',\n",
      "            'max' or 'both', color the values below, above or below and above the\n",
      "            *levels* range.\n",
      "        \n",
      "            Values below ``min(levels)`` and above ``max(levels)`` are mapped to the\n",
      "            under/over values of the `.Colormap`. Note that most colormaps do not have\n",
      "            dedicated colors for these by default, so that the over and under values\n",
      "            are the edge values of the colormap.  You may want to set these values\n",
      "            explicitly using `.Colormap.set_under` and `.Colormap.set_over`.\n",
      "        \n",
      "            .. note::\n",
      "        \n",
      "                An existing `.TriContourSet` does not get notified if properties of its\n",
      "                colormap are changed. Therefore, an explicit call to\n",
      "                `.ContourSet.changed()` is needed after modifying the colormap. The\n",
      "                explicit call can be left out, if a colorbar is assigned to the\n",
      "                `.TriContourSet` because it internally calls `.ContourSet.changed()`.\n",
      "        \n",
      "        xunits, yunits : registered units, optional\n",
      "            Override axis units by specifying an instance of a\n",
      "            :class:`matplotlib.units.ConversionInterface`.\n",
      "        \n",
      "        antialiased : bool, default: True\n",
      "            Whether to use antialiasing.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        `.tricontourf` fills intervals that are closed at the top; that is, for\n",
      "        boundaries *z1* and *z2*, the filled region is::\n",
      "        \n",
      "            z1 < Z <= z2\n",
      "        \n",
      "        except for the lowest interval, which is closed on both sides (i.e. it\n",
      "        includes the lowest value).\n",
      "    \n",
      "    tripcolor(*args, alpha=1.0, norm=None, cmap=None, vmin=None, vmax=None, shading='flat', facecolors=None, **kwargs)\n",
      "        Create a pseudocolor plot of an unstructured triangular grid.\n",
      "        \n",
      "        The triangulation can be specified in one of two ways; either::\n",
      "        \n",
      "          tripcolor(triangulation, ...)\n",
      "        \n",
      "        where triangulation is a `.Triangulation` object, or\n",
      "        \n",
      "        ::\n",
      "        \n",
      "          tripcolor(x, y, ...)\n",
      "          tripcolor(x, y, triangles, ...)\n",
      "          tripcolor(x, y, triangles=triangles, ...)\n",
      "          tripcolor(x, y, mask=mask, ...)\n",
      "          tripcolor(x, y, triangles, mask=mask, ...)\n",
      "        \n",
      "        in which case a Triangulation object will be created.  See `.Triangulation`\n",
      "        for a explanation of these possibilities.\n",
      "        \n",
      "        The next argument must be *C*, the array of color values, either\n",
      "        one per point in the triangulation if color values are defined at\n",
      "        points, or one per triangle in the triangulation if color values\n",
      "        are defined at triangles. If there are the same number of points\n",
      "        and triangles in the triangulation it is assumed that color\n",
      "        values are defined at points; to force the use of color values at\n",
      "        triangles use the kwarg ``facecolors=C`` instead of just ``C``.\n",
      "        \n",
      "        *shading* may be 'flat' (the default) or 'gouraud'. If *shading*\n",
      "        is 'flat' and C values are defined at points, the color values\n",
      "        used for each triangle are from the mean C of the triangle's\n",
      "        three points. If *shading* is 'gouraud' then color values must be\n",
      "        defined at points.\n",
      "        \n",
      "        The remaining kwargs are the same as for `~.Axes.pcolor`.\n",
      "    \n",
      "    triplot(*args, **kwargs)\n",
      "        Draw a unstructured triangular grid as lines and/or markers.\n",
      "        \n",
      "        The triangulation to plot can be specified in one of two ways; either::\n",
      "        \n",
      "          triplot(triangulation, ...)\n",
      "        \n",
      "        where triangulation is a `.Triangulation` object, or\n",
      "        \n",
      "        ::\n",
      "        \n",
      "          triplot(x, y, ...)\n",
      "          triplot(x, y, triangles, ...)\n",
      "          triplot(x, y, triangles=triangles, ...)\n",
      "          triplot(x, y, mask=mask, ...)\n",
      "          triplot(x, y, triangles, mask=mask, ...)\n",
      "        \n",
      "        in which case a Triangulation object will be created.  See `.Triangulation`\n",
      "        for a explanation of these possibilities.\n",
      "        \n",
      "        The remaining args and kwargs are the same as for `~.Axes.plot`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        lines : `~matplotlib.lines.Line2D`\n",
      "            The drawn triangles edges.\n",
      "        markers : `~matplotlib.lines.Line2D`\n",
      "            The drawn marker nodes.\n",
      "    \n",
      "    twinx(ax=None)\n",
      "        Make and return a second axes that shares the *x*-axis.  The new axes will\n",
      "        overlay *ax* (or the current axes if *ax* is *None*), and its ticks will be\n",
      "        on the right.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        :doc:`/gallery/subplots_axes_and_figures/two_scales`\n",
      "    \n",
      "    twiny(ax=None)\n",
      "        Make and return a second axes that shares the *y*-axis.  The new axes will\n",
      "        overlay *ax* (or the current axes if *ax* is *None*), and its ticks will be\n",
      "        on the top.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        :doc:`/gallery/subplots_axes_and_figures/two_scales`\n",
      "    \n",
      "    uninstall_repl_displayhook()\n",
      "        Uninstall the matplotlib display hook.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "           Need IPython >= 2 for this to work.  For IPython < 2 will raise a\n",
      "           ``NotImplementedError``\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "           If you are using vanilla python and have installed another\n",
      "           display hook this will reset ``sys.displayhook`` to what ever\n",
      "           function was there when matplotlib installed it's displayhook,\n",
      "           possibly discarding your changes.\n",
      "    \n",
      "    violinplot(dataset, positions=None, vert=True, widths=0.5, showmeans=False, showextrema=True, showmedians=False, quantiles=None, points=100, bw_method=None, *, data=None)\n",
      "        Make a violin plot.\n",
      "        \n",
      "        Make a violin plot for each column of *dataset* or each vector in\n",
      "        sequence *dataset*.  Each filled area extends to represent the\n",
      "        entire data range, with optional lines at the mean, the median,\n",
      "        the minimum, the maximum, and user-specified quantiles.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        dataset : Array or a sequence of vectors.\n",
      "          The input data.\n",
      "        \n",
      "        positions : array-like, default: [1, 2, ..., n]\n",
      "          Sets the positions of the violins. The ticks and limits are\n",
      "          automatically set to match the positions.\n",
      "        \n",
      "        vert : bool, default: True.\n",
      "          If true, creates a vertical violin plot.\n",
      "          Otherwise, creates a horizontal violin plot.\n",
      "        \n",
      "        widths : array-like, default: 0.5\n",
      "          Either a scalar or a vector that sets the maximal width of\n",
      "          each violin. The default is 0.5, which uses about half of the\n",
      "          available horizontal space.\n",
      "        \n",
      "        showmeans : bool, default: False\n",
      "          If `True`, will toggle rendering of the means.\n",
      "        \n",
      "        showextrema : bool, default: True\n",
      "          If `True`, will toggle rendering of the extrema.\n",
      "        \n",
      "        showmedians : bool, default: False\n",
      "          If `True`, will toggle rendering of the medians.\n",
      "        \n",
      "        quantiles : array-like, default: None\n",
      "          If not None, set a list of floats in interval [0, 1] for each violin,\n",
      "          which stands for the quantiles that will be rendered for that\n",
      "          violin.\n",
      "        \n",
      "        points : int, default: 100\n",
      "          Defines the number of points to evaluate each of the\n",
      "          gaussian kernel density estimations at.\n",
      "        \n",
      "        bw_method : str, scalar or callable, optional\n",
      "          The method used to calculate the estimator bandwidth.  This can be\n",
      "          'scott', 'silverman', a scalar constant or a callable.  If a\n",
      "          scalar, this will be used directly as `kde.factor`.  If a\n",
      "          callable, it should take a `GaussianKDE` instance as its only\n",
      "          parameter and return a scalar. If None (default), 'scott' is used.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        dict\n",
      "          A dictionary mapping each component of the violinplot to a\n",
      "          list of the corresponding collection instances created. The\n",
      "          dictionary has the following keys:\n",
      "        \n",
      "          - ``bodies``: A list of the `~.collections.PolyCollection`\n",
      "            instances containing the filled area of each violin.\n",
      "        \n",
      "          - ``cmeans``: A `~.collections.LineCollection` instance that marks\n",
      "            the mean values of each of the violin's distribution.\n",
      "        \n",
      "          - ``cmins``: A `~.collections.LineCollection` instance that marks\n",
      "            the bottom of each violin's distribution.\n",
      "        \n",
      "          - ``cmaxes``: A `~.collections.LineCollection` instance that marks\n",
      "            the top of each violin's distribution.\n",
      "        \n",
      "          - ``cbars``: A `~.collections.LineCollection` instance that marks\n",
      "            the centers of each violin's distribution.\n",
      "        \n",
      "          - ``cmedians``: A `~.collections.LineCollection` instance that\n",
      "            marks the median values of each of the violin's distribution.\n",
      "        \n",
      "          - ``cquantiles``: A `~.collections.LineCollection` instance created\n",
      "            to identify the quantile values of each of the violin's\n",
      "            distribution.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            the following arguments can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception):\n",
      "            *dataset*.\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    viridis()\n",
      "        Set the colormap to \"viridis\".\n",
      "        \n",
      "        This changes the default colormap as well as the colormap of the current\n",
      "        image if there is one. See ``help(colormaps)`` for more information.\n",
      "    \n",
      "    vlines(x, ymin, ymax, colors=None, linestyles='solid', label='', *, data=None, **kwargs)\n",
      "        Plot vertical lines.\n",
      "        \n",
      "        Plot vertical lines at each *x* from *ymin* to *ymax*.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : float or array-like\n",
      "            x-indexes where to plot the lines.\n",
      "        \n",
      "        ymin, ymax : float or array-like\n",
      "            Respective beginning and end of each line. If scalars are\n",
      "            provided, all lines will have same length.\n",
      "        \n",
      "        colors : list of colors, default: :rc:`lines.color`\n",
      "        \n",
      "        linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional\n",
      "        \n",
      "        label : str, default: ''\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        `~matplotlib.collections.LineCollection`\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs : `~matplotlib.collections.LineCollection` properties.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        hlines : horizontal lines\n",
      "        axvline: vertical line across the axes\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        \n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            the following arguments can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception):\n",
      "            *x*, *ymin*, *ymax*, *colors*.\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    waitforbuttonpress(timeout=-1)\n",
      "        Blocking call to interact with the figure.\n",
      "        \n",
      "        Wait for user input and return True if a key was pressed, False if a\n",
      "        mouse button was pressed and None if no input was given within\n",
      "        *timeout* seconds.  Negative values deactivate *timeout*.\n",
      "    \n",
      "    winter()\n",
      "        Set the colormap to \"winter\".\n",
      "        \n",
      "        This changes the default colormap as well as the colormap of the current\n",
      "        image if there is one. See ``help(colormaps)`` for more information.\n",
      "    \n",
      "    xcorr(x, y, normed=True, detrend=<function detrend_none at 0x113a44a60>, usevlines=True, maxlags=10, *, data=None, **kwargs)\n",
      "        Plot the cross correlation between *x* and *y*.\n",
      "        \n",
      "        The correlation with lag k is defined as\n",
      "        :math:`\\sum_n x[n+k] \\cdot y^*[n]`, where :math:`y^*` is the complex\n",
      "        conjugate of :math:`y`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x, y : array-like of length n\n",
      "        \n",
      "        detrend : callable, default: `.mlab.detrend_none` (no detrending)\n",
      "            A detrending function applied to *x* and *y*.  It must have the\n",
      "            signature ::\n",
      "        \n",
      "                detrend(x: np.ndarray) -> np.ndarray\n",
      "        \n",
      "        normed : bool, default: True\n",
      "            If ``True``, input vectors are normalised to unit length.\n",
      "        \n",
      "        usevlines : bool, default: True\n",
      "            Determines the plot style.\n",
      "        \n",
      "            If ``True``, vertical lines are plotted from 0 to the xcorr value\n",
      "            using `.Axes.vlines`. Additionally, a horizontal line is plotted\n",
      "            at y=0 using `.Axes.axhline`.\n",
      "        \n",
      "            If ``False``, markers are plotted at the xcorr values using\n",
      "            `.Axes.plot`.\n",
      "        \n",
      "        maxlags : int, default: 10\n",
      "            Number of lags to show. If None, will return all ``2 * len(x) - 1``\n",
      "            lags.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        lags : array (length ``2*maxlags+1``)\n",
      "            The lag vector.\n",
      "        c : array  (length ``2*maxlags+1``)\n",
      "            The auto correlation vector.\n",
      "        line : `.LineCollection` or `.Line2D`\n",
      "            `.Artist` added to the axes of the correlation:\n",
      "        \n",
      "            - `.LineCollection` if *usevlines* is True.\n",
      "            - `.Line2D` if *usevlines* is False.\n",
      "        b : `.Line2D` or None\n",
      "            Horizontal line at 0 if *usevlines* is True\n",
      "            None *usevlines* is False.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        linestyle : `.Line2D` property, optional\n",
      "            The linestyle for plotting the data points.\n",
      "            Only used if *usevlines* is ``False``.\n",
      "        \n",
      "        marker : str, default: 'o'\n",
      "            The marker for plotting the data points.\n",
      "            Only used if *usevlines* is ``False``.\n",
      "        \n",
      "        **kwargs\n",
      "            Additional parameters are passed to `.Axes.vlines` and\n",
      "            `.Axes.axhline` if *usevlines* is ``True``; otherwise they are\n",
      "            passed to `.Axes.plot`.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The cross correlation is performed with `numpy.correlate` with\n",
      "        ``mode = \"full\"``.\n",
      "        \n",
      "        .. note::\n",
      "            In addition to the above described arguments, this function can take\n",
      "            a *data* keyword argument. If such a *data* argument is given,\n",
      "            the following arguments can also be string ``s``, which is\n",
      "            interpreted as ``data[s]`` (unless this raises an exception):\n",
      "            *x*, *y*.\n",
      "        \n",
      "            Objects passed as **data** must support item access (``data[s]``) and\n",
      "            membership test (``s in data``).\n",
      "    \n",
      "    xkcd(scale=1, length=100, randomness=2)\n",
      "        Turn on `xkcd <https://xkcd.com/>`_ sketch-style drawing mode.  This will\n",
      "        only have effect on things drawn after this function is called.\n",
      "        \n",
      "        For best results, the \"Humor Sans\" font should be installed: it is\n",
      "        not included with Matplotlib.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        scale : float, optional\n",
      "            The amplitude of the wiggle perpendicular to the source line.\n",
      "        length : float, optional\n",
      "            The length of the wiggle along the line.\n",
      "        randomness : float, optional\n",
      "            The scale factor by which the length is shrunken or expanded.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function works by a number of rcParams, so it will probably\n",
      "        override others you have set before.\n",
      "        \n",
      "        If you want the effects of this function to be temporary, it can\n",
      "        be used as a context manager, for example::\n",
      "        \n",
      "            with plt.xkcd():\n",
      "                # This figure will be in XKCD-style\n",
      "                fig1 = plt.figure()\n",
      "                # ...\n",
      "        \n",
      "            # This figure will be in regular style\n",
      "            fig2 = plt.figure()\n",
      "    \n",
      "    xlabel(xlabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)\n",
      "        Set the label for the x-axis.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        xlabel : str\n",
      "            The label text.\n",
      "        \n",
      "        labelpad : float, default: None\n",
      "            Spacing in points from the axes bounding box including ticks\n",
      "            and tick labels.\n",
      "        \n",
      "        loc : {'left', 'center', 'right'}, default: :rc:`xaxis.labellocation`\n",
      "            The label position. This is a high-level alternative for passing\n",
      "            parameters *x* and *horizontalalignment*.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs : `.Text` properties\n",
      "            `.Text` properties control the appearance of the label.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        text : Documents the properties supported by `.Text`.\n",
      "    \n",
      "    xlim(*args, **kwargs)\n",
      "        Get or set the x limits of the current axes.\n",
      "        \n",
      "        Call signatures::\n",
      "        \n",
      "            left, right = xlim()  # return the current xlim\n",
      "            xlim((left, right))   # set the xlim to left, right\n",
      "            xlim(left, right)     # set the xlim to left, right\n",
      "        \n",
      "        If you do not specify args, you can pass *left* or *right* as kwargs,\n",
      "        i.e.::\n",
      "        \n",
      "            xlim(right=3)  # adjust the right leaving left unchanged\n",
      "            xlim(left=1)  # adjust the left leaving right unchanged\n",
      "        \n",
      "        Setting limits turns autoscaling off for the x-axis.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        left, right\n",
      "            A tuple of the new x-axis limits.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Calling this function with no arguments (e.g. ``xlim()``) is the pyplot\n",
      "        equivalent of calling `~.Axes.get_xlim` on the current axes.\n",
      "        Calling this function with arguments is the pyplot equivalent of calling\n",
      "        `~.Axes.set_xlim` on the current axes. All arguments are passed though.\n",
      "    \n",
      "    xscale(value, **kwargs)\n",
      "        Set the x-axis scale.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        value : {\"linear\", \"log\", \"symlog\", \"logit\", ...}\n",
      "            The axis scale type to apply.\n",
      "        \n",
      "        **kwargs\n",
      "            Different keyword arguments are accepted, depending on the scale.\n",
      "            See the respective class keyword arguments:\n",
      "        \n",
      "            - `matplotlib.scale.LinearScale`\n",
      "            - `matplotlib.scale.LogScale`\n",
      "            - `matplotlib.scale.SymmetricalLogScale`\n",
      "            - `matplotlib.scale.LogitScale`\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        By default, Matplotlib supports the above mentioned scales.\n",
      "        Additionally, custom scales may be registered using\n",
      "        `matplotlib.scale.register_scale`. These scales can then also\n",
      "        be used here.\n",
      "    \n",
      "    xticks(ticks=None, labels=None, **kwargs)\n",
      "        Get or set the current tick locations and labels of the x-axis.\n",
      "        \n",
      "        Pass no arguments to return the current values without modifying them.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        ticks : array-like, optional\n",
      "            The list of xtick locations.  Passing an empty list removes all xticks.\n",
      "        labels : array-like, optional\n",
      "            The labels to place at the given *ticks* locations.  This argument can\n",
      "            only be passed if *ticks* is passed as well.\n",
      "        **kwargs\n",
      "            `.Text` properties can be used to control the appearance of the labels.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        locs\n",
      "            The list of xtick locations.\n",
      "        labels\n",
      "            The list of xlabel `.Text` objects.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Calling this function with no arguments (e.g. ``xticks()``) is the pyplot\n",
      "        equivalent of calling `~.Axes.get_xticks` and `~.Axes.get_xticklabels` on\n",
      "        the current axes.\n",
      "        Calling this function with arguments is the pyplot equivalent of calling\n",
      "        `~.Axes.set_xticks` and `~.Axes.set_xticklabels` on the current axes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> locs, labels = xticks()  # Get the current locations and labels.\n",
      "        >>> xticks(np.arange(0, 1, step=0.2))  # Set label locations.\n",
      "        >>> xticks(np.arange(3), ['Tom', 'Dick', 'Sue'])  # Set text labels.\n",
      "        >>> xticks([0, 1, 2], ['January', 'February', 'March'],\n",
      "        ...        rotation=20)  # Set text labels and properties.\n",
      "        >>> xticks([])  # Disable xticks.\n",
      "    \n",
      "    ylabel(ylabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)\n",
      "        Set the label for the y-axis.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        ylabel : str\n",
      "            The label text.\n",
      "        \n",
      "        labelpad : float, default: None\n",
      "            Spacing in points from the axes bounding box including ticks\n",
      "            and tick labels.\n",
      "        \n",
      "        loc : {'bottom', 'center', 'top'}, default: :rc:`yaxis.labellocation`\n",
      "            The label position. This is a high-level alternative for passing\n",
      "            parameters *y* and *horizontalalignment*.\n",
      "        \n",
      "        Other Parameters\n",
      "        ----------------\n",
      "        **kwargs : `.Text` properties\n",
      "            `.Text` properties control the appearance of the label.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        text : Documents the properties supported by `.Text`.\n",
      "    \n",
      "    ylim(*args, **kwargs)\n",
      "        Get or set the y-limits of the current axes.\n",
      "        \n",
      "        Call signatures::\n",
      "        \n",
      "            bottom, top = ylim()  # return the current ylim\n",
      "            ylim((bottom, top))   # set the ylim to bottom, top\n",
      "            ylim(bottom, top)     # set the ylim to bottom, top\n",
      "        \n",
      "        If you do not specify args, you can alternatively pass *bottom* or\n",
      "        *top* as kwargs, i.e.::\n",
      "        \n",
      "            ylim(top=3)  # adjust the top leaving bottom unchanged\n",
      "            ylim(bottom=1)  # adjust the bottom leaving top unchanged\n",
      "        \n",
      "        Setting limits turns autoscaling off for the y-axis.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        bottom, top\n",
      "            A tuple of the new y-axis limits.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Calling this function with no arguments (e.g. ``ylim()``) is the pyplot\n",
      "        equivalent of calling `~.Axes.get_ylim` on the current axes.\n",
      "        Calling this function with arguments is the pyplot equivalent of calling\n",
      "        `~.Axes.set_ylim` on the current axes. All arguments are passed though.\n",
      "    \n",
      "    yscale(value, **kwargs)\n",
      "        Set the y-axis scale.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        value : {\"linear\", \"log\", \"symlog\", \"logit\", ...}\n",
      "            The axis scale type to apply.\n",
      "        \n",
      "        **kwargs\n",
      "            Different keyword arguments are accepted, depending on the scale.\n",
      "            See the respective class keyword arguments:\n",
      "        \n",
      "            - `matplotlib.scale.LinearScale`\n",
      "            - `matplotlib.scale.LogScale`\n",
      "            - `matplotlib.scale.SymmetricalLogScale`\n",
      "            - `matplotlib.scale.LogitScale`\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        By default, Matplotlib supports the above mentioned scales.\n",
      "        Additionally, custom scales may be registered using\n",
      "        `matplotlib.scale.register_scale`. These scales can then also\n",
      "        be used here.\n",
      "    \n",
      "    yticks(ticks=None, labels=None, **kwargs)\n",
      "        Get or set the current tick locations and labels of the y-axis.\n",
      "        \n",
      "        Pass no arguments to return the current values without modifying them.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        ticks : array-like, optional\n",
      "            The list of ytick locations.  Passing an empty list removes all yticks.\n",
      "        labels : array-like, optional\n",
      "            The labels to place at the given *ticks* locations.  This argument can\n",
      "            only be passed if *ticks* is passed as well.\n",
      "        **kwargs\n",
      "            `.Text` properties can be used to control the appearance of the labels.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        locs\n",
      "            The list of ytick locations.\n",
      "        labels\n",
      "            The list of ylabel `.Text` objects.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Calling this function with no arguments (e.g. ``yticks()``) is the pyplot\n",
      "        equivalent of calling `~.Axes.get_yticks` and `~.Axes.get_yticklabels` on\n",
      "        the current axes.\n",
      "        Calling this function with arguments is the pyplot equivalent of calling\n",
      "        `~.Axes.set_yticks` and `~.Axes.set_yticklabels` on the current axes.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> locs, labels = yticks()  # Get the current locations and labels.\n",
      "        >>> yticks(np.arange(0, 1, step=0.2))  # Set label locations.\n",
      "        >>> yticks(np.arange(3), ['Tom', 'Dick', 'Sue'])  # Set text labels.\n",
      "        >>> yticks([0, 1, 2], ['January', 'February', 'March'],\n",
      "        ...        rotation=45)  # Set text labels and properties.\n",
      "        >>> yticks([])  # Disable yticks.\n",
      "\n",
      "DATA\n",
      "    rcParams = RcParams({'_internal.classic_mode': False,\n",
      "         ...nor.widt...\n",
      "    rcParamsDefault = RcParams({'_internal.classic_mode': False,\n",
      "         ...n...\n",
      "    rcParamsOrig = RcParams({'_internal.classic_mode': False,\n",
      "         ...nor....\n",
      "\n",
      "FILE\n",
      "    /opt/anaconda3/envs/python_intro/lib/python3.8/site-packages/matplotlib/pyplot.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "plt?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "455a6a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86073b1a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Axes3D in module mpl_toolkits.mplot3d.axes3d:\n",
      "\n",
      "class Axes3D(matplotlib.axes._axes.Axes)\n",
      " |  Axes3D(fig, rect=None, *args, azim=-60, elev=30, sharez=None, proj_type='persp', box_aspect=None, **kwargs)\n",
      " |  \n",
      " |  3D axes object.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Axes3D\n",
      " |      matplotlib.axes._axes.Axes\n",
      " |      matplotlib.axes._base._AxesBase\n",
      " |      matplotlib.artist.Artist\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, fig, rect=None, *args, azim=-60, elev=30, sharez=None, proj_type='persp', box_aspect=None, **kwargs)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fig : Figure\n",
      " |          The parent figure.\n",
      " |      rect : (float, float, float, float)\n",
      " |          The ``(left, bottom, width, height)`` axes position.\n",
      " |      azim : float, default: -60\n",
      " |          Azimuthal viewing angle.\n",
      " |      elev : float, default: 30\n",
      " |          Elevation viewing angle.\n",
      " |      sharez : Axes3D, optional\n",
      " |          Other axes to share z-limits with.\n",
      " |      proj_type : {'persp', 'ortho'}\n",
      " |          The projection type, default 'persp'.\n",
      " |      **kwargs\n",
      " |          Other optional keyword arguments:\n",
      " |      \n",
      " |          Properties:\n",
      " |          adjustable: {'box', 'datalim'}\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          anchor: 2-tuple of floats or {'C', 'SW', 'S', 'SE', ...}\n",
      " |          animated: bool\n",
      " |          aspect: {'auto'}\n",
      " |          autoscale_on: bool\n",
      " |          autoscalex_on: bool\n",
      " |          autoscaley_on: bool\n",
      " |          autoscalez_on: bool\n",
      " |          axes_locator: Callable[[Axes, Renderer], Bbox]\n",
      " |          axisbelow: bool or 'line'\n",
      " |          box_aspect: 3-tuple of floats or None\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          contains: unknown\n",
      " |          facecolor or fc: color\n",
      " |          figure: `.Figure`\n",
      " |          frame_on: bool\n",
      " |          gid: str\n",
      " |          in_layout: bool\n",
      " |          label: object\n",
      " |          navigate: bool\n",
      " |          navigate_mode: unknown\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: None or bool or callable\n",
      " |          position: [left, bottom, width, height] or `~matplotlib.transforms.Bbox`\n",
      " |          proj_type: {'persp', 'ortho'}\n",
      " |          prop_cycle: unknown\n",
      " |          rasterization_zorder: float or None\n",
      " |          rasterized: bool or None\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          title: str\n",
      " |          transform: `.Transform`\n",
      " |          url: str\n",
      " |          visible: bool\n",
      " |          xbound: unknown\n",
      " |          xlabel: str\n",
      " |          xlim3d or xlim: unknown\n",
      " |          xmargin: float greater than -0.5\n",
      " |          xscale: {\"linear\"}\n",
      " |          xticklabels: unknown\n",
      " |          xticks: unknown\n",
      " |          ybound: unknown\n",
      " |          ylabel: str\n",
      " |          ylim3d or ylim: unknown\n",
      " |          ymargin: float greater than -0.5\n",
      " |          yscale: {\"linear\"}\n",
      " |          yticklabels: unknown\n",
      " |          yticks: unknown\n",
      " |          zbound: unknown\n",
      " |          zlabel: unknown\n",
      " |          zlim3d or zlim: unknown\n",
      " |          zmargin: unknown\n",
      " |          zorder: float\n",
      " |          zscale: {\"linear\"}\n",
      " |          zticklabels: unknown\n",
      " |          zticks: unknown\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. versionadded:: 1.2.1\n",
      " |          The *sharez* parameter.\n",
      " |  \n",
      " |  add_collection3d(self, col, zs=0, zdir='z')\n",
      " |      Add a 3D collection object to the plot.\n",
      " |      \n",
      " |      2D collection types are converted to a 3D version by\n",
      " |      modifying the object and adding z coordinate information.\n",
      " |      \n",
      " |      Supported are:\n",
      " |          - PolyCollection\n",
      " |          - LineCollection\n",
      " |          - PatchCollection\n",
      " |  \n",
      " |  add_contour_set(self, cset, extend3d=False, stride=5, zdir='z', offset=None)\n",
      " |  \n",
      " |  add_contourf_set(self, cset, zdir='z', offset=None)\n",
      " |  \n",
      " |  apply_aspect(self, position=None)\n",
      " |      Adjust the Axes for a specified data aspect ratio.\n",
      " |      \n",
      " |      Depending on `.get_adjustable` this will modify either the\n",
      " |      Axes box (position) or the view limits. In the former case,\n",
      " |      `~matplotlib.axes.Axes.get_anchor` will affect the position.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is called automatically when each Axes is drawn.  You may need\n",
      " |      to call it yourself if you need to update the Axes position and/or\n",
      " |      view limits before the Figure is drawn.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      matplotlib.axes.Axes.set_aspect\n",
      " |          For a description of aspect ratio handling.\n",
      " |      matplotlib.axes.Axes.set_adjustable\n",
      " |          Set how the Axes adjusts to achieve the required aspect ratio.\n",
      " |      matplotlib.axes.Axes.set_anchor\n",
      " |          Set the position in case of extra space.\n",
      " |  \n",
      " |  auto_scale_xyz(self, X, Y, Z=None, had_data=None)\n",
      " |  \n",
      " |  autoscale(self, enable=True, axis='both', tight=None)\n",
      " |      Convenience method for simple axis view autoscaling.\n",
      " |      See :meth:`matplotlib.axes.Axes.autoscale` for full explanation.\n",
      " |      Note that this function behaves the same, but for all\n",
      " |      three axes.  Therefore, 'z' can be passed for *axis*,\n",
      " |      and 'both' applies to all three axes.\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |  \n",
      " |  autoscale_view(self, tight=None, scalex=True, scaley=True, scalez=True)\n",
      " |      Autoscale the view limits using the data limits.\n",
      " |      See :meth:`matplotlib.axes.Axes.autoscale_view` for documentation.\n",
      " |      Note that this function applies to the 3D axes, and as such\n",
      " |      adds the *scalez* to the function arguments.\n",
      " |      \n",
      " |      .. versionchanged:: 1.1.0\n",
      " |          Function signature was changed to better match the 2D version.\n",
      " |          *tight* is now explicitly a kwarg and placed first.\n",
      " |      \n",
      " |      .. versionchanged:: 1.2.1\n",
      " |          This is now fully functional.\n",
      " |  \n",
      " |  bar(self, left, height, zs=0, zdir='z', *args, **kwargs)\n",
      " |      Add 2D bar(s).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      left : 1D array-like\n",
      " |          The x coordinates of the left sides of the bars.\n",
      " |      height : 1D array-like\n",
      " |          The height of the bars.\n",
      " |      zs : float or 1D array-like\n",
      " |          Z coordinate of bars; if a single value is specified, it will be\n",
      " |          used for all bars.\n",
      " |      zdir : {'x', 'y', 'z'}, default: 'z'\n",
      " |          When plotting 2D data, the direction to use as z ('x', 'y' or 'z').\n",
      " |      **kwargs\n",
      " |          Other arguments are forwarded to `matplotlib.axes.Axes.bar`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      mpl_toolkits.mplot3d.art3d.Patch3DCollection\n",
      " |  \n",
      " |  bar3d(self, x, y, z, dx, dy, dz, color=None, zsort='average', shade=True, lightsource=None, *args, **kwargs)\n",
      " |      Generate a 3D barplot.\n",
      " |      \n",
      " |      This method creates three dimensional barplot where the width,\n",
      " |      depth, height, and color of the bars can all be uniquely set.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x, y, z : array-like\n",
      " |          The coordinates of the anchor point of the bars.\n",
      " |      \n",
      " |      dx, dy, dz : float or array-like\n",
      " |          The width, depth, and height of the bars, respectively.\n",
      " |      \n",
      " |      color : sequence of colors, optional\n",
      " |          The color of the bars can be specified globally or\n",
      " |          individually. This parameter can be:\n",
      " |      \n",
      " |          - A single color, to color all bars the same color.\n",
      " |          - An array of colors of length N bars, to color each bar\n",
      " |            independently.\n",
      " |          - An array of colors of length 6, to color the faces of the\n",
      " |            bars similarly.\n",
      " |          - An array of colors of length 6 * N bars, to color each face\n",
      " |            independently.\n",
      " |      \n",
      " |          When coloring the faces of the boxes specifically, this is\n",
      " |          the order of the coloring:\n",
      " |      \n",
      " |            1. -Z (bottom of box)\n",
      " |            2. +Z (top of box)\n",
      " |            3. -Y\n",
      " |            4. +Y\n",
      " |            5. -X\n",
      " |            6. +X\n",
      " |      \n",
      " |      zsort : str, optional\n",
      " |          The z-axis sorting scheme passed onto `~.art3d.Poly3DCollection`\n",
      " |      \n",
      " |      shade : bool, default: True\n",
      " |          When true, this shades the dark sides of the bars (relative\n",
      " |          to the plot's source of light).\n",
      " |      \n",
      " |      lightsource : `~matplotlib.colors.LightSource`\n",
      " |          The lightsource to use when *shade* is True.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Any additional keyword arguments are passed onto\n",
      " |          `~.art3d.Poly3DCollection`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      collection : `~.art3d.Poly3DCollection`\n",
      " |          A collection of three dimensional polygons representing\n",
      " |          the bars.\n",
      " |  \n",
      " |  can_pan(self)\n",
      " |      Return *True* if this axes supports the pan/zoom button functionality.\n",
      " |      \n",
      " |      3D axes objects do not use the pan/zoom button.\n",
      " |  \n",
      " |  can_zoom(self)\n",
      " |      Return *True* if this axes supports the zoom box button functionality.\n",
      " |      \n",
      " |      3D axes objects do not use the zoom box button.\n",
      " |  \n",
      " |  cla(self)\n",
      " |      Clear the current axes.\n",
      " |  \n",
      " |  clabel(self, *args, **kwargs)\n",
      " |      Currently not implemented for 3D axes, and returns *None*.\n",
      " |  \n",
      " |  contour(self, X, Y, Z, *args, extend3d=False, stride=5, zdir='z', offset=None, **kwargs)\n",
      " |      Create a 3D contour plot.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X, Y, Z : array-like\n",
      " |          Input data.\n",
      " |      extend3d : bool, default: False\n",
      " |          Whether to extend contour in 3D.\n",
      " |      stride : int\n",
      " |          Step size for extending contour.\n",
      " |      zdir : {'x', 'y', 'z'}, default: 'z'\n",
      " |          The direction to use.\n",
      " |      offset : float, optional\n",
      " |          If specified, plot a projection of the contour lines at this\n",
      " |          position in a plane normal to zdir.\n",
      " |      *args, **kwargs\n",
      " |          Other arguments are forwarded to `matplotlib.axes.Axes.contour`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      matplotlib.contour.QuadContourSet\n",
      " |  \n",
      " |  contour3D = contour(self, X, Y, Z, *args, extend3d=False, stride=5, zdir='z', offset=None, **kwargs)\n",
      " |  \n",
      " |  contourf(self, X, Y, Z, *args, zdir='z', offset=None, **kwargs)\n",
      " |      Create a 3D filled contour plot.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X, Y, Z : array-like\n",
      " |          Input data.\n",
      " |      zdir : {'x', 'y', 'z'}, default: 'z'\n",
      " |          The direction to use.\n",
      " |      offset : float, optional\n",
      " |          If specified, plot a projection of the contour lines at this\n",
      " |          position in a plane normal to zdir.\n",
      " |      *args, **kwargs\n",
      " |          Other arguments are forwarded to `matplotlib.axes.Axes.contourf`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      matplotlib.contour.QuadContourSet\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. versionadded:: 1.1.0\n",
      " |          The *zdir* and *offset* parameters.\n",
      " |  \n",
      " |  contourf3D = contourf(self, X, Y, Z, *args, zdir='z', offset=None, **kwargs)\n",
      " |  \n",
      " |  convert_zunits(self, z)\n",
      " |      For artists in an axes, if the zaxis has units support,\n",
      " |      convert *z* using zaxis unit type\n",
      " |      \n",
      " |      .. versionadded:: 1.2.1\n",
      " |  \n",
      " |  disable_mouse_rotation(self)\n",
      " |      Disable mouse buttons for 3D rotation and zooming.\n",
      " |  \n",
      " |  draw(self, renderer)\n",
      " |      Draw the Artist (and its children) using the given renderer.\n",
      " |      \n",
      " |      This has no effect if the artist is not visible (`.Artist.get_visible`\n",
      " |      returns False).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      renderer : `.RendererBase` subclass.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method is overridden in the Artist subclasses.\n",
      " |  \n",
      " |  format_coord(self, xd, yd)\n",
      " |      Given the 2D view coordinates attempt to guess a 3D coordinate.\n",
      " |      Looks for the nearest edge to the point and then assumes that\n",
      " |      the point is at the same z location as the nearest point on the edge.\n",
      " |  \n",
      " |  format_zdata(self, z)\n",
      " |      Return *z* string formatted.  This function will use the\n",
      " |      :attr:`fmt_zdata` attribute if it is callable, else will fall\n",
      " |      back on the zaxis major formatter\n",
      " |  \n",
      " |  get_autoscale_on(self)\n",
      " |      Get whether autoscaling is applied for all axes on plot commands\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |          This function was added, but not tested. Please report any bugs.\n",
      " |  \n",
      " |  get_autoscalez_on(self)\n",
      " |      Get whether autoscaling for the z-axis is applied on plot commands\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |          This function was added, but not tested. Please report any bugs.\n",
      " |  \n",
      " |  get_axis_position(self)\n",
      " |  \n",
      " |  get_frame_on(self)\n",
      " |      Get whether the 3D axes panels are drawn.\n",
      " |  \n",
      " |  get_proj(self)\n",
      " |      Create the projection matrix from the current viewing position.\n",
      " |  \n",
      " |  get_tightbbox(self, renderer, call_axes_locator=True, bbox_extra_artists=None, *, for_layout_only=False)\n",
      " |      Return the tight bounding box of the axes, including axis and their\n",
      " |      decorators (xlabel, title, etc).\n",
      " |      \n",
      " |      Artists that have ``artist.set_in_layout(False)`` are not included\n",
      " |      in the bbox.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      renderer : `.RendererBase` subclass\n",
      " |          renderer that will be used to draw the figures (i.e.\n",
      " |          ``fig.canvas.get_renderer()``)\n",
      " |      \n",
      " |      bbox_extra_artists : list of `.Artist` or ``None``\n",
      " |          List of artists to include in the tight bounding box.  If\n",
      " |          ``None`` (default), then all artist children of the axes are\n",
      " |          included in the tight bounding box.\n",
      " |      \n",
      " |      call_axes_locator : bool, default: True\n",
      " |          If *call_axes_locator* is ``False``, it does not call the\n",
      " |          ``_axes_locator`` attribute, which is necessary to get the correct\n",
      " |          bounding box. ``call_axes_locator=False`` can be used if the\n",
      " |          caller is only interested in the relative size of the tightbbox\n",
      " |          compared to the axes bbox.\n",
      " |      \n",
      " |      for_layout_only : default: False\n",
      " |          The bounding box will *not* include the x-extent of the title and\n",
      " |          the xlabel, or the y-extent of the ylabel.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `.BboxBase`\n",
      " |          Bounding box in figure pixel coordinates.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      matplotlib.axes.Axes.get_window_extent\n",
      " |      matplotlib.axis.Axis.get_tightbbox\n",
      " |      matplotlib.spines.Spine.get_window_extent\n",
      " |  \n",
      " |  get_w_lims(self)\n",
      " |      Get 3D world limits.\n",
      " |  \n",
      " |  get_xlim(self)\n",
      " |      Alias for `get_xlim3d`.\n",
      " |  \n",
      " |  get_xlim3d(self)\n",
      " |      Return the x-axis view limits.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      left, right : (float, float)\n",
      " |          The current x-axis limits in data coordinates.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      set_xlim\n",
      " |      set_xbound, get_xbound\n",
      " |      invert_xaxis, xaxis_inverted\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The x-axis may be inverted, in which case the *left* value will\n",
      " |      be greater than the *right* value.\n",
      " |      \n",
      " |      \n",
      " |      .. versionchanged:: 1.1.0\n",
      " |          This function now correctly refers to the 3D x-limits\n",
      " |  \n",
      " |  get_ylim(self)\n",
      " |      Alias for `get_ylim3d`.\n",
      " |  \n",
      " |  get_ylim3d(self)\n",
      " |      Return the y-axis view limits.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bottom, top : (float, float)\n",
      " |          The current y-axis limits in data coordinates.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      set_ylim\n",
      " |      set_ybound, get_ybound\n",
      " |      invert_yaxis, yaxis_inverted\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The y-axis may be inverted, in which case the *bottom* value\n",
      " |      will be greater than the *top* value.\n",
      " |      \n",
      " |      \n",
      " |      .. versionchanged:: 1.1.0\n",
      " |          This function now correctly refers to the 3D y-limits.\n",
      " |  \n",
      " |  get_zaxis(self)\n",
      " |      Return the ``ZAxis`` (`~.axis3d.Axis`) instance.\n",
      " |  \n",
      " |  get_zbound(self)\n",
      " |      Return the lower and upper z-axis bounds, in increasing order.\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |  \n",
      " |  get_zgridlines(self)\n",
      " |      Return the zaxis' grid lines as a list of `.Line2D`\\s.\n",
      " |  \n",
      " |  get_zlabel(self)\n",
      " |      Get the z-label text string.\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |          This function was added, but not tested. Please report any bugs.\n",
      " |  \n",
      " |  get_zlim(self)\n",
      " |      Alias for `get_zlim3d`.\n",
      " |  \n",
      " |  get_zlim3d(self)\n",
      " |      Get 3D z limits.\n",
      " |  \n",
      " |  get_zmajorticklabels(self)\n",
      " |      Return the zaxis' major tick labels, as a list of `~.text.Text`.\n",
      " |  \n",
      " |  get_zminorticklabels(self)\n",
      " |      Return the zaxis' minor tick labels, as a list of `~.text.Text`.\n",
      " |  \n",
      " |  get_zscale(self)\n",
      " |  \n",
      " |  get_zticklabels(self, minor=False, which=None)\n",
      " |      Get the zaxis' tick labels.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      minor : bool\n",
      " |         Whether to return the minor or the major ticklabels.\n",
      " |      \n",
      " |      which : None, ('minor', 'major', 'both')\n",
      " |         Overrides *minor*.\n",
      " |      \n",
      " |         Selects which ticklabels to return\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of `~matplotlib.text.Text`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The tick label strings are not populated until a ``draw`` method has\n",
      " |      been called.\n",
      " |      \n",
      " |      See also: `~.pyplot.draw` and `~.FigureCanvasBase.draw`.\n",
      " |  \n",
      " |  get_zticklines(self, minor=False)\n",
      " |      Return the zaxis' tick lines as a list of `.Line2D`\\s.\n",
      " |  \n",
      " |  get_zticks(self, *, minor=False)\n",
      " |      Return the zaxis' tick locations in data coordinates.\n",
      " |  \n",
      " |  grid(self, b=True, **kwargs)\n",
      " |      Set / unset 3D grid.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          Currently, this function does not behave the same as\n",
      " |          :meth:`matplotlib.axes.Axes.grid`, but it is intended to\n",
      " |          eventually support that behavior.\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |  \n",
      " |  invert_zaxis(self)\n",
      " |      Invert the z-axis.\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |          This function was added, but not tested. Please report any bugs.\n",
      " |  \n",
      " |  locator_params(self, axis='both', tight=None, **kwargs)\n",
      " |      Convenience method for controlling tick locators.\n",
      " |      \n",
      " |      See :meth:`matplotlib.axes.Axes.locator_params` for full\n",
      " |      documentation.  Note that this is for Axes3D objects,\n",
      " |      therefore, setting *axis* to 'both' will result in the\n",
      " |      parameters being set for all three axes.  Also, *axis*\n",
      " |      can also take a value of 'z' to apply parameters to the\n",
      " |      z axis.\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |          This function was added, but not tested. Please report any bugs.\n",
      " |  \n",
      " |  margins(self, *margins, x=None, y=None, z=None, tight=True)\n",
      " |      Convenience method to set or retrieve autoscaling margins.\n",
      " |      \n",
      " |      Call signatures::\n",
      " |      \n",
      " |          margins()\n",
      " |      \n",
      " |      returns xmargin, ymargin, zmargin\n",
      " |      \n",
      " |      ::\n",
      " |      \n",
      " |          margins(margin)\n",
      " |      \n",
      " |          margins(xmargin, ymargin, zmargin)\n",
      " |      \n",
      " |          margins(x=xmargin, y=ymargin, z=zmargin)\n",
      " |      \n",
      " |          margins(..., tight=False)\n",
      " |      \n",
      " |      All forms above set the xmargin, ymargin and zmargin\n",
      " |      parameters. All keyword parameters are optional.  A single\n",
      " |      positional argument specifies xmargin, ymargin and zmargin.\n",
      " |      Passing both positional and keyword arguments for xmargin,\n",
      " |      ymargin, and/or zmargin is invalid.\n",
      " |      \n",
      " |      The *tight* parameter\n",
      " |      is passed to :meth:`autoscale_view`, which is executed after\n",
      " |      a margin is changed; the default here is *True*, on the\n",
      " |      assumption that when margins are specified, no additional\n",
      " |      padding to match tick marks is usually desired.  Setting\n",
      " |      *tight* to *None* will preserve the previous setting.\n",
      " |      \n",
      " |      Specifying any margin changes only the autoscaling; for example,\n",
      " |      if *xmargin* is not None, then *xmargin* times the X data\n",
      " |      interval will be added to each end of that interval before\n",
      " |      it is used in autoscaling.\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |  \n",
      " |  mouse_init(self, rotate_btn=1, zoom_btn=3)\n",
      " |      Set the mouse buttons for 3D rotation and zooming.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      rotate_btn : int or list of int, default: 1\n",
      " |          The mouse button or buttons to use for 3D rotation of the axes.\n",
      " |      zoom_btn : int or list of int, default: 3\n",
      " |          The mouse button or buttons to use to zoom the 3D axes.\n",
      " |  \n",
      " |  plot(self, xs, ys, *args, zdir='z', **kwargs)\n",
      " |      Plot 2D or 3D data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      xs : 1D array-like\n",
      " |          x coordinates of vertices.\n",
      " |      ys : 1D array-like\n",
      " |          y coordinates of vertices.\n",
      " |      zs : float or 1D array-like\n",
      " |          z coordinates of vertices; either one for all points or one for\n",
      " |          each point.\n",
      " |      zdir : {'x', 'y', 'z'}, default: 'z'\n",
      " |          When plotting 2D data, the direction to use as z ('x', 'y' or 'z').\n",
      " |      **kwargs\n",
      " |          Other arguments are forwarded to `matplotlib.axes.Axes.plot`.\n",
      " |  \n",
      " |  plot3D = plot(self, xs, ys, *args, zdir='z', **kwargs)\n",
      " |  \n",
      " |  plot_surface(self, X, Y, Z, *args, norm=None, vmin=None, vmax=None, lightsource=None, **kwargs)\n",
      " |      Create a surface plot.\n",
      " |      \n",
      " |      By default it will be colored in shades of a solid color, but it also\n",
      " |      supports color mapping by supplying the *cmap* argument.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |         The *rcount* and *ccount* kwargs, which both default to 50,\n",
      " |         determine the maximum number of samples used in each direction.  If\n",
      " |         the input data is larger, it will be downsampled (by slicing) to\n",
      " |         these numbers of points.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |         To maximize rendering speed consider setting *rstride* and *cstride*\n",
      " |         to divisors of the number of rows minus 1 and columns minus 1\n",
      " |         respectively. For example, given 51 rows rstride can be any of the\n",
      " |         divisors of 50.\n",
      " |      \n",
      " |         Similarly, a setting of *rstride* and *cstride* equal to 1 (or\n",
      " |         *rcount* and *ccount* equal the number of rows and columns) can use\n",
      " |         the optimized path.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X, Y, Z : 2d arrays\n",
      " |          Data values.\n",
      " |      \n",
      " |      rcount, ccount : int\n",
      " |          Maximum number of samples used in each direction.  If the input\n",
      " |          data is larger, it will be downsampled (by slicing) to these\n",
      " |          numbers of points.  Defaults to 50.\n",
      " |      \n",
      " |          .. versionadded:: 2.0\n",
      " |      \n",
      " |      rstride, cstride : int\n",
      " |          Downsampling stride in each direction.  These arguments are\n",
      " |          mutually exclusive with *rcount* and *ccount*.  If only one of\n",
      " |          *rstride* or *cstride* is set, the other defaults to 10.\n",
      " |      \n",
      " |          'classic' mode uses a default of ``rstride = cstride = 10`` instead\n",
      " |          of the new default of ``rcount = ccount = 50``.\n",
      " |      \n",
      " |      color : color-like\n",
      " |          Color of the surface patches.\n",
      " |      \n",
      " |      cmap : Colormap\n",
      " |          Colormap of the surface patches.\n",
      " |      \n",
      " |      facecolors : array-like of colors.\n",
      " |          Colors of each individual patch.\n",
      " |      \n",
      " |      norm : Normalize\n",
      " |          Normalization for the colormap.\n",
      " |      \n",
      " |      vmin, vmax : float\n",
      " |          Bounds for the normalization.\n",
      " |      \n",
      " |      shade : bool, default: True\n",
      " |          Whether to shade the facecolors.  Shading is always disabled when\n",
      " |          *cmap* is specified.\n",
      " |      \n",
      " |      lightsource : `~matplotlib.colors.LightSource`\n",
      " |          The lightsource to use when *shade* is True.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Other arguments are forwarded to `.Poly3DCollection`.\n",
      " |  \n",
      " |  plot_trisurf(self, *args, color=None, norm=None, vmin=None, vmax=None, lightsource=None, **kwargs)\n",
      " |      Plot a triangulated surface.\n",
      " |      \n",
      " |      The (optional) triangulation can be specified in one of two ways;\n",
      " |      either::\n",
      " |      \n",
      " |        plot_trisurf(triangulation, ...)\n",
      " |      \n",
      " |      where triangulation is a `~matplotlib.tri.Triangulation` object, or::\n",
      " |      \n",
      " |        plot_trisurf(X, Y, ...)\n",
      " |        plot_trisurf(X, Y, triangles, ...)\n",
      " |        plot_trisurf(X, Y, triangles=triangles, ...)\n",
      " |      \n",
      " |      in which case a Triangulation object will be created.  See\n",
      " |      `.Triangulation` for a explanation of these possibilities.\n",
      " |      \n",
      " |      The remaining arguments are::\n",
      " |      \n",
      " |        plot_trisurf(..., Z)\n",
      " |      \n",
      " |      where *Z* is the array of values to contour, one per point\n",
      " |      in the triangulation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X, Y, Z : array-like\n",
      " |          Data values as 1D arrays.\n",
      " |      color\n",
      " |          Color of the surface patches.\n",
      " |      cmap\n",
      " |          A colormap for the surface patches.\n",
      " |      norm : Normalize\n",
      " |          An instance of Normalize to map values to colors.\n",
      " |      vmin, vmax : float, default: None\n",
      " |          Minimum and maximum value to map.\n",
      " |      shade : bool, default: True\n",
      " |          Whether to shade the facecolors.  Shading is always disabled when\n",
      " |          *cmap* is specified.\n",
      " |      lightsource : `~matplotlib.colors.LightSource`\n",
      " |          The lightsource to use when *shade* is True.\n",
      " |      **kwargs\n",
      " |          All other arguments are passed on to\n",
      " |          :class:`~mpl_toolkits.mplot3d.art3d.Poly3DCollection`\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. plot:: gallery/mplot3d/trisurf3d.py\n",
      " |      .. plot:: gallery/mplot3d/trisurf3d_2.py\n",
      " |      \n",
      " |      .. versionadded:: 1.2.0\n",
      " |  \n",
      " |  plot_wireframe(self, X, Y, Z, *args, **kwargs)\n",
      " |      Plot a 3D wireframe.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |         The *rcount* and *ccount* kwargs, which both default to 50,\n",
      " |         determine the maximum number of samples used in each direction.  If\n",
      " |         the input data is larger, it will be downsampled (by slicing) to\n",
      " |         these numbers of points.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X, Y, Z : 2d arrays\n",
      " |          Data values.\n",
      " |      \n",
      " |      rcount, ccount : int\n",
      " |          Maximum number of samples used in each direction.  If the input\n",
      " |          data is larger, it will be downsampled (by slicing) to these\n",
      " |          numbers of points.  Setting a count to zero causes the data to be\n",
      " |          not sampled in the corresponding direction, producing a 3D line\n",
      " |          plot rather than a wireframe plot.  Defaults to 50.\n",
      " |      \n",
      " |          .. versionadded:: 2.0\n",
      " |      \n",
      " |      rstride, cstride : int\n",
      " |          Downsampling stride in each direction.  These arguments are\n",
      " |          mutually exclusive with *rcount* and *ccount*.  If only one of\n",
      " |          *rstride* or *cstride* is set, the other defaults to 1.  Setting a\n",
      " |          stride to zero causes the data to be not sampled in the\n",
      " |          corresponding direction, producing a 3D line plot rather than a\n",
      " |          wireframe plot.\n",
      " |      \n",
      " |          'classic' mode uses a default of ``rstride = cstride = 1`` instead\n",
      " |          of the new default of ``rcount = ccount = 50``.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Other arguments are forwarded to `.Line3DCollection`.\n",
      " |  \n",
      " |  quiver(self, *args, length=1, arrow_length_ratio=0.3, pivot='tail', normalize=False, **kwargs)\n",
      " |      ax.quiver(X, Y, Z, U, V, W, /, length=1, arrow_length_ratio=.3, pivot='tail', normalize=False, **kwargs)\n",
      " |      \n",
      " |      Plot a 3D field of arrows.\n",
      " |      \n",
      " |      The arguments could be array-like or scalars, so long as they\n",
      " |      they can be broadcast together. The arguments can also be\n",
      " |      masked arrays. If an element in any of argument is masked, then\n",
      " |      that corresponding quiver element will not be plotted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X, Y, Z : array-like\n",
      " |          The x, y and z coordinates of the arrow locations (default is\n",
      " |          tail of arrow; see *pivot* kwarg).\n",
      " |      \n",
      " |      U, V, W : array-like\n",
      " |          The x, y and z components of the arrow vectors.\n",
      " |      \n",
      " |      length : float, default: 1\n",
      " |          The length of each quiver.\n",
      " |      \n",
      " |      arrow_length_ratio : float, default: 0.3\n",
      " |          The ratio of the arrow head with respect to the quiver.\n",
      " |      \n",
      " |      pivot : {'tail', 'middle', 'tip'}, default: 'tail'\n",
      " |          The part of the arrow that is at the grid point; the arrow\n",
      " |          rotates about this point, hence the name *pivot*.\n",
      " |      \n",
      " |      normalize : bool, default: False\n",
      " |          Whether all arrows are normalized to have the same length, or keep\n",
      " |          the lengths defined by *u*, *v*, and *w*.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Any additional keyword arguments are delegated to\n",
      " |          :class:`~matplotlib.collections.LineCollection`\n",
      " |  \n",
      " |  quiver3D = quiver(self, *args, length=1, arrow_length_ratio=0.3, pivot='tail', normalize=False, **kwargs)\n",
      " |  \n",
      " |  scatter(self, xs, ys, zs=0, zdir='z', s=20, c=None, depthshade=True, *args, **kwargs)\n",
      " |      Create a scatter plot.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      xs, ys : array-like\n",
      " |           The data positions.\n",
      " |      zs : float or array-like, default: 0\n",
      " |          The z-positions. Either an array of the same length as *xs* and\n",
      " |          *ys* or a single value to place all points in the same plane.\n",
      " |      zdir : {'x', 'y', 'z', '-x', '-y', '-z'}, default: 'z'\n",
      " |          The axis direction for the *zs*. This is useful when plotting 2D\n",
      " |          data on a 3D Axes. The data must be passed as *xs*, *ys*. Setting\n",
      " |          *zdir* to 'y' then plots the data to the x-z-plane.\n",
      " |      \n",
      " |          See also :doc:`/gallery/mplot3d/2dcollections3d`.\n",
      " |      \n",
      " |      s : float or array-like, default: 20\n",
      " |          The marker size in points**2. Either an array of the same length\n",
      " |          as *xs* and *ys* or a single value to make all markers the same\n",
      " |          size.\n",
      " |      c : color, sequence, or sequence of colors, optional\n",
      " |          The marker color. Possible values:\n",
      " |      \n",
      " |          - A single color format string.\n",
      " |          - A sequence of colors of length n.\n",
      " |          - A sequence of n numbers to be mapped to colors using *cmap* and\n",
      " |            *norm*.\n",
      " |          - A 2-D array in which the rows are RGB or RGBA.\n",
      " |      \n",
      " |          For more details see the *c* argument of `~.axes.Axes.scatter`.\n",
      " |      depthshade : bool, default: True\n",
      " |          Whether to shade the scatter markers to give the appearance of\n",
      " |          depth. Each call to ``scatter()`` will perform its depthshading\n",
      " |          independently.\n",
      " |      **kwargs\n",
      " |          All other arguments are passed on to `~.axes.Axes.scatter`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      paths : `~matplotlib.collections.PathCollection`\n",
      " |  \n",
      " |  scatter3D = scatter(self, xs, ys, zs=0, zdir='z', s=20, c=None, depthshade=True, *args, **kwargs)\n",
      " |  \n",
      " |  set_anchor(self, anchor, share=False)\n",
      " |      Define the anchor location.\n",
      " |      \n",
      " |      The actual drawing area (active position) of the Axes may be smaller\n",
      " |      than the Bbox (original position) when a fixed aspect is required. The\n",
      " |      anchor defines where the drawing area will be located within the\n",
      " |      available space.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      anchor : 2-tuple of floats or {'C', 'SW', 'S', 'SE', ...}\n",
      " |          The anchor position may be either:\n",
      " |      \n",
      " |          - a sequence (*cx*, *cy*). *cx* and *cy* may range from 0\n",
      " |            to 1, where 0 is left or bottom and 1 is right or top.\n",
      " |      \n",
      " |          - a string using cardinal directions as abbreviation:\n",
      " |      \n",
      " |            - 'C' for centered\n",
      " |            - 'S' (south) for bottom-center\n",
      " |            - 'SW' (south west) for bottom-left\n",
      " |            - etc.\n",
      " |      \n",
      " |            Here is an overview of the possible positions:\n",
      " |      \n",
      " |            +------+------+------+\n",
      " |            | 'NW' | 'N'  | 'NE' |\n",
      " |            +------+------+------+\n",
      " |            | 'W'  | 'C'  | 'E'  |\n",
      " |            +------+------+------+\n",
      " |            | 'SW' | 'S'  | 'SE' |\n",
      " |            +------+------+------+\n",
      " |      \n",
      " |      share : bool, default: False\n",
      " |          If ``True``, apply the settings to all shared Axes.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      matplotlib.axes.Axes.set_aspect\n",
      " |          for a description of aspect handling.\n",
      " |  \n",
      " |  set_aspect(self, aspect, adjustable=None, anchor=None, share=False)\n",
      " |      Set the aspect ratios.\n",
      " |      \n",
      " |      Axes 3D does not current support any aspect but 'auto' which fills\n",
      " |      the axes with the data limits.\n",
      " |      \n",
      " |      To simulate having equal aspect in data space, set the ratio\n",
      " |      of your data limits to match the value of `~.get_box_aspect`.\n",
      " |      To control box aspect ratios use `~.Axes3D.set_box_aspect`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      aspect : {'auto'}\n",
      " |          Possible values:\n",
      " |      \n",
      " |          =========   ==================================================\n",
      " |          value       description\n",
      " |          =========   ==================================================\n",
      " |          'auto'      automatic; fill the position rectangle with data.\n",
      " |          =========   ==================================================\n",
      " |      \n",
      " |      adjustable : None\n",
      " |          Currently ignored by Axes3D\n",
      " |      \n",
      " |          If not *None*, this defines which parameter will be adjusted to\n",
      " |          meet the required aspect. See `.set_adjustable` for further\n",
      " |          details.\n",
      " |      \n",
      " |      anchor : None or str or 2-tuple of float, optional\n",
      " |          If not *None*, this defines where the Axes will be drawn if there\n",
      " |          is extra space due to aspect constraints. The most common way to\n",
      " |          to specify the anchor are abbreviations of cardinal directions:\n",
      " |      \n",
      " |          =====   =====================\n",
      " |          value   description\n",
      " |          =====   =====================\n",
      " |          'C'     centered\n",
      " |          'SW'    lower left corner\n",
      " |          'S'     middle of bottom edge\n",
      " |          'SE'    lower right corner\n",
      " |          etc.\n",
      " |          =====   =====================\n",
      " |      \n",
      " |          See `.set_anchor` for further details.\n",
      " |      \n",
      " |      share : bool, default: False\n",
      " |          If ``True``, apply the settings to all shared Axes.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      mpl_toolkits.mplot3d.axes3d.Axes3D.set_box_aspect\n",
      " |  \n",
      " |  set_autoscale_on(self, b)\n",
      " |      Set whether autoscaling is applied on plot commands\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |          This function was added, but not tested. Please report any bugs.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      b : bool\n",
      " |  \n",
      " |  set_autoscalez_on(self, b)\n",
      " |      Set whether autoscaling for the z-axis is applied on plot commands\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      b : bool\n",
      " |  \n",
      " |  set_axis_off(self)\n",
      " |      Turn the x- and y-axis off.\n",
      " |      \n",
      " |      This affects the axis lines, ticks, ticklabels, grid and axis labels.\n",
      " |  \n",
      " |  set_axis_on(self)\n",
      " |      Turn the x- and y-axis on.\n",
      " |      \n",
      " |      This affects the axis lines, ticks, ticklabels, grid and axis labels.\n",
      " |  \n",
      " |  set_box_aspect(self, aspect, *, zoom=1)\n",
      " |      Set the axes box aspect.\n",
      " |      \n",
      " |      The box aspect is the ratio of height to width in display\n",
      " |      units for each face of the box when viewed perpendicular to\n",
      " |      that face.  This is not to be confused with the data aspect\n",
      " |      (which for Axes3D is always 'auto').  The default ratios are\n",
      " |      4:4:3 (x:y:z).\n",
      " |      \n",
      " |      To simulate having equal aspect in data space, set the box\n",
      " |      aspect to match your data range in each dimension.\n",
      " |      \n",
      " |      *zoom* controls the overall size of the Axes3D in the figure.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      aspect : 3-tuple of floats or None\n",
      " |          Changes the physical dimensions of the Axes3D, such that the ratio\n",
      " |          of the axis lengths in display units is x:y:z.\n",
      " |      \n",
      " |          If None, defaults to 4:4:3\n",
      " |      \n",
      " |      zoom : float\n",
      " |          Control overall size of the Axes3D in the figure.\n",
      " |  \n",
      " |  set_frame_on(self, b)\n",
      " |      Set whether the 3D axes panels are drawn.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      b : bool\n",
      " |  \n",
      " |  set_proj_type(self, proj_type)\n",
      " |      Set the projection type.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      proj_type : {'persp', 'ortho'}\n",
      " |  \n",
      " |  set_title(self, label, fontdict=None, loc='center', **kwargs)\n",
      " |      Set a title for the axes.\n",
      " |      \n",
      " |      Set one of the three available axes titles. The available titles\n",
      " |      are positioned above the axes in the center, flush with the left\n",
      " |      edge, and flush with the right edge.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      label : str\n",
      " |          Text to use for the title\n",
      " |      \n",
      " |      fontdict : dict\n",
      " |          A dictionary controlling the appearance of the title text,\n",
      " |          the default *fontdict* is::\n",
      " |      \n",
      " |             {'fontsize': rcParams['axes.titlesize'],\n",
      " |              'fontweight': rcParams['axes.titleweight'],\n",
      " |              'color': rcParams['axes.titlecolor'],\n",
      " |              'verticalalignment': 'baseline',\n",
      " |              'horizontalalignment': loc}\n",
      " |      \n",
      " |      loc : {'center', 'left', 'right'}, default: :rc:`axes.titlelocation`\n",
      " |          Which title to set.\n",
      " |      \n",
      " |      y : float, default: :rc:`axes.titley`\n",
      " |          Vertical axes loation for the title (1.0 is the top).  If\n",
      " |          None (the default), y is determined automatically to avoid\n",
      " |          decorators on the axes.\n",
      " |      \n",
      " |      pad : float, default: :rc:`axes.titlepad`\n",
      " |          The offset of the title from the top of the axes, in points.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `.Text`\n",
      " |          The matplotlib text instance representing the title\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs : `.Text` properties\n",
      " |          Other keyword arguments are text properties, see `.Text` for a list\n",
      " |          of valid text properties.\n",
      " |  \n",
      " |  set_top_view(self)\n",
      " |  \n",
      " |  set_xlim(self, left=None, right=None, emit=True, auto=False, *, xmin=None, xmax=None)\n",
      " |      Alias for `set_xlim3d`.\n",
      " |  \n",
      " |  set_xlim3d(self, left=None, right=None, emit=True, auto=False, *, xmin=None, xmax=None)\n",
      " |      Set 3D x limits.\n",
      " |      \n",
      " |      See :meth:`matplotlib.axes.Axes.set_xlim` for full documentation.\n",
      " |  \n",
      " |  set_xscale(self, value, **kwargs)\n",
      " |      Set the x-axis scale.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value : {\"linear\"}\n",
      " |          The axis scale type to apply.  3D axes currently only support\n",
      " |          linear scales; other scales yield nonsensical results.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Keyword arguments are nominally forwarded to the scale class, but\n",
      " |          none of them is applicable for linear scales.\n",
      " |  \n",
      " |  set_ylim(self, bottom=None, top=None, emit=True, auto=False, *, ymin=None, ymax=None)\n",
      " |      Alias for `set_ylim3d`.\n",
      " |  \n",
      " |  set_ylim3d(self, bottom=None, top=None, emit=True, auto=False, *, ymin=None, ymax=None)\n",
      " |      Set 3D y limits.\n",
      " |      \n",
      " |      See :meth:`matplotlib.axes.Axes.set_ylim` for full documentation.\n",
      " |  \n",
      " |  set_yscale(self, value, **kwargs)\n",
      " |      Set the y-axis scale.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value : {\"linear\"}\n",
      " |          The axis scale type to apply.  3D axes currently only support\n",
      " |          linear scales; other scales yield nonsensical results.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Keyword arguments are nominally forwarded to the scale class, but\n",
      " |          none of them is applicable for linear scales.\n",
      " |  \n",
      " |  set_zbound(self, lower=None, upper=None)\n",
      " |      Set the lower and upper numerical bounds of the z-axis.\n",
      " |      \n",
      " |      This method will honor axes inversion regardless of parameter order.\n",
      " |      It will not change the autoscaling setting (`.get_autoscalez_on()`).\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |  \n",
      " |  set_zlabel(self, zlabel, fontdict=None, labelpad=None, **kwargs)\n",
      " |      Set zlabel.  See doc for `.set_ylabel` for description.\n",
      " |  \n",
      " |  set_zlim(self, bottom=None, top=None, emit=True, auto=False, *, zmin=None, zmax=None)\n",
      " |      Alias for `set_zlim3d`.\n",
      " |  \n",
      " |  set_zlim3d(self, bottom=None, top=None, emit=True, auto=False, *, zmin=None, zmax=None)\n",
      " |      Set 3D z limits.\n",
      " |      \n",
      " |      See :meth:`matplotlib.axes.Axes.set_ylim` for full documentation\n",
      " |  \n",
      " |  set_zmargin(self, m)\n",
      " |      Set padding of Z data limits prior to autoscaling.\n",
      " |      \n",
      " |      *m* times the data interval will be added to each\n",
      " |      end of that interval before it is used in autoscaling.\n",
      " |      \n",
      " |      accepts: float in range 0 to 1\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |  \n",
      " |  set_zscale(self, value, **kwargs)\n",
      " |      Set the z-axis scale.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      value : {\"linear\"}\n",
      " |          The axis scale type to apply.  3D axes currently only support\n",
      " |          linear scales; other scales yield nonsensical results.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Keyword arguments are nominally forwarded to the scale class, but\n",
      " |          none of them is applicable for linear scales.\n",
      " |  \n",
      " |  set_zticklabels(self, labels, *, fontdict=None, minor=False, **kwargs)\n",
      " |      Set the zaxis' labels with list of string labels.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          This method should only be used after fixing the tick positions\n",
      " |          using `.Axes3D.set_zticks`. Otherwise, the labels may end up in\n",
      " |          unexpected positions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      labels : list of str\n",
      " |          The label texts.\n",
      " |      \n",
      " |      fontdict : dict, optional\n",
      " |          A dictionary controlling the appearance of the ticklabels.\n",
      " |          The default *fontdict* is::\n",
      " |      \n",
      " |             {'fontsize': rcParams['axes.titlesize'],\n",
      " |              'fontweight': rcParams['axes.titleweight'],\n",
      " |              'verticalalignment': 'baseline',\n",
      " |              'horizontalalignment': loc}\n",
      " |      \n",
      " |      minor : bool, default: False\n",
      " |          Whether to set the minor ticklabels rather than the major ones.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of `~.Text`\n",
      " |          The labels.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs : `~.text.Text` properties.\n",
      " |  \n",
      " |  set_zticks(self, ticks, *, minor=False)\n",
      " |      Set the zaxis' tick locations.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ticks : list of floats\n",
      " |          List of tick locations.\n",
      " |      minor : bool, default: False\n",
      " |          If ``False``, set the major ticks; if ``True``, the minor ticks.\n",
      " |  \n",
      " |  text(self, x, y, z, s, zdir=None, **kwargs)\n",
      " |      Add text to the plot. kwargs will be passed on to Axes.text,\n",
      " |      except for the *zdir* keyword, which sets the direction to be\n",
      " |      used as the z direction.\n",
      " |  \n",
      " |  text2D = text(self, x, y, s, fontdict=None, **kwargs)\n",
      " |      Add text to the axes.\n",
      " |      \n",
      " |      Add the text *s* to the axes at location *x*, *y* in data coordinates.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x, y : float\n",
      " |          The position to place the text. By default, this is in data\n",
      " |          coordinates. The coordinate system can be changed using the\n",
      " |          *transform* parameter.\n",
      " |      \n",
      " |      s : str\n",
      " |          The text.\n",
      " |      \n",
      " |      fontdict : dict, default: None\n",
      " |          A dictionary to override the default text properties. If fontdict\n",
      " |          is None, the defaults are determined by `.rcParams`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `.Text`\n",
      " |          The created `.Text` instance.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs : `~matplotlib.text.Text` properties.\n",
      " |          Other miscellaneous text parameters.\n",
      " |      \n",
      " |          Properties:\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          animated: bool\n",
      " |          backgroundcolor: color\n",
      " |          bbox: dict with properties for `.patches.FancyBboxPatch`\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          color or c: color\n",
      " |          contains: unknown\n",
      " |          figure: `.Figure`\n",
      " |          fontfamily or family: {FONTNAME, 'serif', 'sans-serif', 'cursive', 'fantasy', 'monospace'}\n",
      " |          fontproperties or font or font_properties: `.font_manager.FontProperties` or `str` or `pathlib.Path`\n",
      " |          fontsize or size: float or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n",
      " |          fontstretch or stretch: {a numeric value in range 0-1000, 'ultra-condensed', 'extra-condensed', 'condensed', 'semi-condensed', 'normal', 'semi-expanded', 'expanded', 'extra-expanded', 'ultra-expanded'}\n",
      " |          fontstyle or style: {'normal', 'italic', 'oblique'}\n",
      " |          fontvariant or variant: {'normal', 'small-caps'}\n",
      " |          fontweight or weight: {a numeric value in range 0-1000, 'ultralight', 'light', 'normal', 'regular', 'book', 'medium', 'roman', 'semibold', 'demibold', 'demi', 'bold', 'heavy', 'extra bold', 'black'}\n",
      " |          gid: str\n",
      " |          horizontalalignment or ha: {'center', 'right', 'left'}\n",
      " |          in_layout: bool\n",
      " |          label: object\n",
      " |          linespacing: float (multiple of font size)\n",
      " |          multialignment or ma: {'left', 'right', 'center'}\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: None or bool or callable\n",
      " |          position: (float, float)\n",
      " |          rasterized: bool or None\n",
      " |          rotation: float or {'vertical', 'horizontal'}\n",
      " |          rotation_mode: {None, 'default', 'anchor'}\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          text: object\n",
      " |          transform: `.Transform`\n",
      " |          url: str\n",
      " |          usetex: bool or None\n",
      " |          verticalalignment or va: {'center', 'top', 'bottom', 'baseline', 'center_baseline'}\n",
      " |          visible: bool\n",
      " |          wrap: bool\n",
      " |          x: float\n",
      " |          y: float\n",
      " |          zorder: float\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Individual keyword arguments can be used to override any given\n",
      " |      parameter::\n",
      " |      \n",
      " |          >>> text(x, y, s, fontsize=12)\n",
      " |      \n",
      " |      The default transform specifies that text is in data coords,\n",
      " |      alternatively, you can specify text in axis coords ((0, 0) is\n",
      " |      lower-left and (1, 1) is upper-right).  The example below places\n",
      " |      text in the center of the axes::\n",
      " |      \n",
      " |          >>> text(0.5, 0.5, 'matplotlib', horizontalalignment='center',\n",
      " |          ...      verticalalignment='center', transform=ax.transAxes)\n",
      " |      \n",
      " |      You can put a rectangular box around the text instance (e.g., to\n",
      " |      set a background color) by using the keyword *bbox*.  *bbox* is\n",
      " |      a dictionary of `~matplotlib.patches.Rectangle`\n",
      " |      properties.  For example::\n",
      " |      \n",
      " |          >>> text(x, y, s, bbox=dict(facecolor='red', alpha=0.5))\n",
      " |  \n",
      " |  text3D = text(self, x, y, z, s, zdir=None, **kwargs)\n",
      " |  \n",
      " |  tick_params(self, axis='both', **kwargs)\n",
      " |      Convenience method for changing the appearance of ticks and\n",
      " |      tick labels.\n",
      " |      \n",
      " |      See :meth:`matplotlib.axes.Axes.tick_params` for more complete\n",
      " |      documentation.\n",
      " |      \n",
      " |      The only difference is that setting *axis* to 'both' will\n",
      " |      mean that the settings are applied to all three axes. Also,\n",
      " |      the *axis* parameter also accepts a value of 'z', which\n",
      " |      would mean to apply to only the z-axis.\n",
      " |      \n",
      " |      Also, because of how Axes3D objects are drawn very differently\n",
      " |      from regular 2D axes, some of these settings may have\n",
      " |      ambiguous meaning.  For simplicity, the 'z' axis will\n",
      " |      accept settings as if it was like the 'y' axis.\n",
      " |      \n",
      " |      .. note::\n",
      " |         Axes3D currently ignores some of these settings.\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |  \n",
      " |  tricontour(self, *args, extend3d=False, stride=5, zdir='z', offset=None, **kwargs)\n",
      " |      Create a 3D contour plot.\n",
      " |      \n",
      " |      .. versionchanged:: 1.3.0\n",
      " |          Added support for custom triangulations\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method currently produces incorrect output due to a\n",
      " |          longstanding bug in 3D PolyCollection rendering.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X, Y, Z : array-like\n",
      " |          Input data.\n",
      " |      extend3d : bool, default: False\n",
      " |          Whether to extend contour in 3D.\n",
      " |      stride : int\n",
      " |          Step size for extending contour.\n",
      " |      zdir : {'x', 'y', 'z'}, default: 'z'\n",
      " |          The direction to use.\n",
      " |      offset : float, optional\n",
      " |          If specified, plot a projection of the contour lines at this\n",
      " |          position in a plane normal to zdir.\n",
      " |      *args, **kwargs\n",
      " |          Other arguments are forwarded to `matplotlib.axes.Axes.tricontour`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      matplotlib.tri.tricontour.TriContourSet\n",
      " |  \n",
      " |  tricontourf(self, *args, zdir='z', offset=None, **kwargs)\n",
      " |      Create a 3D filled contour plot.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method currently produces incorrect output due to a\n",
      " |          longstanding bug in 3D PolyCollection rendering.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X, Y, Z : array-like\n",
      " |          Input data.\n",
      " |      zdir : {'x', 'y', 'z'}, default: 'z'\n",
      " |          The direction to use.\n",
      " |      offset : float, optional\n",
      " |          If specified, plot a projection of the contour lines at this\n",
      " |          position in a plane normal to zdir.\n",
      " |      *args, **kwargs\n",
      " |          Other arguments are forwarded to\n",
      " |          `matplotlib.axes.Axes.tricontourf`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      matplotlib.tri.tricontour.TriContourSet\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. versionadded:: 1.1.0\n",
      " |          The *zdir* and *offset* parameters.\n",
      " |      .. versionchanged:: 1.3.0\n",
      " |          Added support for custom triangulations\n",
      " |  \n",
      " |  tunit_cube(self, vals=None, M=None)\n",
      " |  \n",
      " |  tunit_edges(self, vals=None, M=None)\n",
      " |  \n",
      " |  unit_cube(self, vals=None)\n",
      " |  \n",
      " |  update_datalim(self, xys, **kwargs)\n",
      " |      Extend the `~.Axes.dataLim` Bbox to include the given points.\n",
      " |      \n",
      " |      If no data is set currently, the Bbox will ignore its limits and set\n",
      " |      the bound to be the bounds of the xydata (*xys*). Otherwise, it will\n",
      " |      compute the bounds of the union of its current data and the data in\n",
      " |      *xys*.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      xys : 2D array-like\n",
      " |          The points to include in the data limits Bbox. This can be either\n",
      " |          a list of (x, y) tuples or a Nx2 array.\n",
      " |      \n",
      " |      updatex, updatey : bool, default: True\n",
      " |          Whether to update the x/y limits.\n",
      " |  \n",
      " |  view_init(self, elev=None, azim=None)\n",
      " |      Set the elevation and azimuth of the axes in degrees (not radians).\n",
      " |      \n",
      " |      This can be used to rotate the axes programmatically.\n",
      " |      \n",
      " |      'elev' stores the elevation angle in the z plane (in degrees).\n",
      " |      'azim' stores the azimuth angle in the (x, y) plane (in degrees).\n",
      " |      \n",
      " |      if 'elev' or 'azim' are None (default), then the initial value\n",
      " |      is used which was specified in the :class:`Axes3D` constructor.\n",
      " |  \n",
      " |  voxels(self, *args, facecolors=None, edgecolors=None, shade=True, lightsource=None, **kwargs)\n",
      " |      ax.voxels([x, y, z,] /, filled, facecolors=None, edgecolors=None, **kwargs)\n",
      " |      \n",
      " |      Plot a set of filled voxels\n",
      " |      \n",
      " |      All voxels are plotted as 1x1x1 cubes on the axis, with\n",
      " |      ``filled[0, 0, 0]`` placed with its lower corner at the origin.\n",
      " |      Occluded faces are not plotted.\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      filled : 3D np.array of bool\n",
      " |          A 3d array of values, with truthy values indicating which voxels\n",
      " |          to fill\n",
      " |      \n",
      " |      x, y, z : 3D np.array, optional\n",
      " |          The coordinates of the corners of the voxels. This should broadcast\n",
      " |          to a shape one larger in every dimension than the shape of\n",
      " |          *filled*.  These can be used to plot non-cubic voxels.\n",
      " |      \n",
      " |          If not specified, defaults to increasing integers along each axis,\n",
      " |          like those returned by :func:`~numpy.indices`.\n",
      " |          As indicated by the ``/`` in the function signature, these\n",
      " |          arguments can only be passed positionally.\n",
      " |      \n",
      " |      facecolors, edgecolors : array-like, optional\n",
      " |          The color to draw the faces and edges of the voxels. Can only be\n",
      " |          passed as keyword arguments.\n",
      " |          This parameter can be:\n",
      " |      \n",
      " |            - A single color value, to color all voxels the same color. This\n",
      " |              can be either a string, or a 1D rgb/rgba array\n",
      " |            - ``None``, the default, to use a single color for the faces, and\n",
      " |              the style default for the edges.\n",
      " |            - A 3D ndarray of color names, with each item the color for the\n",
      " |              corresponding voxel. The size must match the voxels.\n",
      " |            - A 4D ndarray of rgb/rgba data, with the components along the\n",
      " |              last axis.\n",
      " |      \n",
      " |      shade : bool, default: True\n",
      " |          Whether to shade the facecolors.  Shading is always disabled when\n",
      " |          *cmap* is specified.\n",
      " |      \n",
      " |          .. versionadded:: 3.1\n",
      " |      \n",
      " |      lightsource : `~matplotlib.colors.LightSource`\n",
      " |          The lightsource to use when *shade* is True.\n",
      " |      \n",
      " |          .. versionadded:: 3.1\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Additional keyword arguments to pass onto\n",
      " |          `~mpl_toolkits.mplot3d.art3d.Poly3DCollection`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      faces : dict\n",
      " |          A dictionary indexed by coordinate, where ``faces[i, j, k]`` is a\n",
      " |          `.Poly3DCollection` of the faces drawn for the voxel\n",
      " |          ``filled[i, j, k]``. If no faces were drawn for a given voxel,\n",
      " |          either because it was not asked to be drawn, or it is fully\n",
      " |          occluded, then ``(i, j, k) not in faces``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. plot:: gallery/mplot3d/voxels.py\n",
      " |      .. plot:: gallery/mplot3d/voxels_rgb.py\n",
      " |      .. plot:: gallery/mplot3d/voxels_torus.py\n",
      " |      .. plot:: gallery/mplot3d/voxels_numpy_logo.py\n",
      " |  \n",
      " |  zaxis_date(self, tz=None)\n",
      " |      Sets up axis ticks and labels to treat data along the zaxis as dates.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tz : str or `datetime.tzinfo`, default: :rc:`timezone`\n",
      " |          The timezone used to create date labels.\n",
      " |  \n",
      " |  zaxis_inverted(self)\n",
      " |      Returns True if the z-axis is inverted.\n",
      " |      \n",
      " |      .. versionadded:: 1.1.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  w_xaxis\n",
      " |  \n",
      " |  w_yaxis\n",
      " |  \n",
      " |  w_zaxis\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  name = '3d'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from matplotlib.axes._axes.Axes:\n",
      " |  \n",
      " |  acorr(self, x, *, data=None, **kwargs)\n",
      " |      Plot the autocorrelation of *x*.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : array-like\n",
      " |      \n",
      " |      detrend : callable, default: `.mlab.detrend_none` (no detrending)\n",
      " |          A detrending function applied to *x*.  It must have the\n",
      " |          signature ::\n",
      " |      \n",
      " |              detrend(x: np.ndarray) -> np.ndarray\n",
      " |      \n",
      " |      normed : bool, default: True\n",
      " |          If ``True``, input vectors are normalised to unit length.\n",
      " |      \n",
      " |      usevlines : bool, default: True\n",
      " |          Determines the plot style.\n",
      " |      \n",
      " |          If ``True``, vertical lines are plotted from 0 to the acorr value\n",
      " |          using `.Axes.vlines`. Additionally, a horizontal line is plotted\n",
      " |          at y=0 using `.Axes.axhline`.\n",
      " |      \n",
      " |          If ``False``, markers are plotted at the acorr values using\n",
      " |          `.Axes.plot`.\n",
      " |      \n",
      " |      maxlags : int, default: 10\n",
      " |          Number of lags to show. If ``None``, will return all\n",
      " |          ``2 * len(x) - 1`` lags.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      lags : array (length ``2*maxlags+1``)\n",
      " |          The lag vector.\n",
      " |      c : array  (length ``2*maxlags+1``)\n",
      " |          The auto correlation vector.\n",
      " |      line : `.LineCollection` or `.Line2D`\n",
      " |          `.Artist` added to the axes of the correlation:\n",
      " |      \n",
      " |          - `.LineCollection` if *usevlines* is True.\n",
      " |          - `.Line2D` if *usevlines* is False.\n",
      " |      b : `.Line2D` or None\n",
      " |          Horizontal line at 0 if *usevlines* is True\n",
      " |          None *usevlines* is False.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      linestyle : `.Line2D` property, optional\n",
      " |          The linestyle for plotting the data points.\n",
      " |          Only used if *usevlines* is ``False``.\n",
      " |      \n",
      " |      marker : str, default: 'o'\n",
      " |          The marker for plotting the data points.\n",
      " |          Only used if *usevlines* is ``False``.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Additional parameters are passed to `.Axes.vlines` and\n",
      " |          `.Axes.axhline` if *usevlines* is ``True``; otherwise they are\n",
      " |          passed to `.Axes.plot`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The cross correlation is performed with `numpy.correlate` with\n",
      " |      ``mode = \"full\"``.\n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          the following arguments can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception):\n",
      " |          *x*.\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  angle_spectrum(self, x, Fs=None, Fc=None, window=None, pad_to=None, sides=None, *, data=None, **kwargs)\n",
      " |      Plot the angle spectrum.\n",
      " |      \n",
      " |      Compute the angle spectrum (wrapped phase spectrum) of *x*.\n",
      " |      Data is padded to a length of *pad_to* and the windowing function\n",
      " |      *window* is applied to the signal.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : 1-D array or sequence\n",
      " |          Array or sequence containing the data.\n",
      " |      \n",
      " |      Fs : float, default: 2\n",
      " |          The sampling frequency (samples per time unit).  It is used to calculate\n",
      " |          the Fourier frequencies, *freqs*, in cycles per time unit.\n",
      " |      \n",
      " |      window : callable or ndarray, default: `.window_hanning`\n",
      " |          A function or a vector of length *NFFT*.  To create window vectors see\n",
      " |          `.window_hanning`, `.window_none`, `numpy.blackman`, `numpy.hamming`,\n",
      " |          `numpy.bartlett`, `scipy.signal`, `scipy.signal.get_window`, etc.  If a\n",
      " |          function is passed as the argument, it must take a data segment as an\n",
      " |          argument and return the windowed version of the segment.\n",
      " |      \n",
      " |      sides : {'default', 'onesided', 'twosided'}, optional\n",
      " |          Which sides of the spectrum to return. 'default' is one-sided for real\n",
      " |          data and two-sided for complex data. 'onesided' forces the return of a\n",
      " |          one-sided spectrum, while 'twosided' forces two-sided.\n",
      " |      \n",
      " |      pad_to : int, optional\n",
      " |          The number of points to which the data segment is padded when performing\n",
      " |          the FFT.  While not increasing the actual resolution of the spectrum (the\n",
      " |          minimum distance between resolvable peaks), this can give more points in\n",
      " |          the plot, allowing for more detail. This corresponds to the *n* parameter\n",
      " |          in the call to fft().  The default is None, which sets *pad_to* equal to\n",
      " |          the length of the input signal (i.e. no padding).\n",
      " |      \n",
      " |      Fc : int, default: 0\n",
      " |          The center frequency of *x*, which offsets the x extents of the\n",
      " |          plot to reflect the frequency range used when a signal is acquired\n",
      " |          and then filtered and downsampled to baseband.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      spectrum : 1-D array\n",
      " |          The values for the angle spectrum in radians (real valued).\n",
      " |      \n",
      " |      freqs : 1-D array\n",
      " |          The frequencies corresponding to the elements in *spectrum*.\n",
      " |      \n",
      " |      line : `~matplotlib.lines.Line2D`\n",
      " |          The line created by this function.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs\n",
      " |          Keyword arguments control the `.Line2D` properties:\n",
      " |      \n",
      " |          Properties:\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          animated: bool\n",
      " |          antialiased or aa: bool\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          color or c: color\n",
      " |          contains: unknown\n",
      " |          dash_capstyle: {'butt', 'round', 'projecting'}\n",
      " |          dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      " |          data: (2, N) array or two 1D arrays\n",
      " |          drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      " |          figure: `.Figure`\n",
      " |          fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      " |          gid: str\n",
      " |          in_layout: bool\n",
      " |          label: object\n",
      " |          linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      " |          linewidth or lw: float\n",
      " |          marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      " |          markeredgecolor or mec: color\n",
      " |          markeredgewidth or mew: float\n",
      " |          markerfacecolor or mfc: color\n",
      " |          markerfacecoloralt or mfcalt: color\n",
      " |          markersize or ms: float\n",
      " |          markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: unknown\n",
      " |          pickradius: float\n",
      " |          rasterized: bool or None\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          solid_capstyle: {'butt', 'round', 'projecting'}\n",
      " |          solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          transform: `matplotlib.transforms.Transform`\n",
      " |          url: str\n",
      " |          visible: bool\n",
      " |          xdata: 1D array\n",
      " |          ydata: 1D array\n",
      " |          zorder: float\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      magnitude_spectrum\n",
      " |          Plots the magnitudes of the corresponding frequencies.\n",
      " |      phase_spectrum\n",
      " |          Plots the unwrapped version of this function.\n",
      " |      specgram\n",
      " |          Can plot the angle spectrum of segments within the signal in a\n",
      " |          colormap.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      \n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          the following arguments can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception):\n",
      " |          *x*.\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  annotate(self, text, xy, *args, **kwargs)\n",
      " |      Annotate the point *xy* with text *text*.\n",
      " |      \n",
      " |      In the simplest form, the text is placed at *xy*.\n",
      " |      \n",
      " |      Optionally, the text can be displayed in another position *xytext*.\n",
      " |      An arrow pointing from the text to the annotated point *xy* can then\n",
      " |      be added by defining *arrowprops*.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      text : str\n",
      " |          The text of the annotation.  *s* is a deprecated synonym for this\n",
      " |          parameter.\n",
      " |      \n",
      " |      xy : (float, float)\n",
      " |          The point *(x, y)* to annotate. The coordinate system is determined\n",
      " |          by *xycoords*.\n",
      " |      \n",
      " |      xytext : (float, float), default: *xy*\n",
      " |          The position *(x, y)* to place the text at. The coordinate system\n",
      " |          is determined by *textcoords*.\n",
      " |      \n",
      " |      xycoords : str or `.Artist` or `.Transform` or callable or (float, float), default: 'data'\n",
      " |      \n",
      " |          The coordinate system that *xy* is given in. The following types\n",
      " |          of values are supported:\n",
      " |      \n",
      " |          - One of the following strings:\n",
      " |      \n",
      " |            =================   =============================================\n",
      " |            Value               Description\n",
      " |            =================   =============================================\n",
      " |            'figure points'     Points from the lower left of the figure\n",
      " |            'figure pixels'     Pixels from the lower left of the figure\n",
      " |            'figure fraction'   Fraction of figure from lower left\n",
      " |            'axes points'       Points from lower left corner of axes\n",
      " |            'axes pixels'       Pixels from lower left corner of axes\n",
      " |            'axes fraction'     Fraction of axes from lower left\n",
      " |            'data'              Use the coordinate system of the object being\n",
      " |                                annotated (default)\n",
      " |            'polar'             *(theta, r)* if not native 'data' coordinates\n",
      " |            =================   =============================================\n",
      " |      \n",
      " |          - An `.Artist`: *xy* is interpreted as a fraction of the artist's\n",
      " |            `~matplotlib.transforms.Bbox`. E.g. *(0, 0)* would be the lower\n",
      " |            left corner of the bounding box and *(0.5, 1)* would be the\n",
      " |            center top of the bounding box.\n",
      " |      \n",
      " |          - A `.Transform` to transform *xy* to screen coordinates.\n",
      " |      \n",
      " |          - A function with one of the following signatures::\n",
      " |      \n",
      " |              def transform(renderer) -> Bbox\n",
      " |              def transform(renderer) -> Transform\n",
      " |      \n",
      " |            where *renderer* is a `.RendererBase` subclass.\n",
      " |      \n",
      " |            The result of the function is interpreted like the `.Artist` and\n",
      " |            `.Transform` cases above.\n",
      " |      \n",
      " |          - A tuple *(xcoords, ycoords)* specifying separate coordinate\n",
      " |            systems for *x* and *y*. *xcoords* and *ycoords* must each be\n",
      " |            of one of the above described types.\n",
      " |      \n",
      " |          See :ref:`plotting-guide-annotation` for more details.\n",
      " |      \n",
      " |      textcoords : str or `.Artist` or `.Transform` or callable or (float, float), default: value of *xycoords*\n",
      " |          The coordinate system that *xytext* is given in.\n",
      " |      \n",
      " |          All *xycoords* values are valid as well as the following\n",
      " |          strings:\n",
      " |      \n",
      " |          =================   =========================================\n",
      " |          Value               Description\n",
      " |          =================   =========================================\n",
      " |          'offset points'     Offset (in points) from the *xy* value\n",
      " |          'offset pixels'     Offset (in pixels) from the *xy* value\n",
      " |          =================   =========================================\n",
      " |      \n",
      " |      arrowprops : dict, optional\n",
      " |          The properties used to draw a `.FancyArrowPatch` arrow between the\n",
      " |          positions *xy* and *xytext*.\n",
      " |      \n",
      " |          If *arrowprops* does not contain the key 'arrowstyle' the\n",
      " |          allowed keys are:\n",
      " |      \n",
      " |          ==========   ======================================================\n",
      " |          Key          Description\n",
      " |          ==========   ======================================================\n",
      " |          width        The width of the arrow in points\n",
      " |          headwidth    The width of the base of the arrow head in points\n",
      " |          headlength   The length of the arrow head in points\n",
      " |          shrink       Fraction of total length to shrink from both ends\n",
      " |          ?            Any key to :class:`matplotlib.patches.FancyArrowPatch`\n",
      " |          ==========   ======================================================\n",
      " |      \n",
      " |          If *arrowprops* contains the key 'arrowstyle' the\n",
      " |          above keys are forbidden.  The allowed values of\n",
      " |          ``'arrowstyle'`` are:\n",
      " |      \n",
      " |          ============   =============================================\n",
      " |          Name           Attrs\n",
      " |          ============   =============================================\n",
      " |          ``'-'``        None\n",
      " |          ``'->'``       head_length=0.4,head_width=0.2\n",
      " |          ``'-['``       widthB=1.0,lengthB=0.2,angleB=None\n",
      " |          ``'|-|'``      widthA=1.0,widthB=1.0\n",
      " |          ``'-|>'``      head_length=0.4,head_width=0.2\n",
      " |          ``'<-'``       head_length=0.4,head_width=0.2\n",
      " |          ``'<->'``      head_length=0.4,head_width=0.2\n",
      " |          ``'<|-'``      head_length=0.4,head_width=0.2\n",
      " |          ``'<|-|>'``    head_length=0.4,head_width=0.2\n",
      " |          ``'fancy'``    head_length=0.4,head_width=0.4,tail_width=0.4\n",
      " |          ``'simple'``   head_length=0.5,head_width=0.5,tail_width=0.2\n",
      " |          ``'wedge'``    tail_width=0.3,shrink_factor=0.5\n",
      " |          ============   =============================================\n",
      " |      \n",
      " |          Valid keys for `~matplotlib.patches.FancyArrowPatch` are:\n",
      " |      \n",
      " |          ===============  ==================================================\n",
      " |          Key              Description\n",
      " |          ===============  ==================================================\n",
      " |          arrowstyle       the arrow style\n",
      " |          connectionstyle  the connection style\n",
      " |          relpos           default is (0.5, 0.5)\n",
      " |          patchA           default is bounding box of the text\n",
      " |          patchB           default is None\n",
      " |          shrinkA          default is 2 points\n",
      " |          shrinkB          default is 2 points\n",
      " |          mutation_scale   default is text size (in points)\n",
      " |          mutation_aspect  default is 1.\n",
      " |          ?                any key for :class:`matplotlib.patches.PathPatch`\n",
      " |          ===============  ==================================================\n",
      " |      \n",
      " |          Defaults to None, i.e. no arrow is drawn.\n",
      " |      \n",
      " |      annotation_clip : bool or None, default: None\n",
      " |          Whether to draw the annotation when the annotation point *xy* is\n",
      " |          outside the axes area.\n",
      " |      \n",
      " |          - If *True*, the annotation will only be drawn when *xy* is\n",
      " |            within the axes.\n",
      " |          - If *False*, the annotation will always be drawn.\n",
      " |          - If *None*, the annotation will only be drawn when *xy* is\n",
      " |            within the axes and *xycoords* is 'data'.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Additional kwargs are passed to `~matplotlib.text.Text`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `.Annotation`\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :ref:`plotting-guide-annotation`\n",
      " |  \n",
      " |  arrow(self, x, y, dx, dy, **kwargs)\n",
      " |      Add an arrow to the axes.\n",
      " |      \n",
      " |      This draws an arrow from ``(x, y)`` to ``(x+dx, y+dy)``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x, y : float\n",
      " |          The x and y coordinates of the arrow base.\n",
      " |      \n",
      " |      dx, dy : float\n",
      " |          The length of the arrow along x and y direction.\n",
      " |      \n",
      " |      width: float, default: 0.001\n",
      " |          Width of full arrow tail.\n",
      " |      \n",
      " |      length_includes_head: bool, default: False\n",
      " |          True if head is to be counted in calculating the length.\n",
      " |      \n",
      " |      head_width: float or None, default: 3*width\n",
      " |          Total width of the full arrow head.\n",
      " |      \n",
      " |      head_length: float or None, default: 1.5*head_width\n",
      " |          Length of arrow head.\n",
      " |      \n",
      " |      shape: ['full', 'left', 'right'], default: 'full'\n",
      " |          Draw the left-half, right-half, or full arrow.\n",
      " |      \n",
      " |      overhang: float, default: 0\n",
      " |          Fraction that the arrow is swept back (0 overhang means\n",
      " |          triangular shape). Can be negative or greater than one.\n",
      " |      \n",
      " |      head_starts_at_zero: bool, default: False\n",
      " |          If True, the head starts being drawn at coordinate 0\n",
      " |          instead of ending at coordinate 0.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          `.Patch` properties:\n",
      " |      \n",
      " |          Properties:\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          animated: bool\n",
      " |          antialiased or aa: unknown\n",
      " |          capstyle: {'butt', 'round', 'projecting'}\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          color: color\n",
      " |          contains: unknown\n",
      " |          edgecolor or ec: color or None or 'auto'\n",
      " |          facecolor or fc: color or None\n",
      " |          figure: `.Figure`\n",
      " |          fill: bool\n",
      " |          gid: str\n",
      " |          hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n",
      " |          in_layout: bool\n",
      " |          joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          label: object\n",
      " |          linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      " |          linewidth or lw: float or None\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: None or bool or callable\n",
      " |          rasterized: bool or None\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          transform: `.Transform`\n",
      " |          url: str\n",
      " |          visible: bool\n",
      " |          zorder: float\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `.FancyArrow`\n",
      " |          The created `.FancyArrow` object.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The resulting arrow is affected by the axes aspect ratio and limits.\n",
      " |      This may produce an arrow whose head is not square with its stem. To\n",
      " |      create an arrow whose head is square with its stem,\n",
      " |      use :meth:`annotate` for example:\n",
      " |      \n",
      " |      >>> ax.annotate(\"\", xy=(0.5, 0.5), xytext=(0, 0),\n",
      " |      ...             arrowprops=dict(arrowstyle=\"->\"))\n",
      " |  \n",
      " |  axhline(self, y=0, xmin=0, xmax=1, **kwargs)\n",
      " |      Add a horizontal line across the axis.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : float, default: 0\n",
      " |          y position in data coordinates of the horizontal line.\n",
      " |      \n",
      " |      xmin : float, default: 0\n",
      " |          Should be between 0 and 1, 0 being the far left of the plot, 1 the\n",
      " |          far right of the plot.\n",
      " |      \n",
      " |      xmax : float, default: 1\n",
      " |          Should be between 0 and 1, 0 being the far left of the plot, 1 the\n",
      " |          far right of the plot.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `~matplotlib.lines.Line2D`\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs\n",
      " |          Valid keyword arguments are `.Line2D` properties, with the\n",
      " |          exception of 'transform':\n",
      " |      \n",
      " |          Properties:\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          animated: bool\n",
      " |          antialiased or aa: bool\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          color or c: color\n",
      " |          contains: unknown\n",
      " |          dash_capstyle: {'butt', 'round', 'projecting'}\n",
      " |          dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      " |          data: (2, N) array or two 1D arrays\n",
      " |          drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      " |          figure: `.Figure`\n",
      " |          fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      " |          gid: str\n",
      " |          in_layout: bool\n",
      " |          label: object\n",
      " |          linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      " |          linewidth or lw: float\n",
      " |          marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      " |          markeredgecolor or mec: color\n",
      " |          markeredgewidth or mew: float\n",
      " |          markerfacecolor or mfc: color\n",
      " |          markerfacecoloralt or mfcalt: color\n",
      " |          markersize or ms: float\n",
      " |          markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: unknown\n",
      " |          pickradius: float\n",
      " |          rasterized: bool or None\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          solid_capstyle: {'butt', 'round', 'projecting'}\n",
      " |          solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          transform: `matplotlib.transforms.Transform`\n",
      " |          url: str\n",
      " |          visible: bool\n",
      " |          xdata: 1D array\n",
      " |          ydata: 1D array\n",
      " |          zorder: float\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      hlines : Add horizontal lines in data coordinates.\n",
      " |      axhspan : Add a horizontal span (rectangle) across the axis.\n",
      " |      axline : Add a line with an arbitrary slope.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      * draw a thick red hline at 'y' = 0 that spans the xrange::\n",
      " |      \n",
      " |          >>> axhline(linewidth=4, color='r')\n",
      " |      \n",
      " |      * draw a default hline at 'y' = 1 that spans the xrange::\n",
      " |      \n",
      " |          >>> axhline(y=1)\n",
      " |      \n",
      " |      * draw a default hline at 'y' = .5 that spans the middle half of\n",
      " |        the xrange::\n",
      " |      \n",
      " |          >>> axhline(y=.5, xmin=0.25, xmax=0.75)\n",
      " |  \n",
      " |  axhspan(self, ymin, ymax, xmin=0, xmax=1, **kwargs)\n",
      " |      Add a horizontal span (rectangle) across the axis.\n",
      " |      \n",
      " |      The rectangle spans from *ymin* to *ymax* vertically, and, by default,\n",
      " |      the whole x-axis horizontally.  The x-span can be set using *xmin*\n",
      " |      (default: 0) and *xmax* (default: 1) which are in axis units; e.g.\n",
      " |      ``xmin = 0.5`` always refers to the middle of the x-axis regardless of\n",
      " |      the limits set by `~.Axes.set_xlim`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ymin : float\n",
      " |          Lower y-coordinate of the span, in data units.\n",
      " |      ymax : float\n",
      " |          Upper y-coordinate of the span, in data units.\n",
      " |      xmin : float, default: 0\n",
      " |          Lower x-coordinate of the span, in x-axis (0-1) units.\n",
      " |      xmax : float, default: 1\n",
      " |          Upper x-coordinate of the span, in x-axis (0-1) units.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `~matplotlib.patches.Polygon`\n",
      " |          Horizontal span (rectangle) from (xmin, ymin) to (xmax, ymax).\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs : `~matplotlib.patches.Polygon` properties\n",
      " |      \n",
      " |      Properties:\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          animated: bool\n",
      " |          antialiased or aa: unknown\n",
      " |          capstyle: {'butt', 'round', 'projecting'}\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          color: color\n",
      " |          contains: unknown\n",
      " |          edgecolor or ec: color or None or 'auto'\n",
      " |          facecolor or fc: color or None\n",
      " |          figure: `.Figure`\n",
      " |          fill: bool\n",
      " |          gid: str\n",
      " |          hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n",
      " |          in_layout: bool\n",
      " |          joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          label: object\n",
      " |          linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      " |          linewidth or lw: float or None\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: None or bool or callable\n",
      " |          rasterized: bool or None\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          transform: `.Transform`\n",
      " |          url: str\n",
      " |          visible: bool\n",
      " |          zorder: float\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      axvspan : Add a vertical span across the axes.\n",
      " |  \n",
      " |  axline(self, xy1, xy2=None, *, slope=None, **kwargs)\n",
      " |      Add an infinitely long straight line.\n",
      " |      \n",
      " |      The line can be defined either by two points *xy1* and *xy2*, or\n",
      " |      by one point *xy1* and a *slope*.\n",
      " |      \n",
      " |      This draws a straight line \"on the screen\", regardless of the x and y\n",
      " |      scales, and is thus also suitable for drawing exponential decays in\n",
      " |      semilog plots, power laws in loglog plots, etc. However, *slope*\n",
      " |      should only be used with linear scales; It has no clear meaning for\n",
      " |      all other scales, and thus the behavior is undefined. Please specify\n",
      " |      the line using the points *xy1*, *xy2* for non-linear scales.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      xy1, xy2 : (float, float)\n",
      " |          Points for the line to pass through.\n",
      " |          Either *xy2* or *slope* has to be given.\n",
      " |      slope : float, optional\n",
      " |          The slope of the line. Either *xy2* or *slope* has to be given.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `.Line2D`\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs\n",
      " |          Valid kwargs are `.Line2D` properties, with the exception of\n",
      " |          'transform':\n",
      " |      \n",
      " |          Properties:\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          animated: bool\n",
      " |          antialiased or aa: bool\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          color or c: color\n",
      " |          contains: unknown\n",
      " |          dash_capstyle: {'butt', 'round', 'projecting'}\n",
      " |          dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      " |          data: (2, N) array or two 1D arrays\n",
      " |          drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      " |          figure: `.Figure`\n",
      " |          fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      " |          gid: str\n",
      " |          in_layout: bool\n",
      " |          label: object\n",
      " |          linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      " |          linewidth or lw: float\n",
      " |          marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      " |          markeredgecolor or mec: color\n",
      " |          markeredgewidth or mew: float\n",
      " |          markerfacecolor or mfc: color\n",
      " |          markerfacecoloralt or mfcalt: color\n",
      " |          markersize or ms: float\n",
      " |          markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: unknown\n",
      " |          pickradius: float\n",
      " |          rasterized: bool or None\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          solid_capstyle: {'butt', 'round', 'projecting'}\n",
      " |          solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          transform: `matplotlib.transforms.Transform`\n",
      " |          url: str\n",
      " |          visible: bool\n",
      " |          xdata: 1D array\n",
      " |          ydata: 1D array\n",
      " |          zorder: float\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      axhline : for horizontal lines\n",
      " |      axvline : for vertical lines\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Draw a thick red line passing through (0, 0) and (1, 1)::\n",
      " |      \n",
      " |          >>> axline((0, 0), (1, 1), linewidth=4, color='r')\n",
      " |  \n",
      " |  axvline(self, x=0, ymin=0, ymax=1, **kwargs)\n",
      " |      Add a vertical line across the axes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : float, default: 0\n",
      " |          x position in data coordinates of the vertical line.\n",
      " |      \n",
      " |      ymin : float, default: 0\n",
      " |          Should be between 0 and 1, 0 being the bottom of the plot, 1 the\n",
      " |          top of the plot.\n",
      " |      \n",
      " |      ymax : float, default: 1\n",
      " |          Should be between 0 and 1, 0 being the bottom of the plot, 1 the\n",
      " |          top of the plot.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `~matplotlib.lines.Line2D`\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs\n",
      " |          Valid keyword arguments are `.Line2D` properties, with the\n",
      " |          exception of 'transform':\n",
      " |      \n",
      " |          Properties:\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          animated: bool\n",
      " |          antialiased or aa: bool\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          color or c: color\n",
      " |          contains: unknown\n",
      " |          dash_capstyle: {'butt', 'round', 'projecting'}\n",
      " |          dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      " |          data: (2, N) array or two 1D arrays\n",
      " |          drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      " |          figure: `.Figure`\n",
      " |          fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      " |          gid: str\n",
      " |          in_layout: bool\n",
      " |          label: object\n",
      " |          linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      " |          linewidth or lw: float\n",
      " |          marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      " |          markeredgecolor or mec: color\n",
      " |          markeredgewidth or mew: float\n",
      " |          markerfacecolor or mfc: color\n",
      " |          markerfacecoloralt or mfcalt: color\n",
      " |          markersize or ms: float\n",
      " |          markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: unknown\n",
      " |          pickradius: float\n",
      " |          rasterized: bool or None\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          solid_capstyle: {'butt', 'round', 'projecting'}\n",
      " |          solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          transform: `matplotlib.transforms.Transform`\n",
      " |          url: str\n",
      " |          visible: bool\n",
      " |          xdata: 1D array\n",
      " |          ydata: 1D array\n",
      " |          zorder: float\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      vlines : Add vertical lines in data coordinates.\n",
      " |      axvspan : Add a vertical span (rectangle) across the axis.\n",
      " |      axline : Add a line with an arbitrary slope.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      * draw a thick red vline at *x* = 0 that spans the yrange::\n",
      " |      \n",
      " |          >>> axvline(linewidth=4, color='r')\n",
      " |      \n",
      " |      * draw a default vline at *x* = 1 that spans the yrange::\n",
      " |      \n",
      " |          >>> axvline(x=1)\n",
      " |      \n",
      " |      * draw a default vline at *x* = .5 that spans the middle half of\n",
      " |        the yrange::\n",
      " |      \n",
      " |          >>> axvline(x=.5, ymin=0.25, ymax=0.75)\n",
      " |  \n",
      " |  axvspan(self, xmin, xmax, ymin=0, ymax=1, **kwargs)\n",
      " |      Add a vertical span (rectangle) across the axes.\n",
      " |      \n",
      " |      The rectangle spans from *xmin* to *xmax* horizontally, and, by\n",
      " |      default, the whole y-axis vertically.  The y-span can be set using\n",
      " |      *ymin* (default: 0) and *ymax* (default: 1) which are in axis units;\n",
      " |      e.g. ``ymin = 0.5`` always refers to the middle of the y-axis\n",
      " |      regardless of the limits set by `~.Axes.set_ylim`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      xmin : float\n",
      " |          Lower x-coordinate of the span, in data units.\n",
      " |      xmax : float\n",
      " |          Upper x-coordinate of the span, in data units.\n",
      " |      ymin : float, default: 0\n",
      " |          Lower y-coordinate of the span, in y-axis units (0-1).\n",
      " |      ymax : float, default: 1\n",
      " |          Upper y-coordinate of the span, in y-axis units (0-1).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `~matplotlib.patches.Polygon`\n",
      " |          Vertical span (rectangle) from (xmin, ymin) to (xmax, ymax).\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs : `~matplotlib.patches.Polygon` properties\n",
      " |      \n",
      " |      %(Polygon)s\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      axhspan : Add a horizontal span across the axes.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Draw a vertical, green, translucent rectangle from x = 1.25 to\n",
      " |      x = 1.55 that spans the yrange of the axes.\n",
      " |      \n",
      " |      >>> axvspan(1.25, 1.55, facecolor='g', alpha=0.5)\n",
      " |  \n",
      " |  barbs(self, *args, data=None, **kw)\n",
      " |      Plot a 2D field of barbs.\n",
      " |      \n",
      " |      Call signature::\n",
      " |      \n",
      " |        barbs([X, Y], U, V, [C], **kw)\n",
      " |      \n",
      " |      Where *X*, *Y* define the barb locations, *U*, *V* define the barb\n",
      " |      directions, and *C* optionally sets the color.\n",
      " |      \n",
      " |      All arguments may be 1D or 2D. *U*, *V*, *C* may be masked arrays, but masked\n",
      " |      *X*, *Y* are not supported at present.\n",
      " |      \n",
      " |      Barbs are traditionally used in meteorology as a way to plot the speed\n",
      " |      and direction of wind observations, but can technically be used to\n",
      " |      plot any two dimensional vector quantity.  As opposed to arrows, which\n",
      " |      give vector magnitude by the length of the arrow, the barbs give more\n",
      " |      quantitative information about the vector magnitude by putting slanted\n",
      " |      lines or a triangle for various increments in magnitude, as show\n",
      " |      schematically below::\n",
      " |      \n",
      " |        :                   /\\    \\\n",
      " |        :                  /  \\    \\\n",
      " |        :                 /    \\    \\    \\\n",
      " |        :                /      \\    \\    \\\n",
      " |        :               ------------------------------\n",
      " |      \n",
      " |      The largest increment is given by a triangle (or \"flag\"). After those\n",
      " |      come full lines (barbs). The smallest increment is a half line.  There\n",
      " |      is only, of course, ever at most 1 half line.  If the magnitude is\n",
      " |      small and only needs a single half-line and no full lines or\n",
      " |      triangles, the half-line is offset from the end of the barb so that it\n",
      " |      can be easily distinguished from barbs with a single full line.  The\n",
      " |      magnitude for the barb shown above would nominally be 65, using the\n",
      " |      standard increments of 50, 10, and 5.\n",
      " |      \n",
      " |      See also https://en.wikipedia.org/wiki/Wind_barb.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X, Y : 1D or 2D array-like, optional\n",
      " |          The x and y coordinates of the barb locations. See *pivot* for how the\n",
      " |          barbs are drawn to the x, y positions.\n",
      " |      \n",
      " |          If not given, they will be generated as a uniform integer meshgrid based\n",
      " |          on the dimensions of *U* and *V*.\n",
      " |      \n",
      " |          If *X* and *Y* are 1D but *U*, *V* are 2D, *X*, *Y* are expanded to 2D\n",
      " |          using ``X, Y = np.meshgrid(X, Y)``. In this case ``len(X)`` and ``len(Y)``\n",
      " |          must match the column and row dimensions of *U* and *V*.\n",
      " |      \n",
      " |      U, V : 1D or 2D array-like\n",
      " |          The x and y components of the barb shaft.\n",
      " |      \n",
      " |      C : 1D or 2D array-like, optional\n",
      " |          Numeric data that defines the barb colors by colormapping via *norm* and\n",
      " |          *cmap*.\n",
      " |      \n",
      " |          This does not support explicit colors. If you want to set colors directly,\n",
      " |          use *barbcolor* instead.\n",
      " |      \n",
      " |      length : float, default: 7\n",
      " |          Length of the barb in points; the other parts of the barb\n",
      " |          are scaled against this.\n",
      " |      \n",
      " |      pivot : {'tip', 'middle'} or float, default: 'tip'\n",
      " |          The part of the arrow that is anchored to the *X*, *Y* grid. The barb\n",
      " |          rotates about this point. This can also be a number, which shifts the\n",
      " |          start of the barb that many points away from grid point.\n",
      " |      \n",
      " |      barbcolor : color or color sequence\n",
      " |          The color of all parts of the barb except for the flags.  This parameter\n",
      " |          is analogous to the *edgecolor* parameter for polygons, which can be used\n",
      " |          instead. However this parameter will override facecolor.\n",
      " |      \n",
      " |      flagcolor : color or color sequence\n",
      " |          The color of any flags on the barb.  This parameter is analogous to the\n",
      " |          *facecolor* parameter for polygons, which can be used instead. However,\n",
      " |          this parameter will override facecolor.  If this is not set (and *C* has\n",
      " |          not either) then *flagcolor* will be set to match *barbcolor* so that the\n",
      " |          barb has a uniform color. If *C* has been set, *flagcolor* has no effect.\n",
      " |      \n",
      " |      sizes : dict, optional\n",
      " |          A dictionary of coefficients specifying the ratio of a given\n",
      " |          feature to the length of the barb. Only those values one wishes to\n",
      " |          override need to be included.  These features include:\n",
      " |      \n",
      " |          - 'spacing' - space between features (flags, full/half barbs)\n",
      " |          - 'height' - height (distance from shaft to top) of a flag or full barb\n",
      " |          - 'width' - width of a flag, twice the width of a full barb\n",
      " |          - 'emptybarb' - radius of the circle used for low magnitudes\n",
      " |      \n",
      " |      fill_empty : bool, default: False\n",
      " |          Whether the empty barbs (circles) that are drawn should be filled with\n",
      " |          the flag color.  If they are not filled, the center is transparent.\n",
      " |      \n",
      " |      rounding : bool, default: True\n",
      " |          Whether the vector magnitude should be rounded when allocating barb\n",
      " |          components.  If True, the magnitude is rounded to the nearest multiple\n",
      " |          of the half-barb increment.  If False, the magnitude is simply truncated\n",
      " |          to the next lowest multiple.\n",
      " |      \n",
      " |      barb_increments : dict, optional\n",
      " |          A dictionary of increments specifying values to associate with\n",
      " |          different parts of the barb. Only those values one wishes to\n",
      " |          override need to be included.\n",
      " |      \n",
      " |          - 'half' - half barbs (Default is 5)\n",
      " |          - 'full' - full barbs (Default is 10)\n",
      " |          - 'flag' - flags (default is 50)\n",
      " |      \n",
      " |      flip_barb : bool or array-like of bool, default: False\n",
      " |          Whether the lines and flags should point opposite to normal.\n",
      " |          Normal behavior is for the barbs and lines to point right (comes from wind\n",
      " |          barbs having these features point towards low pressure in the Northern\n",
      " |          Hemisphere).\n",
      " |      \n",
      " |          A single value is applied to all barbs. Individual barbs can be flipped by\n",
      " |          passing a bool array of the same size as *U* and *V*.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      barbs : `~matplotlib.quiver.Barbs`\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs\n",
      " |          The barbs can further be customized using `.PolyCollection` keyword\n",
      " |          arguments:\n",
      " |      \n",
      " |          Properties:\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          animated: bool\n",
      " |          antialiased or aa or antialiaseds: bool or list of bools\n",
      " |          array: ndarray\n",
      " |          capstyle: {'butt', 'round', 'projecting'}\n",
      " |          clim: (vmin: float, vmax: float)\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          cmap: `.Colormap` or str or None\n",
      " |          color: color or list of rgba tuples\n",
      " |          contains: unknown\n",
      " |          edgecolor or ec or edgecolors: color or list of colors or 'face'\n",
      " |          facecolor or facecolors or fc: color or list of colors\n",
      " |          figure: `.Figure`\n",
      " |          gid: str\n",
      " |          hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n",
      " |          in_layout: bool\n",
      " |          joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          label: object\n",
      " |          linestyle or dashes or linestyles or ls: str or tuple or list thereof\n",
      " |          linewidth or linewidths or lw: float or list of floats\n",
      " |          norm: `.Normalize` or None\n",
      " |          offset_position: unknown\n",
      " |          offsets: array-like (N, 2) or (2,)\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: None or bool or callable\n",
      " |          pickradius: unknown\n",
      " |          rasterized: bool or None\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          transform: `.Transform`\n",
      " |          url: str\n",
      " |          urls: list of str or None\n",
      " |          visible: bool\n",
      " |          zorder: float\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      \n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          every other argument can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception).\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  barh(self, y, width, height=0.8, left=None, *, align='center', **kwargs)\n",
      " |      Make a horizontal bar plot.\n",
      " |      \n",
      " |      The bars are positioned at *y* with the given *align*\\ment. Their\n",
      " |      dimensions are given by *width* and *height*. The horizontal baseline\n",
      " |      is *left* (default 0).\n",
      " |      \n",
      " |      Many parameters can take either a single value applying to all bars\n",
      " |      or a sequence of values, one for each bar.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : float or array-like\n",
      " |          The y coordinates of the bars. See also *align* for the\n",
      " |          alignment of the bars to the coordinates.\n",
      " |      \n",
      " |      width : float or array-like\n",
      " |          The width(s) of the bars.\n",
      " |      \n",
      " |      height : float or array-like, default: 0.8\n",
      " |          The heights of the bars.\n",
      " |      \n",
      " |      left : float or array-like, default: 0\n",
      " |          The x coordinates of the left sides of the bars.\n",
      " |      \n",
      " |      align : {'center', 'edge'}, default: 'center'\n",
      " |          Alignment of the base to the *y* coordinates*:\n",
      " |      \n",
      " |          - 'center': Center the bars on the *y* positions.\n",
      " |          - 'edge': Align the bottom edges of the bars with the *y*\n",
      " |            positions.\n",
      " |      \n",
      " |          To align the bars on the top edge pass a negative *height* and\n",
      " |          ``align='edge'``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `.BarContainer`\n",
      " |          Container with all the bars and optionally errorbars.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      color : color or list of color, optional\n",
      " |          The colors of the bar faces.\n",
      " |      \n",
      " |      edgecolor : color or list of color, optional\n",
      " |          The colors of the bar edges.\n",
      " |      \n",
      " |      linewidth : float or array-like, optional\n",
      " |          Width of the bar edge(s). If 0, don't draw edges.\n",
      " |      \n",
      " |      tick_label : str or list of str, optional\n",
      " |          The tick labels of the bars.\n",
      " |          Default: None (Use default numeric labels.)\n",
      " |      \n",
      " |      xerr, yerr : float or array-like of shape(N,) or shape(2, N), optional\n",
      " |          If not ``None``, add horizontal / vertical errorbars to the\n",
      " |          bar tips. The values are +/- sizes relative to the data:\n",
      " |      \n",
      " |          - scalar: symmetric +/- values for all bars\n",
      " |          - shape(N,): symmetric +/- values for each bar\n",
      " |          - shape(2, N): Separate - and + values for each bar. First row\n",
      " |            contains the lower errors, the second row contains the upper\n",
      " |            errors.\n",
      " |          - *None*: No errorbar. (default)\n",
      " |      \n",
      " |          See :doc:`/gallery/statistics/errorbar_features`\n",
      " |          for an example on the usage of ``xerr`` and ``yerr``.\n",
      " |      \n",
      " |      ecolor : color or list of color, default: 'black'\n",
      " |          The line color of the errorbars.\n",
      " |      \n",
      " |      capsize : float, default: :rc:`errorbar.capsize`\n",
      " |         The length of the error bar caps in points.\n",
      " |      \n",
      " |      error_kw : dict, optional\n",
      " |          Dictionary of kwargs to be passed to the `~.Axes.errorbar`\n",
      " |          method. Values of *ecolor* or *capsize* defined here take\n",
      " |          precedence over the independent kwargs.\n",
      " |      \n",
      " |      log : bool, default: False\n",
      " |          If ``True``, set the x-axis to be log scale.\n",
      " |      \n",
      " |      **kwargs : `.Rectangle` properties\n",
      " |      \n",
      " |      Properties:\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          animated: bool\n",
      " |          antialiased or aa: unknown\n",
      " |          capstyle: {'butt', 'round', 'projecting'}\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          color: color\n",
      " |          contains: unknown\n",
      " |          edgecolor or ec: color or None or 'auto'\n",
      " |          facecolor or fc: color or None\n",
      " |          figure: `.Figure`\n",
      " |          fill: bool\n",
      " |          gid: str\n",
      " |          hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n",
      " |          in_layout: bool\n",
      " |          joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          label: object\n",
      " |          linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      " |          linewidth or lw: float or None\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: None or bool or callable\n",
      " |          rasterized: bool or None\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          transform: `.Transform`\n",
      " |          url: str\n",
      " |          visible: bool\n",
      " |          zorder: float\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      bar: Plot a vertical bar plot.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Stacked bars can be achieved by passing individual *left* values per\n",
      " |      bar. See\n",
      " |      :doc:`/gallery/lines_bars_and_markers/horizontal_barchart_distribution`\n",
      " |      .\n",
      " |  \n",
      " |  boxplot(self, x, notch=None, sym=None, vert=None, whis=None, positions=None, widths=None, patch_artist=None, bootstrap=None, usermedians=None, conf_intervals=None, meanline=None, showmeans=None, showcaps=None, showbox=None, showfliers=None, boxprops=None, labels=None, flierprops=None, medianprops=None, meanprops=None, capprops=None, whiskerprops=None, manage_ticks=True, autorange=False, zorder=None, *, data=None)\n",
      " |      Make a box and whisker plot.\n",
      " |      \n",
      " |      Make a box and whisker plot for each column of *x* or each\n",
      " |      vector in sequence *x*.  The box extends from the lower to\n",
      " |      upper quartile values of the data, with a line at the median.\n",
      " |      The whiskers extend from the box to show the range of the\n",
      " |      data.  Flier points are those past the end of the whiskers.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : Array or a sequence of vectors.\n",
      " |          The input data.\n",
      " |      \n",
      " |      notch : bool, default: False\n",
      " |          Whether to draw a noteched box plot (`True`), or a rectangular box\n",
      " |          plot (`False`).  The notches represent the confidence interval (CI)\n",
      " |          around the median.  The documentation for *bootstrap* describes how\n",
      " |          the locations of the notches are computed.\n",
      " |      \n",
      " |          .. note::\n",
      " |      \n",
      " |              In cases where the values of the CI are less than the\n",
      " |              lower quartile or greater than the upper quartile, the\n",
      " |              notches will extend beyond the box, giving it a\n",
      " |              distinctive \"flipped\" appearance. This is expected\n",
      " |              behavior and consistent with other statistical\n",
      " |              visualization packages.\n",
      " |      \n",
      " |      sym : str, optional\n",
      " |          The default symbol for flier points.  An empty string ('') hides\n",
      " |          the fliers.  If `None`, then the fliers default to 'b+'.  More\n",
      " |          control is provided by the *flierprops* parameter.\n",
      " |      \n",
      " |      vert : bool, default: True\n",
      " |          If `True`, draws vertical boxes.\n",
      " |          If `False`, draw horizontal boxes.\n",
      " |      \n",
      " |      whis : float or (float, float), default: 1.5\n",
      " |          The position of the whiskers.\n",
      " |      \n",
      " |          If a float, the lower whisker is at the lowest datum above\n",
      " |          ``Q1 - whis*(Q3-Q1)``, and the upper whisker at the highest datum\n",
      " |          below ``Q3 + whis*(Q3-Q1)``, where Q1 and Q3 are the first and\n",
      " |          third quartiles.  The default value of ``whis = 1.5`` corresponds\n",
      " |          to Tukey's original definition of boxplots.\n",
      " |      \n",
      " |          If a pair of floats, they indicate the percentiles at which to\n",
      " |          draw the whiskers (e.g., (5, 95)).  In particular, setting this to\n",
      " |          (0, 100) results in whiskers covering the whole range of the data.\n",
      " |          \"range\" is a deprecated synonym for (0, 100).\n",
      " |      \n",
      " |          In the edge case where ``Q1 == Q3``, *whis* is automatically set\n",
      " |          to (0, 100) (cover the whole range of the data) if *autorange* is\n",
      " |          True.\n",
      " |      \n",
      " |          Beyond the whiskers, data are considered outliers and are plotted\n",
      " |          as individual points.\n",
      " |      \n",
      " |      bootstrap : int, optional\n",
      " |          Specifies whether to bootstrap the confidence intervals\n",
      " |          around the median for notched boxplots. If *bootstrap* is\n",
      " |          None, no bootstrapping is performed, and notches are\n",
      " |          calculated using a Gaussian-based asymptotic approximation\n",
      " |          (see McGill, R., Tukey, J.W., and Larsen, W.A., 1978, and\n",
      " |          Kendall and Stuart, 1967). Otherwise, bootstrap specifies\n",
      " |          the number of times to bootstrap the median to determine its\n",
      " |          95% confidence intervals. Values between 1000 and 10000 are\n",
      " |          recommended.\n",
      " |      \n",
      " |      usermedians : array-like, optional\n",
      " |          A 1D array-like of length ``len(x)``.  Each entry that is not\n",
      " |          `None` forces the value of the median for the corresponding\n",
      " |          dataset.  For entries that are `None`, the medians are computed\n",
      " |          by Matplotlib as normal.\n",
      " |      \n",
      " |      conf_intervals : array-like, optional\n",
      " |          A 2D array-like of shape ``(len(x), 2)``.  Each entry that is not\n",
      " |          None forces the location of the corresponding notch (which is\n",
      " |          only drawn if *notch* is `True`).  For entries that are `None`,\n",
      " |          the notches are computed by the method specified by the other\n",
      " |          parameters (e.g., *bootstrap*).\n",
      " |      \n",
      " |      positions : array-like, optional\n",
      " |          Sets the positions of the boxes. The ticks and limits are\n",
      " |          automatically set to match the positions. Defaults to\n",
      " |          ``range(1, N+1)`` where N is the number of boxes to be drawn.\n",
      " |      \n",
      " |      widths : float or array-like\n",
      " |          Sets the width of each box either with a scalar or a\n",
      " |          sequence. The default is 0.5, or ``0.15*(distance between\n",
      " |          extreme positions)``, if that is smaller.\n",
      " |      \n",
      " |      patch_artist : bool, default: False\n",
      " |          If `False` produces boxes with the Line2D artist. Otherwise,\n",
      " |          boxes and drawn with Patch artists.\n",
      " |      \n",
      " |      labels : sequence, optional\n",
      " |          Labels for each dataset (one per dataset).\n",
      " |      \n",
      " |      manage_ticks : bool, default: True\n",
      " |          If True, the tick locations and labels will be adjusted to match\n",
      " |          the boxplot positions.\n",
      " |      \n",
      " |      autorange : bool, default: False\n",
      " |          When `True` and the data are distributed such that the 25th and\n",
      " |          75th percentiles are equal, *whis* is set to (0, 100) such\n",
      " |          that the whisker ends are at the minimum and maximum of the data.\n",
      " |      \n",
      " |      meanline : bool, default: False\n",
      " |          If `True` (and *showmeans* is `True`), will try to render the\n",
      " |          mean as a line spanning the full width of the box according to\n",
      " |          *meanprops* (see below).  Not recommended if *shownotches* is also\n",
      " |          True.  Otherwise, means will be shown as points.\n",
      " |      \n",
      " |      zorder : float, default: ``Line2D.zorder = 2``\n",
      " |          Sets the zorder of the boxplot.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |        A dictionary mapping each component of the boxplot to a list\n",
      " |        of the `.Line2D` instances created. That dictionary has the\n",
      " |        following keys (assuming vertical boxplots):\n",
      " |      \n",
      " |        - ``boxes``: the main body of the boxplot showing the\n",
      " |          quartiles and the median's confidence intervals if\n",
      " |          enabled.\n",
      " |      \n",
      " |        - ``medians``: horizontal lines at the median of each box.\n",
      " |      \n",
      " |        - ``whiskers``: the vertical lines extending to the most\n",
      " |          extreme, non-outlier data points.\n",
      " |      \n",
      " |        - ``caps``: the horizontal lines at the ends of the\n",
      " |          whiskers.\n",
      " |      \n",
      " |        - ``fliers``: points representing data that extend beyond\n",
      " |          the whiskers (fliers).\n",
      " |      \n",
      " |        - ``means``: points or lines representing the means.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      showcaps : bool, default: True\n",
      " |          Show the caps on the ends of whiskers.\n",
      " |      showbox : bool, default: True\n",
      " |          Show the central box.\n",
      " |      showfliers : bool, default: True\n",
      " |          Show the outliers beyond the caps.\n",
      " |      showmeans : bool, default: False\n",
      " |          Show the arithmetic means.\n",
      " |      capprops : dict, default: None\n",
      " |          The style of the caps.\n",
      " |      boxprops : dict, default: None\n",
      " |          The style of the box.\n",
      " |      whiskerprops : dict, default: None\n",
      " |          The style of the whiskers.\n",
      " |      flierprops : dict, default: None\n",
      " |          The style of the fliers.\n",
      " |      medianprops : dict, default: None\n",
      " |          The style of the median.\n",
      " |      meanprops : dict, default: None\n",
      " |          The style of the mean.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      \n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          every other argument can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception).\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  broken_barh(self, xranges, yrange, *, data=None, **kwargs)\n",
      " |      Plot a horizontal sequence of rectangles.\n",
      " |      \n",
      " |      A rectangle is drawn for each element of *xranges*. All rectangles\n",
      " |      have the same vertical position and size defined by *yrange*.\n",
      " |      \n",
      " |      This is a convenience function for instantiating a\n",
      " |      `.BrokenBarHCollection`, adding it to the axes and autoscaling the\n",
      " |      view.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      xranges : sequence of tuples (*xmin*, *xwidth*)\n",
      " |          The x-positions and extends of the rectangles. For each tuple\n",
      " |          (*xmin*, *xwidth*) a rectangle is drawn from *xmin* to *xmin* +\n",
      " |          *xwidth*.\n",
      " |      yrange : (*ymin*, *yheight*)\n",
      " |          The y-position and extend for all the rectangles.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `~.collections.BrokenBarHCollection`\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs : `.BrokenBarHCollection` properties\n",
      " |      \n",
      " |          Each *kwarg* can be either a single argument applying to all\n",
      " |          rectangles, e.g.::\n",
      " |      \n",
      " |              facecolors='black'\n",
      " |      \n",
      " |          or a sequence of arguments over which is cycled, e.g.::\n",
      " |      \n",
      " |              facecolors=('black', 'blue')\n",
      " |      \n",
      " |          would create interleaving black and blue rectangles.\n",
      " |      \n",
      " |          Supported keywords:\n",
      " |      \n",
      " |          Properties:\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          animated: bool\n",
      " |          antialiased or aa or antialiaseds: bool or list of bools\n",
      " |          array: ndarray\n",
      " |          capstyle: {'butt', 'round', 'projecting'}\n",
      " |          clim: (vmin: float, vmax: float)\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          cmap: `.Colormap` or str or None\n",
      " |          color: color or list of rgba tuples\n",
      " |          contains: unknown\n",
      " |          edgecolor or ec or edgecolors: color or list of colors or 'face'\n",
      " |          facecolor or facecolors or fc: color or list of colors\n",
      " |          figure: `.Figure`\n",
      " |          gid: str\n",
      " |          hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n",
      " |          in_layout: bool\n",
      " |          joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          label: object\n",
      " |          linestyle or dashes or linestyles or ls: str or tuple or list thereof\n",
      " |          linewidth or linewidths or lw: float or list of floats\n",
      " |          norm: `.Normalize` or None\n",
      " |          offset_position: unknown\n",
      " |          offsets: array-like (N, 2) or (2,)\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: None or bool or callable\n",
      " |          pickradius: unknown\n",
      " |          rasterized: bool or None\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          transform: `.Transform`\n",
      " |          url: str\n",
      " |          urls: list of str or None\n",
      " |          visible: bool\n",
      " |          zorder: float\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      \n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          every other argument can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception).\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  bxp(self, bxpstats, positions=None, widths=None, vert=True, patch_artist=False, shownotches=False, showmeans=False, showcaps=True, showbox=True, showfliers=True, boxprops=None, whiskerprops=None, flierprops=None, medianprops=None, capprops=None, meanprops=None, meanline=False, manage_ticks=True, zorder=None)\n",
      " |      Drawing function for box and whisker plots.\n",
      " |      \n",
      " |      Make a box and whisker plot for each column of *x* or each\n",
      " |      vector in sequence *x*.  The box extends from the lower to\n",
      " |      upper quartile values of the data, with a line at the median.\n",
      " |      The whiskers extend from the box to show the range of the\n",
      " |      data.  Flier points are those past the end of the whiskers.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      bxpstats : list of dicts\n",
      " |        A list of dictionaries containing stats for each boxplot.\n",
      " |        Required keys are:\n",
      " |      \n",
      " |        - ``med``: The median (scalar float).\n",
      " |      \n",
      " |        - ``q1``: The first quartile (25th percentile) (scalar\n",
      " |          float).\n",
      " |      \n",
      " |        - ``q3``: The third quartile (75th percentile) (scalar\n",
      " |          float).\n",
      " |      \n",
      " |        - ``whislo``: Lower bound of the lower whisker (scalar\n",
      " |          float).\n",
      " |      \n",
      " |        - ``whishi``: Upper bound of the upper whisker (scalar\n",
      " |          float).\n",
      " |      \n",
      " |        Optional keys are:\n",
      " |      \n",
      " |        - ``mean``: The mean (scalar float). Needed if\n",
      " |          ``showmeans=True``.\n",
      " |      \n",
      " |        - ``fliers``: Data beyond the whiskers (sequence of floats).\n",
      " |          Needed if ``showfliers=True``.\n",
      " |      \n",
      " |        - ``cilo`` & ``cihi``: Lower and upper confidence intervals\n",
      " |          about the median. Needed if ``shownotches=True``.\n",
      " |      \n",
      " |        - ``label``: Name of the dataset (string). If available,\n",
      " |          this will be used a tick label for the boxplot\n",
      " |      \n",
      " |      positions : array-like, default: [1, 2, ..., n]\n",
      " |        Sets the positions of the boxes. The ticks and limits\n",
      " |        are automatically set to match the positions.\n",
      " |      \n",
      " |      widths : array-like, default: None\n",
      " |        Either a scalar or a vector and sets the width of each\n",
      " |        box. The default is ``0.15*(distance between extreme\n",
      " |        positions)``, clipped to no less than 0.15 and no more than\n",
      " |        0.5.\n",
      " |      \n",
      " |      vert : bool, default: True\n",
      " |        If `True` (default), makes the boxes vertical.  If `False`,\n",
      " |        makes horizontal boxes.\n",
      " |      \n",
      " |      patch_artist : bool, default: False\n",
      " |        If `False` produces boxes with the `.Line2D` artist.\n",
      " |        If `True` produces boxes with the `~matplotlib.patches.Patch` artist.\n",
      " |      \n",
      " |      shownotches : bool, default: False\n",
      " |        If `False` (default), produces a rectangular box plot.\n",
      " |        If `True`, will produce a notched box plot\n",
      " |      \n",
      " |      showmeans : bool, default: False\n",
      " |        If `True`, will toggle on the rendering of the means\n",
      " |      \n",
      " |      showcaps  : bool, default: True\n",
      " |        If `True`, will toggle on the rendering of the caps\n",
      " |      \n",
      " |      showbox  : bool, default: True\n",
      " |        If `True`, will toggle on the rendering of the box\n",
      " |      \n",
      " |      showfliers : bool, default: True\n",
      " |        If `True`, will toggle on the rendering of the fliers\n",
      " |      \n",
      " |      boxprops : dict or None (default)\n",
      " |        If provided, will set the plotting style of the boxes\n",
      " |      \n",
      " |      whiskerprops : dict or None (default)\n",
      " |        If provided, will set the plotting style of the whiskers\n",
      " |      \n",
      " |      capprops : dict or None (default)\n",
      " |        If provided, will set the plotting style of the caps\n",
      " |      \n",
      " |      flierprops : dict or None (default)\n",
      " |        If provided will set the plotting style of the fliers\n",
      " |      \n",
      " |      medianprops : dict or None (default)\n",
      " |        If provided, will set the plotting style of the medians\n",
      " |      \n",
      " |      meanprops : dict or None (default)\n",
      " |        If provided, will set the plotting style of the means\n",
      " |      \n",
      " |      meanline : bool, default: False\n",
      " |        If `True` (and *showmeans* is `True`), will try to render the mean\n",
      " |        as a line spanning the full width of the box according to\n",
      " |        *meanprops*. Not recommended if *shownotches* is also True.\n",
      " |        Otherwise, means will be shown as points.\n",
      " |      \n",
      " |      manage_ticks : bool, default: True\n",
      " |        If True, the tick locations and labels will be adjusted to match the\n",
      " |        boxplot positions.\n",
      " |      \n",
      " |      zorder : float, default: ``Line2D.zorder = 2``\n",
      " |        The zorder of the resulting boxplot.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |        A dictionary mapping each component of the boxplot to a list\n",
      " |        of the `.Line2D` instances created. That dictionary has the\n",
      " |        following keys (assuming vertical boxplots):\n",
      " |      \n",
      " |        - ``boxes``: the main body of the boxplot showing the\n",
      " |          quartiles and the median's confidence intervals if\n",
      " |          enabled.\n",
      " |      \n",
      " |        - ``medians``: horizontal lines at the median of each box.\n",
      " |      \n",
      " |        - ``whiskers``: the vertical lines extending to the most\n",
      " |          extreme, non-outlier data points.\n",
      " |      \n",
      " |        - ``caps``: the horizontal lines at the ends of the\n",
      " |          whiskers.\n",
      " |      \n",
      " |        - ``fliers``: points representing data that extend beyond\n",
      " |          the whiskers (fliers).\n",
      " |      \n",
      " |        - ``means``: points or lines representing the means.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. plot:: gallery/statistics/bxp.py\n",
      " |  \n",
      " |  cohere(self, x, y, NFFT=256, Fs=2, Fc=0, detrend=<function detrend_none at 0x125d49040>, window=<function window_hanning at 0x125d44ca0>, noverlap=0, pad_to=None, sides='default', scale_by_freq=None, *, data=None, **kwargs)\n",
      " |      Plot the coherence between *x* and *y*.\n",
      " |      \n",
      " |      Plot the coherence between *x* and *y*.  Coherence is the\n",
      " |      normalized cross spectral density:\n",
      " |      \n",
      " |      .. math::\n",
      " |      \n",
      " |        C_{xy} = \\frac{|P_{xy}|^2}{P_{xx}P_{yy}}\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      Fs : float, default: 2\n",
      " |          The sampling frequency (samples per time unit).  It is used to calculate\n",
      " |          the Fourier frequencies, *freqs*, in cycles per time unit.\n",
      " |      \n",
      " |      window : callable or ndarray, default: `.window_hanning`\n",
      " |          A function or a vector of length *NFFT*.  To create window vectors see\n",
      " |          `.window_hanning`, `.window_none`, `numpy.blackman`, `numpy.hamming`,\n",
      " |          `numpy.bartlett`, `scipy.signal`, `scipy.signal.get_window`, etc.  If a\n",
      " |          function is passed as the argument, it must take a data segment as an\n",
      " |          argument and return the windowed version of the segment.\n",
      " |      \n",
      " |      sides : {'default', 'onesided', 'twosided'}, optional\n",
      " |          Which sides of the spectrum to return. 'default' is one-sided for real\n",
      " |          data and two-sided for complex data. 'onesided' forces the return of a\n",
      " |          one-sided spectrum, while 'twosided' forces two-sided.\n",
      " |      \n",
      " |      pad_to : int, optional\n",
      " |          The number of points to which the data segment is padded when performing\n",
      " |          the FFT.  This can be different from *NFFT*, which specifies the number\n",
      " |          of data points used.  While not increasing the actual resolution of the\n",
      " |          spectrum (the minimum distance between resolvable peaks), this can give\n",
      " |          more points in the plot, allowing for more detail. This corresponds to\n",
      " |          the *n* parameter in the call to fft(). The default is None, which sets\n",
      " |          *pad_to* equal to *NFFT*\n",
      " |      \n",
      " |      NFFT : int, default: 256\n",
      " |          The number of data points used in each block for the FFT.  A power 2 is\n",
      " |          most efficient.  This should *NOT* be used to get zero padding, or the\n",
      " |          scaling of the result will be incorrect; use *pad_to* for this instead.\n",
      " |      \n",
      " |      detrend : {'none', 'mean', 'linear'} or callable, default 'none'\n",
      " |          The function applied to each segment before fft-ing, designed to remove\n",
      " |          the mean or linear trend.  Unlike in MATLAB, where the *detrend* parameter\n",
      " |          is a vector, in Matplotlib is it a function.  The :mod:`~matplotlib.mlab`\n",
      " |          module defines `.detrend_none`, `.detrend_mean`, and `.detrend_linear`,\n",
      " |          but you can use a custom function as well.  You can also use a string to\n",
      " |          choose one of the functions: 'none' calls `.detrend_none`. 'mean' calls\n",
      " |          `.detrend_mean`. 'linear' calls `.detrend_linear`.\n",
      " |      \n",
      " |      scale_by_freq : bool, default: True\n",
      " |          Whether the resulting density values should be scaled by the scaling\n",
      " |          frequency, which gives density in units of Hz^-1.  This allows for\n",
      " |          integration over the returned frequency values.  The default is True for\n",
      " |          MATLAB compatibility.\n",
      " |      \n",
      " |      noverlap : int, default: 0 (no overlap)\n",
      " |          The number of points of overlap between blocks.\n",
      " |      \n",
      " |      Fc : int, default: 0\n",
      " |          The center frequency of *x*, which offsets the x extents of the\n",
      " |          plot to reflect the frequency range used when a signal is acquired\n",
      " |          and then filtered and downsampled to baseband.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Cxy : 1-D array\n",
      " |          The coherence vector.\n",
      " |      \n",
      " |      freqs : 1-D array\n",
      " |          The frequencies for the elements in *Cxy*.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs\n",
      " |          Keyword arguments control the `.Line2D` properties:\n",
      " |      \n",
      " |          Properties:\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          animated: bool\n",
      " |          antialiased or aa: bool\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          color or c: color\n",
      " |          contains: unknown\n",
      " |          dash_capstyle: {'butt', 'round', 'projecting'}\n",
      " |          dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      " |          data: (2, N) array or two 1D arrays\n",
      " |          drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      " |          figure: `.Figure`\n",
      " |          fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      " |          gid: str\n",
      " |          in_layout: bool\n",
      " |          label: object\n",
      " |          linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      " |          linewidth or lw: float\n",
      " |          marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      " |          markeredgecolor or mec: color\n",
      " |          markeredgewidth or mew: float\n",
      " |          markerfacecolor or mfc: color\n",
      " |          markerfacecoloralt or mfcalt: color\n",
      " |          markersize or ms: float\n",
      " |          markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: unknown\n",
      " |          pickradius: float\n",
      " |          rasterized: bool or None\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          solid_capstyle: {'butt', 'round', 'projecting'}\n",
      " |          solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          transform: `matplotlib.transforms.Transform`\n",
      " |          url: str\n",
      " |          visible: bool\n",
      " |          xdata: 1D array\n",
      " |          ydata: 1D array\n",
      " |          zorder: float\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      Bendat & Piersol -- Random Data: Analysis and Measurement Procedures,\n",
      " |      John Wiley & Sons (1986)\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      \n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          the following arguments can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception):\n",
      " |          *x*, *y*.\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  csd(self, x, y, NFFT=None, Fs=None, Fc=None, detrend=None, window=None, noverlap=None, pad_to=None, sides=None, scale_by_freq=None, return_line=None, *, data=None, **kwargs)\n",
      " |      Plot the cross-spectral density.\n",
      " |      \n",
      " |      The cross spectral density :math:`P_{xy}` by Welch's average\n",
      " |      periodogram method.  The vectors *x* and *y* are divided into\n",
      " |      *NFFT* length segments.  Each segment is detrended by function\n",
      " |      *detrend* and windowed by function *window*.  *noverlap* gives\n",
      " |      the length of the overlap between segments.  The product of\n",
      " |      the direct FFTs of *x* and *y* are averaged over each segment\n",
      " |      to compute :math:`P_{xy}`, with a scaling to correct for power\n",
      " |      loss due to windowing.\n",
      " |      \n",
      " |      If len(*x*) < *NFFT* or len(*y*) < *NFFT*, they will be zero\n",
      " |      padded to *NFFT*.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x, y : 1-D arrays or sequences\n",
      " |          Arrays or sequences containing the data.\n",
      " |      \n",
      " |      Fs : float, default: 2\n",
      " |          The sampling frequency (samples per time unit).  It is used to calculate\n",
      " |          the Fourier frequencies, *freqs*, in cycles per time unit.\n",
      " |      \n",
      " |      window : callable or ndarray, default: `.window_hanning`\n",
      " |          A function or a vector of length *NFFT*.  To create window vectors see\n",
      " |          `.window_hanning`, `.window_none`, `numpy.blackman`, `numpy.hamming`,\n",
      " |          `numpy.bartlett`, `scipy.signal`, `scipy.signal.get_window`, etc.  If a\n",
      " |          function is passed as the argument, it must take a data segment as an\n",
      " |          argument and return the windowed version of the segment.\n",
      " |      \n",
      " |      sides : {'default', 'onesided', 'twosided'}, optional\n",
      " |          Which sides of the spectrum to return. 'default' is one-sided for real\n",
      " |          data and two-sided for complex data. 'onesided' forces the return of a\n",
      " |          one-sided spectrum, while 'twosided' forces two-sided.\n",
      " |      \n",
      " |      pad_to : int, optional\n",
      " |          The number of points to which the data segment is padded when performing\n",
      " |          the FFT.  This can be different from *NFFT*, which specifies the number\n",
      " |          of data points used.  While not increasing the actual resolution of the\n",
      " |          spectrum (the minimum distance between resolvable peaks), this can give\n",
      " |          more points in the plot, allowing for more detail. This corresponds to\n",
      " |          the *n* parameter in the call to fft(). The default is None, which sets\n",
      " |          *pad_to* equal to *NFFT*\n",
      " |      \n",
      " |      NFFT : int, default: 256\n",
      " |          The number of data points used in each block for the FFT.  A power 2 is\n",
      " |          most efficient.  This should *NOT* be used to get zero padding, or the\n",
      " |          scaling of the result will be incorrect; use *pad_to* for this instead.\n",
      " |      \n",
      " |      detrend : {'none', 'mean', 'linear'} or callable, default 'none'\n",
      " |          The function applied to each segment before fft-ing, designed to remove\n",
      " |          the mean or linear trend.  Unlike in MATLAB, where the *detrend* parameter\n",
      " |          is a vector, in Matplotlib is it a function.  The :mod:`~matplotlib.mlab`\n",
      " |          module defines `.detrend_none`, `.detrend_mean`, and `.detrend_linear`,\n",
      " |          but you can use a custom function as well.  You can also use a string to\n",
      " |          choose one of the functions: 'none' calls `.detrend_none`. 'mean' calls\n",
      " |          `.detrend_mean`. 'linear' calls `.detrend_linear`.\n",
      " |      \n",
      " |      scale_by_freq : bool, default: True\n",
      " |          Whether the resulting density values should be scaled by the scaling\n",
      " |          frequency, which gives density in units of Hz^-1.  This allows for\n",
      " |          integration over the returned frequency values.  The default is True for\n",
      " |          MATLAB compatibility.\n",
      " |      \n",
      " |      noverlap : int, default: 0 (no overlap)\n",
      " |          The number of points of overlap between segments.\n",
      " |      \n",
      " |      Fc : int, default: 0\n",
      " |          The center frequency of *x*, which offsets the x extents of the\n",
      " |          plot to reflect the frequency range used when a signal is acquired\n",
      " |          and then filtered and downsampled to baseband.\n",
      " |      \n",
      " |      return_line : bool, default: False\n",
      " |          Whether to include the line object plotted in the returned values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Pxy : 1-D array\n",
      " |          The values for the cross spectrum :math:`P_{xy}` before scaling\n",
      " |          (complex valued).\n",
      " |      \n",
      " |      freqs : 1-D array\n",
      " |          The frequencies corresponding to the elements in *Pxy*.\n",
      " |      \n",
      " |      line : `~matplotlib.lines.Line2D`\n",
      " |          The line created by this function.\n",
      " |          Only returned if *return_line* is True.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs\n",
      " |          Keyword arguments control the `.Line2D` properties:\n",
      " |      \n",
      " |          Properties:\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          animated: bool\n",
      " |          antialiased or aa: bool\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          color or c: color\n",
      " |          contains: unknown\n",
      " |          dash_capstyle: {'butt', 'round', 'projecting'}\n",
      " |          dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      " |          data: (2, N) array or two 1D arrays\n",
      " |          drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      " |          figure: `.Figure`\n",
      " |          fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      " |          gid: str\n",
      " |          in_layout: bool\n",
      " |          label: object\n",
      " |          linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      " |          linewidth or lw: float\n",
      " |          marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      " |          markeredgecolor or mec: color\n",
      " |          markeredgewidth or mew: float\n",
      " |          markerfacecolor or mfc: color\n",
      " |          markerfacecoloralt or mfcalt: color\n",
      " |          markersize or ms: float\n",
      " |          markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: unknown\n",
      " |          pickradius: float\n",
      " |          rasterized: bool or None\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          solid_capstyle: {'butt', 'round', 'projecting'}\n",
      " |          solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          transform: `matplotlib.transforms.Transform`\n",
      " |          url: str\n",
      " |          visible: bool\n",
      " |          xdata: 1D array\n",
      " |          ydata: 1D array\n",
      " |          zorder: float\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      psd : is equivalent to setting ``y = x``.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For plotting, the power is plotted as\n",
      " |      :math:`10 \\log_{10}(P_{xy})` for decibels, though :math:`P_{xy}` itself\n",
      " |      is returned.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      Bendat & Piersol -- Random Data: Analysis and Measurement Procedures,\n",
      " |      John Wiley & Sons (1986)\n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          the following arguments can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception):\n",
      " |          *x*, *y*.\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  errorbar(self, x, y, yerr=None, xerr=None, fmt='', ecolor=None, elinewidth=None, capsize=None, barsabove=False, lolims=False, uplims=False, xlolims=False, xuplims=False, errorevery=1, capthick=None, *, data=None, **kwargs)\n",
      " |      Plot y versus x as lines and/or markers with attached errorbars.\n",
      " |      \n",
      " |      *x*, *y* define the data locations, *xerr*, *yerr* define the errorbar\n",
      " |      sizes. By default, this draws the data markers/lines as well the\n",
      " |      errorbars. Use fmt='none' to draw errorbars without any data markers.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x, y : float or array-like\n",
      " |          The data positions.\n",
      " |      \n",
      " |      xerr, yerr : float or array-like, shape(N,) or shape(2, N), optional\n",
      " |          The errorbar sizes:\n",
      " |      \n",
      " |          - scalar: Symmetric +/- values for all data points.\n",
      " |          - shape(N,): Symmetric +/-values for each data point.\n",
      " |          - shape(2, N): Separate - and + values for each bar. First row\n",
      " |            contains the lower errors, the second row contains the upper\n",
      " |            errors.\n",
      " |          - *None*: No errorbar.\n",
      " |      \n",
      " |          Note that all error arrays should have *positive* values.\n",
      " |      \n",
      " |          See :doc:`/gallery/statistics/errorbar_features`\n",
      " |          for an example on the usage of ``xerr`` and ``yerr``.\n",
      " |      \n",
      " |      fmt : str, default: ''\n",
      " |          The format for the data points / data lines. See `.plot` for\n",
      " |          details.\n",
      " |      \n",
      " |          Use 'none' (case insensitive) to plot errorbars without any data\n",
      " |          markers.\n",
      " |      \n",
      " |      ecolor : color, default: None\n",
      " |          The color of the errorbar lines.  If None, use the color of the\n",
      " |          line connecting the markers.\n",
      " |      \n",
      " |      elinewidth : float, default: None\n",
      " |          The linewidth of the errorbar lines. If None, the linewidth of\n",
      " |          the current style is used.\n",
      " |      \n",
      " |      capsize : float, default: :rc:`errorbar.capsize`\n",
      " |          The length of the error bar caps in points.\n",
      " |      \n",
      " |      capthick : float, default: None\n",
      " |          An alias to the keyword argument *markeredgewidth* (a.k.a. *mew*).\n",
      " |          This setting is a more sensible name for the property that\n",
      " |          controls the thickness of the error bar cap in points. For\n",
      " |          backwards compatibility, if *mew* or *markeredgewidth* are given,\n",
      " |          then they will over-ride *capthick*. This may change in future\n",
      " |          releases.\n",
      " |      \n",
      " |      barsabove : bool, default: False\n",
      " |          If True, will plot the errorbars above the plot\n",
      " |          symbols. Default is below.\n",
      " |      \n",
      " |      lolims, uplims, xlolims, xuplims : bool, default: False\n",
      " |          These arguments can be used to indicate that a value gives only\n",
      " |          upper/lower limits.  In that case a caret symbol is used to\n",
      " |          indicate this. *lims*-arguments may be scalars, or array-likes of\n",
      " |          the same length as *xerr* and *yerr*.  To use limits with inverted\n",
      " |          axes, `~.Axes.set_xlim` or `~.Axes.set_ylim` must be called before\n",
      " |          :meth:`errorbar`.  Note the tricky parameter names: setting e.g.\n",
      " |          *lolims* to True means that the y-value is a *lower* limit of the\n",
      " |          True value, so, only an *upward*-pointing arrow will be drawn!\n",
      " |      \n",
      " |      errorevery : int or (int, int), default: 1\n",
      " |          draws error bars on a subset of the data. *errorevery* =N draws\n",
      " |          error bars on the points (x[::N], y[::N]).\n",
      " |          *errorevery* =(start, N) draws error bars on the points\n",
      " |          (x[start::N], y[start::N]). e.g. errorevery=(6, 3)\n",
      " |          adds error bars to the data at (x[6], x[9], x[12], x[15], ...).\n",
      " |          Used to avoid overlapping error bars when two series share x-axis\n",
      " |          values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `.ErrorbarContainer`\n",
      " |          The container contains:\n",
      " |      \n",
      " |          - plotline: `.Line2D` instance of x, y plot markers and/or line.\n",
      " |          - caplines: A tuple of `.Line2D` instances of the error bar caps.\n",
      " |          - barlinecols: A tuple of `.LineCollection` with the horizontal and\n",
      " |            vertical error ranges.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs\n",
      " |          All other keyword arguments are passed on to the `~.Axes.plot` call\n",
      " |          drawing the markers. For example, this code makes big red squares\n",
      " |          with thick green edges::\n",
      " |      \n",
      " |              x, y, yerr = rand(3, 10)\n",
      " |              errorbar(x, y, yerr, marker='s', mfc='red',\n",
      " |                       mec='green', ms=20, mew=4)\n",
      " |      \n",
      " |          where *mfc*, *mec*, *ms* and *mew* are aliases for the longer\n",
      " |          property names, *markerfacecolor*, *markeredgecolor*, *markersize*\n",
      " |          and *markeredgewidth*.\n",
      " |      \n",
      " |          Valid kwargs for the marker properties are `.Line2D` properties:\n",
      " |      \n",
      " |          Properties:\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          animated: bool\n",
      " |          antialiased or aa: bool\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          color or c: color\n",
      " |          contains: unknown\n",
      " |          dash_capstyle: {'butt', 'round', 'projecting'}\n",
      " |          dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      " |          data: (2, N) array or two 1D arrays\n",
      " |          drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      " |          figure: `.Figure`\n",
      " |          fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      " |          gid: str\n",
      " |          in_layout: bool\n",
      " |          label: object\n",
      " |          linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      " |          linewidth or lw: float\n",
      " |          marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      " |          markeredgecolor or mec: color\n",
      " |          markeredgewidth or mew: float\n",
      " |          markerfacecolor or mfc: color\n",
      " |          markerfacecoloralt or mfcalt: color\n",
      " |          markersize or ms: float\n",
      " |          markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: unknown\n",
      " |          pickradius: float\n",
      " |          rasterized: bool or None\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          solid_capstyle: {'butt', 'round', 'projecting'}\n",
      " |          solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          transform: `matplotlib.transforms.Transform`\n",
      " |          url: str\n",
      " |          visible: bool\n",
      " |          xdata: 1D array\n",
      " |          ydata: 1D array\n",
      " |          zorder: float\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      \n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          the following arguments can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception):\n",
      " |          *x*, *y*, *xerr*, *yerr*.\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  eventplot(self, positions, orientation='horizontal', lineoffsets=1, linelengths=1, linewidths=None, colors=None, linestyles='solid', *, data=None, **kwargs)\n",
      " |      Plot identical parallel lines at the given positions.\n",
      " |      \n",
      " |      This type of plot is commonly used in neuroscience for representing\n",
      " |      neural events, where it is usually called a spike raster, dot raster,\n",
      " |      or raster plot.\n",
      " |      \n",
      " |      However, it is useful in any situation where you wish to show the\n",
      " |      timing or position of multiple sets of discrete events, such as the\n",
      " |      arrival times of people to a business on each day of the month or the\n",
      " |      date of hurricanes each year of the last century.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positions : array-like or list of array-like\n",
      " |          A 1D array-like defines the positions of one sequence of events.\n",
      " |      \n",
      " |          Multiple groups of events may be passed as a list of array-likes.\n",
      " |          Each group can be styled independently by passing lists of values\n",
      " |          to *lineoffsets*, *linelengths*, *linewidths*, *colors* and\n",
      " |          *linestyles*.\n",
      " |      \n",
      " |          Note that *positions* can be a 2D array, but in practice different\n",
      " |          event groups usually have different counts so that one will use a\n",
      " |          list of different-length arrays rather than a 2D array.\n",
      " |      \n",
      " |      orientation : {'horizontal', 'vertical'}, default: 'horizontal'\n",
      " |          The direction of the event sequence:\n",
      " |      \n",
      " |          - 'horizontal': the events are arranged horizontally.\n",
      " |            The indicator lines are vertical.\n",
      " |          - 'vertical': the events are arranged vertically.\n",
      " |            The indicator lines are horizontal.\n",
      " |      \n",
      " |      lineoffsets : float or array-like, default: 1\n",
      " |          The offset of the center of the lines from the origin, in the\n",
      " |          direction orthogonal to *orientation*.\n",
      " |      \n",
      " |          If *positions* is 2D, this can be a sequence with length matching\n",
      " |          the length of *positions*.\n",
      " |      \n",
      " |      linelengths : float or array-like, default: 1\n",
      " |          The total height of the lines (i.e. the lines stretches from\n",
      " |          ``lineoffset - linelength/2`` to ``lineoffset + linelength/2``).\n",
      " |      \n",
      " |          If *positions* is 2D, this can be a sequence with length matching\n",
      " |          the length of *positions*.\n",
      " |      \n",
      " |      linewidths : float or array-like, default: :rc:`lines.linewidth`\n",
      " |          The line width(s) of the event lines, in points.\n",
      " |      \n",
      " |          If *positions* is 2D, this can be a sequence with length matching\n",
      " |          the length of *positions*.\n",
      " |      \n",
      " |      colors : color or list of colors, default: :rc:`lines.color`\n",
      " |          The color(s) of the event lines.\n",
      " |      \n",
      " |          If *positions* is 2D, this can be a sequence with length matching\n",
      " |          the length of *positions*.\n",
      " |      \n",
      " |      linestyles : str or tuple or list of such values, default: 'solid'\n",
      " |          Default is 'solid'. Valid strings are ['solid', 'dashed',\n",
      " |          'dashdot', 'dotted', '-', '--', '-.', ':']. Dash tuples\n",
      " |          should be of the form::\n",
      " |      \n",
      " |              (offset, onoffseq),\n",
      " |      \n",
      " |          where *onoffseq* is an even length tuple of on and off ink\n",
      " |          in points.\n",
      " |      \n",
      " |          If *positions* is 2D, this can be a sequence with length matching\n",
      " |          the length of *positions*.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Other keyword arguments are line collection properties.  See\n",
      " |          `.LineCollection` for a list of the valid properties.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of `.EventCollection`\n",
      " |          The `.EventCollection` that were added.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For *linelengths*, *linewidths*, *colors*, and *linestyles*, if only\n",
      " |      a single value is given, that value is applied to all lines.  If an\n",
      " |      array-like is given, it must have the same length as *positions*, and\n",
      " |      each value will be applied to the corresponding row of the array.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. plot:: gallery/lines_bars_and_markers/eventplot_demo.py\n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          the following arguments can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception):\n",
      " |          *positions*, *lineoffsets*, *linelengths*, *linewidths*, *colors*, *linestyles*.\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  fill(self, *args, data=None, **kwargs)\n",
      " |      Plot filled polygons.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      *args : sequence of x, y, [color]\n",
      " |          Each polygon is defined by the lists of *x* and *y* positions of\n",
      " |          its nodes, optionally followed by a *color* specifier. See\n",
      " |          :mod:`matplotlib.colors` for supported color specifiers. The\n",
      " |          standard color cycle is used for polygons without a color\n",
      " |          specifier.\n",
      " |      \n",
      " |          You can plot multiple polygons by providing multiple *x*, *y*,\n",
      " |          *[color]* groups.\n",
      " |      \n",
      " |          For example, each of the following is legal::\n",
      " |      \n",
      " |              ax.fill(x, y)                    # a polygon with default color\n",
      " |              ax.fill(x, y, \"b\")               # a blue polygon\n",
      " |              ax.fill(x, y, x2, y2)            # two polygons\n",
      " |              ax.fill(x, y, \"b\", x2, y2, \"r\")  # a blue and a red polygon\n",
      " |      \n",
      " |      data : indexable object, optional\n",
      " |          An object with labelled data. If given, provide the label names to\n",
      " |          plot in *x* and *y*, e.g.::\n",
      " |      \n",
      " |              ax.fill(\"time\", \"signal\",\n",
      " |                      data={\"time\": [0, 1, 2], \"signal\": [0, 1, 0]})\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of `~matplotlib.patches.Polygon`\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs : `~matplotlib.patches.Polygon` properties\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Use :meth:`fill_between` if you would like to fill the region between\n",
      " |      two curves.\n",
      " |  \n",
      " |  fill_between(self, x, y1, y2=0, where=None, interpolate=False, step=None, *, data=None, **kwargs)\n",
      " |      Fill the area between two horizontal curves.\n",
      " |      \n",
      " |      The curves are defined by the points (*x*, *y1*) and (*x*,\n",
      " |      *y2*).  This creates one or multiple polygons describing the filled\n",
      " |      area.\n",
      " |      \n",
      " |      You may exclude some horizontal sections from filling using *where*.\n",
      " |      \n",
      " |      By default, the edges connect the given points directly.  Use *step*\n",
      " |      if the filling should be a step function, i.e. constant in between\n",
      " |      *x*.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : array (length N)\n",
      " |          The x coordinates of the nodes defining the curves.\n",
      " |      \n",
      " |      y1 : array (length N) or scalar\n",
      " |          The y coordinates of the nodes defining the first curve.\n",
      " |      \n",
      " |      y2 : array (length N) or scalar, default: 0\n",
      " |          The y coordinates of the nodes defining the second curve.\n",
      " |      \n",
      " |      where : array of bool (length N), optional\n",
      " |          Define *where* to exclude some horizontal regions from being filled.\n",
      " |          The filled regions are defined by the coordinates ``x[where]``.\n",
      " |          More precisely, fill between ``x[i]`` and ``x[i+1]`` if\n",
      " |          ``where[i] and where[i+1]``.  Note that this definition implies\n",
      " |          that an isolated *True* value between two *False* values in *where*\n",
      " |          will not result in filling.  Both sides of the *True* position\n",
      " |          remain unfilled due to the adjacent *False* values.\n",
      " |      \n",
      " |      interpolate : bool, default: False\n",
      " |          This option is only relevant if *where* is used and the two curves\n",
      " |          are crossing each other.\n",
      " |      \n",
      " |          Semantically, *where* is often used for *y1* > *y2* or\n",
      " |          similar.  By default, the nodes of the polygon defining the filled\n",
      " |          region will only be placed at the positions in the *x* array.\n",
      " |          Such a polygon cannot describe the above semantics close to the\n",
      " |          intersection.  The x-sections containing the intersection are\n",
      " |          simply clipped.\n",
      " |      \n",
      " |          Setting *interpolate* to *True* will calculate the actual\n",
      " |          intersection point and extend the filled region up to this point.\n",
      " |      \n",
      " |      step : {'pre', 'post', 'mid'}, optional\n",
      " |          Define *step* if the filling should be a step function,\n",
      " |          i.e. constant in between *x*.  The value determines where the\n",
      " |          step will occur:\n",
      " |      \n",
      " |          - 'pre': The y value is continued constantly to the left from\n",
      " |            every *x* position, i.e. the interval ``(x[i-1], x[i]]`` has the\n",
      " |            value ``y[i]``.\n",
      " |          - 'post': The y value is continued constantly to the right from\n",
      " |            every *x* position, i.e. the interval ``[x[i], x[i+1])`` has the\n",
      " |            value ``y[i]``.\n",
      " |          - 'mid': Steps occur half-way between the *x* positions.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `.PolyCollection`\n",
      " |          A `.PolyCollection` containing the plotted polygons.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs\n",
      " |          All other keyword arguments are passed on to `.PolyCollection`.\n",
      " |          They control the `.Polygon` properties:\n",
      " |      \n",
      " |          Properties:\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          animated: bool\n",
      " |          antialiased or aa or antialiaseds: bool or list of bools\n",
      " |          array: ndarray\n",
      " |          capstyle: {'butt', 'round', 'projecting'}\n",
      " |          clim: (vmin: float, vmax: float)\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          cmap: `.Colormap` or str or None\n",
      " |          color: color or list of rgba tuples\n",
      " |          contains: unknown\n",
      " |          edgecolor or ec or edgecolors: color or list of colors or 'face'\n",
      " |          facecolor or facecolors or fc: color or list of colors\n",
      " |          figure: `.Figure`\n",
      " |          gid: str\n",
      " |          hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n",
      " |          in_layout: bool\n",
      " |          joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          label: object\n",
      " |          linestyle or dashes or linestyles or ls: str or tuple or list thereof\n",
      " |          linewidth or linewidths or lw: float or list of floats\n",
      " |          norm: `.Normalize` or None\n",
      " |          offset_position: unknown\n",
      " |          offsets: array-like (N, 2) or (2,)\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: None or bool or callable\n",
      " |          pickradius: unknown\n",
      " |          rasterized: bool or None\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          transform: `.Transform`\n",
      " |          url: str\n",
      " |          urls: list of str or None\n",
      " |          visible: bool\n",
      " |          zorder: float\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      fill_between : Fill between two sets of y-values.\n",
      " |      fill_betweenx : Fill between two sets of x-values.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. [notes section required to get data note injection right]\n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          the following arguments can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception):\n",
      " |          *x*, *y1*, *y2*, *where*.\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  fill_betweenx(self, y, x1, x2=0, where=None, step=None, interpolate=False, *, data=None, **kwargs)\n",
      " |      Fill the area between two vertical curves.\n",
      " |      \n",
      " |      The curves are defined by the points (*y*, *x1*) and (*y*,\n",
      " |      *x2*).  This creates one or multiple polygons describing the filled\n",
      " |      area.\n",
      " |      \n",
      " |      You may exclude some vertical sections from filling using *where*.\n",
      " |      \n",
      " |      By default, the edges connect the given points directly.  Use *step*\n",
      " |      if the filling should be a step function, i.e. constant in between\n",
      " |      *y*.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : array (length N)\n",
      " |          The y coordinates of the nodes defining the curves.\n",
      " |      \n",
      " |      x1 : array (length N) or scalar\n",
      " |          The x coordinates of the nodes defining the first curve.\n",
      " |      \n",
      " |      x2 : array (length N) or scalar, default: 0\n",
      " |          The x coordinates of the nodes defining the second curve.\n",
      " |      \n",
      " |      where : array of bool (length N), optional\n",
      " |          Define *where* to exclude some vertical regions from being filled.\n",
      " |          The filled regions are defined by the coordinates ``y[where]``.\n",
      " |          More precisely, fill between ``y[i]`` and ``y[i+1]`` if\n",
      " |          ``where[i] and where[i+1]``.  Note that this definition implies\n",
      " |          that an isolated *True* value between two *False* values in *where*\n",
      " |          will not result in filling.  Both sides of the *True* position\n",
      " |          remain unfilled due to the adjacent *False* values.\n",
      " |      \n",
      " |      interpolate : bool, default: False\n",
      " |          This option is only relevant if *where* is used and the two curves\n",
      " |          are crossing each other.\n",
      " |      \n",
      " |          Semantically, *where* is often used for *x1* > *x2* or\n",
      " |          similar.  By default, the nodes of the polygon defining the filled\n",
      " |          region will only be placed at the positions in the *y* array.\n",
      " |          Such a polygon cannot describe the above semantics close to the\n",
      " |          intersection.  The y-sections containing the intersection are\n",
      " |          simply clipped.\n",
      " |      \n",
      " |          Setting *interpolate* to *True* will calculate the actual\n",
      " |          intersection point and extend the filled region up to this point.\n",
      " |      \n",
      " |      step : {'pre', 'post', 'mid'}, optional\n",
      " |          Define *step* if the filling should be a step function,\n",
      " |          i.e. constant in between *y*.  The value determines where the\n",
      " |          step will occur:\n",
      " |      \n",
      " |          - 'pre': The y value is continued constantly to the left from\n",
      " |            every *x* position, i.e. the interval ``(x[i-1], x[i]]`` has the\n",
      " |            value ``y[i]``.\n",
      " |          - 'post': The y value is continued constantly to the right from\n",
      " |            every *x* position, i.e. the interval ``[x[i], x[i+1])`` has the\n",
      " |            value ``y[i]``.\n",
      " |          - 'mid': Steps occur half-way between the *x* positions.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `.PolyCollection`\n",
      " |          A `.PolyCollection` containing the plotted polygons.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs\n",
      " |          All other keyword arguments are passed on to `.PolyCollection`.\n",
      " |          They control the `.Polygon` properties:\n",
      " |      \n",
      " |          Properties:\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          animated: bool\n",
      " |          antialiased or aa or antialiaseds: bool or list of bools\n",
      " |          array: ndarray\n",
      " |          capstyle: {'butt', 'round', 'projecting'}\n",
      " |          clim: (vmin: float, vmax: float)\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          cmap: `.Colormap` or str or None\n",
      " |          color: color or list of rgba tuples\n",
      " |          contains: unknown\n",
      " |          edgecolor or ec or edgecolors: color or list of colors or 'face'\n",
      " |          facecolor or facecolors or fc: color or list of colors\n",
      " |          figure: `.Figure`\n",
      " |          gid: str\n",
      " |          hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n",
      " |          in_layout: bool\n",
      " |          joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          label: object\n",
      " |          linestyle or dashes or linestyles or ls: str or tuple or list thereof\n",
      " |          linewidth or linewidths or lw: float or list of floats\n",
      " |          norm: `.Normalize` or None\n",
      " |          offset_position: unknown\n",
      " |          offsets: array-like (N, 2) or (2,)\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: None or bool or callable\n",
      " |          pickradius: unknown\n",
      " |          rasterized: bool or None\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          transform: `.Transform`\n",
      " |          url: str\n",
      " |          urls: list of str or None\n",
      " |          visible: bool\n",
      " |          zorder: float\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      fill_between : Fill between two sets of y-values.\n",
      " |      fill_betweenx : Fill between two sets of x-values.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. [notes section required to get data note injection right]\n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          the following arguments can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception):\n",
      " |          *y*, *x1*, *x2*, *where*.\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  get_legend_handles_labels(self, legend_handler_map=None)\n",
      " |      Return handles and labels for legend\n",
      " |      \n",
      " |      ``ax.legend()`` is equivalent to ::\n",
      " |      \n",
      " |        h, l = ax.get_legend_handles_labels()\n",
      " |        ax.legend(h, l)\n",
      " |  \n",
      " |  get_title(self, loc='center')\n",
      " |      Get an axes title.\n",
      " |      \n",
      " |      Get one of the three available axes titles. The available titles\n",
      " |      are positioned above the axes in the center, flush with the left\n",
      " |      edge, and flush with the right edge.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      loc : {'center', 'left', 'right'}, str, default: 'center'\n",
      " |          Which title to return.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          The title text string.\n",
      " |  \n",
      " |  get_xlabel(self)\n",
      " |      Get the xlabel text string.\n",
      " |  \n",
      " |  get_ylabel(self)\n",
      " |      Get the ylabel text string.\n",
      " |  \n",
      " |  hexbin(self, x, y, C=None, gridsize=100, bins=None, xscale='linear', yscale='linear', extent=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, edgecolors='face', reduce_C_function=<function mean at 0x112543e50>, mincnt=None, marginals=False, *, data=None, **kwargs)\n",
      " |      Make a 2D hexagonal binning plot of points *x*, *y*.\n",
      " |      \n",
      " |      If *C* is *None*, the value of the hexagon is determined by the number\n",
      " |      of points in the hexagon. Otherwise, *C* specifies values at the\n",
      " |      coordinate (x[i], y[i]). For each hexagon, these values are reduced\n",
      " |      using *reduce_C_function*.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x, y : array-like\n",
      " |          The data positions. *x* and *y* must be of the same length.\n",
      " |      \n",
      " |      C : array-like, optional\n",
      " |          If given, these values are accumulated in the bins. Otherwise,\n",
      " |          every point has a value of 1. Must be of the same length as *x*\n",
      " |          and *y*.\n",
      " |      \n",
      " |      gridsize : int or (int, int), default: 100\n",
      " |          If a single int, the number of hexagons in the *x*-direction.\n",
      " |          The number of hexagons in the *y*-direction is chosen such that\n",
      " |          the hexagons are approximately regular.\n",
      " |      \n",
      " |          Alternatively, if a tuple (*nx*, *ny*), the number of hexagons\n",
      " |          in the *x*-direction and the *y*-direction.\n",
      " |      \n",
      " |      bins : 'log' or int or sequence, default: None\n",
      " |          Discretization of the hexagon values.\n",
      " |      \n",
      " |          - If *None*, no binning is applied; the color of each hexagon\n",
      " |            directly corresponds to its count value.\n",
      " |          - If 'log', use a logarithmic scale for the color map.\n",
      " |            Internally, :math:`log_{10}(i+1)` is used to determine the\n",
      " |            hexagon color. This is equivalent to ``norm=LogNorm()``.\n",
      " |          - If an integer, divide the counts in the specified number\n",
      " |            of bins, and color the hexagons accordingly.\n",
      " |          - If a sequence of values, the values of the lower bound of\n",
      " |            the bins to be used.\n",
      " |      \n",
      " |      xscale : {'linear', 'log'}, default: 'linear'\n",
      " |          Use a linear or log10 scale on the horizontal axis.\n",
      " |      \n",
      " |      yscale : {'linear', 'log'}, default: 'linear'\n",
      " |          Use a linear or log10 scale on the vertical axis.\n",
      " |      \n",
      " |      mincnt : int > 0, default: *None*\n",
      " |          If not *None*, only display cells with more than *mincnt*\n",
      " |          number of points in the cell.\n",
      " |      \n",
      " |      marginals : bool, default: *False*\n",
      " |          If marginals is *True*, plot the marginal density as\n",
      " |          colormapped rectangles along the bottom of the x-axis and\n",
      " |          left of the y-axis.\n",
      " |      \n",
      " |      extent : float, default: *None*\n",
      " |          The limits of the bins. The default assigns the limits\n",
      " |          based on *gridsize*, *x*, *y*, *xscale* and *yscale*.\n",
      " |      \n",
      " |          If *xscale* or *yscale* is set to 'log', the limits are\n",
      " |          expected to be the exponent for a power of 10. E.g. for\n",
      " |          x-limits of 1 and 50 in 'linear' scale and y-limits\n",
      " |          of 10 and 1000 in 'log' scale, enter (1, 50, 1, 3).\n",
      " |      \n",
      " |          Order of scalars is (left, right, bottom, top).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `~matplotlib.collections.PolyCollection`\n",
      " |          A `.PolyCollection` defining the hexagonal bins.\n",
      " |      \n",
      " |          - `.PolyCollection.get_offsets` contains a Mx2 array containing\n",
      " |            the x, y positions of the M hexagon centers.\n",
      " |          - `.PolyCollection.get_array` contains the values of the M\n",
      " |            hexagons.\n",
      " |      \n",
      " |          If *marginals* is *True*, horizontal\n",
      " |          bar and vertical bar (both PolyCollections) will be attached\n",
      " |          to the return collection as attributes *hbar* and *vbar*.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      cmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n",
      " |          The Colormap instance or registered colormap name used to map\n",
      " |          the bin values to colors.\n",
      " |      \n",
      " |      norm : `~matplotlib.colors.Normalize`, optional\n",
      " |          The Normalize instance scales the bin values to the canonical\n",
      " |          colormap range [0, 1] for mapping to colors. By default, the data\n",
      " |          range is mapped to the colorbar range using linear scaling.\n",
      " |      \n",
      " |      vmin, vmax : float, default: None\n",
      " |          The colorbar range. If *None*, suitable min/max values are\n",
      " |          automatically chosen by the `~.Normalize` instance (defaults to\n",
      " |          the respective min/max values of the bins in case of the default\n",
      " |          linear scaling).\n",
      " |          It is deprecated to use *vmin*/*vmax* when *norm* is given.\n",
      " |      \n",
      " |      alpha : float between 0 and 1, optional\n",
      " |          The alpha blending value, between 0 (transparent) and 1 (opaque).\n",
      " |      \n",
      " |      linewidths : float, default: *None*\n",
      " |          If *None*, defaults to 1.0.\n",
      " |      \n",
      " |      edgecolors : {'face', 'none', *None*} or color, default: 'face'\n",
      " |          The color of the hexagon edges. Possible values are:\n",
      " |      \n",
      " |          - 'face': Draw the edges in the same color as the fill color.\n",
      " |          - 'none': No edges are drawn. This can sometimes lead to unsightly\n",
      " |            unpainted pixels between the hexagons.\n",
      " |          - *None*: Draw outlines in the default color.\n",
      " |          - An explicit color.\n",
      " |      \n",
      " |      reduce_C_function : callable, default: `numpy.mean`\n",
      " |          The function to aggregate *C* within the bins. It is ignored if\n",
      " |          *C* is not given. This must have the signature::\n",
      " |      \n",
      " |              def reduce_C_function(C: array) -> float\n",
      " |      \n",
      " |          Commonly used functions are:\n",
      " |      \n",
      " |          - `numpy.mean`: average of the points\n",
      " |          - `numpy.sum`: integral of the point values\n",
      " |          - `numpy.max`: value taken from the largest point\n",
      " |      \n",
      " |      **kwargs : `~matplotlib.collections.PolyCollection` properties\n",
      " |          All other keyword arguments are passed on to `.PolyCollection`:\n",
      " |      \n",
      " |          Properties:\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          animated: bool\n",
      " |          antialiased or aa or antialiaseds: bool or list of bools\n",
      " |          array: ndarray\n",
      " |          capstyle: {'butt', 'round', 'projecting'}\n",
      " |          clim: (vmin: float, vmax: float)\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          cmap: `.Colormap` or str or None\n",
      " |          color: color or list of rgba tuples\n",
      " |          contains: unknown\n",
      " |          edgecolor or ec or edgecolors: color or list of colors or 'face'\n",
      " |          facecolor or facecolors or fc: color or list of colors\n",
      " |          figure: `.Figure`\n",
      " |          gid: str\n",
      " |          hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n",
      " |          in_layout: bool\n",
      " |          joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          label: object\n",
      " |          linestyle or dashes or linestyles or ls: str or tuple or list thereof\n",
      " |          linewidth or linewidths or lw: float or list of floats\n",
      " |          norm: `.Normalize` or None\n",
      " |          offset_position: unknown\n",
      " |          offsets: array-like (N, 2) or (2,)\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: None or bool or callable\n",
      " |          pickradius: unknown\n",
      " |          rasterized: bool or None\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          transform: `.Transform`\n",
      " |          url: str\n",
      " |          urls: list of str or None\n",
      " |          visible: bool\n",
      " |          zorder: float\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      \n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          the following arguments can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception):\n",
      " |          *x*, *y*.\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  hist(self, x, bins=None, range=None, density=False, weights=None, cumulative=False, bottom=None, histtype='bar', align='mid', orientation='vertical', rwidth=None, log=False, color=None, label=None, stacked=False, *, data=None, **kwargs)\n",
      " |      Plot a histogram.\n",
      " |      \n",
      " |      Compute and draw the histogram of *x*.  The return value is a tuple\n",
      " |      (*n*, *bins*, *patches*) or ([*n0*, *n1*, ...], *bins*, [*patches0*,\n",
      " |      *patches1*, ...]) if the input contains multiple data.  See the\n",
      " |      documentation of the *weights* parameter to draw a histogram of\n",
      " |      already-binned data.\n",
      " |      \n",
      " |      Multiple data can be provided via *x* as a list of datasets\n",
      " |      of potentially different length ([*x0*, *x1*, ...]), or as\n",
      " |      a 2-D ndarray in which each column is a dataset.  Note that\n",
      " |      the ndarray form is transposed relative to the list form.\n",
      " |      \n",
      " |      Masked arrays are not supported.\n",
      " |      \n",
      " |      The *bins*, *range*, *weights*, and *density* parameters behave as in\n",
      " |      `numpy.histogram`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : (n,) array or sequence of (n,) arrays\n",
      " |          Input values, this takes either a single array or a sequence of\n",
      " |          arrays which are not required to be of the same length.\n",
      " |      \n",
      " |      bins : int or sequence or str, default: :rc:`hist.bins`\n",
      " |          If *bins* is an integer, it defines the number of equal-width bins\n",
      " |          in the range.\n",
      " |      \n",
      " |          If *bins* is a sequence, it defines the bin edges, including the\n",
      " |          left edge of the first bin and the right edge of the last bin;\n",
      " |          in this case, bins may be unequally spaced.  All but the last\n",
      " |          (righthand-most) bin is half-open.  In other words, if *bins* is::\n",
      " |      \n",
      " |              [1, 2, 3, 4]\n",
      " |      \n",
      " |          then the first bin is ``[1, 2)`` (including 1, but excluding 2) and\n",
      " |          the second ``[2, 3)``.  The last bin, however, is ``[3, 4]``, which\n",
      " |          *includes* 4.\n",
      " |      \n",
      " |          If *bins* is a string, it is one of the binning strategies\n",
      " |          supported by `numpy.histogram_bin_edges`: 'auto', 'fd', 'doane',\n",
      " |          'scott', 'stone', 'rice', 'sturges', or 'sqrt'.\n",
      " |      \n",
      " |      range : tuple or None, default: None\n",
      " |          The lower and upper range of the bins. Lower and upper outliers\n",
      " |          are ignored. If not provided, *range* is ``(x.min(), x.max())``.\n",
      " |          Range has no effect if *bins* is a sequence.\n",
      " |      \n",
      " |          If *bins* is a sequence or *range* is specified, autoscaling\n",
      " |          is based on the specified bin range instead of the\n",
      " |          range of x.\n",
      " |      \n",
      " |      density : bool, default: False\n",
      " |          If ``True``, draw and return a probability density: each bin\n",
      " |          will display the bin's raw count divided by the total number of\n",
      " |          counts *and the bin width*\n",
      " |          (``density = counts / (sum(counts) * np.diff(bins))``),\n",
      " |          so that the area under the histogram integrates to 1\n",
      " |          (``np.sum(density * np.diff(bins)) == 1``).\n",
      " |      \n",
      " |          If *stacked* is also ``True``, the sum of the histograms is\n",
      " |          normalized to 1.\n",
      " |      \n",
      " |      weights : (n,) array-like or None, default: None\n",
      " |          An array of weights, of the same shape as *x*.  Each value in\n",
      " |          *x* only contributes its associated weight towards the bin count\n",
      " |          (instead of 1).  If *density* is ``True``, the weights are\n",
      " |          normalized, so that the integral of the density over the range\n",
      " |          remains 1.\n",
      " |      \n",
      " |          This parameter can be used to draw a histogram of data that has\n",
      " |          already been binned, e.g. using `numpy.histogram` (by treating each\n",
      " |          bin as a single point with a weight equal to its count) ::\n",
      " |      \n",
      " |              counts, bins = np.histogram(data)\n",
      " |              plt.hist(bins[:-1], bins, weights=counts)\n",
      " |      \n",
      " |          (or you may alternatively use `~.bar()`).\n",
      " |      \n",
      " |      cumulative : bool or -1, default: False\n",
      " |          If ``True``, then a histogram is computed where each bin gives the\n",
      " |          counts in that bin plus all bins for smaller values. The last bin\n",
      " |          gives the total number of datapoints.\n",
      " |      \n",
      " |          If *density* is also ``True`` then the histogram is normalized such\n",
      " |          that the last bin equals 1.\n",
      " |      \n",
      " |          If *cumulative* is a number less than 0 (e.g., -1), the direction\n",
      " |          of accumulation is reversed.  In this case, if *density* is also\n",
      " |          ``True``, then the histogram is normalized such that the first bin\n",
      " |          equals 1.\n",
      " |      \n",
      " |      bottom : array-like, scalar, or None, default: None\n",
      " |          Location of the bottom of each bin, ie. bins are drawn from\n",
      " |          ``bottom`` to ``bottom + hist(x, bins)`` If a scalar, the bottom\n",
      " |          of each bin is shifted by the same amount. If an array, each bin\n",
      " |          is shifted independently and the length of bottom must match the\n",
      " |          number of bins. If None, defaults to 0.\n",
      " |      \n",
      " |      histtype : {'bar', 'barstacked', 'step', 'stepfilled'}, default: 'bar'\n",
      " |          The type of histogram to draw.\n",
      " |      \n",
      " |          - 'bar' is a traditional bar-type histogram.  If multiple data\n",
      " |            are given the bars are arranged side by side.\n",
      " |          - 'barstacked' is a bar-type histogram where multiple\n",
      " |            data are stacked on top of each other.\n",
      " |          - 'step' generates a lineplot that is by default unfilled.\n",
      " |          - 'stepfilled' generates a lineplot that is by default filled.\n",
      " |      \n",
      " |      align : {'left', 'mid', 'right'}, default: 'mid'\n",
      " |          The horizontal alignment of the histogram bars.\n",
      " |      \n",
      " |          - 'left': bars are centered on the left bin edges.\n",
      " |          - 'mid': bars are centered between the bin edges.\n",
      " |          - 'right': bars are centered on the right bin edges.\n",
      " |      \n",
      " |      orientation : {'vertical', 'horizontal'}, default: 'vertical'\n",
      " |          If 'horizontal', `~.Axes.barh` will be used for bar-type histograms\n",
      " |          and the *bottom* kwarg will be the left edges.\n",
      " |      \n",
      " |      rwidth : float or None, default: None\n",
      " |          The relative width of the bars as a fraction of the bin width.  If\n",
      " |          ``None``, automatically compute the width.\n",
      " |      \n",
      " |          Ignored if *histtype* is 'step' or 'stepfilled'.\n",
      " |      \n",
      " |      log : bool, default: False\n",
      " |          If ``True``, the histogram axis will be set to a log scale. If\n",
      " |          *log* is ``True`` and *x* is a 1D array, empty bins will be\n",
      " |          filtered out and only the non-empty ``(n, bins, patches)``\n",
      " |          will be returned.\n",
      " |      \n",
      " |      color : color or array-like of colors or None, default: None\n",
      " |          Color or sequence of colors, one per dataset.  Default (``None``)\n",
      " |          uses the standard line color sequence.\n",
      " |      \n",
      " |      label : str or None, default: None\n",
      " |          String, or sequence of strings to match multiple datasets.  Bar\n",
      " |          charts yield multiple patches per dataset, but only the first gets\n",
      " |          the label, so that `~.Axes.legend` will work as expected.\n",
      " |      \n",
      " |      stacked : bool, default: False\n",
      " |          If ``True``, multiple data are stacked on top of each other If\n",
      " |          ``False`` multiple data are arranged side by side if histtype is\n",
      " |          'bar' or on top of each other if histtype is 'step'\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      n : array or list of arrays\n",
      " |          The values of the histogram bins. See *density* and *weights* for a\n",
      " |          description of the possible semantics.  If input *x* is an array,\n",
      " |          then this is an array of length *nbins*. If input is a sequence of\n",
      " |          arrays ``[data1, data2, ...]``, then this is a list of arrays with\n",
      " |          the values of the histograms for each of the arrays in the same\n",
      " |          order.  The dtype of the array *n* (or of its element arrays) will\n",
      " |          always be float even if no weighting or normalization is used.\n",
      " |      \n",
      " |      bins : array\n",
      " |          The edges of the bins. Length nbins + 1 (nbins left edges and right\n",
      " |          edge of last bin).  Always a single array even when multiple data\n",
      " |          sets are passed in.\n",
      " |      \n",
      " |      patches : `.BarContainer` or list of a single `.Polygon` or list of such objects\n",
      " |          Container of individual artists used to create the histogram\n",
      " |          or list of such containers if there are multiple input datasets.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs\n",
      " |          `~matplotlib.patches.Patch` properties\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      hist2d : 2D histograms\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For large numbers of bins (>1000), 'step' and 'stepfilled' can be\n",
      " |      significantly faster than 'bar' and 'barstacked'.\n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          the following arguments can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception):\n",
      " |          *x*, *weights*.\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  hist2d(self, x, y, bins=10, range=None, density=False, weights=None, cmin=None, cmax=None, *, data=None, **kwargs)\n",
      " |      Make a 2D histogram plot.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x, y : array-like, shape (n, )\n",
      " |          Input values\n",
      " |      \n",
      " |      bins : None or int or [int, int] or array-like or [array, array]\n",
      " |      \n",
      " |          The bin specification:\n",
      " |      \n",
      " |          - If int, the number of bins for the two dimensions\n",
      " |            (nx=ny=bins).\n",
      " |          - If ``[int, int]``, the number of bins in each dimension\n",
      " |            (nx, ny = bins).\n",
      " |          - If array-like, the bin edges for the two dimensions\n",
      " |            (x_edges=y_edges=bins).\n",
      " |          - If ``[array, array]``, the bin edges in each dimension\n",
      " |            (x_edges, y_edges = bins).\n",
      " |      \n",
      " |          The default value is 10.\n",
      " |      \n",
      " |      range : array-like shape(2, 2), optional\n",
      " |          The leftmost and rightmost edges of the bins along each dimension\n",
      " |          (if not specified explicitly in the bins parameters): ``[[xmin,\n",
      " |          xmax], [ymin, ymax]]``. All values outside of this range will be\n",
      " |          considered outliers and not tallied in the histogram.\n",
      " |      \n",
      " |      density : bool, default: False\n",
      " |          Normalize histogram.  See the documentation for the *density*\n",
      " |          parameter of `~.Axes.hist` for more details.\n",
      " |      \n",
      " |      weights : array-like, shape (n, ), optional\n",
      " |          An array of values w_i weighing each sample (x_i, y_i).\n",
      " |      \n",
      " |      cmin, cmax : float, default: None\n",
      " |          All bins that has count less than *cmin* or more than *cmax* will\n",
      " |          not be displayed (set to NaN before passing to imshow) and these\n",
      " |          count values in the return value count histogram will also be set\n",
      " |          to nan upon return.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      h : 2D array\n",
      " |          The bi-dimensional histogram of samples x and y. Values in x are\n",
      " |          histogrammed along the first dimension and values in y are\n",
      " |          histogrammed along the second dimension.\n",
      " |      xedges : 1D array\n",
      " |          The bin edges along the x axis.\n",
      " |      yedges : 1D array\n",
      " |          The bin edges along the y axis.\n",
      " |      image : `~.matplotlib.collections.QuadMesh`\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      cmap : Colormap or str, optional\n",
      " |          A `.colors.Colormap` instance.  If not set, use rc settings.\n",
      " |      \n",
      " |      norm : Normalize, optional\n",
      " |          A `.colors.Normalize` instance is used to\n",
      " |          scale luminance data to ``[0, 1]``. If not set, defaults to\n",
      " |          `.colors.Normalize()`.\n",
      " |      \n",
      " |      vmin/vmax : None or scalar, optional\n",
      " |          Arguments passed to the `~.colors.Normalize` instance.\n",
      " |      \n",
      " |      alpha : ``0 <= scalar <= 1`` or ``None``, optional\n",
      " |          The alpha blending value.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Additional parameters are passed along to the\n",
      " |          `~.Axes.pcolormesh` method and `~matplotlib.collections.QuadMesh`\n",
      " |          constructor.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      hist : 1D histogram plotting\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      - Currently ``hist2d`` calculates its own axis limits, and any limits\n",
      " |        previously set are ignored.\n",
      " |      - Rendering the histogram with a logarithmic color scale is\n",
      " |        accomplished by passing a `.colors.LogNorm` instance to the *norm*\n",
      " |        keyword argument. Likewise, power-law normalization (similar\n",
      " |        in effect to gamma correction) can be accomplished with\n",
      " |        `.colors.PowerNorm`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          the following arguments can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception):\n",
      " |          *x*, *y*, *weights*.\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  hlines(self, y, xmin, xmax, colors=None, linestyles='solid', label='', *, data=None, **kwargs)\n",
      " |      Plot horizontal lines at each *y* from *xmin* to *xmax*.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      y : float or array-like\n",
      " |          y-indexes where to plot the lines.\n",
      " |      \n",
      " |      xmin, xmax : float or array-like\n",
      " |          Respective beginning and end of each line. If scalars are\n",
      " |          provided, all lines will have same length.\n",
      " |      \n",
      " |      colors : list of colors, default: :rc:`lines.color`\n",
      " |      \n",
      " |      linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional\n",
      " |      \n",
      " |      label : str, default: ''\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `~matplotlib.collections.LineCollection`\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs :  `~matplotlib.collections.LineCollection` properties.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      vlines : vertical lines\n",
      " |      axhline: horizontal line across the axes\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      \n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          the following arguments can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception):\n",
      " |          *y*, *xmin*, *xmax*, *colors*.\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  imshow(self, X, cmap=None, norm=None, aspect=None, interpolation=None, alpha=None, vmin=None, vmax=None, origin=None, extent=None, *, filternorm=True, filterrad=4.0, resample=None, url=None, data=None, **kwargs)\n",
      " |      Display data as an image, i.e., on a 2D regular raster.\n",
      " |      \n",
      " |      The input may either be actual RGB(A) data, or 2D scalar data, which\n",
      " |      will be rendered as a pseudocolor image. For displaying a grayscale\n",
      " |      image set up the color mapping using the parameters\n",
      " |      ``cmap='gray', vmin=0, vmax=255``.\n",
      " |      \n",
      " |      The number of pixels used to render an image is set by the axes size\n",
      " |      and the *dpi* of the figure. This can lead to aliasing artifacts when\n",
      " |      the image is resampled because the displayed image size will usually\n",
      " |      not match the size of *X* (see\n",
      " |      :doc:`/gallery/images_contours_and_fields/image_antialiasing`).\n",
      " |      The resampling can be controlled via the *interpolation* parameter\n",
      " |      and/or :rc:`image.interpolation`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or PIL image\n",
      " |          The image data. Supported array shapes are:\n",
      " |      \n",
      " |          - (M, N): an image with scalar data. The values are mapped to\n",
      " |            colors using normalization and a colormap. See parameters *norm*,\n",
      " |            *cmap*, *vmin*, *vmax*.\n",
      " |          - (M, N, 3): an image with RGB values (0-1 float or 0-255 int).\n",
      " |          - (M, N, 4): an image with RGBA values (0-1 float or 0-255 int),\n",
      " |            i.e. including transparency.\n",
      " |      \n",
      " |          The first two dimensions (M, N) define the rows and columns of\n",
      " |          the image.\n",
      " |      \n",
      " |          Out-of-range RGB(A) values are clipped.\n",
      " |      \n",
      " |      cmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n",
      " |          The Colormap instance or registered colormap name used to map\n",
      " |          scalar data to colors. This parameter is ignored for RGB(A) data.\n",
      " |      \n",
      " |      norm : `~matplotlib.colors.Normalize`, optional\n",
      " |          The `.Normalize` instance used to scale scalar data to the [0, 1]\n",
      " |          range before mapping to colors using *cmap*. By default, a linear\n",
      " |          scaling mapping the lowest value to 0 and the highest to 1 is used.\n",
      " |          This parameter is ignored for RGB(A) data.\n",
      " |      \n",
      " |      aspect : {'equal', 'auto'} or float, default: :rc:`image.aspect`\n",
      " |          The aspect ratio of the axes.  This parameter is particularly\n",
      " |          relevant for images since it determines whether data pixels are\n",
      " |          square.\n",
      " |      \n",
      " |          This parameter is a shortcut for explicitly calling\n",
      " |          `.Axes.set_aspect`. See there for further details.\n",
      " |      \n",
      " |          - 'equal': Ensures an aspect ratio of 1. Pixels will be square\n",
      " |            (unless pixel sizes are explicitly made non-square in data\n",
      " |            coordinates using *extent*).\n",
      " |          - 'auto': The axes is kept fixed and the aspect is adjusted so\n",
      " |            that the data fit in the axes. In general, this will result in\n",
      " |            non-square pixels.\n",
      " |      \n",
      " |      interpolation : str, default: :rc:`image.interpolation`\n",
      " |          The interpolation method used.\n",
      " |      \n",
      " |          Supported values are 'none', 'antialiased', 'nearest', 'bilinear',\n",
      " |          'bicubic', 'spline16', 'spline36', 'hanning', 'hamming', 'hermite',\n",
      " |          'kaiser', 'quadric', 'catrom', 'gaussian', 'bessel', 'mitchell',\n",
      " |          'sinc', 'lanczos'.\n",
      " |      \n",
      " |          If *interpolation* is 'none', then no interpolation is performed\n",
      " |          on the Agg, ps, pdf and svg backends. Other backends will fall back\n",
      " |          to 'nearest'. Note that most SVG renderers perform interpolation at\n",
      " |          rendering and that the default interpolation method they implement\n",
      " |          may differ.\n",
      " |      \n",
      " |          If *interpolation* is the default 'antialiased', then 'nearest'\n",
      " |          interpolation is used if the image is upsampled by more than a\n",
      " |          factor of three (i.e. the number of display pixels is at least\n",
      " |          three times the size of the data array).  If the upsampling rate is\n",
      " |          smaller than 3, or the image is downsampled, then 'hanning'\n",
      " |          interpolation is used to act as an anti-aliasing filter, unless the\n",
      " |          image happens to be upsampled by exactly a factor of two or one.\n",
      " |      \n",
      " |          See\n",
      " |          :doc:`/gallery/images_contours_and_fields/interpolation_methods`\n",
      " |          for an overview of the supported interpolation methods, and\n",
      " |          :doc:`/gallery/images_contours_and_fields/image_antialiasing` for\n",
      " |          a discussion of image antialiasing.\n",
      " |      \n",
      " |          Some interpolation methods require an additional radius parameter,\n",
      " |          which can be set by *filterrad*. Additionally, the antigrain image\n",
      " |          resize filter is controlled by the parameter *filternorm*.\n",
      " |      \n",
      " |      alpha : float or array-like, optional\n",
      " |          The alpha blending value, between 0 (transparent) and 1 (opaque).\n",
      " |          If *alpha* is an array, the alpha blending values are applied pixel\n",
      " |          by pixel, and *alpha* must have the same shape as *X*.\n",
      " |      \n",
      " |      vmin, vmax : float, optional\n",
      " |          When using scalar data and no explicit *norm*, *vmin* and *vmax*\n",
      " |          define the data range that the colormap covers. By default,\n",
      " |          the colormap covers the complete value range of the supplied\n",
      " |          data. It is deprecated to use *vmin*/*vmax* when *norm* is given.\n",
      " |      \n",
      " |      origin : {'upper', 'lower'}, default: :rc:`image.origin`\n",
      " |          Place the [0, 0] index of the array in the upper left or lower\n",
      " |          left corner of the axes. The convention (the default) 'upper' is\n",
      " |          typically used for matrices and images.\n",
      " |      \n",
      " |          Note that the vertical axes points upward for 'lower'\n",
      " |          but downward for 'upper'.\n",
      " |      \n",
      " |          See the :doc:`/tutorials/intermediate/imshow_extent` tutorial for\n",
      " |          examples and a more detailed description.\n",
      " |      \n",
      " |      extent : floats (left, right, bottom, top), optional\n",
      " |          The bounding box in data coordinates that the image will fill.\n",
      " |          The image is stretched individually along x and y to fill the box.\n",
      " |      \n",
      " |          The default extent is determined by the following conditions.\n",
      " |          Pixels have unit size in data coordinates. Their centers are on\n",
      " |          integer coordinates, and their center coordinates range from 0 to\n",
      " |          columns-1 horizontally and from 0 to rows-1 vertically.\n",
      " |      \n",
      " |          Note that the direction of the vertical axis and thus the default\n",
      " |          values for top and bottom depend on *origin*:\n",
      " |      \n",
      " |          - For ``origin == 'upper'`` the default is\n",
      " |            ``(-0.5, numcols-0.5, numrows-0.5, -0.5)``.\n",
      " |          - For ``origin == 'lower'`` the default is\n",
      " |            ``(-0.5, numcols-0.5, -0.5, numrows-0.5)``.\n",
      " |      \n",
      " |          See the :doc:`/tutorials/intermediate/imshow_extent` tutorial for\n",
      " |          examples and a more detailed description.\n",
      " |      \n",
      " |      filternorm : bool, default: True\n",
      " |          A parameter for the antigrain image resize filter (see the\n",
      " |          antigrain documentation).  If *filternorm* is set, the filter\n",
      " |          normalizes integer values and corrects the rounding errors. It\n",
      " |          doesn't do anything with the source floating point values, it\n",
      " |          corrects only integers according to the rule of 1.0 which means\n",
      " |          that any sum of pixel weights must be equal to 1.0.  So, the\n",
      " |          filter function must produce a graph of the proper shape.\n",
      " |      \n",
      " |      filterrad : float > 0, default: 4.0\n",
      " |          The filter radius for filters that have a radius parameter, i.e.\n",
      " |          when interpolation is one of: 'sinc', 'lanczos' or 'blackman'.\n",
      " |      \n",
      " |      resample : bool, default: :rc:`image.resample`\n",
      " |          When *True*, use a full resampling method.  When *False*, only\n",
      " |          resample when the output image is larger than the input image.\n",
      " |      \n",
      " |      url : str, optional\n",
      " |          Set the url of the created `.AxesImage`. See `.Artist.set_url`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `~matplotlib.image.AxesImage`\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs : `~matplotlib.artist.Artist` properties\n",
      " |          These parameters are passed on to the constructor of the\n",
      " |          `.AxesImage` artist.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      matshow : Plot a matrix or an array as an image.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Unless *extent* is used, pixel centers will be located at integer\n",
      " |      coordinates. In other words: the origin will coincide with the center\n",
      " |      of pixel (0, 0).\n",
      " |      \n",
      " |      There are two common representations for RGB images with an alpha\n",
      " |      channel:\n",
      " |      \n",
      " |      -   Straight (unassociated) alpha: R, G, and B channels represent the\n",
      " |          color of the pixel, disregarding its opacity.\n",
      " |      -   Premultiplied (associated) alpha: R, G, and B channels represent\n",
      " |          the color of the pixel, adjusted for its opacity by multiplication.\n",
      " |      \n",
      " |      `~matplotlib.pyplot.imshow` expects RGB images adopting the straight\n",
      " |      (unassociated) alpha representation.\n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          every other argument can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception).\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  indicate_inset(self, bounds, inset_ax=None, *, transform=None, facecolor='none', edgecolor='0.5', alpha=0.5, zorder=4.99, **kwargs)\n",
      " |      Add an inset indicator to the axes.  This is a rectangle on the plot\n",
      " |      at the position indicated by *bounds* that optionally has lines that\n",
      " |      connect the rectangle to an inset axes (`.Axes.inset_axes`).\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      This method is experimental as of 3.0, and the API may change.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      bounds : [x0, y0, width, height]\n",
      " |          Lower-left corner of rectangle to be marked, and its width\n",
      " |          and height.\n",
      " |      \n",
      " |      inset_ax : `.Axes`\n",
      " |          An optional inset axes to draw connecting lines to.  Two lines are\n",
      " |          drawn connecting the indicator box to the inset axes on corners\n",
      " |          chosen so as to not overlap with the indicator box.\n",
      " |      \n",
      " |      transform : `.Transform`\n",
      " |          Transform for the rectangle coordinates. Defaults to\n",
      " |          `ax.transAxes`, i.e. the units of *rect* are in axes-relative\n",
      " |          coordinates.\n",
      " |      \n",
      " |      facecolor : color, default: 'none'\n",
      " |          Facecolor of the rectangle.\n",
      " |      \n",
      " |      edgecolor : color, default: '0.5'\n",
      " |          Color of the rectangle and color of the connecting lines.\n",
      " |      \n",
      " |      alpha : float, default: 0.5\n",
      " |          Transparency of the rectangle and connector lines.\n",
      " |      \n",
      " |      zorder : float, default: 4.99\n",
      " |          Drawing order of the rectangle and connector lines.  The default,\n",
      " |          4.99, is just below the default level of inset axes.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Other keyword arguments are passed on to the `.Rectangle` patch:\n",
      " |      \n",
      " |          %(Rectangle)s\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      rectangle_patch : `.patches.Rectangle`\n",
      " |           The indicator frame.\n",
      " |      \n",
      " |      connector_lines : 4-tuple of `.patches.ConnectionPatch`\n",
      " |          The four connector lines connecting to (lower_left, upper_left,\n",
      " |          lower_right upper_right) corners of *inset_ax*. Two lines are\n",
      " |          set with visibility to *False*,  but the user can set the\n",
      " |          visibility to True if the automatic choice is not deemed correct.\n",
      " |  \n",
      " |  indicate_inset_zoom(self, inset_ax, **kwargs)\n",
      " |      Add an inset indicator rectangle to the axes based on the axis\n",
      " |      limits for an *inset_ax* and draw connectors between *inset_ax*\n",
      " |      and the rectangle.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      This method is experimental as of 3.0, and the API may change.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      inset_ax : `.Axes`\n",
      " |          Inset axes to draw connecting lines to.  Two lines are\n",
      " |          drawn connecting the indicator box to the inset axes on corners\n",
      " |          chosen so as to not overlap with the indicator box.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Other keyword arguments are passed on to `.Axes.indicate_inset`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      rectangle_patch : `.patches.Rectangle`\n",
      " |           Rectangle artist.\n",
      " |      \n",
      " |      connector_lines : 4-tuple of `.patches.ConnectionPatch`\n",
      " |          Each of four connector lines coming from the rectangle drawn on\n",
      " |          this axis, in the order lower left, upper left, lower right,\n",
      " |          upper right.\n",
      " |          Two are set with visibility to *False*,  but the user can\n",
      " |          set the visibility to *True* if the automatic choice is not deemed\n",
      " |          correct.\n",
      " |  \n",
      " |  inset_axes(self, bounds, *, transform=None, zorder=5, **kwargs)\n",
      " |      Add a child inset axes to this existing axes.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      This method is experimental as of 3.0, and the API may change.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      bounds : [x0, y0, width, height]\n",
      " |          Lower-left corner of inset axes, and its width and height.\n",
      " |      \n",
      " |      transform : `.Transform`\n",
      " |          Defaults to `ax.transAxes`, i.e. the units of *rect* are in\n",
      " |          axes-relative coordinates.\n",
      " |      \n",
      " |      zorder : number\n",
      " |          Defaults to 5 (same as `.Axes.legend`).  Adjust higher or lower\n",
      " |          to change whether it is above or below data plotted on the\n",
      " |          parent axes.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Other keyword arguments are passed on to the child `.Axes`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ax\n",
      " |          The created `~.axes.Axes` instance.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      This example makes two inset axes, the first is in axes-relative\n",
      " |      coordinates, and the second in data-coordinates::\n",
      " |      \n",
      " |          fig, ax = plt.subplots()\n",
      " |          ax.plot(range(10))\n",
      " |          axin1 = ax.inset_axes([0.8, 0.1, 0.15, 0.15])\n",
      " |          axin2 = ax.inset_axes(\n",
      " |                  [5, 7, 2.3, 2.3], transform=ax.transData)\n",
      " |  \n",
      " |  legend(self, *args, **kwargs)\n",
      " |      Place a legend on the axes.\n",
      " |      \n",
      " |      Call signatures::\n",
      " |      \n",
      " |          legend()\n",
      " |          legend(labels)\n",
      " |          legend(handles, labels)\n",
      " |      \n",
      " |      The call signatures correspond to three different ways how to use\n",
      " |      this method.\n",
      " |      \n",
      " |      **1. Automatic detection of elements to be shown in the legend**\n",
      " |      \n",
      " |      The elements to be added to the legend are automatically determined,\n",
      " |      when you do not pass in any extra arguments.\n",
      " |      \n",
      " |      In this case, the labels are taken from the artist. You can specify\n",
      " |      them either at artist creation or by calling the\n",
      " |      :meth:`~.Artist.set_label` method on the artist::\n",
      " |      \n",
      " |          line, = ax.plot([1, 2, 3], label='Inline label')\n",
      " |          ax.legend()\n",
      " |      \n",
      " |      or::\n",
      " |      \n",
      " |          line, = ax.plot([1, 2, 3])\n",
      " |          line.set_label('Label via method')\n",
      " |          ax.legend()\n",
      " |      \n",
      " |      Specific lines can be excluded from the automatic legend element\n",
      " |      selection by defining a label starting with an underscore.\n",
      " |      This is default for all artists, so calling `.Axes.legend` without\n",
      " |      any arguments and without setting the labels manually will result in\n",
      " |      no legend being drawn.\n",
      " |      \n",
      " |      \n",
      " |      **2. Labeling existing plot elements**\n",
      " |      \n",
      " |      To make a legend for lines which already exist on the axes\n",
      " |      (via plot for instance), simply call this function with an iterable\n",
      " |      of strings, one for each legend item. For example::\n",
      " |      \n",
      " |          ax.plot([1, 2, 3])\n",
      " |          ax.legend(['A simple line'])\n",
      " |      \n",
      " |      Note: This way of using is discouraged, because the relation between\n",
      " |      plot elements and labels is only implicit by their order and can\n",
      " |      easily be mixed up.\n",
      " |      \n",
      " |      \n",
      " |      **3. Explicitly defining the elements in the legend**\n",
      " |      \n",
      " |      For full control of which artists have a legend entry, it is possible\n",
      " |      to pass an iterable of legend artists followed by an iterable of\n",
      " |      legend labels respectively::\n",
      " |      \n",
      " |          legend((line1, line2, line3), ('label1', 'label2', 'label3'))\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      handles : sequence of `.Artist`, optional\n",
      " |          A list of Artists (lines, patches) to be added to the legend.\n",
      " |          Use this together with *labels*, if you need full control on what\n",
      " |          is shown in the legend and the automatic mechanism described above\n",
      " |          is not sufficient.\n",
      " |      \n",
      " |          The length of handles and labels should be the same in this\n",
      " |          case. If they are not, they are truncated to the smaller length.\n",
      " |      \n",
      " |      labels : list of str, optional\n",
      " |          A list of labels to show next to the artists.\n",
      " |          Use this together with *handles*, if you need full control on what\n",
      " |          is shown in the legend and the automatic mechanism described above\n",
      " |          is not sufficient.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `~matplotlib.legend.Legend`\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      \n",
      " |      loc : str or pair of floats, default: :rc:`legend.loc` ('best' for axes, 'upper right' for figures)\n",
      " |          The location of the legend.\n",
      " |      \n",
      " |          The strings\n",
      " |          ``'upper left', 'upper right', 'lower left', 'lower right'``\n",
      " |          place the legend at the corresponding corner of the axes/figure.\n",
      " |      \n",
      " |          The strings\n",
      " |          ``'upper center', 'lower center', 'center left', 'center right'``\n",
      " |          place the legend at the center of the corresponding edge of the\n",
      " |          axes/figure.\n",
      " |      \n",
      " |          The string ``'center'`` places the legend at the center of the axes/figure.\n",
      " |      \n",
      " |          The string ``'best'`` places the legend at the location, among the nine\n",
      " |          locations defined so far, with the minimum overlap with other drawn\n",
      " |          artists.  This option can be quite slow for plots with large amounts of\n",
      " |          data; your plotting speed may benefit from providing a specific location.\n",
      " |      \n",
      " |          The location can also be a 2-tuple giving the coordinates of the lower-left\n",
      " |          corner of the legend in axes coordinates (in which case *bbox_to_anchor*\n",
      " |          will be ignored).\n",
      " |      \n",
      " |          For back-compatibility, ``'center right'`` (but no other location) can also\n",
      " |          be spelled ``'right'``, and each \"string\" locations can also be given as a\n",
      " |          numeric value:\n",
      " |      \n",
      " |              ===============   =============\n",
      " |              Location String   Location Code\n",
      " |              ===============   =============\n",
      " |              'best'            0\n",
      " |              'upper right'     1\n",
      " |              'upper left'      2\n",
      " |              'lower left'      3\n",
      " |              'lower right'     4\n",
      " |              'right'           5\n",
      " |              'center left'     6\n",
      " |              'center right'    7\n",
      " |              'lower center'    8\n",
      " |              'upper center'    9\n",
      " |              'center'          10\n",
      " |              ===============   =============\n",
      " |      \n",
      " |      bbox_to_anchor : `.BboxBase`, 2-tuple, or 4-tuple of floats\n",
      " |          Box that is used to position the legend in conjunction with *loc*.\n",
      " |          Defaults to `axes.bbox` (if called as a method to `.Axes.legend`) or\n",
      " |          `figure.bbox` (if `.Figure.legend`).  This argument allows arbitrary\n",
      " |          placement of the legend.\n",
      " |      \n",
      " |          Bbox coordinates are interpreted in the coordinate system given by\n",
      " |          *bbox_transform*, with the default transform\n",
      " |          Axes or Figure coordinates, depending on which ``legend`` is called.\n",
      " |      \n",
      " |          If a 4-tuple or `.BboxBase` is given, then it specifies the bbox\n",
      " |          ``(x, y, width, height)`` that the legend is placed in.\n",
      " |          To put the legend in the best location in the bottom right\n",
      " |          quadrant of the axes (or figure)::\n",
      " |      \n",
      " |              loc='best', bbox_to_anchor=(0.5, 0., 0.5, 0.5)\n",
      " |      \n",
      " |          A 2-tuple ``(x, y)`` places the corner of the legend specified by *loc* at\n",
      " |          x, y.  For example, to put the legend's upper right-hand corner in the\n",
      " |          center of the axes (or figure) the following keywords can be used::\n",
      " |      \n",
      " |              loc='upper right', bbox_to_anchor=(0.5, 0.5)\n",
      " |      \n",
      " |      ncol : int, default: 1\n",
      " |          The number of columns that the legend has.\n",
      " |      \n",
      " |      prop : None or `matplotlib.font_manager.FontProperties` or dict\n",
      " |          The font properties of the legend. If None (default), the current\n",
      " |          :data:`matplotlib.rcParams` will be used.\n",
      " |      \n",
      " |      fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}\n",
      " |          The font size of the legend. If the value is numeric the size will be the\n",
      " |          absolute font size in points. String values are relative to the current\n",
      " |          default font size. This argument is only used if *prop* is not specified.\n",
      " |      \n",
      " |      labelcolor : str or list\n",
      " |          Sets the color of the text in the legend. Can be a valid color string\n",
      " |          (for example, 'red'), or a list of color strings. The labelcolor can\n",
      " |          also be made to match the color of the line or marker using 'linecolor',\n",
      " |          'markerfacecolor' (or 'mfc'), or 'markeredgecolor' (or 'mec').\n",
      " |      \n",
      " |      numpoints : int, default: :rc:`legend.numpoints`\n",
      " |          The number of marker points in the legend when creating a legend\n",
      " |          entry for a `.Line2D` (line).\n",
      " |      \n",
      " |      scatterpoints : int, default: :rc:`legend.scatterpoints`\n",
      " |          The number of marker points in the legend when creating\n",
      " |          a legend entry for a `.PathCollection` (scatter plot).\n",
      " |      \n",
      " |      scatteryoffsets : iterable of floats, default: ``[0.375, 0.5, 0.3125]``\n",
      " |          The vertical offset (relative to the font size) for the markers\n",
      " |          created for a scatter plot legend entry. 0.0 is at the base the\n",
      " |          legend text, and 1.0 is at the top. To draw all markers at the\n",
      " |          same height, set to ``[0.5]``.\n",
      " |      \n",
      " |      markerscale : float, default: :rc:`legend.markerscale`\n",
      " |          The relative size of legend markers compared with the originally\n",
      " |          drawn ones.\n",
      " |      \n",
      " |      markerfirst : bool, default: True\n",
      " |          If *True*, legend marker is placed to the left of the legend label.\n",
      " |          If *False*, legend marker is placed to the right of the legend label.\n",
      " |      \n",
      " |      frameon : bool, default: :rc:`legend.frameon`\n",
      " |          Whether the legend should be drawn on a patch (frame).\n",
      " |      \n",
      " |      fancybox : bool, default: :rc:`legend.fancybox`\n",
      " |          Whether round edges should be enabled around the `~.FancyBboxPatch` which\n",
      " |          makes up the legend's background.\n",
      " |      \n",
      " |      shadow : bool, default: :rc:`legend.shadow`\n",
      " |          Whether to draw a shadow behind the legend.\n",
      " |      \n",
      " |      framealpha : float, default: :rc:`legend.framealpha`\n",
      " |          The alpha transparency of the legend's background.\n",
      " |          If *shadow* is activated and *framealpha* is ``None``, the default value is\n",
      " |          ignored.\n",
      " |      \n",
      " |      facecolor : \"inherit\" or color, default: :rc:`legend.facecolor`\n",
      " |          The legend's background color.\n",
      " |          If ``\"inherit\"``, use :rc:`axes.facecolor`.\n",
      " |      \n",
      " |      edgecolor : \"inherit\" or color, default: :rc:`legend.edgecolor`\n",
      " |          The legend's background patch edge color.\n",
      " |          If ``\"inherit\"``, use take :rc:`axes.edgecolor`.\n",
      " |      \n",
      " |      mode : {\"expand\", None}\n",
      " |          If *mode* is set to ``\"expand\"`` the legend will be horizontally\n",
      " |          expanded to fill the axes area (or *bbox_to_anchor* if defines\n",
      " |          the legend's size).\n",
      " |      \n",
      " |      bbox_transform : None or `matplotlib.transforms.Transform`\n",
      " |          The transform for the bounding box (*bbox_to_anchor*). For a value\n",
      " |          of ``None`` (default) the Axes'\n",
      " |          :data:`~matplotlib.axes.Axes.transAxes` transform will be used.\n",
      " |      \n",
      " |      title : str or None\n",
      " |          The legend's title. Default is no title (``None``).\n",
      " |      \n",
      " |      title_fontsize : int or {'xx-small', 'x-small', 'small', 'medium', 'large', 'x-large', 'xx-large'}, default: :rc:`legend.title_fontsize`\n",
      " |          The font size of the legend's title.\n",
      " |      \n",
      " |      borderpad : float, default: :rc:`legend.borderpad`\n",
      " |          The fractional whitespace inside the legend border, in font-size units.\n",
      " |      \n",
      " |      labelspacing : float, default: :rc:`legend.labelspacing`\n",
      " |          The vertical space between the legend entries, in font-size units.\n",
      " |      \n",
      " |      handlelength : float, default: :rc:`legend.handlelength`\n",
      " |          The length of the legend handles, in font-size units.\n",
      " |      \n",
      " |      handletextpad : float, default: :rc:`legend.handletextpad`\n",
      " |          The pad between the legend handle and text, in font-size units.\n",
      " |      \n",
      " |      borderaxespad : float, default: :rc:`legend.borderaxespad`\n",
      " |          The pad between the axes and legend border, in font-size units.\n",
      " |      \n",
      " |      columnspacing : float, default: :rc:`legend.columnspacing`\n",
      " |          The spacing between columns, in font-size units.\n",
      " |      \n",
      " |      handler_map : dict or None\n",
      " |          The custom dictionary mapping instances or types to a legend\n",
      " |          handler. This *handler_map* updates the default handler map\n",
      " |          found at `matplotlib.legend.Legend.get_legend_handler_map`.\n",
      " |      \n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Some artists are not supported by this function.  See\n",
      " |      :doc:`/tutorials/intermediate/legend_guide` for details.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      .. plot:: gallery/text_labels_and_annotations/legend.py\n",
      " |  \n",
      " |  loglog(self, *args, **kwargs)\n",
      " |      Make a plot with log scaling on both the x and y axis.\n",
      " |      \n",
      " |      Call signatures::\n",
      " |      \n",
      " |          loglog([x], y, [fmt], data=None, **kwargs)\n",
      " |          loglog([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n",
      " |      \n",
      " |      This is just a thin wrapper around `.plot` which additionally changes\n",
      " |      both the x-axis and the y-axis to log scaling. All of the concepts and\n",
      " |      parameters of plot can be used here as well.\n",
      " |      \n",
      " |      The additional parameters *base*, *subs* and *nonpositive* control the\n",
      " |      x/y-axis properties. They are just forwarded to `.Axes.set_xscale` and\n",
      " |      `.Axes.set_yscale`. To use different properties on the x-axis and the\n",
      " |      y-axis, use e.g.\n",
      " |      ``ax.set_xscale(\"log\", base=10); ax.set_yscale(\"log\", base=2)``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      base : float, default: 10\n",
      " |          Base of the logarithm.\n",
      " |      \n",
      " |      subs : sequence, optional\n",
      " |          The location of the minor ticks. If *None*, reasonable locations\n",
      " |          are automatically chosen depending on the number of decades in the\n",
      " |          plot. See `.Axes.set_xscale`/`.Axes.set_yscale` for details.\n",
      " |      \n",
      " |      nonpositive : {'mask', 'clip'}, default: 'mask'\n",
      " |          Non-positive values can be masked as invalid, or clipped to a very\n",
      " |          small positive number.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      lines\n",
      " |          A list of `.Line2D` objects representing the plotted data.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs\n",
      " |          All parameters supported by `.plot`.\n",
      " |  \n",
      " |  magnitude_spectrum(self, x, Fs=None, Fc=None, window=None, pad_to=None, sides=None, scale=None, *, data=None, **kwargs)\n",
      " |      Plot the magnitude spectrum.\n",
      " |      \n",
      " |      Compute the magnitude spectrum of *x*.  Data is padded to a\n",
      " |      length of *pad_to* and the windowing function *window* is applied to\n",
      " |      the signal.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : 1-D array or sequence\n",
      " |          Array or sequence containing the data.\n",
      " |      \n",
      " |      Fs : float, default: 2\n",
      " |          The sampling frequency (samples per time unit).  It is used to calculate\n",
      " |          the Fourier frequencies, *freqs*, in cycles per time unit.\n",
      " |      \n",
      " |      window : callable or ndarray, default: `.window_hanning`\n",
      " |          A function or a vector of length *NFFT*.  To create window vectors see\n",
      " |          `.window_hanning`, `.window_none`, `numpy.blackman`, `numpy.hamming`,\n",
      " |          `numpy.bartlett`, `scipy.signal`, `scipy.signal.get_window`, etc.  If a\n",
      " |          function is passed as the argument, it must take a data segment as an\n",
      " |          argument and return the windowed version of the segment.\n",
      " |      \n",
      " |      sides : {'default', 'onesided', 'twosided'}, optional\n",
      " |          Which sides of the spectrum to return. 'default' is one-sided for real\n",
      " |          data and two-sided for complex data. 'onesided' forces the return of a\n",
      " |          one-sided spectrum, while 'twosided' forces two-sided.\n",
      " |      \n",
      " |      pad_to : int, optional\n",
      " |          The number of points to which the data segment is padded when performing\n",
      " |          the FFT.  While not increasing the actual resolution of the spectrum (the\n",
      " |          minimum distance between resolvable peaks), this can give more points in\n",
      " |          the plot, allowing for more detail. This corresponds to the *n* parameter\n",
      " |          in the call to fft().  The default is None, which sets *pad_to* equal to\n",
      " |          the length of the input signal (i.e. no padding).\n",
      " |      \n",
      " |      scale : {'default', 'linear', 'dB'}\n",
      " |          The scaling of the values in the *spec*.  'linear' is no scaling.\n",
      " |          'dB' returns the values in dB scale, i.e., the dB amplitude\n",
      " |          (20 * log10). 'default' is 'linear'.\n",
      " |      \n",
      " |      Fc : int, default: 0\n",
      " |          The center frequency of *x*, which offsets the x extents of the\n",
      " |          plot to reflect the frequency range used when a signal is acquired\n",
      " |          and then filtered and downsampled to baseband.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      spectrum : 1-D array\n",
      " |          The values for the magnitude spectrum before scaling (real valued).\n",
      " |      \n",
      " |      freqs : 1-D array\n",
      " |          The frequencies corresponding to the elements in *spectrum*.\n",
      " |      \n",
      " |      line : `~matplotlib.lines.Line2D`\n",
      " |          The line created by this function.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs\n",
      " |          Keyword arguments control the `.Line2D` properties:\n",
      " |      \n",
      " |          Properties:\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          animated: bool\n",
      " |          antialiased or aa: bool\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          color or c: color\n",
      " |          contains: unknown\n",
      " |          dash_capstyle: {'butt', 'round', 'projecting'}\n",
      " |          dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      " |          data: (2, N) array or two 1D arrays\n",
      " |          drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      " |          figure: `.Figure`\n",
      " |          fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      " |          gid: str\n",
      " |          in_layout: bool\n",
      " |          label: object\n",
      " |          linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      " |          linewidth or lw: float\n",
      " |          marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      " |          markeredgecolor or mec: color\n",
      " |          markeredgewidth or mew: float\n",
      " |          markerfacecolor or mfc: color\n",
      " |          markerfacecoloralt or mfcalt: color\n",
      " |          markersize or ms: float\n",
      " |          markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: unknown\n",
      " |          pickradius: float\n",
      " |          rasterized: bool or None\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          solid_capstyle: {'butt', 'round', 'projecting'}\n",
      " |          solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          transform: `matplotlib.transforms.Transform`\n",
      " |          url: str\n",
      " |          visible: bool\n",
      " |          xdata: 1D array\n",
      " |          ydata: 1D array\n",
      " |          zorder: float\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      psd\n",
      " |          Plots the power spectral density.\n",
      " |      angle_spectrum\n",
      " |          Plots the angles of the corresponding frequencies.\n",
      " |      phase_spectrum\n",
      " |          Plots the phase (unwrapped angle) of the corresponding frequencies.\n",
      " |      specgram\n",
      " |          Can plot the magnitude spectrum of segments within the signal in a\n",
      " |          colormap.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      \n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          the following arguments can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception):\n",
      " |          *x*.\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  matshow(self, Z, **kwargs)\n",
      " |      Plot the values of a 2D matrix or array as color-coded image.\n",
      " |      \n",
      " |      The matrix will be shown the way it would be printed, with the first\n",
      " |      row at the top.  Row and column numbering is zero-based.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      Z : array-like(M, N)\n",
      " |          The matrix to be displayed.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `~matplotlib.image.AxesImage`\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs : `~matplotlib.axes.Axes.imshow` arguments\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      imshow : More general function to plot data on a 2D regular raster.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is just a convenience function wrapping `.imshow` to set useful\n",
      " |      defaults for displaying a matrix. In particular:\n",
      " |      \n",
      " |      - Set ``origin='upper'``.\n",
      " |      - Set ``interpolation='nearest'``.\n",
      " |      - Set ``aspect='equal'``.\n",
      " |      - Ticks are placed to the left and above.\n",
      " |      - Ticks are formatted to show integer indices.\n",
      " |  \n",
      " |  pcolor(self, *args, shading=None, alpha=None, norm=None, cmap=None, vmin=None, vmax=None, data=None, **kwargs)\n",
      " |      Create a pseudocolor plot with a non-regular rectangular grid.\n",
      " |      \n",
      " |      Call signature::\n",
      " |      \n",
      " |          pcolor([X, Y,] C, **kwargs)\n",
      " |      \n",
      " |      *X* and *Y* can be used to specify the corners of the quadrilaterals.\n",
      " |      \n",
      " |      .. hint::\n",
      " |      \n",
      " |          ``pcolor()`` can be very slow for large arrays. In most\n",
      " |          cases you should use the similar but much faster\n",
      " |          `~.Axes.pcolormesh` instead. See\n",
      " |          :ref:`Differences between pcolor() and pcolormesh()\n",
      " |          <differences-pcolor-pcolormesh>` for a discussion of the\n",
      " |          differences.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      C : array-like\n",
      " |          A scalar 2-D array. The values will be color-mapped.\n",
      " |      \n",
      " |      X, Y : array-like, optional\n",
      " |          The coordinates of the corners of quadrilaterals of a pcolormesh::\n",
      " |      \n",
      " |              (X[i+1, j], Y[i+1, j])       (X[i+1, j+1], Y[i+1, j+1])\n",
      " |                                    +-----+\n",
      " |                                    |     |\n",
      " |                                    +-----+\n",
      " |                  (X[i, j], Y[i, j])       (X[i, j+1], Y[i, j+1])\n",
      " |      \n",
      " |          Note that the column index corresponds to the x-coordinate, and\n",
      " |          the row index corresponds to y. For details, see the\n",
      " |          :ref:`Notes <axes-pcolormesh-grid-orientation>` section below.\n",
      " |      \n",
      " |          If ``shading='flat'`` the dimensions of *X* and *Y* should be one\n",
      " |          greater than those of *C*, and the quadrilateral is colored due\n",
      " |          to the value at ``C[i, j]``.  If *X*, *Y* and *C* have equal\n",
      " |          dimensions, a warning will be raised and the last row and column\n",
      " |          of *C* will be ignored.\n",
      " |      \n",
      " |          If ``shading='nearest'``, the dimensions of *X* and *Y* should be\n",
      " |          the same as those of *C* (if not, a ValueError will be raised). The\n",
      " |          color ``C[i, j]`` will be centered on ``(X[i, j], Y[i, j])``.\n",
      " |      \n",
      " |          If *X* and/or *Y* are 1-D arrays or column vectors they will be\n",
      " |          expanded as needed into the appropriate 2-D arrays, making a\n",
      " |          rectangular grid.\n",
      " |      \n",
      " |      shading : {'flat', 'nearest', 'auto'}, optional\n",
      " |          The fill style for the quadrilateral; defaults to 'flat' or\n",
      " |          :rc:`pcolor.shading`. Possible values:\n",
      " |      \n",
      " |          - 'flat': A solid color is used for each quad. The color of the\n",
      " |            quad (i, j), (i+1, j), (i, j+1), (i+1, j+1) is given by\n",
      " |            ``C[i, j]``. The dimensions of *X* and *Y* should be\n",
      " |            one greater than those of *C*; if they are the same as *C*,\n",
      " |            then a deprecation warning is raised, and the last row\n",
      " |            and column of *C* are dropped.\n",
      " |          - 'nearest': Each grid point will have a color centered on it,\n",
      " |            extending halfway between the adjacent grid centers.  The\n",
      " |            dimensions of *X* and *Y* must be the same as *C*.\n",
      " |          - 'auto': Choose 'flat' if dimensions of *X* and *Y* are one\n",
      " |            larger than *C*.  Choose 'nearest' if dimensions are the same.\n",
      " |      \n",
      " |          See :doc:`/gallery/images_contours_and_fields/pcolormesh_grids`\n",
      " |          for more description.\n",
      " |      \n",
      " |      cmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n",
      " |          A Colormap instance or registered colormap name. The colormap\n",
      " |          maps the *C* values to colors.\n",
      " |      \n",
      " |      norm : `~matplotlib.colors.Normalize`, optional\n",
      " |          The Normalize instance scales the data values to the canonical\n",
      " |          colormap range [0, 1] for mapping to colors. By default, the data\n",
      " |          range is mapped to the colorbar range using linear scaling.\n",
      " |      \n",
      " |      vmin, vmax : float, default: None\n",
      " |          The colorbar range. If *None*, suitable min/max values are\n",
      " |          automatically chosen by the `~.Normalize` instance (defaults to\n",
      " |          the respective min/max values of *C* in case of the default linear\n",
      " |          scaling).\n",
      " |          It is deprecated to use *vmin*/*vmax* when *norm* is given.\n",
      " |      \n",
      " |      edgecolors : {'none', None, 'face', color, color sequence}, optional\n",
      " |          The color of the edges. Defaults to 'none'. Possible values:\n",
      " |      \n",
      " |          - 'none' or '': No edge.\n",
      " |          - *None*: :rc:`patch.edgecolor` will be used. Note that currently\n",
      " |            :rc:`patch.force_edgecolor` has to be True for this to work.\n",
      " |          - 'face': Use the adjacent face color.\n",
      " |          - A color or sequence of colors will set the edge color.\n",
      " |      \n",
      " |          The singular form *edgecolor* works as an alias.\n",
      " |      \n",
      " |      alpha : float, default: None\n",
      " |          The alpha blending value of the face color, between 0 (transparent)\n",
      " |          and 1 (opaque). Note: The edgecolor is currently not affected by\n",
      " |          this.\n",
      " |      \n",
      " |      snap : bool, default: False\n",
      " |          Whether to snap the mesh to pixel boundaries.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `matplotlib.collections.Collection`\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      antialiaseds : bool, default: False\n",
      " |          The default *antialiaseds* is False if the default\n",
      " |          *edgecolors*\\ =\"none\" is used.  This eliminates artificial lines\n",
      " |          at patch boundaries, and works regardless of the value of alpha.\n",
      " |          If *edgecolors* is not \"none\", then the default *antialiaseds*\n",
      " |          is taken from :rc:`patch.antialiased`.\n",
      " |          Stroking the edges may be preferred if *alpha* is 1, but will\n",
      " |          cause artifacts otherwise.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Additionally, the following arguments are allowed. They are passed\n",
      " |          along to the `~matplotlib.collections.PolyCollection` constructor:\n",
      " |      \n",
      " |      Properties:\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          animated: bool\n",
      " |          antialiased or aa or antialiaseds: bool or list of bools\n",
      " |          array: ndarray\n",
      " |          capstyle: {'butt', 'round', 'projecting'}\n",
      " |          clim: (vmin: float, vmax: float)\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          cmap: `.Colormap` or str or None\n",
      " |          color: color or list of rgba tuples\n",
      " |          contains: unknown\n",
      " |          edgecolor or ec or edgecolors: color or list of colors or 'face'\n",
      " |          facecolor or facecolors or fc: color or list of colors\n",
      " |          figure: `.Figure`\n",
      " |          gid: str\n",
      " |          hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n",
      " |          in_layout: bool\n",
      " |          joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          label: object\n",
      " |          linestyle or dashes or linestyles or ls: str or tuple or list thereof\n",
      " |          linewidth or linewidths or lw: float or list of floats\n",
      " |          norm: `.Normalize` or None\n",
      " |          offset_position: unknown\n",
      " |          offsets: array-like (N, 2) or (2,)\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: None or bool or callable\n",
      " |          pickradius: unknown\n",
      " |          rasterized: bool or None\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          transform: `.Transform`\n",
      " |          url: str\n",
      " |          urls: list of str or None\n",
      " |          visible: bool\n",
      " |          zorder: float\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pcolormesh : for an explanation of the differences between\n",
      " |          pcolor and pcolormesh.\n",
      " |      imshow : If *X* and *Y* are each equidistant, `~.Axes.imshow` can be a\n",
      " |          faster alternative.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      **Masked arrays**\n",
      " |      \n",
      " |      *X*, *Y* and *C* may be masked arrays. If either ``C[i, j]``, or one\n",
      " |      of the vertices surrounding ``C[i, j]`` (*X* or *Y* at\n",
      " |      ``[i, j], [i+1, j], [i, j+1], [i+1, j+1]``) is masked, nothing is\n",
      " |      plotted.\n",
      " |      \n",
      " |      .. _axes-pcolor-grid-orientation:\n",
      " |      \n",
      " |      **Grid orientation**\n",
      " |      \n",
      " |      The grid orientation follows the standard matrix convention: An array\n",
      " |      *C* with shape (nrows, ncolumns) is plotted with the column number as\n",
      " |      *X* and the row number as *Y*.\n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          every other argument can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception).\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  pcolorfast(self, *args, alpha=None, norm=None, cmap=None, vmin=None, vmax=None, data=None, **kwargs)\n",
      " |      Create a pseudocolor plot with a non-regular rectangular grid.\n",
      " |      \n",
      " |      Call signature::\n",
      " |      \n",
      " |        ax.pcolorfast([X, Y], C, /, **kwargs)\n",
      " |      \n",
      " |      This method is similar to `~.Axes.pcolor` and `~.Axes.pcolormesh`.\n",
      " |      It's designed to provide the fastest pcolor-type plotting with the\n",
      " |      Agg backend. To achieve this, it uses different algorithms internally\n",
      " |      depending on the complexity of the input grid (regular rectangular,\n",
      " |      non-regular rectangular or arbitrary quadrilateral).\n",
      " |      \n",
      " |      .. warning::\n",
      " |      \n",
      " |         This method is experimental. Compared to `~.Axes.pcolor` or\n",
      " |         `~.Axes.pcolormesh` it has some limitations:\n",
      " |      \n",
      " |         - It supports only flat shading (no outlines)\n",
      " |         - It lacks support for log scaling of the axes.\n",
      " |         - It does not have a have a pyplot wrapper.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      C : array-like(M, N)\n",
      " |          The image data. Supported array shapes are:\n",
      " |      \n",
      " |          - (M, N): an image with scalar data. The data is visualized\n",
      " |            using a colormap.\n",
      " |          - (M, N, 3): an image with RGB values (0-1 float or 0-255 int).\n",
      " |          - (M, N, 4): an image with RGBA values (0-1 float or 0-255 int),\n",
      " |            i.e. including transparency.\n",
      " |      \n",
      " |          The first two dimensions (M, N) define the rows and columns of\n",
      " |          the image.\n",
      " |      \n",
      " |          This parameter can only be passed positionally.\n",
      " |      \n",
      " |      X, Y : tuple or array-like, default: ``(0, N)``, ``(0, M)``\n",
      " |          *X* and *Y* are used to specify the coordinates of the\n",
      " |          quadrilaterals. There are different ways to do this:\n",
      " |      \n",
      " |          - Use tuples ``X=(xmin, xmax)`` and ``Y=(ymin, ymax)`` to define\n",
      " |            a *uniform rectangular grid*.\n",
      " |      \n",
      " |            The tuples define the outer edges of the grid. All individual\n",
      " |            quadrilaterals will be of the same size. This is the fastest\n",
      " |            version.\n",
      " |      \n",
      " |          - Use 1D arrays *X*, *Y* to specify a *non-uniform rectangular\n",
      " |            grid*.\n",
      " |      \n",
      " |            In this case *X* and *Y* have to be monotonic 1D arrays of length\n",
      " |            *N+1* and *M+1*, specifying the x and y boundaries of the cells.\n",
      " |      \n",
      " |            The speed is intermediate. Note: The grid is checked, and if\n",
      " |            found to be uniform the fast version is used.\n",
      " |      \n",
      " |          - Use 2D arrays *X*, *Y* if you need an *arbitrary quadrilateral\n",
      " |            grid* (i.e. if the quadrilaterals are not rectangular).\n",
      " |      \n",
      " |            In this case *X* and *Y* are 2D arrays with shape (M + 1, N + 1),\n",
      " |            specifying the x and y coordinates of the corners of the colored\n",
      " |            quadrilaterals.\n",
      " |      \n",
      " |            This is the most general, but the slowest to render.  It may\n",
      " |            produce faster and more compact output using ps, pdf, and\n",
      " |            svg backends, however.\n",
      " |      \n",
      " |          These arguments can only be passed positionally.\n",
      " |      \n",
      " |      cmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n",
      " |          A Colormap instance or registered colormap name. The colormap\n",
      " |          maps the *C* values to colors.\n",
      " |      \n",
      " |      norm : `~matplotlib.colors.Normalize`, optional\n",
      " |          The Normalize instance scales the data values to the canonical\n",
      " |          colormap range [0, 1] for mapping to colors. By default, the data\n",
      " |          range is mapped to the colorbar range using linear scaling.\n",
      " |      \n",
      " |      vmin, vmax : float, default: None\n",
      " |          The colorbar range. If *None*, suitable min/max values are\n",
      " |          automatically chosen by the `~.Normalize` instance (defaults to\n",
      " |          the respective min/max values of *C* in case of the default linear\n",
      " |          scaling).\n",
      " |          It is deprecated to use *vmin*/*vmax* when *norm* is given.\n",
      " |      \n",
      " |      alpha : float, default: None\n",
      " |          The alpha blending value, between 0 (transparent) and 1 (opaque).\n",
      " |      \n",
      " |      snap : bool, default: False\n",
      " |          Whether to snap the mesh to pixel boundaries.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `.AxesImage` or `.PcolorImage` or `.QuadMesh`\n",
      " |          The return type depends on the type of grid:\n",
      " |      \n",
      " |          - `.AxesImage` for a regular rectangular grid.\n",
      " |          - `.PcolorImage` for a non-regular rectangular grid.\n",
      " |          - `.QuadMesh` for a non-rectangular grid.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs\n",
      " |          Supported additional parameters depend on the type of grid.\n",
      " |          See return types of *image* for further description.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. [notes section required to get data note injection right]\n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          every other argument can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception).\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  pcolormesh(self, *args, alpha=None, norm=None, cmap=None, vmin=None, vmax=None, shading=None, antialiased=False, data=None, **kwargs)\n",
      " |      Create a pseudocolor plot with a non-regular rectangular grid.\n",
      " |      \n",
      " |      Call signature::\n",
      " |      \n",
      " |          pcolormesh([X, Y,] C, **kwargs)\n",
      " |      \n",
      " |      *X* and *Y* can be used to specify the corners of the quadrilaterals.\n",
      " |      \n",
      " |      .. hint::\n",
      " |      \n",
      " |         `~.Axes.pcolormesh` is similar to `~.Axes.pcolor`. It is much faster\n",
      " |         and preferred in most cases. For a detailed discussion on the\n",
      " |         differences see :ref:`Differences between pcolor() and pcolormesh()\n",
      " |         <differences-pcolor-pcolormesh>`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      C : array-like\n",
      " |          A scalar 2-D array. The values will be color-mapped.\n",
      " |      \n",
      " |      X, Y : array-like, optional\n",
      " |          The coordinates of the corners of quadrilaterals of a pcolormesh::\n",
      " |      \n",
      " |              (X[i+1, j], Y[i+1, j])       (X[i+1, j+1], Y[i+1, j+1])\n",
      " |                                    +-----+\n",
      " |                                    |     |\n",
      " |                                    +-----+\n",
      " |                  (X[i, j], Y[i, j])       (X[i, j+1], Y[i, j+1])\n",
      " |      \n",
      " |          Note that the column index corresponds to the x-coordinate, and\n",
      " |          the row index corresponds to y. For details, see the\n",
      " |          :ref:`Notes <axes-pcolormesh-grid-orientation>` section below.\n",
      " |      \n",
      " |          If ``shading='flat'`` the dimensions of *X* and *Y* should be one\n",
      " |          greater than those of *C*, and the quadrilateral is colored due\n",
      " |          to the value at ``C[i, j]``.  If *X*, *Y* and *C* have equal\n",
      " |          dimensions, a warning will be raised and the last row and column\n",
      " |          of *C* will be ignored.\n",
      " |      \n",
      " |          If ``shading='nearest'`` or ``'gouraud'``, the dimensions of *X*\n",
      " |          and *Y* should be the same as those of *C* (if not, a ValueError\n",
      " |          will be raised).  For ``'nearest'`` the color ``C[i, j]`` is\n",
      " |          centered on ``(X[i, j], Y[i, j])``.  For ``'gouraud'``, a smooth\n",
      " |          interpolation is caried out between the quadrilateral corners.\n",
      " |      \n",
      " |          If *X* and/or *Y* are 1-D arrays or column vectors they will be\n",
      " |          expanded as needed into the appropriate 2-D arrays, making a\n",
      " |          rectangular grid.\n",
      " |      \n",
      " |      cmap : str or `~matplotlib.colors.Colormap`, default: :rc:`image.cmap`\n",
      " |          A Colormap instance or registered colormap name. The colormap\n",
      " |          maps the *C* values to colors.\n",
      " |      \n",
      " |      norm : `~matplotlib.colors.Normalize`, optional\n",
      " |          The Normalize instance scales the data values to the canonical\n",
      " |          colormap range [0, 1] for mapping to colors. By default, the data\n",
      " |          range is mapped to the colorbar range using linear scaling.\n",
      " |      \n",
      " |      vmin, vmax : float, default: None\n",
      " |          The colorbar range. If *None*, suitable min/max values are\n",
      " |          automatically chosen by the `~.Normalize` instance (defaults to\n",
      " |          the respective min/max values of *C* in case of the default linear\n",
      " |          scaling).\n",
      " |          It is deprecated to use *vmin*/*vmax* when *norm* is given.\n",
      " |      \n",
      " |      edgecolors : {'none', None, 'face', color, color sequence}, optional\n",
      " |          The color of the edges. Defaults to 'none'. Possible values:\n",
      " |      \n",
      " |          - 'none' or '': No edge.\n",
      " |          - *None*: :rc:`patch.edgecolor` will be used. Note that currently\n",
      " |            :rc:`patch.force_edgecolor` has to be True for this to work.\n",
      " |          - 'face': Use the adjacent face color.\n",
      " |          - A color or sequence of colors will set the edge color.\n",
      " |      \n",
      " |          The singular form *edgecolor* works as an alias.\n",
      " |      \n",
      " |      alpha : float, default: None\n",
      " |          The alpha blending value, between 0 (transparent) and 1 (opaque).\n",
      " |      \n",
      " |      shading : {'flat', 'nearest', 'gouraud', 'auto'}, optional\n",
      " |          The fill style for the quadrilateral; defaults to\n",
      " |          'flat' or :rc:`pcolor.shading`. Possible values:\n",
      " |      \n",
      " |          - 'flat': A solid color is used for each quad. The color of the\n",
      " |            quad (i, j), (i+1, j), (i, j+1), (i+1, j+1) is given by\n",
      " |            ``C[i, j]``. The dimensions of *X* and *Y* should be\n",
      " |            one greater than those of *C*; if they are the same as *C*,\n",
      " |            then a deprecation warning is raised, and the last row\n",
      " |            and column of *C* are dropped.\n",
      " |          - 'nearest': Each grid point will have a color centered on it,\n",
      " |            extending halfway between the adjacent grid centers.  The\n",
      " |            dimensions of *X* and *Y* must be the same as *C*.\n",
      " |          - 'gouraud': Each quad will be Gouraud shaded: The color of the\n",
      " |            corners (i', j') are given by ``C[i', j']``. The color values of\n",
      " |            the area in between is interpolated from the corner values.\n",
      " |            The dimensions of *X* and *Y* must be the same as *C*. When\n",
      " |            Gouraud shading is used, *edgecolors* is ignored.\n",
      " |          - 'auto': Choose 'flat' if dimensions of *X* and *Y* are one\n",
      " |            larger than *C*.  Choose 'nearest' if dimensions are the same.\n",
      " |      \n",
      " |          See :doc:`/gallery/images_contours_and_fields/pcolormesh_grids`\n",
      " |          for more description.\n",
      " |      \n",
      " |      snap : bool, default: False\n",
      " |          Whether to snap the mesh to pixel boundaries.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `matplotlib.collections.QuadMesh`\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs\n",
      " |          Additionally, the following arguments are allowed. They are passed\n",
      " |          along to the `~matplotlib.collections.QuadMesh` constructor:\n",
      " |      \n",
      " |      Properties:\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          animated: bool\n",
      " |          antialiased or aa or antialiaseds: bool or list of bools\n",
      " |          array: ndarray\n",
      " |          capstyle: {'butt', 'round', 'projecting'}\n",
      " |          clim: (vmin: float, vmax: float)\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          cmap: `.Colormap` or str or None\n",
      " |          color: color or list of rgba tuples\n",
      " |          contains: unknown\n",
      " |          edgecolor or ec or edgecolors: color or list of colors or 'face'\n",
      " |          facecolor or facecolors or fc: color or list of colors\n",
      " |          figure: `.Figure`\n",
      " |          gid: str\n",
      " |          hatch: {'/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.', '*'}\n",
      " |          in_layout: bool\n",
      " |          joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          label: object\n",
      " |          linestyle or dashes or linestyles or ls: str or tuple or list thereof\n",
      " |          linewidth or linewidths or lw: float or list of floats\n",
      " |          norm: `.Normalize` or None\n",
      " |          offset_position: unknown\n",
      " |          offsets: array-like (N, 2) or (2,)\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: None or bool or callable\n",
      " |          pickradius: unknown\n",
      " |          rasterized: bool or None\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          transform: `.Transform`\n",
      " |          url: str\n",
      " |          urls: list of str or None\n",
      " |          visible: bool\n",
      " |          zorder: float\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      pcolor : An alternative implementation with slightly different\n",
      " |          features. For a detailed discussion on the differences see\n",
      " |          :ref:`Differences between pcolor() and pcolormesh()\n",
      " |          <differences-pcolor-pcolormesh>`.\n",
      " |      imshow : If *X* and *Y* are each equidistant, `~.Axes.imshow` can be a\n",
      " |          faster alternative.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      **Masked arrays**\n",
      " |      \n",
      " |      *C* may be a masked array. If ``C[i, j]`` is masked, the corresponding\n",
      " |      quadrilateral will be transparent. Masking of *X* and *Y* is not\n",
      " |      supported. Use `~.Axes.pcolor` if you need this functionality.\n",
      " |      \n",
      " |      .. _axes-pcolormesh-grid-orientation:\n",
      " |      \n",
      " |      **Grid orientation**\n",
      " |      \n",
      " |      The grid orientation follows the standard matrix convention: An array\n",
      " |      *C* with shape (nrows, ncolumns) is plotted with the column number as\n",
      " |      *X* and the row number as *Y*.\n",
      " |      \n",
      " |      .. _differences-pcolor-pcolormesh:\n",
      " |      \n",
      " |      **Differences between pcolor() and pcolormesh()**\n",
      " |      \n",
      " |      Both methods are used to create a pseudocolor plot of a 2-D array\n",
      " |      using quadrilaterals.\n",
      " |      \n",
      " |      The main difference lies in the created object and internal data\n",
      " |      handling:\n",
      " |      While `~.Axes.pcolor` returns a `.PolyCollection`, `~.Axes.pcolormesh`\n",
      " |      returns a `.QuadMesh`. The latter is more specialized for the given\n",
      " |      purpose and thus is faster. It should almost always be preferred.\n",
      " |      \n",
      " |      There is also a slight difference in the handling of masked arrays.\n",
      " |      Both `~.Axes.pcolor` and `~.Axes.pcolormesh` support masked arrays\n",
      " |      for *C*. However, only `~.Axes.pcolor` supports masked arrays for *X*\n",
      " |      and *Y*. The reason lies in the internal handling of the masked values.\n",
      " |      `~.Axes.pcolor` leaves out the respective polygons from the\n",
      " |      PolyCollection. `~.Axes.pcolormesh` sets the facecolor of the masked\n",
      " |      elements to transparent. You can see the difference when using\n",
      " |      edgecolors. While all edges are drawn irrespective of masking in a\n",
      " |      QuadMesh, the edge between two adjacent masked quadrilaterals in\n",
      " |      `~.Axes.pcolor` is not drawn as the corresponding polygons do not\n",
      " |      exist in the PolyCollection.\n",
      " |      \n",
      " |      Another difference is the support of Gouraud shading in\n",
      " |      `~.Axes.pcolormesh`, which is not available with `~.Axes.pcolor`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          every other argument can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception).\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  phase_spectrum(self, x, Fs=None, Fc=None, window=None, pad_to=None, sides=None, *, data=None, **kwargs)\n",
      " |      Plot the phase spectrum.\n",
      " |      \n",
      " |      Compute the phase spectrum (unwrapped angle spectrum) of *x*.\n",
      " |      Data is padded to a length of *pad_to* and the windowing function\n",
      " |      *window* is applied to the signal.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : 1-D array or sequence\n",
      " |          Array or sequence containing the data\n",
      " |      \n",
      " |      Fs : float, default: 2\n",
      " |          The sampling frequency (samples per time unit).  It is used to calculate\n",
      " |          the Fourier frequencies, *freqs*, in cycles per time unit.\n",
      " |      \n",
      " |      window : callable or ndarray, default: `.window_hanning`\n",
      " |          A function or a vector of length *NFFT*.  To create window vectors see\n",
      " |          `.window_hanning`, `.window_none`, `numpy.blackman`, `numpy.hamming`,\n",
      " |          `numpy.bartlett`, `scipy.signal`, `scipy.signal.get_window`, etc.  If a\n",
      " |          function is passed as the argument, it must take a data segment as an\n",
      " |          argument and return the windowed version of the segment.\n",
      " |      \n",
      " |      sides : {'default', 'onesided', 'twosided'}, optional\n",
      " |          Which sides of the spectrum to return. 'default' is one-sided for real\n",
      " |          data and two-sided for complex data. 'onesided' forces the return of a\n",
      " |          one-sided spectrum, while 'twosided' forces two-sided.\n",
      " |      \n",
      " |      pad_to : int, optional\n",
      " |          The number of points to which the data segment is padded when performing\n",
      " |          the FFT.  While not increasing the actual resolution of the spectrum (the\n",
      " |          minimum distance between resolvable peaks), this can give more points in\n",
      " |          the plot, allowing for more detail. This corresponds to the *n* parameter\n",
      " |          in the call to fft().  The default is None, which sets *pad_to* equal to\n",
      " |          the length of the input signal (i.e. no padding).\n",
      " |      \n",
      " |      Fc : int, default: 0\n",
      " |          The center frequency of *x*, which offsets the x extents of the\n",
      " |          plot to reflect the frequency range used when a signal is acquired\n",
      " |          and then filtered and downsampled to baseband.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      spectrum : 1-D array\n",
      " |          The values for the phase spectrum in radians (real valued).\n",
      " |      \n",
      " |      freqs : 1-D array\n",
      " |          The frequencies corresponding to the elements in *spectrum*.\n",
      " |      \n",
      " |      line : `~matplotlib.lines.Line2D`\n",
      " |          The line created by this function.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs\n",
      " |          Keyword arguments control the `.Line2D` properties:\n",
      " |      \n",
      " |          Properties:\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          animated: bool\n",
      " |          antialiased or aa: bool\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          color or c: color\n",
      " |          contains: unknown\n",
      " |          dash_capstyle: {'butt', 'round', 'projecting'}\n",
      " |          dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      " |          data: (2, N) array or two 1D arrays\n",
      " |          drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      " |          figure: `.Figure`\n",
      " |          fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      " |          gid: str\n",
      " |          in_layout: bool\n",
      " |          label: object\n",
      " |          linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      " |          linewidth or lw: float\n",
      " |          marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      " |          markeredgecolor or mec: color\n",
      " |          markeredgewidth or mew: float\n",
      " |          markerfacecolor or mfc: color\n",
      " |          markerfacecoloralt or mfcalt: color\n",
      " |          markersize or ms: float\n",
      " |          markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: unknown\n",
      " |          pickradius: float\n",
      " |          rasterized: bool or None\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          solid_capstyle: {'butt', 'round', 'projecting'}\n",
      " |          solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          transform: `matplotlib.transforms.Transform`\n",
      " |          url: str\n",
      " |          visible: bool\n",
      " |          xdata: 1D array\n",
      " |          ydata: 1D array\n",
      " |          zorder: float\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      magnitude_spectrum\n",
      " |          Plots the magnitudes of the corresponding frequencies.\n",
      " |      angle_spectrum\n",
      " |          Plots the wrapped version of this function.\n",
      " |      specgram\n",
      " |          Can plot the phase spectrum of segments within the signal in a\n",
      " |          colormap.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      \n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          the following arguments can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception):\n",
      " |          *x*.\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  pie(self, x, explode=None, labels=None, colors=None, autopct=None, pctdistance=0.6, shadow=False, labeldistance=1.1, startangle=0, radius=1, counterclock=True, wedgeprops=None, textprops=None, center=(0, 0), frame=False, rotatelabels=False, *, normalize=None, data=None)\n",
      " |      Plot a pie chart.\n",
      " |      \n",
      " |      Make a pie chart of array *x*.  The fractional area of each wedge is\n",
      " |      given by ``x/sum(x)``.  If ``sum(x) < 1``, then the values of *x* give\n",
      " |      the fractional area directly and the array will not be normalized. The\n",
      " |      resulting pie will have an empty wedge of size ``1 - sum(x)``.\n",
      " |      \n",
      " |      The wedges are plotted counterclockwise, by default starting from the\n",
      " |      x-axis.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : 1D array-like\n",
      " |          The wedge sizes.\n",
      " |      \n",
      " |      explode : array-like, default: None\n",
      " |          If not *None*, is a ``len(x)`` array which specifies the fraction\n",
      " |          of the radius with which to offset each wedge.\n",
      " |      \n",
      " |      labels : list, default: None\n",
      " |          A sequence of strings providing the labels for each wedge\n",
      " |      \n",
      " |      colors : array-like, default: None\n",
      " |          A sequence of colors through which the pie chart will cycle.  If\n",
      " |          *None*, will use the colors in the currently active cycle.\n",
      " |      \n",
      " |      autopct : None or str or callable, default: None\n",
      " |          If not *None*, is a string or function used to label the wedges\n",
      " |          with their numeric value.  The label will be placed inside the\n",
      " |          wedge.  If it is a format string, the label will be ``fmt % pct``.\n",
      " |          If it is a function, it will be called.\n",
      " |      \n",
      " |      pctdistance : float, default: 0.6\n",
      " |          The ratio between the center of each pie slice and the start of\n",
      " |          the text generated by *autopct*.  Ignored if *autopct* is *None*.\n",
      " |      \n",
      " |      shadow : bool, default: False\n",
      " |          Draw a shadow beneath the pie.\n",
      " |      \n",
      " |      normalize: None or bool, default: None\n",
      " |          When *True*, always make a full pie by normalizing x so that\n",
      " |          ``sum(x) == 1``. *False* makes a partial pie if ``sum(x) <= 1``\n",
      " |          and raises a `ValueError` for ``sum(x) > 1``.\n",
      " |      \n",
      " |          When *None*, defaults to *True* if ``sum(x) >= 1`` and *False* if\n",
      " |          ``sum(x) < 1``.\n",
      " |      \n",
      " |          Please note that the previous default value of *None* is now\n",
      " |          deprecated, and the default will change to *True* in the next\n",
      " |          release. Please pass ``normalize=False`` explicitly if you want to\n",
      " |          draw a partial pie.\n",
      " |      \n",
      " |      labeldistance : float or None, default: 1.1\n",
      " |          The radial distance at which the pie labels are drawn.\n",
      " |          If set to ``None``, label are not drawn, but are stored for use in\n",
      " |          ``legend()``\n",
      " |      \n",
      " |      startangle : float, default: 0 degrees\n",
      " |          The angle by which the start of the pie is rotated,\n",
      " |          counterclockwise from the x-axis.\n",
      " |      \n",
      " |      radius : float, default: 1\n",
      " |          The radius of the pie.\n",
      " |      \n",
      " |      counterclock : bool, default: True\n",
      " |          Specify fractions direction, clockwise or counterclockwise.\n",
      " |      \n",
      " |      wedgeprops : dict, default: None\n",
      " |          Dict of arguments passed to the wedge objects making the pie.\n",
      " |          For example, you can pass in ``wedgeprops = {'linewidth': 3}``\n",
      " |          to set the width of the wedge border lines equal to 3.\n",
      " |          For more details, look at the doc/arguments of the wedge object.\n",
      " |          By default ``clip_on=False``.\n",
      " |      \n",
      " |      textprops : dict, default: None\n",
      " |          Dict of arguments to pass to the text objects.\n",
      " |      \n",
      " |      center : (float, float), default: (0, 0)\n",
      " |          The coordinates of the center of the chart.\n",
      " |      \n",
      " |      frame : bool, default: False\n",
      " |          Plot axes frame with the chart if true.\n",
      " |      \n",
      " |      rotatelabels : bool, default: False\n",
      " |          Rotate each label to the angle of the corresponding slice if true.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      patches : list\n",
      " |          A sequence of `matplotlib.patches.Wedge` instances\n",
      " |      \n",
      " |      texts : list\n",
      " |          A list of the label `.Text` instances.\n",
      " |      \n",
      " |      autotexts : list\n",
      " |          A list of `.Text` instances for the numeric labels. This will only\n",
      " |          be returned if the parameter *autopct* is not *None*.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The pie chart will probably look best if the figure and axes are\n",
      " |      square, or the Axes aspect is equal.\n",
      " |      This method sets the aspect ratio of the axis to \"equal\".\n",
      " |      The axes aspect ratio can be controlled with `.Axes.set_aspect`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          the following arguments can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception):\n",
      " |          *x*, *explode*, *labels*, *colors*.\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  plot_date(self, x, y, fmt='o', tz=None, xdate=True, ydate=False, *, data=None, **kwargs)\n",
      " |      Plot data that contains dates.\n",
      " |      \n",
      " |      Similar to `.plot`, this plots *y* vs. *x* as lines or markers.\n",
      " |      However, the axis labels are formatted as dates depending on *xdate*\n",
      " |      and *ydate*.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x, y : array-like\n",
      " |          The coordinates of the data points. If *xdate* or *ydate* is\n",
      " |          *True*, the respective values *x* or *y* are interpreted as\n",
      " |          :ref:`Matplotlib dates <date-format>`.\n",
      " |      \n",
      " |      fmt : str, optional\n",
      " |          The plot format string. For details, see the corresponding\n",
      " |          parameter in `.plot`.\n",
      " |      \n",
      " |      tz : timezone string or `datetime.tzinfo`, default: :rc:`timezone`\n",
      " |          The time zone to use in labeling dates.\n",
      " |      \n",
      " |      xdate : bool, default: True\n",
      " |          If *True*, the *x*-axis will be interpreted as Matplotlib dates.\n",
      " |      \n",
      " |      ydate : bool, default: False\n",
      " |          If *True*, the *y*-axis will be interpreted as Matplotlib dates.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      lines\n",
      " |          A list of `.Line2D` objects representing the plotted data.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs\n",
      " |          Keyword arguments control the `.Line2D` properties:\n",
      " |      \n",
      " |          Properties:\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          animated: bool\n",
      " |          antialiased or aa: bool\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          color or c: color\n",
      " |          contains: unknown\n",
      " |          dash_capstyle: {'butt', 'round', 'projecting'}\n",
      " |          dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      " |          data: (2, N) array or two 1D arrays\n",
      " |          drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      " |          figure: `.Figure`\n",
      " |          fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      " |          gid: str\n",
      " |          in_layout: bool\n",
      " |          label: object\n",
      " |          linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      " |          linewidth or lw: float\n",
      " |          marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      " |          markeredgecolor or mec: color\n",
      " |          markeredgewidth or mew: float\n",
      " |          markerfacecolor or mfc: color\n",
      " |          markerfacecoloralt or mfcalt: color\n",
      " |          markersize or ms: float\n",
      " |          markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: unknown\n",
      " |          pickradius: float\n",
      " |          rasterized: bool or None\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          solid_capstyle: {'butt', 'round', 'projecting'}\n",
      " |          solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          transform: `matplotlib.transforms.Transform`\n",
      " |          url: str\n",
      " |          visible: bool\n",
      " |          xdata: 1D array\n",
      " |          ydata: 1D array\n",
      " |          zorder: float\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      matplotlib.dates : Helper functions on dates.\n",
      " |      matplotlib.dates.date2num : Convert dates to num.\n",
      " |      matplotlib.dates.num2date : Convert num to dates.\n",
      " |      matplotlib.dates.drange : Create an equally spaced sequence of dates.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      If you are using custom date tickers and formatters, it may be\n",
      " |      necessary to set the formatters/locators after the call to\n",
      " |      `.plot_date`. `.plot_date` will set the default tick locator to\n",
      " |      `.AutoDateLocator` (if the tick locator is not already set to a\n",
      " |      `.DateLocator` instance) and the default tick formatter to\n",
      " |      `.AutoDateFormatter` (if the tick formatter is not already set to a\n",
      " |      `.DateFormatter` instance).\n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          the following arguments can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception):\n",
      " |          *x*, *y*.\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  psd(self, x, NFFT=None, Fs=None, Fc=None, detrend=None, window=None, noverlap=None, pad_to=None, sides=None, scale_by_freq=None, return_line=None, *, data=None, **kwargs)\n",
      " |      Plot the power spectral density.\n",
      " |      \n",
      " |      The power spectral density :math:`P_{xx}` by Welch's average\n",
      " |      periodogram method.  The vector *x* is divided into *NFFT* length\n",
      " |      segments.  Each segment is detrended by function *detrend* and\n",
      " |      windowed by function *window*.  *noverlap* gives the length of\n",
      " |      the overlap between segments.  The :math:`|\\mathrm{fft}(i)|^2`\n",
      " |      of each segment :math:`i` are averaged to compute :math:`P_{xx}`,\n",
      " |      with a scaling to correct for power loss due to windowing.\n",
      " |      \n",
      " |      If len(*x*) < *NFFT*, it will be zero padded to *NFFT*.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : 1-D array or sequence\n",
      " |          Array or sequence containing the data\n",
      " |      \n",
      " |      Fs : float, default: 2\n",
      " |          The sampling frequency (samples per time unit).  It is used to calculate\n",
      " |          the Fourier frequencies, *freqs*, in cycles per time unit.\n",
      " |      \n",
      " |      window : callable or ndarray, default: `.window_hanning`\n",
      " |          A function or a vector of length *NFFT*.  To create window vectors see\n",
      " |          `.window_hanning`, `.window_none`, `numpy.blackman`, `numpy.hamming`,\n",
      " |          `numpy.bartlett`, `scipy.signal`, `scipy.signal.get_window`, etc.  If a\n",
      " |          function is passed as the argument, it must take a data segment as an\n",
      " |          argument and return the windowed version of the segment.\n",
      " |      \n",
      " |      sides : {'default', 'onesided', 'twosided'}, optional\n",
      " |          Which sides of the spectrum to return. 'default' is one-sided for real\n",
      " |          data and two-sided for complex data. 'onesided' forces the return of a\n",
      " |          one-sided spectrum, while 'twosided' forces two-sided.\n",
      " |      \n",
      " |      pad_to : int, optional\n",
      " |          The number of points to which the data segment is padded when performing\n",
      " |          the FFT.  This can be different from *NFFT*, which specifies the number\n",
      " |          of data points used.  While not increasing the actual resolution of the\n",
      " |          spectrum (the minimum distance between resolvable peaks), this can give\n",
      " |          more points in the plot, allowing for more detail. This corresponds to\n",
      " |          the *n* parameter in the call to fft(). The default is None, which sets\n",
      " |          *pad_to* equal to *NFFT*\n",
      " |      \n",
      " |      NFFT : int, default: 256\n",
      " |          The number of data points used in each block for the FFT.  A power 2 is\n",
      " |          most efficient.  This should *NOT* be used to get zero padding, or the\n",
      " |          scaling of the result will be incorrect; use *pad_to* for this instead.\n",
      " |      \n",
      " |      detrend : {'none', 'mean', 'linear'} or callable, default 'none'\n",
      " |          The function applied to each segment before fft-ing, designed to remove\n",
      " |          the mean or linear trend.  Unlike in MATLAB, where the *detrend* parameter\n",
      " |          is a vector, in Matplotlib is it a function.  The :mod:`~matplotlib.mlab`\n",
      " |          module defines `.detrend_none`, `.detrend_mean`, and `.detrend_linear`,\n",
      " |          but you can use a custom function as well.  You can also use a string to\n",
      " |          choose one of the functions: 'none' calls `.detrend_none`. 'mean' calls\n",
      " |          `.detrend_mean`. 'linear' calls `.detrend_linear`.\n",
      " |      \n",
      " |      scale_by_freq : bool, default: True\n",
      " |          Whether the resulting density values should be scaled by the scaling\n",
      " |          frequency, which gives density in units of Hz^-1.  This allows for\n",
      " |          integration over the returned frequency values.  The default is True for\n",
      " |          MATLAB compatibility.\n",
      " |      \n",
      " |      noverlap : int, default: 0 (no overlap)\n",
      " |          The number of points of overlap between segments.\n",
      " |      \n",
      " |      Fc : int, default: 0\n",
      " |          The center frequency of *x*, which offsets the x extents of the\n",
      " |          plot to reflect the frequency range used when a signal is acquired\n",
      " |          and then filtered and downsampled to baseband.\n",
      " |      \n",
      " |      return_line : bool, default: False\n",
      " |          Whether to include the line object plotted in the returned values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Pxx : 1-D array\n",
      " |          The values for the power spectrum :math:`P_{xx}` before scaling\n",
      " |          (real valued).\n",
      " |      \n",
      " |      freqs : 1-D array\n",
      " |          The frequencies corresponding to the elements in *Pxx*.\n",
      " |      \n",
      " |      line : `~matplotlib.lines.Line2D`\n",
      " |          The line created by this function.\n",
      " |          Only returned if *return_line* is True.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs\n",
      " |          Keyword arguments control the `.Line2D` properties:\n",
      " |      \n",
      " |          Properties:\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          animated: bool\n",
      " |          antialiased or aa: bool\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          color or c: color\n",
      " |          contains: unknown\n",
      " |          dash_capstyle: {'butt', 'round', 'projecting'}\n",
      " |          dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      " |          data: (2, N) array or two 1D arrays\n",
      " |          drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      " |          figure: `.Figure`\n",
      " |          fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      " |          gid: str\n",
      " |          in_layout: bool\n",
      " |          label: object\n",
      " |          linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      " |          linewidth or lw: float\n",
      " |          marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      " |          markeredgecolor or mec: color\n",
      " |          markeredgewidth or mew: float\n",
      " |          markerfacecolor or mfc: color\n",
      " |          markerfacecoloralt or mfcalt: color\n",
      " |          markersize or ms: float\n",
      " |          markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: unknown\n",
      " |          pickradius: float\n",
      " |          rasterized: bool or None\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          solid_capstyle: {'butt', 'round', 'projecting'}\n",
      " |          solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          transform: `matplotlib.transforms.Transform`\n",
      " |          url: str\n",
      " |          visible: bool\n",
      " |          xdata: 1D array\n",
      " |          ydata: 1D array\n",
      " |          zorder: float\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      specgram\n",
      " |          Differs in the default overlap; in not returning the mean of the\n",
      " |          segment periodograms; in returning the times of the segments; and\n",
      " |          in plotting a colormap instead of a line.\n",
      " |      magnitude_spectrum\n",
      " |          Plots the magnitude spectrum.\n",
      " |      csd\n",
      " |          Plots the spectral density between two signals.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For plotting, the power is plotted as\n",
      " |      :math:`10\\log_{10}(P_{xx})` for decibels, though *Pxx* itself\n",
      " |      is returned.\n",
      " |      \n",
      " |      References\n",
      " |      ----------\n",
      " |      Bendat & Piersol -- Random Data: Analysis and Measurement Procedures,\n",
      " |      John Wiley & Sons (1986)\n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          the following arguments can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception):\n",
      " |          *x*.\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  quiverkey(self, Q, X, Y, U, label, **kw)\n",
      " |      Add a key to a quiver plot.\n",
      " |      \n",
      " |      The positioning of the key depends on *X*, *Y*, *coordinates*, and\n",
      " |      *labelpos*.  If *labelpos* is 'N' or 'S', *X*, *Y* give the position of\n",
      " |      the middle of the key arrow.  If *labelpos* is 'E', *X*, *Y* positions\n",
      " |      the head, and if *labelpos* is 'W', *X*, *Y* positions the tail; in\n",
      " |      either of these two cases, *X*, *Y* is somewhere in the middle of the\n",
      " |      arrow+label key object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      Q : `matplotlib.quiver.Quiver`\n",
      " |          A `.Quiver` object as returned by a call to `~.Axes.quiver()`.\n",
      " |      X, Y : float\n",
      " |          The location of the key.\n",
      " |      U : float\n",
      " |          The length of the key.\n",
      " |      label : str\n",
      " |          The key label (e.g., length and units of the key).\n",
      " |      angle : float, default: 0\n",
      " |          The angle of the key arrow, in degrees anti-clockwise from the\n",
      " |          x-axis.\n",
      " |      coordinates : {'axes', 'figure', 'data', 'inches'}, default: 'axes'\n",
      " |          Coordinate system and units for *X*, *Y*: 'axes' and 'figure' are\n",
      " |          normalized coordinate systems with (0, 0) in the lower left and\n",
      " |          (1, 1) in the upper right; 'data' are the axes data coordinates\n",
      " |          (used for the locations of the vectors in the quiver plot itself);\n",
      " |          'inches' is position in the figure in inches, with (0, 0) at the\n",
      " |          lower left corner.\n",
      " |      color : color\n",
      " |          Overrides face and edge colors from *Q*.\n",
      " |      labelpos : {'N', 'S', 'E', 'W'}\n",
      " |          Position the label above, below, to the right, to the left of the\n",
      " |          arrow, respectively.\n",
      " |      labelsep : float, default: 0.1\n",
      " |          Distance in inches between the arrow and the label.\n",
      " |      labelcolor : color, default: :rc:`text.color`\n",
      " |          Label color.\n",
      " |      fontproperties : dict, optional\n",
      " |          A dictionary with keyword arguments accepted by the\n",
      " |          `~matplotlib.font_manager.FontProperties` initializer:\n",
      " |          *family*, *style*, *variant*, *size*, *weight*.\n",
      " |      **kwargs\n",
      " |          Any additional keyword arguments are used to override vector\n",
      " |          properties taken from *Q*.\n",
      " |  \n",
      " |  secondary_xaxis(self, location, *, functions=None, **kwargs)\n",
      " |      Add a second x-axis to this axes.\n",
      " |      \n",
      " |      For example if we want to have a second scale for the data plotted on\n",
      " |      the xaxis.\n",
      " |      \n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      This method is experimental as of 3.1, and the API may change.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      location : {'top', 'bottom', 'left', 'right'} or float\n",
      " |          The position to put the secondary axis.  Strings can be 'top' or\n",
      " |          'bottom' for orientation='x' and 'right' or 'left' for\n",
      " |          orientation='y'. A float indicates the relative position on the\n",
      " |          parent axes to put the new axes, 0.0 being the bottom (or left)\n",
      " |          and 1.0 being the top (or right).\n",
      " |      \n",
      " |      functions : 2-tuple of func, or Transform with an inverse\n",
      " |      \n",
      " |          If a 2-tuple of functions, the user specifies the transform\n",
      " |          function and its inverse.  i.e.\n",
      " |          ``functions=(lambda x: 2 / x, lambda x: 2 / x)`` would be an\n",
      " |          reciprocal transform with a factor of 2.\n",
      " |      \n",
      " |          The user can also directly supply a subclass of\n",
      " |          `.transforms.Transform` so long as it has an inverse.\n",
      " |      \n",
      " |          See :doc:`/gallery/subplots_axes_and_figures/secondary_axis`\n",
      " |          for examples of making these conversions.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ax : axes._secondary_axes.SecondaryAxis\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs : `~matplotlib.axes.Axes` properties.\n",
      " |          Other miscellaneous axes parameters.\n",
      " |      \n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      The main axis shows frequency, and the secondary axis shows period.\n",
      " |      \n",
      " |      .. plot::\n",
      " |      \n",
      " |          fig, ax = plt.subplots()\n",
      " |          ax.loglog(range(1, 360, 5), range(1, 360, 5))\n",
      " |          ax.set_xlabel('frequency [Hz]')\n",
      " |      \n",
      " |          def invert(x):\n",
      " |              return 1 / x\n",
      " |      \n",
      " |          secax = ax.secondary_xaxis('top', functions=(invert, invert))\n",
      " |          secax.set_xlabel('Period [s]')\n",
      " |          plt.show()\n",
      " |  \n",
      " |  secondary_yaxis(self, location, *, functions=None, **kwargs)\n",
      " |      Add a second y-axis to this axes.\n",
      " |      \n",
      " |      For example if we want to have a second scale for the data plotted on\n",
      " |      the yaxis.\n",
      " |      \n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      This method is experimental as of 3.1, and the API may change.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      location : {'top', 'bottom', 'left', 'right'} or float\n",
      " |          The position to put the secondary axis.  Strings can be 'top' or\n",
      " |          'bottom' for orientation='x' and 'right' or 'left' for\n",
      " |          orientation='y'. A float indicates the relative position on the\n",
      " |          parent axes to put the new axes, 0.0 being the bottom (or left)\n",
      " |          and 1.0 being the top (or right).\n",
      " |      \n",
      " |      functions : 2-tuple of func, or Transform with an inverse\n",
      " |      \n",
      " |          If a 2-tuple of functions, the user specifies the transform\n",
      " |          function and its inverse.  i.e.\n",
      " |          ``functions=(lambda x: 2 / x, lambda x: 2 / x)`` would be an\n",
      " |          reciprocal transform with a factor of 2.\n",
      " |      \n",
      " |          The user can also directly supply a subclass of\n",
      " |          `.transforms.Transform` so long as it has an inverse.\n",
      " |      \n",
      " |          See :doc:`/gallery/subplots_axes_and_figures/secondary_axis`\n",
      " |          for examples of making these conversions.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      ax : axes._secondary_axes.SecondaryAxis\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs : `~matplotlib.axes.Axes` properties.\n",
      " |          Other miscellaneous axes parameters.\n",
      " |      \n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Add a secondary axes that converts from radians to degrees\n",
      " |      \n",
      " |      .. plot::\n",
      " |      \n",
      " |          fig, ax = plt.subplots()\n",
      " |          ax.plot(range(1, 360, 5), range(1, 360, 5))\n",
      " |          ax.set_ylabel('degrees')\n",
      " |          secax = ax.secondary_yaxis('right', functions=(np.deg2rad,\n",
      " |                                                         np.rad2deg))\n",
      " |          secax.set_ylabel('radians')\n",
      " |  \n",
      " |  semilogx(self, *args, **kwargs)\n",
      " |      Make a plot with log scaling on the x axis.\n",
      " |      \n",
      " |      Call signatures::\n",
      " |      \n",
      " |          semilogx([x], y, [fmt], data=None, **kwargs)\n",
      " |          semilogx([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n",
      " |      \n",
      " |      This is just a thin wrapper around `.plot` which additionally changes\n",
      " |      the x-axis to log scaling. All of the concepts and parameters of plot\n",
      " |      can be used here as well.\n",
      " |      \n",
      " |      The additional parameters *base*, *subs*, and *nonpositive* control the\n",
      " |      x-axis properties. They are just forwarded to `.Axes.set_xscale`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      base : float, default: 10\n",
      " |          Base of the x logarithm.\n",
      " |      \n",
      " |      subs : array-like, optional\n",
      " |          The location of the minor xticks. If *None*, reasonable locations\n",
      " |          are automatically chosen depending on the number of decades in the\n",
      " |          plot. See `.Axes.set_xscale` for details.\n",
      " |      \n",
      " |      nonpositive : {'mask', 'clip'}, default: 'mask'\n",
      " |          Non-positive values in x can be masked as invalid, or clipped to a\n",
      " |          very small positive number.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      lines\n",
      " |          A list of `.Line2D` objects representing the plotted data.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs\n",
      " |          All parameters supported by `.plot`.\n",
      " |  \n",
      " |  semilogy(self, *args, **kwargs)\n",
      " |      Make a plot with log scaling on the y axis.\n",
      " |      \n",
      " |      Call signatures::\n",
      " |      \n",
      " |          semilogy([x], y, [fmt], data=None, **kwargs)\n",
      " |          semilogy([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n",
      " |      \n",
      " |      This is just a thin wrapper around `.plot` which additionally changes\n",
      " |      the y-axis to log scaling. All of the concepts and parameters of plot\n",
      " |      can be used here as well.\n",
      " |      \n",
      " |      The additional parameters *base*, *subs*, and *nonpositive* control the\n",
      " |      y-axis properties. They are just forwarded to `.Axes.set_yscale`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      base : float, default: 10\n",
      " |          Base of the y logarithm.\n",
      " |      \n",
      " |      subs : array-like, optional\n",
      " |          The location of the minor yticks. If *None*, reasonable locations\n",
      " |          are automatically chosen depending on the number of decades in the\n",
      " |          plot. See `.Axes.set_yscale` for details.\n",
      " |      \n",
      " |      nonpositive : {'mask', 'clip'}, default: 'mask'\n",
      " |          Non-positive values in y can be masked as invalid, or clipped to a\n",
      " |          very small positive number.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      lines\n",
      " |          A list of `.Line2D` objects representing the plotted data.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs\n",
      " |          All parameters supported by `.plot`.\n",
      " |  \n",
      " |  set_xlabel(self, xlabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)\n",
      " |      Set the label for the x-axis.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      xlabel : str\n",
      " |          The label text.\n",
      " |      \n",
      " |      labelpad : float, default: None\n",
      " |          Spacing in points from the axes bounding box including ticks\n",
      " |          and tick labels.\n",
      " |      \n",
      " |      loc : {'left', 'center', 'right'}, default: :rc:`xaxis.labellocation`\n",
      " |          The label position. This is a high-level alternative for passing\n",
      " |          parameters *x* and *horizontalalignment*.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs : `.Text` properties\n",
      " |          `.Text` properties control the appearance of the label.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      text : Documents the properties supported by `.Text`.\n",
      " |  \n",
      " |  set_ylabel(self, ylabel, fontdict=None, labelpad=None, *, loc=None, **kwargs)\n",
      " |      Set the label for the y-axis.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ylabel : str\n",
      " |          The label text.\n",
      " |      \n",
      " |      labelpad : float, default: None\n",
      " |          Spacing in points from the axes bounding box including ticks\n",
      " |          and tick labels.\n",
      " |      \n",
      " |      loc : {'bottom', 'center', 'top'}, default: :rc:`yaxis.labellocation`\n",
      " |          The label position. This is a high-level alternative for passing\n",
      " |          parameters *y* and *horizontalalignment*.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs : `.Text` properties\n",
      " |          `.Text` properties control the appearance of the label.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      text : Documents the properties supported by `.Text`.\n",
      " |  \n",
      " |  specgram(self, x, NFFT=None, Fs=None, Fc=None, detrend=None, window=None, noverlap=None, cmap=None, xextent=None, pad_to=None, sides=None, scale_by_freq=None, mode=None, scale=None, vmin=None, vmax=None, *, data=None, **kwargs)\n",
      " |      Plot a spectrogram.\n",
      " |      \n",
      " |      Compute and plot a spectrogram of data in *x*.  Data are split into\n",
      " |      *NFFT* length segments and the spectrum of each section is\n",
      " |      computed.  The windowing function *window* is applied to each\n",
      " |      segment, and the amount of overlap of each segment is\n",
      " |      specified with *noverlap*. The spectrogram is plotted as a colormap\n",
      " |      (using imshow).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : 1-D array or sequence\n",
      " |          Array or sequence containing the data.\n",
      " |      \n",
      " |      Fs : float, default: 2\n",
      " |          The sampling frequency (samples per time unit).  It is used to calculate\n",
      " |          the Fourier frequencies, *freqs*, in cycles per time unit.\n",
      " |      \n",
      " |      window : callable or ndarray, default: `.window_hanning`\n",
      " |          A function or a vector of length *NFFT*.  To create window vectors see\n",
      " |          `.window_hanning`, `.window_none`, `numpy.blackman`, `numpy.hamming`,\n",
      " |          `numpy.bartlett`, `scipy.signal`, `scipy.signal.get_window`, etc.  If a\n",
      " |          function is passed as the argument, it must take a data segment as an\n",
      " |          argument and return the windowed version of the segment.\n",
      " |      \n",
      " |      sides : {'default', 'onesided', 'twosided'}, optional\n",
      " |          Which sides of the spectrum to return. 'default' is one-sided for real\n",
      " |          data and two-sided for complex data. 'onesided' forces the return of a\n",
      " |          one-sided spectrum, while 'twosided' forces two-sided.\n",
      " |      \n",
      " |      pad_to : int, optional\n",
      " |          The number of points to which the data segment is padded when performing\n",
      " |          the FFT.  This can be different from *NFFT*, which specifies the number\n",
      " |          of data points used.  While not increasing the actual resolution of the\n",
      " |          spectrum (the minimum distance between resolvable peaks), this can give\n",
      " |          more points in the plot, allowing for more detail. This corresponds to\n",
      " |          the *n* parameter in the call to fft(). The default is None, which sets\n",
      " |          *pad_to* equal to *NFFT*\n",
      " |      \n",
      " |      NFFT : int, default: 256\n",
      " |          The number of data points used in each block for the FFT.  A power 2 is\n",
      " |          most efficient.  This should *NOT* be used to get zero padding, or the\n",
      " |          scaling of the result will be incorrect; use *pad_to* for this instead.\n",
      " |      \n",
      " |      detrend : {'none', 'mean', 'linear'} or callable, default 'none'\n",
      " |          The function applied to each segment before fft-ing, designed to remove\n",
      " |          the mean or linear trend.  Unlike in MATLAB, where the *detrend* parameter\n",
      " |          is a vector, in Matplotlib is it a function.  The :mod:`~matplotlib.mlab`\n",
      " |          module defines `.detrend_none`, `.detrend_mean`, and `.detrend_linear`,\n",
      " |          but you can use a custom function as well.  You can also use a string to\n",
      " |          choose one of the functions: 'none' calls `.detrend_none`. 'mean' calls\n",
      " |          `.detrend_mean`. 'linear' calls `.detrend_linear`.\n",
      " |      \n",
      " |      scale_by_freq : bool, default: True\n",
      " |          Whether the resulting density values should be scaled by the scaling\n",
      " |          frequency, which gives density in units of Hz^-1.  This allows for\n",
      " |          integration over the returned frequency values.  The default is True for\n",
      " |          MATLAB compatibility.\n",
      " |      \n",
      " |      mode : {'default', 'psd', 'magnitude', 'angle', 'phase'}\n",
      " |          What sort of spectrum to use.  Default is 'psd', which takes the\n",
      " |          power spectral density.  'magnitude' returns the magnitude\n",
      " |          spectrum.  'angle' returns the phase spectrum without unwrapping.\n",
      " |          'phase' returns the phase spectrum with unwrapping.\n",
      " |      \n",
      " |      noverlap : int\n",
      " |          The number of points of overlap between blocks.  The\n",
      " |          default value is 128.\n",
      " |      \n",
      " |      scale : {'default', 'linear', 'dB'}\n",
      " |          The scaling of the values in the *spec*.  'linear' is no scaling.\n",
      " |          'dB' returns the values in dB scale.  When *mode* is 'psd',\n",
      " |          this is dB power (10 * log10).  Otherwise this is dB amplitude\n",
      " |          (20 * log10). 'default' is 'dB' if *mode* is 'psd' or\n",
      " |          'magnitude' and 'linear' otherwise.  This must be 'linear'\n",
      " |          if *mode* is 'angle' or 'phase'.\n",
      " |      \n",
      " |      Fc : int, default: 0\n",
      " |          The center frequency of *x*, which offsets the x extents of the\n",
      " |          plot to reflect the frequency range used when a signal is acquired\n",
      " |          and then filtered and downsampled to baseband.\n",
      " |      \n",
      " |      cmap : `.Colormap`, default: :rc:`image.cmap`\n",
      " |      \n",
      " |      xextent : *None* or (xmin, xmax)\n",
      " |          The image extent along the x-axis. The default sets *xmin* to the\n",
      " |          left border of the first bin (*spectrum* column) and *xmax* to the\n",
      " |          right border of the last bin. Note that for *noverlap>0* the width\n",
      " |          of the bins is smaller than those of the segments.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Additional keyword arguments are passed on to `~.axes.Axes.imshow`\n",
      " |          which makes the specgram image.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      spectrum : 2-D array\n",
      " |          Columns are the periodograms of successive segments.\n",
      " |      \n",
      " |      freqs : 1-D array\n",
      " |          The frequencies corresponding to the rows in *spectrum*.\n",
      " |      \n",
      " |      t : 1-D array\n",
      " |          The times corresponding to midpoints of segments (i.e., the columns\n",
      " |          in *spectrum*).\n",
      " |      \n",
      " |      im : `.AxesImage`\n",
      " |          The image created by imshow containing the spectrogram.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      psd\n",
      " |          Differs in the default overlap; in returning the mean of the\n",
      " |          segment periodograms; in not returning times; and in generating a\n",
      " |          line plot instead of colormap.\n",
      " |      magnitude_spectrum\n",
      " |          A single spectrum, similar to having a single segment when *mode*\n",
      " |          is 'magnitude'. Plots a line instead of a colormap.\n",
      " |      angle_spectrum\n",
      " |          A single spectrum, similar to having a single segment when *mode*\n",
      " |          is 'angle'. Plots a line instead of a colormap.\n",
      " |      phase_spectrum\n",
      " |          A single spectrum, similar to having a single segment when *mode*\n",
      " |          is 'phase'. Plots a line instead of a colormap.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The parameters *detrend* and *scale_by_freq* do only apply when *mode*\n",
      " |      is set to 'psd'.\n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          the following arguments can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception):\n",
      " |          *x*.\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  spy(self, Z, precision=0, marker=None, markersize=None, aspect='equal', origin='upper', **kwargs)\n",
      " |      Plot the sparsity pattern of a 2D array.\n",
      " |      \n",
      " |      This visualizes the non-zero values of the array.\n",
      " |      \n",
      " |      Two plotting styles are available: image and marker. Both\n",
      " |      are available for full arrays, but only the marker style\n",
      " |      works for `scipy.sparse.spmatrix` instances.\n",
      " |      \n",
      " |      **Image style**\n",
      " |      \n",
      " |      If *marker* and *markersize* are *None*, `~.Axes.imshow` is used. Any\n",
      " |      extra remaining keyword arguments are passed to this method.\n",
      " |      \n",
      " |      **Marker style**\n",
      " |      \n",
      " |      If *Z* is a `scipy.sparse.spmatrix` or *marker* or *markersize* are\n",
      " |      *None*, a `.Line2D` object will be returned with the value of marker\n",
      " |      determining the marker type, and any remaining keyword arguments\n",
      " |      passed to `~.Axes.plot`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      Z : array-like (M, N)\n",
      " |          The array to be plotted.\n",
      " |      \n",
      " |      precision : float or 'present', default: 0\n",
      " |          If *precision* is 0, any non-zero value will be plotted. Otherwise,\n",
      " |          values of :math:`|Z| > precision` will be plotted.\n",
      " |      \n",
      " |          For `scipy.sparse.spmatrix` instances, you can also\n",
      " |          pass 'present'. In this case any value present in the array\n",
      " |          will be plotted, even if it is identically zero.\n",
      " |      \n",
      " |      aspect : {'equal', 'auto', None} or float, default: 'equal'\n",
      " |          The aspect ratio of the axes.  This parameter is particularly\n",
      " |          relevant for images since it determines whether data pixels are\n",
      " |          square.\n",
      " |      \n",
      " |          This parameter is a shortcut for explicitly calling\n",
      " |          `.Axes.set_aspect`. See there for further details.\n",
      " |      \n",
      " |          - 'equal': Ensures an aspect ratio of 1. Pixels will be square.\n",
      " |          - 'auto': The axes is kept fixed and the aspect is adjusted so\n",
      " |            that the data fit in the axes. In general, this will result in\n",
      " |            non-square pixels.\n",
      " |          - *None*: Use :rc:`image.aspect`.\n",
      " |      \n",
      " |      origin : {'upper', 'lower'}, default: :rc:`image.origin`\n",
      " |          Place the [0, 0] index of the array in the upper left or lower left\n",
      " |          corner of the axes. The convention 'upper' is typically used for\n",
      " |          matrices and images.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `~matplotlib.image.AxesImage` or `.Line2D`\n",
      " |          The return type depends on the plotting style (see above).\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs\n",
      " |          The supported additional parameters depend on the plotting style.\n",
      " |      \n",
      " |          For the image style, you can pass the following additional\n",
      " |          parameters of `~.Axes.imshow`:\n",
      " |      \n",
      " |          - *cmap*\n",
      " |          - *alpha*\n",
      " |          - *url*\n",
      " |          - any `.Artist` properties (passed on to the `.AxesImage`)\n",
      " |      \n",
      " |          For the marker style, you can pass any `.Line2D` property except\n",
      " |          for *linestyle*:\n",
      " |      \n",
      " |          Properties:\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          animated: bool\n",
      " |          antialiased or aa: bool\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          color or c: color\n",
      " |          contains: unknown\n",
      " |          dash_capstyle: {'butt', 'round', 'projecting'}\n",
      " |          dash_joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          dashes: sequence of floats (on/off ink in points) or (None, None)\n",
      " |          data: (2, N) array or two 1D arrays\n",
      " |          drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
      " |          figure: `.Figure`\n",
      " |          fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
      " |          gid: str\n",
      " |          in_layout: bool\n",
      " |          label: object\n",
      " |          linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
      " |          linewidth or lw: float\n",
      " |          marker: marker style string, `~.path.Path` or `~.markers.MarkerStyle`\n",
      " |          markeredgecolor or mec: color\n",
      " |          markeredgewidth or mew: float\n",
      " |          markerfacecolor or mfc: color\n",
      " |          markerfacecoloralt or mfcalt: color\n",
      " |          markersize or ms: float\n",
      " |          markevery: None or int or (int, int) or slice or List[int] or float or (float, float) or List[bool]\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: unknown\n",
      " |          pickradius: float\n",
      " |          rasterized: bool or None\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          solid_capstyle: {'butt', 'round', 'projecting'}\n",
      " |          solid_joinstyle: {'miter', 'round', 'bevel'}\n",
      " |          transform: `matplotlib.transforms.Transform`\n",
      " |          url: str\n",
      " |          visible: bool\n",
      " |          xdata: 1D array\n",
      " |          ydata: 1D array\n",
      " |          zorder: float\n",
      " |  \n",
      " |  stackplot(axes, x, *args, labels=(), colors=None, baseline='zero', data=None, **kwargs)\n",
      " |      Draw a stacked area plot.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : 1d array of dimension N\n",
      " |      \n",
      " |      y : 2d array (dimension MxN), or sequence of 1d arrays (each dimension 1xN)\n",
      " |      \n",
      " |          The data is assumed to be unstacked. Each of the following\n",
      " |          calls is legal::\n",
      " |      \n",
      " |              stackplot(x, y)               # where y is MxN\n",
      " |              stackplot(x, y1, y2, y3, y4)  # where y1, y2, y3, y4, are all 1xNm\n",
      " |      \n",
      " |      baseline : {'zero', 'sym', 'wiggle', 'weighted_wiggle'}\n",
      " |          Method used to calculate the baseline:\n",
      " |      \n",
      " |          - ``'zero'``: Constant zero baseline, i.e. a simple stacked plot.\n",
      " |          - ``'sym'``:  Symmetric around zero and is sometimes called\n",
      " |            'ThemeRiver'.\n",
      " |          - ``'wiggle'``: Minimizes the sum of the squared slopes.\n",
      " |          - ``'weighted_wiggle'``: Does the same but weights to account for\n",
      " |            size of each layer. It is also called 'Streamgraph'-layout. More\n",
      " |            details can be found at http://leebyron.com/streamgraph/.\n",
      " |      \n",
      " |      labels : Length N sequence of strings\n",
      " |          Labels to assign to each data series.\n",
      " |      \n",
      " |      colors : Length N sequence of colors\n",
      " |          A list or tuple of colors. These will be cycled through and used to\n",
      " |          colour the stacked areas.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          All other keyword arguments are passed to `.Axes.fill_between`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of `.PolyCollection`\n",
      " |          A list of `.PolyCollection` instances, one for each element in the\n",
      " |          stacked area plot.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      \n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          every other argument can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception).\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  stem(self, *args, linefmt=None, markerfmt=None, basefmt=None, bottom=0, label=None, use_line_collection=True, data=None)\n",
      " |      Create a stem plot.\n",
      " |      \n",
      " |      A stem plot plots vertical lines at each *x* location from the baseline\n",
      " |      to *y*, and places a marker there.\n",
      " |      \n",
      " |      Call signature::\n",
      " |      \n",
      " |        stem([x,] y, linefmt=None, markerfmt=None, basefmt=None)\n",
      " |      \n",
      " |      The x-positions are optional. The formats may be provided either as\n",
      " |      positional or as keyword-arguments.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : array-like, optional\n",
      " |          The x-positions of the stems. Default: (0, 1, ..., len(y) - 1).\n",
      " |      \n",
      " |      y : array-like\n",
      " |          The y-values of the stem heads.\n",
      " |      \n",
      " |      linefmt : str, optional\n",
      " |          A string defining the properties of the vertical lines. Usually,\n",
      " |          this will be a color or a color and a linestyle:\n",
      " |      \n",
      " |          =========  =============\n",
      " |          Character  Line Style\n",
      " |          =========  =============\n",
      " |          ``'-'``    solid line\n",
      " |          ``'--'``   dashed line\n",
      " |          ``'-.'``   dash-dot line\n",
      " |          ``':'``    dotted line\n",
      " |          =========  =============\n",
      " |      \n",
      " |          Default: 'C0-', i.e. solid line with the first color of the color\n",
      " |          cycle.\n",
      " |      \n",
      " |          Note: While it is technically possible to specify valid formats\n",
      " |          other than color or color and linestyle (e.g. 'rx' or '-.'), this\n",
      " |          is beyond the intention of the method and will most likely not\n",
      " |          result in a reasonable plot.\n",
      " |      \n",
      " |      markerfmt : str, optional\n",
      " |          A string defining the properties of the markers at the stem heads.\n",
      " |          Default: 'C0o', i.e. filled circles with the first color of the\n",
      " |          color cycle.\n",
      " |      \n",
      " |      basefmt : str, default: 'C3-' ('C2-' in classic mode)\n",
      " |          A format string defining the properties of the baseline.\n",
      " |      \n",
      " |      bottom : float, default: 0\n",
      " |          The y-position of the baseline.\n",
      " |      \n",
      " |      label : str, default: None\n",
      " |          The label to use for the stems in legends.\n",
      " |      \n",
      " |      use_line_collection : bool, default: True\n",
      " |          If ``True``, store and plot the stem lines as a\n",
      " |          `~.collections.LineCollection` instead of individual lines, which\n",
      " |          significantly increases performance.  If ``False``, defaults to the\n",
      " |          old behavior of using a list of `.Line2D` objects.  This parameter\n",
      " |          may be deprecated in the future.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `.StemContainer`\n",
      " |          The container may be treated like a tuple\n",
      " |          (*markerline*, *stemlines*, *baseline*)\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. seealso::\n",
      " |          The MATLAB function\n",
      " |          `stem <https://www.mathworks.com/help/matlab/ref/stem.html>`_\n",
      " |          which inspired this method.\n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          every other argument can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception).\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  step(self, x, y, *args, where='pre', data=None, **kwargs)\n",
      " |      Make a step plot.\n",
      " |      \n",
      " |      Call signatures::\n",
      " |      \n",
      " |          step(x, y, [fmt], *, data=None, where='pre', **kwargs)\n",
      " |          step(x, y, [fmt], x2, y2, [fmt2], ..., *, where='pre', **kwargs)\n",
      " |      \n",
      " |      This is just a thin wrapper around `.plot` which changes some\n",
      " |      formatting options. Most of the concepts and parameters of plot can be\n",
      " |      used here as well.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : array-like\n",
      " |          1-D sequence of x positions. It is assumed, but not checked, that\n",
      " |          it is uniformly increasing.\n",
      " |      \n",
      " |      y : array-like\n",
      " |          1-D sequence of y levels.\n",
      " |      \n",
      " |      fmt : str, optional\n",
      " |          A format string, e.g. 'g' for a green line. See `.plot` for a more\n",
      " |          detailed description.\n",
      " |      \n",
      " |          Note: While full format strings are accepted, it is recommended to\n",
      " |          only specify the color. Line styles are currently ignored (use\n",
      " |          the keyword argument *linestyle* instead). Markers are accepted\n",
      " |          and plotted on the given positions, however, this is a rarely\n",
      " |          needed feature for step plots.\n",
      " |      \n",
      " |      data : indexable object, optional\n",
      " |          An object with labelled data. If given, provide the label names to\n",
      " |          plot in *x* and *y*.\n",
      " |      \n",
      " |      where : {'pre', 'post', 'mid'}, default: 'pre'\n",
      " |          Define where the steps should be placed:\n",
      " |      \n",
      " |          - 'pre': The y value is continued constantly to the left from\n",
      " |            every *x* position, i.e. the interval ``(x[i-1], x[i]]`` has the\n",
      " |            value ``y[i]``.\n",
      " |          - 'post': The y value is continued constantly to the right from\n",
      " |            every *x* position, i.e. the interval ``[x[i], x[i+1])`` has the\n",
      " |            value ``y[i]``.\n",
      " |          - 'mid': Steps occur half-way between the *x* positions.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      lines\n",
      " |          A list of `.Line2D` objects representing the plotted data.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs\n",
      " |          Additional parameters are the same as those for `.plot`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. [notes section required to get data note injection right]\n",
      " |  \n",
      " |  streamplot(axes, x, y, u, v, density=1, linewidth=None, color=None, cmap=None, norm=None, arrowsize=1, arrowstyle='-|>', minlength=0.1, transform=None, zorder=None, start_points=None, maxlength=4.0, integration_direction='both', *, data=None)\n",
      " |      Draw streamlines of a vector flow.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x, y : 1D arrays\n",
      " |          An evenly spaced grid.\n",
      " |      u, v : 2D arrays\n",
      " |          *x* and *y*-velocities. The number of rows and columns must match\n",
      " |          the length of *y* and *x*, respectively.\n",
      " |      density : float or (float, float)\n",
      " |          Controls the closeness of streamlines. When ``density = 1``, the domain\n",
      " |          is divided into a 30x30 grid. *density* linearly scales this grid.\n",
      " |          Each cell in the grid can have, at most, one traversing streamline.\n",
      " |          For different densities in each direction, use a tuple\n",
      " |          (density_x, density_y).\n",
      " |      linewidth : float or 2D array\n",
      " |          The width of the stream lines. With a 2D array the line width can be\n",
      " |          varied across the grid. The array must have the same shape as *u*\n",
      " |          and *v*.\n",
      " |      color : color or 2D array\n",
      " |          The streamline color. If given an array, its values are converted to\n",
      " |          colors using *cmap* and *norm*.  The array must have the same shape\n",
      " |          as *u* and *v*.\n",
      " |      cmap : `~matplotlib.colors.Colormap`\n",
      " |          Colormap used to plot streamlines and arrows. This is only used if\n",
      " |          *color* is an array.\n",
      " |      norm : `~matplotlib.colors.Normalize`\n",
      " |          Normalize object used to scale luminance data to 0, 1. If ``None``,\n",
      " |          stretch (min, max) to (0, 1). This is only used if *color* is an array.\n",
      " |      arrowsize : float\n",
      " |          Scaling factor for the arrow size.\n",
      " |      arrowstyle : str\n",
      " |          Arrow style specification.\n",
      " |          See `~matplotlib.patches.FancyArrowPatch`.\n",
      " |      minlength : float\n",
      " |          Minimum length of streamline in axes coordinates.\n",
      " |      start_points : Nx2 array\n",
      " |          Coordinates of starting points for the streamlines in data coordinates\n",
      " |          (the same coordinates as the *x* and *y* arrays).\n",
      " |      zorder : int\n",
      " |          The zorder of the stream lines and arrows.\n",
      " |          Artists with lower zorder values are drawn first.\n",
      " |      maxlength : float\n",
      " |          Maximum length of streamline in axes coordinates.\n",
      " |      integration_direction : {'forward', 'backward', 'both'}, default: 'both'\n",
      " |          Integrate the streamline in forward, backward or both directions.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      StreamplotSet\n",
      " |          Container object with attributes\n",
      " |      \n",
      " |          - ``lines``: `.LineCollection` of streamlines\n",
      " |      \n",
      " |          - ``arrows``: `.PatchCollection` containing `.FancyArrowPatch`\n",
      " |            objects representing the arrows half-way along stream lines.\n",
      " |      \n",
      " |          This container will probably change in the future to allow changes\n",
      " |          to the colormap, alpha, etc. for both lines and arrows, but these\n",
      " |          changes should be backward compatible.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      \n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          the following arguments can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception):\n",
      " |          *x*, *y*, *u*, *v*, *start_points*.\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  table(ax, cellText=None, cellColours=None, cellLoc='right', colWidths=None, rowLabels=None, rowColours=None, rowLoc='left', colLabels=None, colColours=None, colLoc='center', loc='bottom', bbox=None, edges='closed', **kwargs)\n",
      " |      Add a table to an `~.axes.Axes`.\n",
      " |      \n",
      " |      At least one of *cellText* or *cellColours* must be specified. These\n",
      " |      parameters must be 2D lists, in which the outer lists define the rows and\n",
      " |      the inner list define the column values per row. Each row must have the\n",
      " |      same number of elements.\n",
      " |      \n",
      " |      The table can optionally have row and column headers, which are configured\n",
      " |      using *rowLabels*, *rowColours*, *rowLoc* and *colLabels*, *colColours*,\n",
      " |      *colLoc* respectively.\n",
      " |      \n",
      " |      For finer grained control over tables, use the `.Table` class and add it to\n",
      " |      the axes with `.Axes.add_table`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cellText : 2D list of str, optional\n",
      " |          The texts to place into the table cells.\n",
      " |      \n",
      " |          *Note*: Line breaks in the strings are currently not accounted for and\n",
      " |          will result in the text exceeding the cell boundaries.\n",
      " |      \n",
      " |      cellColours : 2D list of colors, optional\n",
      " |          The background colors of the cells.\n",
      " |      \n",
      " |      cellLoc : {'left', 'center', 'right'}, default: 'right'\n",
      " |          The alignment of the text within the cells.\n",
      " |      \n",
      " |      colWidths : list of float, optional\n",
      " |          The column widths in units of the axes. If not given, all columns will\n",
      " |          have a width of *1 / ncols*.\n",
      " |      \n",
      " |      rowLabels : list of str, optional\n",
      " |          The text of the row header cells.\n",
      " |      \n",
      " |      rowColours : list of colors, optional\n",
      " |          The colors of the row header cells.\n",
      " |      \n",
      " |      rowLoc : {'left', 'center', 'right'}, default: 'left'\n",
      " |          The text alignment of the row header cells.\n",
      " |      \n",
      " |      colLabels : list of str, optional\n",
      " |          The text of the column header cells.\n",
      " |      \n",
      " |      colColours : list of colors, optional\n",
      " |          The colors of the column header cells.\n",
      " |      \n",
      " |      colLoc : {'left', 'center', 'right'}, default: 'left'\n",
      " |          The text alignment of the column header cells.\n",
      " |      \n",
      " |      loc : str, optional\n",
      " |          The position of the cell with respect to *ax*. This must be one of\n",
      " |          the `~.Table.codes`.\n",
      " |      \n",
      " |      bbox : `.Bbox`, optional\n",
      " |          A bounding box to draw the table into. If this is not *None*, this\n",
      " |          overrides *loc*.\n",
      " |      \n",
      " |      edges : substring of 'BRTL' or {'open', 'closed', 'horizontal', 'vertical'}\n",
      " |          The cell edges to be drawn with a line. See also\n",
      " |          `~.Cell.visible_edges`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `~matplotlib.table.Table`\n",
      " |          The created table.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs\n",
      " |          `.Table` properties.\n",
      " |      \n",
      " |      Properties:\n",
      " |          agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
      " |          alpha: float or None\n",
      " |          animated: bool\n",
      " |          clip_box: `.Bbox`\n",
      " |          clip_on: bool\n",
      " |          clip_path: Patch or (Path, Transform) or None\n",
      " |          contains: unknown\n",
      " |          figure: `.Figure`\n",
      " |          fontsize: float\n",
      " |          gid: str\n",
      " |          in_layout: bool\n",
      " |          label: object\n",
      " |          path_effects: `.AbstractPathEffect`\n",
      " |          picker: None or bool or callable\n",
      " |          rasterized: bool or None\n",
      " |          sketch_params: (scale: float, length: float, randomness: float)\n",
      " |          snap: bool or None\n",
      " |          transform: `.Transform`\n",
      " |          url: str\n",
      " |          visible: bool\n",
      " |          zorder: float\n",
      " |  \n",
      " |  tripcolor(ax, *args, alpha=1.0, norm=None, cmap=None, vmin=None, vmax=None, shading='flat', facecolors=None, **kwargs)\n",
      " |      Create a pseudocolor plot of an unstructured triangular grid.\n",
      " |      \n",
      " |      The triangulation can be specified in one of two ways; either::\n",
      " |      \n",
      " |        tripcolor(triangulation, ...)\n",
      " |      \n",
      " |      where triangulation is a `.Triangulation` object, or\n",
      " |      \n",
      " |      ::\n",
      " |      \n",
      " |        tripcolor(x, y, ...)\n",
      " |        tripcolor(x, y, triangles, ...)\n",
      " |        tripcolor(x, y, triangles=triangles, ...)\n",
      " |        tripcolor(x, y, mask=mask, ...)\n",
      " |        tripcolor(x, y, triangles, mask=mask, ...)\n",
      " |      \n",
      " |      in which case a Triangulation object will be created.  See `.Triangulation`\n",
      " |      for a explanation of these possibilities.\n",
      " |      \n",
      " |      The next argument must be *C*, the array of color values, either\n",
      " |      one per point in the triangulation if color values are defined at\n",
      " |      points, or one per triangle in the triangulation if color values\n",
      " |      are defined at triangles. If there are the same number of points\n",
      " |      and triangles in the triangulation it is assumed that color\n",
      " |      values are defined at points; to force the use of color values at\n",
      " |      triangles use the kwarg ``facecolors=C`` instead of just ``C``.\n",
      " |      \n",
      " |      *shading* may be 'flat' (the default) or 'gouraud'. If *shading*\n",
      " |      is 'flat' and C values are defined at points, the color values\n",
      " |      used for each triangle are from the mean C of the triangle's\n",
      " |      three points. If *shading* is 'gouraud' then color values must be\n",
      " |      defined at points.\n",
      " |      \n",
      " |      The remaining kwargs are the same as for `~.Axes.pcolor`.\n",
      " |  \n",
      " |  triplot(ax, *args, **kwargs)\n",
      " |      Draw a unstructured triangular grid as lines and/or markers.\n",
      " |      \n",
      " |      The triangulation to plot can be specified in one of two ways; either::\n",
      " |      \n",
      " |        triplot(triangulation, ...)\n",
      " |      \n",
      " |      where triangulation is a `.Triangulation` object, or\n",
      " |      \n",
      " |      ::\n",
      " |      \n",
      " |        triplot(x, y, ...)\n",
      " |        triplot(x, y, triangles, ...)\n",
      " |        triplot(x, y, triangles=triangles, ...)\n",
      " |        triplot(x, y, mask=mask, ...)\n",
      " |        triplot(x, y, triangles, mask=mask, ...)\n",
      " |      \n",
      " |      in which case a Triangulation object will be created.  See `.Triangulation`\n",
      " |      for a explanation of these possibilities.\n",
      " |      \n",
      " |      The remaining args and kwargs are the same as for `~.Axes.plot`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      lines : `~matplotlib.lines.Line2D`\n",
      " |          The drawn triangles edges.\n",
      " |      markers : `~matplotlib.lines.Line2D`\n",
      " |          The drawn marker nodes.\n",
      " |  \n",
      " |  violin(self, vpstats, positions=None, vert=True, widths=0.5, showmeans=False, showextrema=True, showmedians=False)\n",
      " |      Drawing function for violin plots.\n",
      " |      \n",
      " |      Draw a violin plot for each column of *vpstats*. Each filled area\n",
      " |      extends to represent the entire data range, with optional lines at the\n",
      " |      mean, the median, the minimum, the maximum, and the quantiles values.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vpstats : list of dicts\n",
      " |        A list of dictionaries containing stats for each violin plot.\n",
      " |        Required keys are:\n",
      " |      \n",
      " |        - ``coords``: A list of scalars containing the coordinates that\n",
      " |          the violin's kernel density estimate were evaluated at.\n",
      " |      \n",
      " |        - ``vals``: A list of scalars containing the values of the\n",
      " |          kernel density estimate at each of the coordinates given\n",
      " |          in *coords*.\n",
      " |      \n",
      " |        - ``mean``: The mean value for this violin's dataset.\n",
      " |      \n",
      " |        - ``median``: The median value for this violin's dataset.\n",
      " |      \n",
      " |        - ``min``: The minimum value for this violin's dataset.\n",
      " |      \n",
      " |        - ``max``: The maximum value for this violin's dataset.\n",
      " |      \n",
      " |        Optional keys are:\n",
      " |      \n",
      " |        - ``quantiles``: A list of scalars containing the quantile values\n",
      " |          for this violin's dataset.\n",
      " |      \n",
      " |      positions : array-like, default: [1, 2, ..., n]\n",
      " |        Sets the positions of the violins. The ticks and limits are\n",
      " |        automatically set to match the positions.\n",
      " |      \n",
      " |      vert : bool, default: True.\n",
      " |        If true, plots the violins vertically.\n",
      " |        Otherwise, plots the violins horizontally.\n",
      " |      \n",
      " |      widths : array-like, default: 0.5\n",
      " |        Either a scalar or a vector that sets the maximal width of\n",
      " |        each violin. The default is 0.5, which uses about half of the\n",
      " |        available horizontal space.\n",
      " |      \n",
      " |      showmeans : bool, default: False\n",
      " |        If true, will toggle rendering of the means.\n",
      " |      \n",
      " |      showextrema : bool, default: True\n",
      " |        If true, will toggle rendering of the extrema.\n",
      " |      \n",
      " |      showmedians : bool, default: False\n",
      " |        If true, will toggle rendering of the medians.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |        A dictionary mapping each component of the violinplot to a\n",
      " |        list of the corresponding collection instances created. The\n",
      " |        dictionary has the following keys:\n",
      " |      \n",
      " |        - ``bodies``: A list of the `~.collections.PolyCollection`\n",
      " |          instances containing the filled area of each violin.\n",
      " |      \n",
      " |        - ``cmeans``: A `~.collections.LineCollection` instance that marks\n",
      " |          the mean values of each of the violin's distribution.\n",
      " |      \n",
      " |        - ``cmins``: A `~.collections.LineCollection` instance that marks\n",
      " |          the bottom of each violin's distribution.\n",
      " |      \n",
      " |        - ``cmaxes``: A `~.collections.LineCollection` instance that marks\n",
      " |          the top of each violin's distribution.\n",
      " |      \n",
      " |        - ``cbars``: A `~.collections.LineCollection` instance that marks\n",
      " |          the centers of each violin's distribution.\n",
      " |      \n",
      " |        - ``cmedians``: A `~.collections.LineCollection` instance that\n",
      " |          marks the median values of each of the violin's distribution.\n",
      " |      \n",
      " |        - ``cquantiles``: A `~.collections.LineCollection` instance created\n",
      " |          to identify the quantiles values of each of the violin's\n",
      " |          distribution.\n",
      " |  \n",
      " |  violinplot(self, dataset, positions=None, vert=True, widths=0.5, showmeans=False, showextrema=True, showmedians=False, quantiles=None, points=100, bw_method=None, *, data=None)\n",
      " |      Make a violin plot.\n",
      " |      \n",
      " |      Make a violin plot for each column of *dataset* or each vector in\n",
      " |      sequence *dataset*.  Each filled area extends to represent the\n",
      " |      entire data range, with optional lines at the mean, the median,\n",
      " |      the minimum, the maximum, and user-specified quantiles.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dataset : Array or a sequence of vectors.\n",
      " |        The input data.\n",
      " |      \n",
      " |      positions : array-like, default: [1, 2, ..., n]\n",
      " |        Sets the positions of the violins. The ticks and limits are\n",
      " |        automatically set to match the positions.\n",
      " |      \n",
      " |      vert : bool, default: True.\n",
      " |        If true, creates a vertical violin plot.\n",
      " |        Otherwise, creates a horizontal violin plot.\n",
      " |      \n",
      " |      widths : array-like, default: 0.5\n",
      " |        Either a scalar or a vector that sets the maximal width of\n",
      " |        each violin. The default is 0.5, which uses about half of the\n",
      " |        available horizontal space.\n",
      " |      \n",
      " |      showmeans : bool, default: False\n",
      " |        If `True`, will toggle rendering of the means.\n",
      " |      \n",
      " |      showextrema : bool, default: True\n",
      " |        If `True`, will toggle rendering of the extrema.\n",
      " |      \n",
      " |      showmedians : bool, default: False\n",
      " |        If `True`, will toggle rendering of the medians.\n",
      " |      \n",
      " |      quantiles : array-like, default: None\n",
      " |        If not None, set a list of floats in interval [0, 1] for each violin,\n",
      " |        which stands for the quantiles that will be rendered for that\n",
      " |        violin.\n",
      " |      \n",
      " |      points : int, default: 100\n",
      " |        Defines the number of points to evaluate each of the\n",
      " |        gaussian kernel density estimations at.\n",
      " |      \n",
      " |      bw_method : str, scalar or callable, optional\n",
      " |        The method used to calculate the estimator bandwidth.  This can be\n",
      " |        'scott', 'silverman', a scalar constant or a callable.  If a\n",
      " |        scalar, this will be used directly as `kde.factor`.  If a\n",
      " |        callable, it should take a `GaussianKDE` instance as its only\n",
      " |        parameter and return a scalar. If None (default), 'scott' is used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict\n",
      " |        A dictionary mapping each component of the violinplot to a\n",
      " |        list of the corresponding collection instances created. The\n",
      " |        dictionary has the following keys:\n",
      " |      \n",
      " |        - ``bodies``: A list of the `~.collections.PolyCollection`\n",
      " |          instances containing the filled area of each violin.\n",
      " |      \n",
      " |        - ``cmeans``: A `~.collections.LineCollection` instance that marks\n",
      " |          the mean values of each of the violin's distribution.\n",
      " |      \n",
      " |        - ``cmins``: A `~.collections.LineCollection` instance that marks\n",
      " |          the bottom of each violin's distribution.\n",
      " |      \n",
      " |        - ``cmaxes``: A `~.collections.LineCollection` instance that marks\n",
      " |          the top of each violin's distribution.\n",
      " |      \n",
      " |        - ``cbars``: A `~.collections.LineCollection` instance that marks\n",
      " |          the centers of each violin's distribution.\n",
      " |      \n",
      " |        - ``cmedians``: A `~.collections.LineCollection` instance that\n",
      " |          marks the median values of each of the violin's distribution.\n",
      " |      \n",
      " |        - ``cquantiles``: A `~.collections.LineCollection` instance created\n",
      " |          to identify the quantile values of each of the violin's\n",
      " |          distribution.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      \n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          the following arguments can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception):\n",
      " |          *dataset*.\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  vlines(self, x, ymin, ymax, colors=None, linestyles='solid', label='', *, data=None, **kwargs)\n",
      " |      Plot vertical lines.\n",
      " |      \n",
      " |      Plot vertical lines at each *x* from *ymin* to *ymax*.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : float or array-like\n",
      " |          x-indexes where to plot the lines.\n",
      " |      \n",
      " |      ymin, ymax : float or array-like\n",
      " |          Respective beginning and end of each line. If scalars are\n",
      " |          provided, all lines will have same length.\n",
      " |      \n",
      " |      colors : list of colors, default: :rc:`lines.color`\n",
      " |      \n",
      " |      linestyles : {'solid', 'dashed', 'dashdot', 'dotted'}, optional\n",
      " |      \n",
      " |      label : str, default: ''\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `~matplotlib.collections.LineCollection`\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs : `~matplotlib.collections.LineCollection` properties.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      hlines : horizontal lines\n",
      " |      axvline: vertical line across the axes\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      \n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          the following arguments can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception):\n",
      " |          *x*, *ymin*, *ymax*, *colors*.\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  xcorr(self, x, y, normed=True, detrend=<function detrend_none at 0x125d49040>, usevlines=True, maxlags=10, *, data=None, **kwargs)\n",
      " |      Plot the cross correlation between *x* and *y*.\n",
      " |      \n",
      " |      The correlation with lag k is defined as\n",
      " |      :math:`\\sum_n x[n+k] \\cdot y^*[n]`, where :math:`y^*` is the complex\n",
      " |      conjugate of :math:`y`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x, y : array-like of length n\n",
      " |      \n",
      " |      detrend : callable, default: `.mlab.detrend_none` (no detrending)\n",
      " |          A detrending function applied to *x* and *y*.  It must have the\n",
      " |          signature ::\n",
      " |      \n",
      " |              detrend(x: np.ndarray) -> np.ndarray\n",
      " |      \n",
      " |      normed : bool, default: True\n",
      " |          If ``True``, input vectors are normalised to unit length.\n",
      " |      \n",
      " |      usevlines : bool, default: True\n",
      " |          Determines the plot style.\n",
      " |      \n",
      " |          If ``True``, vertical lines are plotted from 0 to the xcorr value\n",
      " |          using `.Axes.vlines`. Additionally, a horizontal line is plotted\n",
      " |          at y=0 using `.Axes.axhline`.\n",
      " |      \n",
      " |          If ``False``, markers are plotted at the xcorr values using\n",
      " |          `.Axes.plot`.\n",
      " |      \n",
      " |      maxlags : int, default: 10\n",
      " |          Number of lags to show. If None, will return all ``2 * len(x) - 1``\n",
      " |          lags.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      lags : array (length ``2*maxlags+1``)\n",
      " |          The lag vector.\n",
      " |      c : array  (length ``2*maxlags+1``)\n",
      " |          The auto correlation vector.\n",
      " |      line : `.LineCollection` or `.Line2D`\n",
      " |          `.Artist` added to the axes of the correlation:\n",
      " |      \n",
      " |          - `.LineCollection` if *usevlines* is True.\n",
      " |          - `.Line2D` if *usevlines* is False.\n",
      " |      b : `.Line2D` or None\n",
      " |          Horizontal line at 0 if *usevlines* is True\n",
      " |          None *usevlines* is False.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      linestyle : `.Line2D` property, optional\n",
      " |          The linestyle for plotting the data points.\n",
      " |          Only used if *usevlines* is ``False``.\n",
      " |      \n",
      " |      marker : str, default: 'o'\n",
      " |          The marker for plotting the data points.\n",
      " |          Only used if *usevlines* is ``False``.\n",
      " |      \n",
      " |      **kwargs\n",
      " |          Additional parameters are passed to `.Axes.vlines` and\n",
      " |          `.Axes.axhline` if *usevlines* is ``True``; otherwise they are\n",
      " |          passed to `.Axes.plot`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The cross correlation is performed with `numpy.correlate` with\n",
      " |      ``mode = \"full\"``.\n",
      " |      \n",
      " |      .. note::\n",
      " |          In addition to the above described arguments, this function can take\n",
      " |          a *data* keyword argument. If such a *data* argument is given,\n",
      " |          the following arguments can also be string ``s``, which is\n",
      " |          interpreted as ``data[s]`` (unless this raises an exception):\n",
      " |          *x*, *y*.\n",
      " |      \n",
      " |          Objects passed as **data** must support item access (``data[s]``) and\n",
      " |          membership test (``s in data``).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from matplotlib.axes._base._AxesBase:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  add_artist(self, a)\n",
      " |      Add an `~.Artist` to the axes, and return the artist.\n",
      " |      \n",
      " |      Use `add_artist` only for artists for which there is no dedicated\n",
      " |      \"add\" method; and if necessary, use a method such as `update_datalim`\n",
      " |      to manually update the dataLim if the artist is to be included in\n",
      " |      autoscaling.\n",
      " |      \n",
      " |      If no ``transform`` has been specified when creating the artist (e.g.\n",
      " |      ``artist.get_transform() == None``) then the transform is set to\n",
      " |      ``ax.transData``.\n",
      " |  \n",
      " |  add_child_axes(self, ax)\n",
      " |      Add an `~.AxesBase` to the axes' children; return the child axes.\n",
      " |      \n",
      " |      This is the lowlevel version.  See `.axes.Axes.inset_axes`.\n",
      " |  \n",
      " |  add_collection(self, collection, autolim=True)\n",
      " |      Add a `~.Collection` to the axes' collections; return the collection.\n",
      " |  \n",
      " |  add_container(self, container)\n",
      " |      Add a `~.Container` to the axes' containers; return the container.\n",
      " |  \n",
      " |  add_image(self, image)\n",
      " |      Add an `~.AxesImage` to the axes' images; return the image.\n",
      " |  \n",
      " |  add_line(self, line)\n",
      " |      Add a `.Line2D` to the axes' lines; return the line.\n",
      " |  \n",
      " |  add_patch(self, p)\n",
      " |      Add a `~.Patch` to the axes' patches; return the patch.\n",
      " |  \n",
      " |  add_table(self, tab)\n",
      " |      Add a `~.Table` to the axes' tables; return the table.\n",
      " |  \n",
      " |  axis(self, *args, emit=True, **kwargs)\n",
      " |      Convenience method to get or set some axis properties.\n",
      " |      \n",
      " |      Call signatures::\n",
      " |      \n",
      " |        xmin, xmax, ymin, ymax = axis()\n",
      " |        xmin, xmax, ymin, ymax = axis([xmin, xmax, ymin, ymax])\n",
      " |        xmin, xmax, ymin, ymax = axis(option)\n",
      " |        xmin, xmax, ymin, ymax = axis(**kwargs)\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      xmin, xmax, ymin, ymax : float, optional\n",
      " |          The axis limits to be set.  This can also be achieved using ::\n",
      " |      \n",
      " |              ax.set(xlim=(xmin, xmax), ylim=(ymin, ymax))\n",
      " |      \n",
      " |      option : bool or str\n",
      " |          If a bool, turns axis lines and labels on or off. If a string,\n",
      " |          possible values are:\n",
      " |      \n",
      " |          ======== ==========================================================\n",
      " |          Value    Description\n",
      " |          ======== ==========================================================\n",
      " |          'on'     Turn on axis lines and labels. Same as ``True``.\n",
      " |          'off'    Turn off axis lines and labels. Same as ``False``.\n",
      " |          'equal'  Set equal scaling (i.e., make circles circular) by\n",
      " |                   changing axis limits. This is the same as\n",
      " |                   ``ax.set_aspect('equal', adjustable='datalim')``.\n",
      " |                   Explicit data limits may not be respected in this case.\n",
      " |          'scaled' Set equal scaling (i.e., make circles circular) by\n",
      " |                   changing dimensions of the plot box. This is the same as\n",
      " |                   ``ax.set_aspect('equal', adjustable='box', anchor='C')``.\n",
      " |                   Additionally, further autoscaling will be disabled.\n",
      " |          'tight'  Set limits just large enough to show all data, then\n",
      " |                   disable further autoscaling.\n",
      " |          'auto'   Automatic scaling (fill plot box with data).\n",
      " |          'image'  'scaled' with axis limits equal to data limits.\n",
      " |          'square' Square plot; similar to 'scaled', but initially forcing\n",
      " |                   ``xmax-xmin == ymax-ymin``.\n",
      " |          ======== ==========================================================\n",
      " |      \n",
      " |      emit : bool, default: True\n",
      " |          Whether observers are notified of the axis limit change.\n",
      " |          This option is passed on to `~.Axes.set_xlim` and\n",
      " |          `~.Axes.set_ylim`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      xmin, xmax, ymin, ymax : float\n",
      " |          The axis limits.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      matplotlib.axes.Axes.set_xlim\n",
      " |      matplotlib.axes.Axes.set_ylim\n",
      " |  \n",
      " |  clear(self)\n",
      " |      Clear the axes.\n",
      " |  \n",
      " |  contains(self, mouseevent)\n",
      " |      Test whether the artist contains the mouse event.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      mouseevent : `matplotlib.backend_bases.MouseEvent`\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      contains : bool\n",
      " |          Whether any values are within the radius.\n",
      " |      details : dict\n",
      " |          An artist-specific dictionary of details of the event context,\n",
      " |          such as which points are contained in the pick radius. See the\n",
      " |          individual Artist subclasses for details.\n",
      " |  \n",
      " |  contains_point(self, point)\n",
      " |      Return whether *point* (pair of pixel coordinates) is inside the axes\n",
      " |      patch.\n",
      " |  \n",
      " |  drag_pan(self, button, key, x, y)\n",
      " |      Called when the mouse moves during a pan operation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      button : `.MouseButton`\n",
      " |          The pressed mouse button.\n",
      " |      key : str or None\n",
      " |          The pressed key, if any.\n",
      " |      x, y : float\n",
      " |          The mouse coordinates in display coords.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is intended to be overridden by new projection types.\n",
      " |  \n",
      " |  draw_artist(self, a)\n",
      " |      Efficiently redraw a single artist.\n",
      " |      \n",
      " |      This method can only be used after an initial draw which caches the\n",
      " |      renderer.\n",
      " |  \n",
      " |  end_pan(self)\n",
      " |      Called when a pan operation completes (when the mouse button is up.)\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is intended to be overridden by new projection types.\n",
      " |  \n",
      " |  format_xdata(self, x)\n",
      " |      Return *x* formatted as an x-value.\n",
      " |      \n",
      " |      This function will use the `.fmt_xdata` attribute if it is not None,\n",
      " |      else will fall back on the xaxis major formatter.\n",
      " |  \n",
      " |  format_ydata(self, y)\n",
      " |      Return *y* formatted as an y-value.\n",
      " |      \n",
      " |      This function will use the `.fmt_ydata` attribute if it is not None,\n",
      " |      else will fall back on the yaxis major formatter.\n",
      " |  \n",
      " |  get_adjustable(self)\n",
      " |      Return whether the Axes will adjust its physical dimension ('box') or\n",
      " |      its data limits ('datalim') to achieve the desired aspect ratio.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      matplotlib.axes.Axes.set_adjustable\n",
      " |          Set how the Axes adjusts to achieve the required aspect ratio.\n",
      " |      matplotlib.axes.Axes.set_aspect\n",
      " |          For a description of aspect handling.\n",
      " |  \n",
      " |  get_anchor(self)\n",
      " |      Get the anchor location.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      matplotlib.axes.Axes.set_anchor\n",
      " |          for a description of the anchor.\n",
      " |      matplotlib.axes.Axes.set_aspect\n",
      " |          for a description of aspect handling.\n",
      " |  \n",
      " |  get_aspect(self)\n",
      " |  \n",
      " |  get_autoscalex_on(self)\n",
      " |      Get whether autoscaling for the x-axis is applied on plot commands\n",
      " |  \n",
      " |  get_autoscaley_on(self)\n",
      " |      Get whether autoscaling for the y-axis is applied on plot commands\n",
      " |  \n",
      " |  get_axes_locator(self)\n",
      " |      Return the axes_locator.\n",
      " |  \n",
      " |  get_axisbelow(self)\n",
      " |      Get whether axis ticks and gridlines are above or below most artists.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      bool or 'line'\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      set_axisbelow\n",
      " |  \n",
      " |  get_box_aspect(self)\n",
      " |      Get the axes box aspect.\n",
      " |      Will be ``None`` if not explicitly specified.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      matplotlib.axes.Axes.set_box_aspect\n",
      " |          for a description of box aspect.\n",
      " |      matplotlib.axes.Axes.set_aspect\n",
      " |          for a description of aspect handling.\n",
      " |  \n",
      " |  get_children(self)\n",
      " |      Return a list of the child `.Artist`\\s of this `.Artist`.\n",
      " |  \n",
      " |  get_data_ratio(self)\n",
      " |      Return the aspect ratio of the scaled data.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This method is intended to be overridden by new projection types.\n",
      " |  \n",
      " |  get_data_ratio_log(self)\n",
      " |      [*Deprecated*] Return the aspect ratio of the raw data in log scale.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Will be used when both axis are in log scale.\n",
      " |      \n",
      " |      .. deprecated:: 3.2\n",
      " |  \n",
      " |  get_default_bbox_extra_artists(self)\n",
      " |      Return a default list of artists that are used for the bounding box\n",
      " |      calculation.\n",
      " |      \n",
      " |      Artists are excluded either by not being visible or\n",
      " |      ``artist.set_in_layout(False)``.\n",
      " |  \n",
      " |  get_facecolor(self)\n",
      " |      Get the facecolor of the Axes.\n",
      " |  \n",
      " |  get_fc(self)\n",
      " |      Alias for `get_facecolor`.\n",
      " |  \n",
      " |  get_images(self)\n",
      " |      Return a list of `.AxesImage`\\s contained by the Axes.\n",
      " |  \n",
      " |  get_legend(self)\n",
      " |      Return the `.Legend` instance, or None if no legend is defined.\n",
      " |  \n",
      " |  get_lines(self)\n",
      " |      Return a list of lines contained by the Axes.\n",
      " |  \n",
      " |  get_navigate(self)\n",
      " |      Get whether the axes responds to navigation commands\n",
      " |  \n",
      " |  get_navigate_mode(self)\n",
      " |      Get the navigation toolbar button status: 'PAN', 'ZOOM', or None\n",
      " |  \n",
      " |  get_position(self, original=False)\n",
      " |      Get a copy of the axes rectangle as a `.Bbox`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      original : bool\n",
      " |          If ``True``, return the original position. Otherwise return the\n",
      " |          active position. For an explanation of the positions see\n",
      " |          `.set_position`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `.Bbox`\n",
      " |  \n",
      " |  get_rasterization_zorder(self)\n",
      " |      Return the zorder value below which artists will be rasterized.\n",
      " |  \n",
      " |  get_renderer_cache(self)\n",
      " |  \n",
      " |  get_shared_x_axes(self)\n",
      " |      Return a reference to the shared axes Grouper object for x axes.\n",
      " |  \n",
      " |  get_shared_y_axes(self)\n",
      " |      Return a reference to the shared axes Grouper object for y axes.\n",
      " |  \n",
      " |  get_window_extent(self, *args, **kwargs)\n",
      " |      Return the axes bounding box in display space; *args* and *kwargs*\n",
      " |      are empty.\n",
      " |      \n",
      " |      This bounding box does not include the spines, ticks, ticklables,\n",
      " |      or other labels.  For a bounding box including these elements use\n",
      " |      `~matplotlib.axes.Axes.get_tightbbox`.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      matplotlib.axes.Axes.get_tightbbox\n",
      " |      matplotlib.axis.Axis.get_tightbbox\n",
      " |      matplotlib.spines.get_window_extent\n",
      " |  \n",
      " |  get_xaxis(self)\n",
      " |      Return the XAxis instance.\n",
      " |  \n",
      " |  get_xaxis_text1_transform(self, pad_points)\n",
      " |      Returns\n",
      " |      -------\n",
      " |      transform : Transform\n",
      " |          The transform used for drawing x-axis labels, which will add\n",
      " |          *pad_points* of padding (in points) between the axes and the label.\n",
      " |          The x-direction is in data coordinates and the y-direction is in\n",
      " |          axis corrdinates\n",
      " |      valign : {'center', 'top', 'bottom', 'baseline', 'center_baseline'}\n",
      " |          The text vertical alignment.\n",
      " |      halign : {'center', 'left', 'right'}\n",
      " |          The text horizontal alignment.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This transformation is primarily used by the `~matplotlib.axis.Axis`\n",
      " |      class, and is meant to be overridden by new kinds of projections that\n",
      " |      may need to place axis elements in different locations.\n",
      " |  \n",
      " |  get_xaxis_text2_transform(self, pad_points)\n",
      " |      Returns\n",
      " |      -------\n",
      " |      transform : Transform\n",
      " |          The transform used for drawing secondary x-axis labels, which will\n",
      " |          add *pad_points* of padding (in points) between the axes and the\n",
      " |          label.  The x-direction is in data coordinates and the y-direction\n",
      " |          is in axis corrdinates\n",
      " |      valign : {'center', 'top', 'bottom', 'baseline', 'center_baseline'}\n",
      " |          The text vertical alignment.\n",
      " |      halign : {'center', 'left', 'right'}\n",
      " |          The text horizontal alignment.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This transformation is primarily used by the `~matplotlib.axis.Axis`\n",
      " |      class, and is meant to be overridden by new kinds of projections that\n",
      " |      may need to place axis elements in different locations.\n",
      " |  \n",
      " |  get_xaxis_transform(self, which='grid')\n",
      " |      Get the transformation used for drawing x-axis labels, ticks\n",
      " |      and gridlines.  The x-direction is in data coordinates and the\n",
      " |      y-direction is in axis coordinates.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          This transformation is primarily used by the\n",
      " |          `~matplotlib.axis.Axis` class, and is meant to be\n",
      " |          overridden by new kinds of projections that may need to\n",
      " |          place axis elements in different locations.\n",
      " |  \n",
      " |  get_xbound(self)\n",
      " |      Return the lower and upper x-axis bounds, in increasing order.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      set_xbound\n",
      " |      get_xlim, set_xlim\n",
      " |      invert_xaxis, xaxis_inverted\n",
      " |  \n",
      " |  get_xgridlines(self)\n",
      " |      Return the xaxis' grid lines as a list of `.Line2D`\\s.\n",
      " |  \n",
      " |  get_xmajorticklabels(self)\n",
      " |      Return the xaxis' major tick labels, as a list of `~.text.Text`.\n",
      " |  \n",
      " |  get_xminorticklabels(self)\n",
      " |      Return the xaxis' minor tick labels, as a list of `~.text.Text`.\n",
      " |  \n",
      " |  get_xscale(self)\n",
      " |      Return the xaxis' scale (as a str).\n",
      " |  \n",
      " |  get_xticklabels(self, minor=False, which=None)\n",
      " |      Get the xaxis' tick labels.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      minor : bool\n",
      " |         Whether to return the minor or the major ticklabels.\n",
      " |      \n",
      " |      which : None, ('minor', 'major', 'both')\n",
      " |         Overrides *minor*.\n",
      " |      \n",
      " |         Selects which ticklabels to return\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of `~matplotlib.text.Text`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The tick label strings are not populated until a ``draw`` method has\n",
      " |      been called.\n",
      " |      \n",
      " |      See also: `~.pyplot.draw` and `~.FigureCanvasBase.draw`.\n",
      " |  \n",
      " |  get_xticklines(self, minor=False)\n",
      " |      Return the xaxis' tick lines as a list of `.Line2D`\\s.\n",
      " |  \n",
      " |  get_xticks(self, *, minor=False)\n",
      " |      Return the xaxis' tick locations in data coordinates.\n",
      " |  \n",
      " |  get_yaxis(self)\n",
      " |      Return the YAxis instance.\n",
      " |  \n",
      " |  get_yaxis_text1_transform(self, pad_points)\n",
      " |      Returns\n",
      " |      -------\n",
      " |      transform : Transform\n",
      " |          The transform used for drawing y-axis labels, which will add\n",
      " |          *pad_points* of padding (in points) between the axes and the label.\n",
      " |          The x-direction is in axis coordinates and the y-direction is in\n",
      " |          data corrdinates\n",
      " |      valign : {'center', 'top', 'bottom', 'baseline', 'center_baseline'}\n",
      " |          The text vertical alignment.\n",
      " |      halign : {'center', 'left', 'right'}\n",
      " |          The text horizontal alignment.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This transformation is primarily used by the `~matplotlib.axis.Axis`\n",
      " |      class, and is meant to be overridden by new kinds of projections that\n",
      " |      may need to place axis elements in different locations.\n",
      " |  \n",
      " |  get_yaxis_text2_transform(self, pad_points)\n",
      " |      Returns\n",
      " |      -------\n",
      " |      transform : Transform\n",
      " |          The transform used for drawing secondart y-axis labels, which will\n",
      " |          add *pad_points* of padding (in points) between the axes and the\n",
      " |          label.  The x-direction is in axis coordinates and the y-direction\n",
      " |          is in data corrdinates\n",
      " |      valign : {'center', 'top', 'bottom', 'baseline', 'center_baseline'}\n",
      " |          The text vertical alignment.\n",
      " |      halign : {'center', 'left', 'right'}\n",
      " |          The text horizontal alignment.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This transformation is primarily used by the `~matplotlib.axis.Axis`\n",
      " |      class, and is meant to be overridden by new kinds of projections that\n",
      " |      may need to place axis elements in different locations.\n",
      " |  \n",
      " |  get_yaxis_transform(self, which='grid')\n",
      " |      Get the transformation used for drawing y-axis labels, ticks\n",
      " |      and gridlines.  The x-direction is in axis coordinates and the\n",
      " |      y-direction is in data coordinates.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          This transformation is primarily used by the\n",
      " |          `~matplotlib.axis.Axis` class, and is meant to be\n",
      " |          overridden by new kinds of projections that may need to\n",
      " |          place axis elements in different locations.\n",
      " |  \n",
      " |  get_ybound(self)\n",
      " |      Return the lower and upper y-axis bounds, in increasing order.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      set_ybound\n",
      " |      get_ylim, set_ylim\n",
      " |      invert_yaxis, yaxis_inverted\n",
      " |  \n",
      " |  get_ygridlines(self)\n",
      " |      Return the yaxis' grid lines as a list of `.Line2D`\\s.\n",
      " |  \n",
      " |  get_ymajorticklabels(self)\n",
      " |      Return the yaxis' major tick labels, as a list of `~.text.Text`.\n",
      " |  \n",
      " |  get_yminorticklabels(self)\n",
      " |      Return the yaxis' minor tick labels, as a list of `~.text.Text`.\n",
      " |  \n",
      " |  get_yscale(self)\n",
      " |      Return the yaxis' scale (as a str).\n",
      " |  \n",
      " |  get_yticklabels(self, minor=False, which=None)\n",
      " |      Get the yaxis' tick labels.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      minor : bool\n",
      " |         Whether to return the minor or the major ticklabels.\n",
      " |      \n",
      " |      which : None, ('minor', 'major', 'both')\n",
      " |         Overrides *minor*.\n",
      " |      \n",
      " |         Selects which ticklabels to return\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of `~matplotlib.text.Text`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The tick label strings are not populated until a ``draw`` method has\n",
      " |      been called.\n",
      " |      \n",
      " |      See also: `~.pyplot.draw` and `~.FigureCanvasBase.draw`.\n",
      " |  \n",
      " |  get_yticklines(self, minor=False)\n",
      " |      Return the yaxis' tick lines as a list of `.Line2D`\\s.\n",
      " |  \n",
      " |  get_yticks(self, *, minor=False)\n",
      " |      Return the yaxis' tick locations in data coordinates.\n",
      " |  \n",
      " |  has_data(self)\n",
      " |      Return *True* if any artists have been added to axes.\n",
      " |      \n",
      " |      This should not be used to determine whether the *dataLim*\n",
      " |      need to be updated, and may not actually be useful for\n",
      " |      anything.\n",
      " |  \n",
      " |  in_axes(self, mouseevent)\n",
      " |      Return *True* if the given *mouseevent* (in display coords)\n",
      " |      is in the Axes\n",
      " |  \n",
      " |  invert_xaxis(self)\n",
      " |      Invert the x-axis.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      xaxis_inverted\n",
      " |      get_xlim, set_xlim\n",
      " |      get_xbound, set_xbound\n",
      " |  \n",
      " |  invert_yaxis(self)\n",
      " |      Invert the y-axis.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      yaxis_inverted\n",
      " |      get_ylim, set_ylim\n",
      " |      get_ybound, set_ybound\n",
      " |  \n",
      " |  minorticks_off(self)\n",
      " |      Remove minor ticks from the axes.\n",
      " |  \n",
      " |  minorticks_on(self)\n",
      " |      Display minor ticks on the axes.\n",
      " |      \n",
      " |      Displaying minor ticks may reduce performance; you may turn them off\n",
      " |      using `minorticks_off()` if drawing speed is a problem.\n",
      " |  \n",
      " |  redraw_in_frame(self)\n",
      " |      Efficiently redraw Axes data, but not axis ticks, labels, etc.\n",
      " |      \n",
      " |      This method can only be used after an initial draw which caches the\n",
      " |      renderer.\n",
      " |  \n",
      " |  relim(self, visible_only=False)\n",
      " |      Recompute the data limits based on current artists.\n",
      " |      \n",
      " |      At present, `~.Collection` instances are not supported.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      visible_only : bool, default: False\n",
      " |          Whether to exclude invisible artists.\n",
      " |  \n",
      " |  reset_position(self)\n",
      " |      Reset the active position to the original position.\n",
      " |      \n",
      " |      This resets the a possible position change due to aspect constraints.\n",
      " |      For an explanation of the positions see `.set_position`.\n",
      " |  \n",
      " |  set_adjustable(self, adjustable, share=False)\n",
      " |      Set how the Axes adjusts to achieve the required aspect ratio.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      adjustable : {'box', 'datalim'}\n",
      " |          If 'box', change the physical dimensions of the Axes.\n",
      " |          If 'datalim', change the ``x`` or ``y`` data limits.\n",
      " |      \n",
      " |      share : bool, default: False\n",
      " |          If ``True``, apply the settings to all shared Axes.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      matplotlib.axes.Axes.set_aspect\n",
      " |          For a description of aspect handling.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Shared Axes (of which twinned Axes are a special case)\n",
      " |      impose restrictions on how aspect ratios can be imposed.\n",
      " |      For twinned Axes, use 'datalim'.  For Axes that share both\n",
      " |      x and y, use 'box'.  Otherwise, either 'datalim' or 'box'\n",
      " |      may be used.  These limitations are partly a requirement\n",
      " |      to avoid over-specification, and partly a result of the\n",
      " |      particular implementation we are currently using, in\n",
      " |      which the adjustments for aspect ratios are done sequentially\n",
      " |      and independently on each Axes as it is drawn.\n",
      " |  \n",
      " |  set_autoscalex_on(self, b)\n",
      " |      Set whether autoscaling for the x-axis is applied on plot commands\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      b : bool\n",
      " |  \n",
      " |  set_autoscaley_on(self, b)\n",
      " |      Set whether autoscaling for the y-axis is applied on plot commands\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      b : bool\n",
      " |  \n",
      " |  set_axes_locator(self, locator)\n",
      " |      Set the axes locator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      locator : Callable[[Axes, Renderer], Bbox]\n",
      " |  \n",
      " |  set_axisbelow(self, b)\n",
      " |      Set whether axis ticks and gridlines are above or below most artists.\n",
      " |      \n",
      " |      This controls the zorder of the ticks and gridlines. For more\n",
      " |      information on the zorder see :doc:`/gallery/misc/zorder_demo`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      b : bool or 'line'\n",
      " |          Possible values:\n",
      " |      \n",
      " |          - *True* (zorder = 0.5): Ticks and gridlines are below all Artists.\n",
      " |          - 'line' (zorder = 1.5): Ticks and gridlines are above patches\n",
      " |            (e.g. rectangles, with default zorder = 1) but still below lines\n",
      " |            and markers (with their default zorder = 2).\n",
      " |          - *False* (zorder = 2.5): Ticks and gridlines are above patches\n",
      " |            and lines / markers.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      get_axisbelow\n",
      " |  \n",
      " |  set_facecolor(self, color)\n",
      " |      Set the facecolor of the Axes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      color : color\n",
      " |  \n",
      " |  set_fc(self, color)\n",
      " |      Alias for `set_facecolor`.\n",
      " |  \n",
      " |  set_figure(self, fig)\n",
      " |      Set the `.Figure` instance the artist belongs to.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fig : `.Figure`\n",
      " |  \n",
      " |  set_navigate(self, b)\n",
      " |      Set whether the axes responds to navigation toolbar commands\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      b : bool\n",
      " |  \n",
      " |  set_navigate_mode(self, b)\n",
      " |      Set the navigation toolbar button status;\n",
      " |      \n",
      " |      .. warning::\n",
      " |          this is not a user-API function.\n",
      " |  \n",
      " |  set_position(self, pos, which='both')\n",
      " |      Set the axes position.\n",
      " |      \n",
      " |      Axes have two position attributes. The 'original' position is the\n",
      " |      position allocated for the Axes. The 'active' position is the\n",
      " |      position the Axes is actually drawn at. These positions are usually\n",
      " |      the same unless a fixed aspect is set to the Axes. See `.set_aspect`\n",
      " |      for details.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pos : [left, bottom, width, height] or `~matplotlib.transforms.Bbox`\n",
      " |          The new position of the in `.Figure` coordinates.\n",
      " |      \n",
      " |      which : {'both', 'active', 'original'}, default: 'both'\n",
      " |          Determines which position variables to change.\n",
      " |  \n",
      " |  set_prop_cycle(self, *args, **kwargs)\n",
      " |      Set the property cycle of the Axes.\n",
      " |      \n",
      " |      The property cycle controls the style properties such as color,\n",
      " |      marker and linestyle of future plot commands. The style properties\n",
      " |      of data already added to the Axes are not modified.\n",
      " |      \n",
      " |      Call signatures::\n",
      " |      \n",
      " |        set_prop_cycle(cycler)\n",
      " |        set_prop_cycle(label=values[, label2=values2[, ...]])\n",
      " |        set_prop_cycle(label, values)\n",
      " |      \n",
      " |      Form 1 sets given `~cycler.Cycler` object.\n",
      " |      \n",
      " |      Form 2 creates a `~cycler.Cycler` which cycles over one or more\n",
      " |      properties simultaneously and set it as the property cycle of the\n",
      " |      axes. If multiple properties are given, their value lists must have\n",
      " |      the same length. This is just a shortcut for explicitly creating a\n",
      " |      cycler and passing it to the function, i.e. it's short for\n",
      " |      ``set_prop_cycle(cycler(label=values label2=values2, ...))``.\n",
      " |      \n",
      " |      Form 3 creates a `~cycler.Cycler` for a single property and set it\n",
      " |      as the property cycle of the axes. This form exists for compatibility\n",
      " |      with the original `cycler.cycler` interface. Its use is discouraged\n",
      " |      in favor of the kwarg form, i.e. ``set_prop_cycle(label=values)``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      cycler : Cycler\n",
      " |          Set the given Cycler. *None* resets to the cycle defined by the\n",
      " |          current style.\n",
      " |      \n",
      " |      label : str\n",
      " |          The property key. Must be a valid `.Artist` property.\n",
      " |          For example, 'color' or 'linestyle'. Aliases are allowed,\n",
      " |          such as 'c' for 'color' and 'lw' for 'linewidth'.\n",
      " |      \n",
      " |      values : iterable\n",
      " |          Finite-length iterable of the property values. These values\n",
      " |          are validated and will raise a ValueError if invalid.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      matplotlib.rcsetup.cycler\n",
      " |          Convenience function for creating validated cyclers for properties.\n",
      " |      cycler.cycler\n",
      " |          The original function for creating unvalidated cyclers.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      Setting the property cycle for a single property:\n",
      " |      \n",
      " |      >>> ax.set_prop_cycle(color=['red', 'green', 'blue'])\n",
      " |      \n",
      " |      Setting the property cycle for simultaneously cycling over multiple\n",
      " |      properties (e.g. red circle, green plus, blue cross):\n",
      " |      \n",
      " |      >>> ax.set_prop_cycle(color=['red', 'green', 'blue'],\n",
      " |      ...                   marker=['o', '+', 'x'])\n",
      " |  \n",
      " |  set_rasterization_zorder(self, z)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      z : float or None\n",
      " |          zorder below which artists are rasterized.  ``None`` means that\n",
      " |          artists do not get rasterized based on zorder.\n",
      " |  \n",
      " |  set_xbound(self, lower=None, upper=None)\n",
      " |      Set the lower and upper numerical bounds of the x-axis.\n",
      " |      \n",
      " |      This method will honor axes inversion regardless of parameter order.\n",
      " |      It will not change the autoscaling setting (`.get_autoscalex_on()`).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      lower, upper : float or None\n",
      " |          The lower and upper bounds. If *None*, the respective axis bound\n",
      " |          is not modified.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      get_xbound\n",
      " |      get_xlim, set_xlim\n",
      " |      invert_xaxis, xaxis_inverted\n",
      " |  \n",
      " |  set_xmargin(self, m)\n",
      " |      Set padding of X data limits prior to autoscaling.\n",
      " |      \n",
      " |      *m* times the data interval will be added to each\n",
      " |      end of that interval before it is used in autoscaling.\n",
      " |      For example, if your data is in the range [0, 2], a factor of\n",
      " |      ``m = 0.1`` will result in a range [-0.2, 2.2].\n",
      " |      \n",
      " |      Negative values -0.5 < m < 0 will result in clipping of the data range.\n",
      " |      I.e. for a data range [0, 2], a factor of ``m = -0.1`` will result in\n",
      " |      a range [0.2, 1.8].\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      m : float greater than -0.5\n",
      " |  \n",
      " |  set_xticklabels(self, labels, *, fontdict=None, minor=False, **kwargs)\n",
      " |      Set the xaxis' labels with list of string labels.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          This method should only be used after fixing the tick positions\n",
      " |          using `.Axes.set_xticks`. Otherwise, the labels may end up in\n",
      " |          unexpected positions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      labels : list of str\n",
      " |          The label texts.\n",
      " |      \n",
      " |      fontdict : dict, optional\n",
      " |          A dictionary controlling the appearance of the ticklabels.\n",
      " |          The default *fontdict* is::\n",
      " |      \n",
      " |             {'fontsize': rcParams['axes.titlesize'],\n",
      " |              'fontweight': rcParams['axes.titleweight'],\n",
      " |              'verticalalignment': 'baseline',\n",
      " |              'horizontalalignment': loc}\n",
      " |      \n",
      " |      minor : bool, default: False\n",
      " |          Whether to set the minor ticklabels rather than the major ones.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of `~.Text`\n",
      " |          The labels.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs : `~.text.Text` properties.\n",
      " |  \n",
      " |  set_xticks(self, ticks, *, minor=False)\n",
      " |      Set the xaxis' tick locations.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ticks : list of floats\n",
      " |          List of tick locations.\n",
      " |      minor : bool, default: False\n",
      " |          If ``False``, set the major ticks; if ``True``, the minor ticks.\n",
      " |  \n",
      " |  set_ybound(self, lower=None, upper=None)\n",
      " |      Set the lower and upper numerical bounds of the y-axis.\n",
      " |      \n",
      " |      This method will honor axes inversion regardless of parameter order.\n",
      " |      It will not change the autoscaling setting (`.get_autoscaley_on()`).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      lower, upper : float or None\n",
      " |          The lower and upper bounds. If *None*, the respective axis bound\n",
      " |          is not modified.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      get_ybound\n",
      " |      get_ylim, set_ylim\n",
      " |      invert_yaxis, yaxis_inverted\n",
      " |  \n",
      " |  set_ymargin(self, m)\n",
      " |      Set padding of Y data limits prior to autoscaling.\n",
      " |      \n",
      " |      *m* times the data interval will be added to each\n",
      " |      end of that interval before it is used in autoscaling.\n",
      " |      For example, if your data is in the range [0, 2], a factor of\n",
      " |      ``m = 0.1`` will result in a range [-0.2, 2.2].\n",
      " |      \n",
      " |      Negative values -0.5 < m < 0 will result in clipping of the data range.\n",
      " |      I.e. for a data range [0, 2], a factor of ``m = -0.1`` will result in\n",
      " |      a range [0.2, 1.8].\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      m : float greater than -0.5\n",
      " |  \n",
      " |  set_yticklabels(self, labels, *, fontdict=None, minor=False, **kwargs)\n",
      " |      Set the yaxis' labels with list of string labels.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          This method should only be used after fixing the tick positions\n",
      " |          using `.Axes.set_yticks`. Otherwise, the labels may end up in\n",
      " |          unexpected positions.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      labels : list of str\n",
      " |          The label texts.\n",
      " |      \n",
      " |      fontdict : dict, optional\n",
      " |          A dictionary controlling the appearance of the ticklabels.\n",
      " |          The default *fontdict* is::\n",
      " |      \n",
      " |             {'fontsize': rcParams['axes.titlesize'],\n",
      " |              'fontweight': rcParams['axes.titleweight'],\n",
      " |              'verticalalignment': 'baseline',\n",
      " |              'horizontalalignment': loc}\n",
      " |      \n",
      " |      minor : bool, default: False\n",
      " |          Whether to set the minor ticklabels rather than the major ones.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of `~.Text`\n",
      " |          The labels.\n",
      " |      \n",
      " |      Other Parameters\n",
      " |      ----------------\n",
      " |      **kwargs : `~.text.Text` properties.\n",
      " |  \n",
      " |  set_yticks(self, ticks, *, minor=False)\n",
      " |      Set the yaxis' tick locations.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ticks : list of floats\n",
      " |          List of tick locations.\n",
      " |      minor : bool, default: False\n",
      " |          If ``False``, set the major ticks; if ``True``, the minor ticks.\n",
      " |  \n",
      " |  sharex(self, other)\n",
      " |      Share the x-axis with *other*.\n",
      " |      \n",
      " |      This is equivalent to passing ``sharex=other`` when constructing the\n",
      " |      axes, and cannot be used if the x-axis is already being shared with\n",
      " |      another axes.\n",
      " |  \n",
      " |  sharey(self, other)\n",
      " |      Share the y-axis with *other*.\n",
      " |      \n",
      " |      This is equivalent to passing ``sharey=other`` when constructing the\n",
      " |      axes, and cannot be used if the y-axis is already being shared with\n",
      " |      another axes.\n",
      " |  \n",
      " |  start_pan(self, x, y, button)\n",
      " |      Called when a pan operation has started.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x, y : float\n",
      " |          The mouse coordinates in display coords.\n",
      " |      button : `.MouseButton`\n",
      " |          The pressed mouse button.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is intended to be overridden by new projection types.\n",
      " |  \n",
      " |  ticklabel_format(self, *, axis='both', style='', scilimits=None, useOffset=None, useLocale=None, useMathText=None)\n",
      " |      Configure the `.ScalarFormatter` used by default for linear axes.\n",
      " |      \n",
      " |      If a parameter is not set, the corresponding property of the formatter\n",
      " |      is left unchanged.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      axis : {'x', 'y', 'both'}, default: 'both'\n",
      " |          The axes to configure.  Only major ticks are affected.\n",
      " |      \n",
      " |      style : {'sci', 'scientific', 'plain'}\n",
      " |          Whether to use scientific notation.\n",
      " |          The formatter default is to use scientific notation.\n",
      " |      \n",
      " |      scilimits : pair of ints (m, n)\n",
      " |          Scientific notation is used only for numbers outside the range\n",
      " |          10\\ :sup:`m` to 10\\ :sup:`n` (and only if the formatter is\n",
      " |          configured to use scientific notation at all).  Use (0, 0) to\n",
      " |          include all numbers.  Use (m, m) where m != 0 to fix the order of\n",
      " |          magnitude to 10\\ :sup:`m`.\n",
      " |          The formatter default is :rc:`axes.formatter.limits`.\n",
      " |      \n",
      " |      useOffset : bool or float\n",
      " |          If True, the offset is calculated as needed.\n",
      " |          If False, no offset is used.\n",
      " |          If a numeric value, it sets the offset.\n",
      " |          The formatter default is :rc:`axes.formatter.useoffset`.\n",
      " |      \n",
      " |      useLocale : bool\n",
      " |          Whether to format the number using the current locale or using the\n",
      " |          C (English) locale.  This affects e.g. the decimal separator.  The\n",
      " |          formatter default is :rc:`axes.formatter.use_locale`.\n",
      " |      \n",
      " |      useMathText : bool\n",
      " |          Render the offset and scientific notation in mathtext.\n",
      " |          The formatter default is :rc:`axes.formatter.use_mathtext`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          If the current formatter is not a `.ScalarFormatter`.\n",
      " |  \n",
      " |  twinx(self)\n",
      " |      Create a twin Axes sharing the xaxis.\n",
      " |      \n",
      " |      Create a new Axes with an invisible x-axis and an independent\n",
      " |      y-axis positioned opposite to the original one (i.e. at right). The\n",
      " |      x-axis autoscale setting will be inherited from the original\n",
      " |      Axes.  To ensure that the tick marks of both y-axes align, see\n",
      " |      `~matplotlib.ticker.LinearLocator`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Axes\n",
      " |          The newly created Axes instance\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For those who are 'picking' artists while using twinx, pick\n",
      " |      events are only called for the artists in the top-most axes.\n",
      " |  \n",
      " |  twiny(self)\n",
      " |      Create a twin Axes sharing the yaxis.\n",
      " |      \n",
      " |      Create a new Axes with an invisible y-axis and an independent\n",
      " |      x-axis positioned opposite to the original one (i.e. at top). The\n",
      " |      y-axis autoscale setting will be inherited from the original Axes.\n",
      " |      To ensure that the tick marks of both x-axes align, see\n",
      " |      `~matplotlib.ticker.LinearLocator`.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Axes\n",
      " |          The newly created Axes instance\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For those who are 'picking' artists while using twiny, pick\n",
      " |      events are only called for the artists in the top-most axes.\n",
      " |  \n",
      " |  update_datalim_bounds(self, bounds)\n",
      " |      [*Deprecated*] Extend the `~.Axes.datalim` Bbox to include the given\n",
      " |      `~matplotlib.transforms.Bbox`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      bounds : `~matplotlib.transforms.Bbox`\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: 3.3\n",
      " |  \n",
      " |  xaxis_date(self, tz=None)\n",
      " |      Sets up axis ticks and labels to treat data along the xaxis as dates.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tz : str or `datetime.tzinfo`, default: :rc:`timezone`\n",
      " |          The timezone used to create date labels.\n",
      " |  \n",
      " |  xaxis_inverted(self)\n",
      " |      Return whether the xaxis is oriented in the \"inverse\" direction.\n",
      " |      \n",
      " |      The \"normal\" direction is increasing to the right for the x-axis and to\n",
      " |      the top for the y-axis; the \"inverse\" direction is increasing to the\n",
      " |      left for the x-axis and to the bottom for the y-axis.\n",
      " |  \n",
      " |  yaxis_date(self, tz=None)\n",
      " |      Sets up axis ticks and labels to treat data along the yaxis as dates.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      tz : str or `datetime.tzinfo`, default: :rc:`timezone`\n",
      " |          The timezone used to create date labels.\n",
      " |  \n",
      " |  yaxis_inverted(self)\n",
      " |      Return whether the yaxis is oriented in the \"inverse\" direction.\n",
      " |      \n",
      " |      The \"normal\" direction is increasing to the right for the x-axis and to\n",
      " |      the top for the y-axis; the \"inverse\" direction is increasing to the\n",
      " |      left for the x-axis and to the bottom for the y-axis.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from matplotlib.axes._base._AxesBase:\n",
      " |  \n",
      " |  viewLim\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from matplotlib.axes._base._AxesBase:\n",
      " |  \n",
      " |  use_sticky_edges\n",
      " |      When autoscaling, whether to obey all `Artist.sticky_edges`.\n",
      " |      \n",
      " |      Default is ``True``.\n",
      " |      \n",
      " |      Setting this to ``False`` ensures that the specified margins\n",
      " |      will be applied, even if the plot includes an image, for\n",
      " |      example, which would otherwise force a view limit to coincide\n",
      " |      with its data limit.\n",
      " |      \n",
      " |      The changing this property does not change the plot until\n",
      " |      `autoscale` or `autoscale_view` is called.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from matplotlib.artist.Artist:\n",
      " |  \n",
      " |  add_callback(self, func)\n",
      " |      Add a callback function that will be called whenever one of the\n",
      " |      `.Artist`'s properties changes.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : callable\n",
      " |          The callback function. It must have the signature::\n",
      " |      \n",
      " |              def func(artist: Artist) -> Any\n",
      " |      \n",
      " |          where *artist* is the calling `.Artist`. Return values may exist\n",
      " |          but are ignored.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      int\n",
      " |          The observer id associated with the callback. This id can be\n",
      " |          used for removing the callback with `.remove_callback` later.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      remove_callback\n",
      " |  \n",
      " |  convert_xunits(self, x)\n",
      " |      Convert *x* using the unit type of the xaxis.\n",
      " |      \n",
      " |      If the artist is not in contained in an Axes or if the xaxis does not\n",
      " |      have units, *x* itself is returned.\n",
      " |  \n",
      " |  convert_yunits(self, y)\n",
      " |      Convert *y* using the unit type of the yaxis.\n",
      " |      \n",
      " |      If the artist is not in contained in an Axes or if the yaxis does not\n",
      " |      have units, *y* itself is returned.\n",
      " |  \n",
      " |  findobj(self, match=None, include_self=True)\n",
      " |      Find artist objects.\n",
      " |      \n",
      " |      Recursively find all `.Artist` instances contained in the artist.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      match\n",
      " |          A filter criterion for the matches. This can be\n",
      " |      \n",
      " |          - *None*: Return all objects contained in artist.\n",
      " |          - A function with signature ``def match(artist: Artist) -> bool``.\n",
      " |            The result will only contain artists for which the function\n",
      " |            returns *True*.\n",
      " |          - A class instance: e.g., `.Line2D`. The result will only contain\n",
      " |            artists of this class or its subclasses (``isinstance`` check).\n",
      " |      \n",
      " |      include_self : bool\n",
      " |          Include *self* in the list to be checked for a match.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of `.Artist`\n",
      " |  \n",
      " |  format_cursor_data(self, data)\n",
      " |      Return a string representation of *data*.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is intended to be overridden by artist subclasses.\n",
      " |          As an end-user of Matplotlib you will most likely not call this\n",
      " |          method yourself.\n",
      " |      \n",
      " |      The default implementation converts ints and floats and arrays of ints\n",
      " |      and floats into a comma-separated string enclosed in square brackets.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      get_cursor_data\n",
      " |  \n",
      " |  get_agg_filter(self)\n",
      " |      Return filter function to be used for agg filter.\n",
      " |  \n",
      " |  get_alpha(self)\n",
      " |      Return the alpha value used for blending - not supported on all\n",
      " |      backends.\n",
      " |  \n",
      " |  get_animated(self)\n",
      " |      Return whether the artist is animated.\n",
      " |  \n",
      " |  get_clip_box(self)\n",
      " |      Return the clipbox.\n",
      " |  \n",
      " |  get_clip_on(self)\n",
      " |      Return whether the artist uses clipping.\n",
      " |  \n",
      " |  get_clip_path(self)\n",
      " |      Return the clip path.\n",
      " |  \n",
      " |  get_contains(self)\n",
      " |      [*Deprecated*] Return the custom contains function of the artist if set, or *None*.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      set_contains\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: 3.3\n",
      " |  \n",
      " |  get_cursor_data(self, event)\n",
      " |      Return the cursor data for a given event.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method is intended to be overridden by artist subclasses.\n",
      " |          As an end-user of Matplotlib you will most likely not call this\n",
      " |          method yourself.\n",
      " |      \n",
      " |      Cursor data can be used by Artists to provide additional context\n",
      " |      information for a given event. The default implementation just returns\n",
      " |      *None*.\n",
      " |      \n",
      " |      Subclasses can override the method and return arbitrary data. However,\n",
      " |      when doing so, they must ensure that `.format_cursor_data` can convert\n",
      " |      the data to a string representation.\n",
      " |      \n",
      " |      The only current use case is displaying the z-value of an `.AxesImage`\n",
      " |      in the status bar of a plot window, while moving the mouse.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      event : `matplotlib.backend_bases.MouseEvent`\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      format_cursor_data\n",
      " |  \n",
      " |  get_figure(self)\n",
      " |      Return the `.Figure` instance the artist belongs to.\n",
      " |  \n",
      " |  get_gid(self)\n",
      " |      Return the group id.\n",
      " |  \n",
      " |  get_in_layout(self)\n",
      " |      Return boolean flag, ``True`` if artist is included in layout\n",
      " |      calculations.\n",
      " |      \n",
      " |      E.g. :doc:`/tutorials/intermediate/constrainedlayout_guide`,\n",
      " |      `.Figure.tight_layout()`, and\n",
      " |      ``fig.savefig(fname, bbox_inches='tight')``.\n",
      " |  \n",
      " |  get_label(self)\n",
      " |      Return the label used for this artist in the legend.\n",
      " |  \n",
      " |  get_path_effects(self)\n",
      " |  \n",
      " |  get_picker(self)\n",
      " |      Return the picking behavior of the artist.\n",
      " |      \n",
      " |      The possible values are described in `.set_picker`.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      set_picker, pickable, pick\n",
      " |  \n",
      " |  get_rasterized(self)\n",
      " |      Return whether the artist is to be rasterized.\n",
      " |  \n",
      " |  get_sketch_params(self)\n",
      " |      Return the sketch parameters for the artist.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      tuple or None\n",
      " |      \n",
      " |          A 3-tuple with the following elements:\n",
      " |      \n",
      " |          - *scale*: The amplitude of the wiggle perpendicular to the\n",
      " |            source line.\n",
      " |          - *length*: The length of the wiggle along the line.\n",
      " |          - *randomness*: The scale factor by which the length is\n",
      " |            shrunken or expanded.\n",
      " |      \n",
      " |          Returns *None* if no sketch parameters were set.\n",
      " |  \n",
      " |  get_snap(self)\n",
      " |      Return the snap setting.\n",
      " |      \n",
      " |      See `.set_snap` for details.\n",
      " |  \n",
      " |  get_transform(self)\n",
      " |      Return the `.Transform` instance used by this artist.\n",
      " |  \n",
      " |  get_transformed_clip_path_and_affine(self)\n",
      " |      Return the clip path with the non-affine part of its\n",
      " |      transformation applied, and the remaining affine part of its\n",
      " |      transformation.\n",
      " |  \n",
      " |  get_url(self)\n",
      " |      Return the url.\n",
      " |  \n",
      " |  get_visible(self)\n",
      " |      Return the visibility.\n",
      " |  \n",
      " |  get_zorder(self)\n",
      " |      Return the artist's zorder.\n",
      " |  \n",
      " |  have_units(self)\n",
      " |      Return *True* if units are set on any axis.\n",
      " |  \n",
      " |  is_transform_set(self)\n",
      " |      Return whether the Artist has an explicitly set transform.\n",
      " |      \n",
      " |      This is *True* after `.set_transform` has been called.\n",
      " |  \n",
      " |  pchanged(self)\n",
      " |      Call all of the registered callbacks.\n",
      " |      \n",
      " |      This function is triggered internally when a property is changed.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      add_callback\n",
      " |      remove_callback\n",
      " |  \n",
      " |  pick(self, mouseevent)\n",
      " |      Process a pick event.\n",
      " |      \n",
      " |      Each child artist will fire a pick event if *mouseevent* is over\n",
      " |      the artist and the artist has picker set.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      set_picker, get_picker, pickable\n",
      " |  \n",
      " |  pickable(self)\n",
      " |      Return whether the artist is pickable.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      set_picker, get_picker, pick\n",
      " |  \n",
      " |  properties(self)\n",
      " |      Return a dictionary of all the properties of the artist.\n",
      " |  \n",
      " |  remove(self)\n",
      " |      Remove the artist from the figure if possible.\n",
      " |      \n",
      " |      The effect will not be visible until the figure is redrawn, e.g.,\n",
      " |      with `.FigureCanvasBase.draw_idle`.  Call `~.axes.Axes.relim` to\n",
      " |      update the axes limits if desired.\n",
      " |      \n",
      " |      Note: `~.axes.Axes.relim` will not see collections even if the\n",
      " |      collection was added to the axes with *autolim* = True.\n",
      " |      \n",
      " |      Note: there is no support for removing the artist's legend entry.\n",
      " |  \n",
      " |  remove_callback(self, oid)\n",
      " |      Remove a callback based on its observer id.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      add_callback\n",
      " |  \n",
      " |  set(self, **kwargs)\n",
      " |      A property batch setter.  Pass *kwargs* to set properties.\n",
      " |  \n",
      " |  set_agg_filter(self, filter_func)\n",
      " |      Set the agg filter.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      filter_func : callable\n",
      " |          A filter function, which takes a (m, n, 3) float array and a dpi\n",
      " |          value, and returns a (m, n, 3) array.\n",
      " |      \n",
      " |          .. ACCEPTS: a filter function, which takes a (m, n, 3) float array\n",
      " |              and a dpi value, and returns a (m, n, 3) array\n",
      " |  \n",
      " |  set_alpha(self, alpha)\n",
      " |      Set the alpha value used for blending - not supported on all backends.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      alpha : float or None\n",
      " |  \n",
      " |  set_animated(self, b)\n",
      " |      Set the artist's animation state.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      b : bool\n",
      " |  \n",
      " |  set_clip_box(self, clipbox)\n",
      " |      Set the artist's clip `.Bbox`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      clipbox : `.Bbox`\n",
      " |  \n",
      " |  set_clip_on(self, b)\n",
      " |      Set whether the artist uses clipping.\n",
      " |      \n",
      " |      When False artists will be visible outside of the axes which\n",
      " |      can lead to unexpected results.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      b : bool\n",
      " |  \n",
      " |  set_clip_path(self, path, transform=None)\n",
      " |      Set the artist's clip path.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path : `.Patch` or `.Path` or `.TransformedPath` or None\n",
      " |          The clip path. If given a `.Path`, *transform* must be provided as\n",
      " |          well. If *None*, a previously set clip path is removed.\n",
      " |      transform : `~matplotlib.transforms.Transform`, optional\n",
      " |          Only used if *path* is a `.Path`, in which case the given `.Path`\n",
      " |          is converted to a `.TransformedPath` using *transform*.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For efficiency, if *path* is a `.Rectangle` this method will set the\n",
      " |      clipping box to the corresponding rectangle and set the clipping path\n",
      " |      to ``None``.\n",
      " |      \n",
      " |      For technical reasons (support of `~.Artist.set`), a tuple\n",
      " |      (*path*, *transform*) is also accepted as a single positional\n",
      " |      parameter.\n",
      " |      \n",
      " |      .. ACCEPTS: Patch or (Path, Transform) or None\n",
      " |  \n",
      " |  set_contains(self, picker)\n",
      " |      [*Deprecated*] Define a custom contains test for the artist.\n",
      " |      \n",
      " |      The provided callable replaces the default `.contains` method\n",
      " |      of the artist.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      picker : callable\n",
      " |          A custom picker function to evaluate if an event is within the\n",
      " |          artist. The function must have the signature::\n",
      " |      \n",
      " |              def contains(artist: Artist, event: MouseEvent) -> bool, dict\n",
      " |      \n",
      " |          that returns:\n",
      " |      \n",
      " |          - a bool indicating if the event is within the artist\n",
      " |          - a dict of additional information. The dict should at least\n",
      " |            return the same information as the default ``contains()``\n",
      " |            implementation of the respective artist, but may provide\n",
      " |            additional information.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      .. deprecated:: 3.3\n",
      " |  \n",
      " |  set_gid(self, gid)\n",
      " |      Set the (group) id for the artist.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      gid : str\n",
      " |  \n",
      " |  set_in_layout(self, in_layout)\n",
      " |      Set if artist is to be included in layout calculations,\n",
      " |      E.g. :doc:`/tutorials/intermediate/constrainedlayout_guide`,\n",
      " |      `.Figure.tight_layout()`, and\n",
      " |      ``fig.savefig(fname, bbox_inches='tight')``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      in_layout : bool\n",
      " |  \n",
      " |  set_label(self, s)\n",
      " |      Set a label that will be displayed in the legend.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      s : object\n",
      " |          *s* will be converted to a string by calling `str`.\n",
      " |  \n",
      " |  set_path_effects(self, path_effects)\n",
      " |      Set the path effects.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      path_effects : `.AbstractPathEffect`\n",
      " |  \n",
      " |  set_picker(self, picker)\n",
      " |      Define the picking behavior of the artist.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      picker : None or bool or callable\n",
      " |          This can be one of the following:\n",
      " |      \n",
      " |          - *None*: Picking is disabled for this artist (default).\n",
      " |      \n",
      " |          - A boolean: If *True* then picking will be enabled and the\n",
      " |            artist will fire a pick event if the mouse event is over\n",
      " |            the artist.\n",
      " |      \n",
      " |          - A function: If picker is callable, it is a user supplied\n",
      " |            function which determines whether the artist is hit by the\n",
      " |            mouse event::\n",
      " |      \n",
      " |              hit, props = picker(artist, mouseevent)\n",
      " |      \n",
      " |            to determine the hit test.  if the mouse event is over the\n",
      " |            artist, return *hit=True* and props is a dictionary of\n",
      " |            properties you want added to the PickEvent attributes.\n",
      " |      \n",
      " |          - *deprecated*: For `.Line2D` only, *picker* can also be a float\n",
      " |            that sets the tolerance for checking whether an event occurred\n",
      " |            \"on\" the line; this is deprecated.  Use `.Line2D.set_pickradius`\n",
      " |            instead.\n",
      " |  \n",
      " |  set_rasterized(self, rasterized)\n",
      " |      Force rasterized (bitmap) drawing in vector backend output.\n",
      " |      \n",
      " |      Defaults to None, which implies the backend's default behavior.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      rasterized : bool or None\n",
      " |  \n",
      " |  set_sketch_params(self, scale=None, length=None, randomness=None)\n",
      " |      Sets the sketch parameters.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      scale : float, optional\n",
      " |          The amplitude of the wiggle perpendicular to the source\n",
      " |          line, in pixels.  If scale is `None`, or not provided, no\n",
      " |          sketch filter will be provided.\n",
      " |      length : float, optional\n",
      " |           The length of the wiggle along the line, in pixels\n",
      " |           (default 128.0)\n",
      " |      randomness : float, optional\n",
      " |          The scale factor by which the length is shrunken or\n",
      " |          expanded (default 16.0)\n",
      " |      \n",
      " |          .. ACCEPTS: (scale: float, length: float, randomness: float)\n",
      " |  \n",
      " |  set_snap(self, snap)\n",
      " |      Set the snapping behavior.\n",
      " |      \n",
      " |      Snapping aligns positions with the pixel grid, which results in\n",
      " |      clearer images. For example, if a black line of 1px width was\n",
      " |      defined at a position in between two pixels, the resulting image\n",
      " |      would contain the interpolated value of that line in the pixel grid,\n",
      " |      which would be a grey value on both adjacent pixel positions. In\n",
      " |      contrast, snapping will move the line to the nearest integer pixel\n",
      " |      value, so that the resulting image will really contain a 1px wide\n",
      " |      black line.\n",
      " |      \n",
      " |      Snapping is currently only supported by the Agg and MacOSX backends.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      snap : bool or None\n",
      " |          Possible values:\n",
      " |      \n",
      " |          - *True*: Snap vertices to the nearest pixel center.\n",
      " |          - *False*: Do not modify vertex positions.\n",
      " |          - *None*: (auto) If the path contains only rectilinear line\n",
      " |            segments, round to the nearest pixel center.\n",
      " |  \n",
      " |  set_transform(self, t)\n",
      " |      Set the artist transform.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      t : `.Transform`\n",
      " |  \n",
      " |  set_url(self, url)\n",
      " |      Set the url for the artist.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      url : str\n",
      " |  \n",
      " |  set_visible(self, b)\n",
      " |      Set the artist's visibility.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      b : bool\n",
      " |  \n",
      " |  set_zorder(self, level)\n",
      " |      Set the zorder for the artist.  Artists with lower zorder\n",
      " |      values are drawn first.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      level : float\n",
      " |  \n",
      " |  update(self, props)\n",
      " |      Update this artist's properties from the dict *props*.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      props : dict\n",
      " |  \n",
      " |  update_from(self, other)\n",
      " |      Copy properties from *other* to *self*.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from matplotlib.artist.Artist:\n",
      " |  \n",
      " |  sticky_edges\n",
      " |      ``x`` and ``y`` sticky edge lists for autoscaling.\n",
      " |      \n",
      " |      When performing autoscaling, if a data limit coincides with a value in\n",
      " |      the corresponding sticky_edges list, then no margin will be added--the\n",
      " |      view limit \"sticks\" to the edge. A typical use case is histograms,\n",
      " |      where one usually expects no margin on the bottom edge (0) of the\n",
      " |      histogram.\n",
      " |      \n",
      " |      This attribute cannot be assigned to; however, the ``x`` and ``y``\n",
      " |      lists can be modified in place as needed.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      >>> artist.sticky_edges.x[:] = (xmin, xmax)\n",
      " |      >>> artist.sticky_edges.y[:] = (ymin, ymax)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from matplotlib.artist.Artist:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  axes\n",
      " |      The `~.axes.Axes` instance the artist resides in, or *None*.\n",
      " |  \n",
      " |  mouseover\n",
      " |      If this property is set to *True*, the artist will be queried for\n",
      " |      custom context information when the mouse cursor moves over it.\n",
      " |      \n",
      " |      See also :meth:`get_cursor_data`, :class:`.ToolCursorPosition` and\n",
      " |      :class:`.NavigationToolbar2`.\n",
      " |  \n",
      " |  stale\n",
      " |      Whether the artist is 'stale' and needs to be re-drawn for the output\n",
      " |      to match the internal state of the artist.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from matplotlib.artist.Artist:\n",
      " |  \n",
      " |  zorder = 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Axes3D?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-pursuit",
   "metadata": {},
   "source": [
    "### Seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-modern",
   "metadata": {},
   "source": [
    "It includes very useful algebra tools: norm, inv, solve, det, eig, eigvalues, etc. It provides a high level interface for drawing attractive and informative statistical grahics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bound-bonus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "jewish-sunglasses",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package seaborn:\n",
      "\n",
      "NAME\n",
      "    seaborn - # Import seaborn objects\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    _core\n",
      "    _decorators\n",
      "    _docstrings\n",
      "    _statistics\n",
      "    _testing\n",
      "    algorithms\n",
      "    axisgrid\n",
      "    categorical\n",
      "    cm\n",
      "    colors (package)\n",
      "    conftest\n",
      "    distributions\n",
      "    external (package)\n",
      "    matrix\n",
      "    miscplot\n",
      "    palettes\n",
      "    rcmod\n",
      "    regression\n",
      "    relational\n",
      "    tests (package)\n",
      "    utils\n",
      "    widgets\n",
      "\n",
      "DATA\n",
      "    crayons = {'Almond': '#EFDECD', 'Antique Brass': '#CD9575', 'Apricot':...\n",
      "    xkcd_rgb = {'acid green': '#8ffe09', 'adobe': '#bd6c48', 'algae': '#54...\n",
      "\n",
      "VERSION\n",
      "    0.11.1\n",
      "\n",
      "FILE\n",
      "    /opt/anaconda3/envs/python_intro/lib/python3.8/site-packages/seaborn/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sn?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposite-supervision",
   "metadata": {},
   "source": [
    "### Zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lyric-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-moral",
   "metadata": {},
   "outputs": [],
   "source": [
    "zf = ZipFile('ruta del fichero')\n",
    "\n",
    "zf.filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZipFile?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frank-shoulder",
   "metadata": {},
   "source": [
    "### Web Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "announced-shirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cultural-freeware",
   "metadata": {},
   "source": [
    "Requests is an HTTP library, written in Python, for human beings.\n",
    "    Basic GET usage:\n",
    "    \n",
    "       >>> import requests\n",
    "       >>> r = requests.get('https://www.python.org')\n",
    "       >>> r.status_code\n",
    "       200\n",
    "       >>> b'Python is a programming language' in r.content\n",
    "       True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "tough-shaft",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package requests:\n",
      "\n",
      "NAME\n",
      "    requests\n",
      "\n",
      "DESCRIPTION\n",
      "    Requests HTTP Library\n",
      "    ~~~~~~~~~~~~~~~~~~~~~\n",
      "    \n",
      "    Requests is an HTTP library, written in Python, for human beings.\n",
      "    Basic GET usage:\n",
      "    \n",
      "       >>> import requests\n",
      "       >>> r = requests.get('https://www.python.org')\n",
      "       >>> r.status_code\n",
      "       200\n",
      "       >>> b'Python is a programming language' in r.content\n",
      "       True\n",
      "    \n",
      "    ... or POST:\n",
      "    \n",
      "       >>> payload = dict(key1='value1', key2='value2')\n",
      "       >>> r = requests.post('https://httpbin.org/post', data=payload)\n",
      "       >>> print(r.text)\n",
      "       {\n",
      "         ...\n",
      "         \"form\": {\n",
      "           \"key1\": \"value1\",\n",
      "           \"key2\": \"value2\"\n",
      "         },\n",
      "         ...\n",
      "       }\n",
      "    \n",
      "    The other HTTP methods are supported - see `requests.api`. Full documentation\n",
      "    is at <https://requests.readthedocs.io>.\n",
      "    \n",
      "    :copyright: (c) 2017 by Kenneth Reitz.\n",
      "    :license: Apache 2.0, see LICENSE for more details.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __version__\n",
      "    _internal_utils\n",
      "    adapters\n",
      "    api\n",
      "    auth\n",
      "    certs\n",
      "    compat\n",
      "    cookies\n",
      "    exceptions\n",
      "    help\n",
      "    hooks\n",
      "    models\n",
      "    packages\n",
      "    sessions\n",
      "    status_codes\n",
      "    structures\n",
      "    utils\n",
      "\n",
      "FUNCTIONS\n",
      "    check_compatibility(urllib3_version, chardet_version)\n",
      "\n",
      "DATA\n",
      "    __author_email__ = 'me@kennethreitz.org'\n",
      "    __build__ = 140545\n",
      "    __cake__ = '  '\n",
      "    __copyright__ = 'Copyright 2020 Kenneth Reitz'\n",
      "    __description__ = 'Python HTTP for Humans.'\n",
      "    __license__ = 'Apache 2.0'\n",
      "    __title__ = 'requests'\n",
      "    __url__ = 'https://requests.readthedocs.io'\n",
      "    codes = <lookup 'status_codes'>\n",
      "\n",
      "VERSION\n",
      "    2.25.1\n",
      "\n",
      "AUTHOR\n",
      "    Kenneth Reitz\n",
      "\n",
      "FILE\n",
      "    /opt/anaconda3/envs/python_intro/lib/python3.8/site-packages/requests/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "requests?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0beb31",
   "metadata": {},
   "source": [
    "#### Beautiful Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef8c60a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "005a8f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BeautifulSoup?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-estate",
   "metadata": {},
   "source": [
    "#### IPython.display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nasty-reasoning",
   "metadata": {},
   "source": [
    "For web scrapping:\n",
    "- _Image_ and _display_ allow to see images extracted from webistes.\n",
    "- _IFrame_ allows to render a website into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "declared-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display, IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "Example\n",
    "\n",
    "IFrame('https://en.wikipedia.org/',800,600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "driving-newsletter",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Image in module IPython.core.display:\n",
      "\n",
      "class Image(DisplayObject)\n",
      " |  Image(data=None, url=None, filename=None, format=None, embed=None, width=None, height=None, retina=False, unconfined=False, metadata=None)\n",
      " |  \n",
      " |  An object that wraps data to be displayed.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Image\n",
      " |      DisplayObject\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, data=None, url=None, filename=None, format=None, embed=None, width=None, height=None, retina=False, unconfined=False, metadata=None)\n",
      " |      Create a PNG/JPEG/GIF image object given raw data.\n",
      " |      \n",
      " |      When this object is returned by an input cell or passed to the\n",
      " |      display function, it will result in the image being displayed\n",
      " |      in the frontend.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : unicode, str or bytes\n",
      " |          The raw image data or a URL or filename to load the data from.\n",
      " |          This always results in embedded image data.\n",
      " |      url : unicode\n",
      " |          A URL to download the data from. If you specify `url=`,\n",
      " |          the image data will not be embedded unless you also specify `embed=True`.\n",
      " |      filename : unicode\n",
      " |          Path to a local file to load the data from.\n",
      " |          Images from a file are always embedded.\n",
      " |      format : unicode\n",
      " |          The format of the image data (png/jpeg/jpg/gif). If a filename or URL is given\n",
      " |          for format will be inferred from the filename extension.\n",
      " |      embed : bool\n",
      " |          Should the image data be embedded using a data URI (True) or be\n",
      " |          loaded using an <img> tag. Set this to True if you want the image\n",
      " |          to be viewable later with no internet connection in the notebook.\n",
      " |      \n",
      " |          Default is `True`, unless the keyword argument `url` is set, then\n",
      " |          default value is `False`.\n",
      " |      \n",
      " |          Note that QtConsole is not able to display images if `embed` is set to `False`\n",
      " |      width : int\n",
      " |          Width in pixels to which to constrain the image in html\n",
      " |      height : int\n",
      " |          Height in pixels to which to constrain the image in html\n",
      " |      retina : bool\n",
      " |          Automatically set the width and height to half of the measured\n",
      " |          width and height.\n",
      " |          This only works for embedded images because it reads the width/height\n",
      " |          from image data.\n",
      " |          For non-embedded images, you can just set the desired display width\n",
      " |          and height directly.\n",
      " |      unconfined: bool\n",
      " |          Set unconfined=True to disable max-width confinement of the image.\n",
      " |      metadata: dict\n",
      " |          Specify extra metadata to attach to the image.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      # embedded image data, works in qtconsole and notebook\n",
      " |      # when passed positionally, the first arg can be any of raw image data,\n",
      " |      # a URL, or a filename from which to load image data.\n",
      " |      # The result is always embedding image data for inline images.\n",
      " |      Image('http://www.google.fr/images/srpr/logo3w.png')\n",
      " |      Image('/path/to/image.jpg')\n",
      " |      Image(b'RAW_PNG_DATA...')\n",
      " |      \n",
      " |      # Specifying Image(url=...) does not embed the image data,\n",
      " |      # it only generates `<img>` tag with a link to the source.\n",
      " |      # This will not work in the qtconsole or offline.\n",
      " |      Image(url='http://www.google.fr/images/srpr/logo3w.png')\n",
      " |  \n",
      " |  reload(self)\n",
      " |      Reload the raw data from file or URL.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from DisplayObject:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from DisplayObject:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from DisplayObject:\n",
      " |  \n",
      " |  metadata = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "shared-society",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function display in module IPython.core.display:\n",
      "\n",
      "display(*objs, include=None, exclude=None, metadata=None, transient=None, display_id=None, **kwargs)\n",
      "    Display a Python object in all frontends.\n",
      "    \n",
      "    By default all representations will be computed and sent to the frontends.\n",
      "    Frontends can decide which representation is used and how.\n",
      "    \n",
      "    In terminal IPython this will be similar to using :func:`print`, for use in richer\n",
      "    frontends see Jupyter notebook examples with rich display logic.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    objs : tuple of objects\n",
      "        The Python objects to display.\n",
      "    raw : bool, optional\n",
      "        Are the objects to be displayed already mimetype-keyed dicts of raw display data,\n",
      "        or Python objects that need to be formatted before display? [default: False]\n",
      "    include : list, tuple or set, optional\n",
      "        A list of format type strings (MIME types) to include in the\n",
      "        format data dict. If this is set *only* the format types included\n",
      "        in this list will be computed.\n",
      "    exclude : list, tuple or set, optional\n",
      "        A list of format type strings (MIME types) to exclude in the format\n",
      "        data dict. If this is set all format types will be computed,\n",
      "        except for those included in this argument.\n",
      "    metadata : dict, optional\n",
      "        A dictionary of metadata to associate with the output.\n",
      "        mime-type keys in this dictionary will be associated with the individual\n",
      "        representation formats, if they exist.\n",
      "    transient : dict, optional\n",
      "        A dictionary of transient data to associate with the output.\n",
      "        Data in this dict should not be persisted to files (e.g. notebooks).\n",
      "    display_id : str, bool optional\n",
      "        Set an id for the display.\n",
      "        This id can be used for updating this display area later via update_display.\n",
      "        If given as `True`, generate a new `display_id`\n",
      "    kwargs: additional keyword-args, optional\n",
      "        Additional keyword-arguments are passed through to the display publisher.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    \n",
      "    handle: DisplayHandle\n",
      "        Returns a handle on updatable displays for use with :func:`update_display`,\n",
      "        if `display_id` is given. Returns :any:`None` if no `display_id` is given\n",
      "        (default).\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    \n",
      "    >>> class Json(object):\n",
      "    ...     def __init__(self, json):\n",
      "    ...         self.json = json\n",
      "    ...     def _repr_pretty_(self, pp, cycle):\n",
      "    ...         import json\n",
      "    ...         pp.text(json.dumps(self.json, indent=2))\n",
      "    ...     def __repr__(self):\n",
      "    ...         return str(self.json)\n",
      "    ...\n",
      "    \n",
      "    >>> d = Json({1:2, 3: {4:5}})\n",
      "    \n",
      "    >>> print(d)\n",
      "    {1: 2, 3: {4: 5}}\n",
      "    \n",
      "    >>> display(d)\n",
      "    {\n",
      "      \"1\": 2,\n",
      "      \"3\": {\n",
      "        \"4\": 5\n",
      "      }\n",
      "    }\n",
      "    \n",
      "    >>> def int_formatter(integer, pp, cycle):\n",
      "    ...     pp.text('I'*integer)\n",
      "    \n",
      "    >>> plain = get_ipython().display_formatter.formatters['text/plain']\n",
      "    >>> plain.for_type(int, int_formatter)\n",
      "    <function _repr_pprint at 0x...>\n",
      "    >>> display(7-5)\n",
      "    II\n",
      "    \n",
      "    >>> del plain.type_printers[int]\n",
      "    >>> display(7-5)\n",
      "    2\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    \n",
      "    :func:`update_display`\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    \n",
      "    In Python, objects can declare their textual representation using the\n",
      "    `__repr__` method. IPython expands on this idea and allows objects to declare\n",
      "    other, rich representations including:\n",
      "    \n",
      "      - HTML\n",
      "      - JSON\n",
      "      - PNG\n",
      "      - JPEG\n",
      "      - SVG\n",
      "      - LaTeX\n",
      "    \n",
      "    A single object can declare some or all of these representations; all are\n",
      "    handled by IPython's display system.\n",
      "    \n",
      "    The main idea of the first approach is that you have to implement special\n",
      "    display methods when you define your class, one for each representation you\n",
      "    want to use. Here is a list of the names of the special methods and the\n",
      "    values they must return:\n",
      "    \n",
      "      - `_repr_html_`: return raw HTML as a string, or a tuple (see below).\n",
      "      - `_repr_json_`: return a JSONable dict, or a tuple (see below).\n",
      "      - `_repr_jpeg_`: return raw JPEG data, or a tuple (see below).\n",
      "      - `_repr_png_`: return raw PNG data, or a tuple (see below).\n",
      "      - `_repr_svg_`: return raw SVG data as a string, or a tuple (see below).\n",
      "      - `_repr_latex_`: return LaTeX commands in a string surrounded by \"$\",\n",
      "                        or a tuple (see below).\n",
      "      - `_repr_mimebundle_`: return a full mimebundle containing the mapping\n",
      "                             from all mimetypes to data.\n",
      "                             Use this for any mime-type not listed above.\n",
      "    \n",
      "    The above functions may also return the object's metadata alonside the\n",
      "    data.  If the metadata is available, the functions will return a tuple\n",
      "    containing the data and metadata, in that order.  If there is no metadata\n",
      "    available, then the functions will return the data only.\n",
      "    \n",
      "    When you are directly writing your own classes, you can adapt them for\n",
      "    display in IPython by following the above approach. But in practice, you\n",
      "    often need to work with existing classes that you can't easily modify.\n",
      "    \n",
      "    You can refer to the documentation on integrating with the display system in\n",
      "    order to register custom formatters for already existing types\n",
      "    (:ref:`integrating_rich_display`).\n",
      "    \n",
      "    .. versionadded:: 5.4 display available without import\n",
      "    .. versionadded:: 6.1 display available without import\n",
      "    \n",
      "    Since IPython 5.4 and 6.1 :func:`display` is automatically made available to\n",
      "    the user without import. If you are using display in a document that might\n",
      "    be used in a pure python context or with older version of IPython, use the\n",
      "    following import at the top of your file::\n",
      "    \n",
      "        from IPython.display import display\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "invisible-director",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class IFrame in module IPython.lib.display:\n",
      "\n",
      "class IFrame(builtins.object)\n",
      " |  IFrame(src, width, height, **kwargs)\n",
      " |  \n",
      " |  Generic class to embed an iframe in an IPython notebook\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, src, width, height, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  iframe = '\\n        <iframe\\n            width=\"{width}\"\\n   ...      ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "IFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-equality",
   "metadata": {},
   "source": [
    "#### BS4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-graham",
   "metadata": {},
   "source": [
    "It helps to read the content of a site and pull data out of it. We can create a BeautifulSoup object out of the content and use __prettify__ to get a more readable version of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "agreed-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "armed-bottom",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BeautifulSoup in module bs4:\n",
      "\n",
      "class BeautifulSoup(bs4.element.Tag)\n",
      " |  BeautifulSoup(markup='', features=None, builder=None, parse_only=None, from_encoding=None, exclude_encodings=None, element_classes=None, **kwargs)\n",
      " |  \n",
      " |  A data structure representing a parsed HTML or XML document.\n",
      " |  \n",
      " |  Most of the methods you'll call on a BeautifulSoup object are inherited from\n",
      " |  PageElement or Tag.\n",
      " |  \n",
      " |  Internally, this class defines the basic interface called by the\n",
      " |  tree builders when converting an HTML/XML document into a data\n",
      " |  structure. The interface abstracts away the differences between\n",
      " |  parsers. To write a new tree builder, you'll need to understand\n",
      " |  these methods as a whole.\n",
      " |  \n",
      " |  These methods will be called by the BeautifulSoup constructor:\n",
      " |    * reset()\n",
      " |    * feed(markup)\n",
      " |  \n",
      " |  The tree builder may call these methods from its feed() implementation:\n",
      " |    * handle_starttag(name, attrs) # See note about return value\n",
      " |    * handle_endtag(name)\n",
      " |    * handle_data(data) # Appends to the current data node\n",
      " |    * endData(containerClass) # Ends the current data node\n",
      " |  \n",
      " |  No matter how complicated the underlying parser is, you should be\n",
      " |  able to build a tree using 'start tag' events, 'end tag' events,\n",
      " |  'data' events, and \"done with data\" events.\n",
      " |  \n",
      " |  If you encounter an empty-element tag (aka a self-closing tag,\n",
      " |  like HTML's <br> tag), call handle_starttag and then\n",
      " |  handle_endtag.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      BeautifulSoup\n",
      " |      bs4.element.Tag\n",
      " |      bs4.element.PageElement\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __copy__(self)\n",
      " |      Copy a BeautifulSoup object by converting the document to a string and parsing it again.\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __init__(self, markup='', features=None, builder=None, parse_only=None, from_encoding=None, exclude_encodings=None, element_classes=None, **kwargs)\n",
      " |      Constructor.\n",
      " |      \n",
      " |      :param markup: A string or a file-like object representing\n",
      " |       markup to be parsed.\n",
      " |      \n",
      " |      :param features: Desirable features of the parser to be\n",
      " |       used. This may be the name of a specific parser (\"lxml\",\n",
      " |       \"lxml-xml\", \"html.parser\", or \"html5lib\") or it may be the\n",
      " |       type of markup to be used (\"html\", \"html5\", \"xml\"). It's\n",
      " |       recommended that you name a specific parser, so that\n",
      " |       Beautiful Soup gives you the same results across platforms\n",
      " |       and virtual environments.\n",
      " |      \n",
      " |      :param builder: A TreeBuilder subclass to instantiate (or\n",
      " |       instance to use) instead of looking one up based on\n",
      " |       `features`. You only need to use this if you've implemented a\n",
      " |       custom TreeBuilder.\n",
      " |      \n",
      " |      :param parse_only: A SoupStrainer. Only parts of the document\n",
      " |       matching the SoupStrainer will be considered. This is useful\n",
      " |       when parsing part of a document that would otherwise be too\n",
      " |       large to fit into memory.\n",
      " |      \n",
      " |      :param from_encoding: A string indicating the encoding of the\n",
      " |       document to be parsed. Pass this in if Beautiful Soup is\n",
      " |       guessing wrongly about the document's encoding.\n",
      " |      \n",
      " |      :param exclude_encodings: A list of strings indicating\n",
      " |       encodings known to be wrong. Pass this in if you don't know\n",
      " |       the document's encoding but you know Beautiful Soup's guess is\n",
      " |       wrong.\n",
      " |      \n",
      " |      :param element_classes: A dictionary mapping BeautifulSoup\n",
      " |       classes like Tag and NavigableString, to other classes you'd\n",
      " |       like to be instantiated instead as the parse tree is\n",
      " |       built. This is useful for subclassing Tag or NavigableString\n",
      " |       to modify default behavior.\n",
      " |      \n",
      " |      :param kwargs: For backwards compatibility purposes, the\n",
      " |       constructor accepts certain keyword arguments used in\n",
      " |       Beautiful Soup 3. None of these arguments do anything in\n",
      " |       Beautiful Soup 4; they will result in a warning and then be\n",
      " |       ignored.\n",
      " |       \n",
      " |       Apart from this, any keyword arguments passed into the\n",
      " |       BeautifulSoup constructor are propagated to the TreeBuilder\n",
      " |       constructor. This makes it possible to configure a\n",
      " |       TreeBuilder by passing in arguments, not just by saying which\n",
      " |       one to use.\n",
      " |  \n",
      " |  decode(self, pretty_print=False, eventual_encoding='utf-8', formatter='minimal')\n",
      " |      Returns a string or Unicode representation of the parse tree\n",
      " |          as an HTML or XML document.\n",
      " |      \n",
      " |      :param pretty_print: If this is True, indentation will be used to\n",
      " |          make the document more readable.\n",
      " |      :param eventual_encoding: The encoding of the final document.\n",
      " |          If this is None, the document will be a Unicode string.\n",
      " |  \n",
      " |  endData(self, containerClass=None)\n",
      " |      Method called by the TreeBuilder when the end of a data segment\n",
      " |      occurs.\n",
      " |  \n",
      " |  handle_data(self, data)\n",
      " |      Called by the tree builder when a chunk of textual data is encountered.\n",
      " |  \n",
      " |  handle_endtag(self, name, nsprefix=None)\n",
      " |      Called by the tree builder when an ending tag is encountered.\n",
      " |      \n",
      " |      :param name: Name of the tag.\n",
      " |      :param nsprefix: Namespace prefix for the tag.\n",
      " |  \n",
      " |  handle_starttag(self, name, namespace, nsprefix, attrs, sourceline=None, sourcepos=None)\n",
      " |      Called by the tree builder when a new tag is encountered.\n",
      " |      \n",
      " |      :param name: Name of the tag.\n",
      " |      :param nsprefix: Namespace prefix for the tag.\n",
      " |      :param attrs: A dictionary of attribute values.\n",
      " |      :param sourceline: The line number where this tag was found in its\n",
      " |          source document.\n",
      " |      :param sourcepos: The character position within `sourceline` where this\n",
      " |          tag was found.\n",
      " |      \n",
      " |      If this method returns None, the tag was rejected by an active\n",
      " |      SoupStrainer. You should proceed as if the tag had not occurred\n",
      " |      in the document. For instance, if this was a self-closing tag,\n",
      " |      don't call handle_endtag.\n",
      " |  \n",
      " |  insert_after(self, *args)\n",
      " |      This method is part of the PageElement API, but `BeautifulSoup` doesn't implement\n",
      " |      it because there is nothing before or after it in the parse tree.\n",
      " |  \n",
      " |  insert_before(self, *args)\n",
      " |      This method is part of the PageElement API, but `BeautifulSoup` doesn't implement\n",
      " |      it because there is nothing before or after it in the parse tree.\n",
      " |  \n",
      " |  new_string(self, s, subclass=None)\n",
      " |      Create a new NavigableString associated with this BeautifulSoup\n",
      " |      object.\n",
      " |  \n",
      " |  new_tag(self, name, namespace=None, nsprefix=None, attrs={}, sourceline=None, sourcepos=None, **kwattrs)\n",
      " |      Create a new Tag associated with this BeautifulSoup object.\n",
      " |      \n",
      " |      :param name: The name of the new Tag.\n",
      " |      :param namespace: The URI of the new Tag's XML namespace, if any.\n",
      " |      :param prefix: The prefix for the new Tag's XML namespace, if any.\n",
      " |      :param attrs: A dictionary of this Tag's attribute values; can\n",
      " |          be used instead of `kwattrs` for attributes like 'class'\n",
      " |          that are reserved words in Python.\n",
      " |      :param sourceline: The line number where this tag was\n",
      " |          (purportedly) found in its source document.\n",
      " |      :param sourcepos: The character position within `sourceline` where this\n",
      " |          tag was (purportedly) found.\n",
      " |      :param kwattrs: Keyword arguments for the new Tag's attribute values.\n",
      " |  \n",
      " |  object_was_parsed(self, o, parent=None, most_recent_element=None)\n",
      " |      Method called by the TreeBuilder to integrate an object into the parse tree.\n",
      " |  \n",
      " |  popTag(self)\n",
      " |      Internal method called by _popToTag when a tag is closed.\n",
      " |  \n",
      " |  pushTag(self, tag)\n",
      " |      Internal method called by handle_starttag when a tag is opened.\n",
      " |  \n",
      " |  reset(self)\n",
      " |      Reset this object to a state as though it had never parsed any\n",
      " |      markup.\n",
      " |  \n",
      " |  string_container(self, base_class=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  ASCII_SPACES = ' \\n\\t\\x0c\\r'\n",
      " |  \n",
      " |  DEFAULT_BUILDER_FEATURES = ['html', 'fast']\n",
      " |  \n",
      " |  NO_PARSER_SPECIFIED_WARNING = 'No parser was explicitly specified, so ...\n",
      " |  \n",
      " |  ROOT_TAG_NAME = '[document]'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from bs4.element.Tag:\n",
      " |  \n",
      " |  __bool__(self)\n",
      " |      A tag is non-None even if it has no contents.\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Calling a Tag like a function is the same as calling its\n",
      " |      find_all() method. Eg. tag('a') returns a list of all the A tags\n",
      " |      found within this tag.\n",
      " |  \n",
      " |  __contains__(self, x)\n",
      " |  \n",
      " |  __delitem__(self, key)\n",
      " |      Deleting tag[key] deletes all 'key' attributes for the tag.\n",
      " |  \n",
      " |  __eq__(self, other)\n",
      " |      Returns true iff this Tag has the same name, the same attributes,\n",
      " |      and the same contents (recursively) as `other`.\n",
      " |  \n",
      " |  __getattr__(self, tag)\n",
      " |      Calling tag.subtag is the same as calling tag.find(name=\"subtag\")\n",
      " |  \n",
      " |  __getitem__(self, key)\n",
      " |      tag[key] returns the value of the 'key' attribute for the Tag,\n",
      " |      and throws an exception if it's not there.\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Iterating over a Tag iterates over its contents.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      The length of a Tag is the length of its list of contents.\n",
      " |  \n",
      " |  __ne__(self, other)\n",
      " |      Returns true iff this Tag is not identical to `other`,\n",
      " |      as defined in __eq__.\n",
      " |  \n",
      " |  __repr__ = __unicode__(self)\n",
      " |  \n",
      " |  __setitem__(self, key, value)\n",
      " |      Setting tag[key] sets the value of the 'key' attribute for the\n",
      " |      tag.\n",
      " |  \n",
      " |  __str__ = __unicode__(self)\n",
      " |  \n",
      " |  __unicode__(self)\n",
      " |      Renders this PageElement as a Unicode string.\n",
      " |  \n",
      " |  childGenerator(self)\n",
      " |      Deprecated generator.\n",
      " |  \n",
      " |  clear(self, decompose=False)\n",
      " |      Wipe out all children of this PageElement by calling extract()\n",
      " |         on them.\n",
      " |      \n",
      " |      :param decompose: If this is True, decompose() (a more\n",
      " |          destructive method) will be called instead of extract().\n",
      " |  \n",
      " |  decode_contents(self, indent_level=None, eventual_encoding='utf-8', formatter='minimal')\n",
      " |      Renders the contents of this tag as a Unicode string.\n",
      " |      \n",
      " |      :param indent_level: Each line of the rendering will be\n",
      " |         indented this many spaces. Used internally in\n",
      " |         recursive calls while pretty-printing.\n",
      " |      \n",
      " |      :param eventual_encoding: The tag is destined to be\n",
      " |         encoded into this encoding. decode_contents() is _not_\n",
      " |         responsible for performing that encoding. This information\n",
      " |         is passed in so that it can be substituted in if the\n",
      " |         document contains a <META> tag that mentions the document's\n",
      " |         encoding.\n",
      " |      \n",
      " |      :param formatter: A Formatter object, or a string naming one of\n",
      " |          the standard Formatters.\n",
      " |  \n",
      " |  decompose(self)\n",
      " |      Recursively destroys this PageElement and its children.\n",
      " |      \n",
      " |      This element will be removed from the tree and wiped out; so\n",
      " |      will everything beneath it.\n",
      " |      \n",
      " |      The behavior of a decomposed PageElement is undefined and you\n",
      " |      should never use one for anything, but if you need to _check_\n",
      " |      whether an element has been decomposed, you can use the\n",
      " |      `decomposed` property.\n",
      " |  \n",
      " |  encode(self, encoding='utf-8', indent_level=None, formatter='minimal', errors='xmlcharrefreplace')\n",
      " |      Render a bytestring representation of this PageElement and its\n",
      " |      contents.\n",
      " |      \n",
      " |      :param encoding: The destination encoding.\n",
      " |      :param indent_level: Each line of the rendering will be\n",
      " |          indented this many spaces. Used internally in\n",
      " |          recursive calls while pretty-printing.\n",
      " |      :param formatter: A Formatter object, or a string naming one of\n",
      " |          the standard formatters.\n",
      " |      :param errors: An error handling strategy such as\n",
      " |          'xmlcharrefreplace'. This value is passed along into\n",
      " |          encode() and its value should be one of the constants\n",
      " |          defined by Python.\n",
      " |      :return: A bytestring.\n",
      " |  \n",
      " |  encode_contents(self, indent_level=None, encoding='utf-8', formatter='minimal')\n",
      " |      Renders the contents of this PageElement as a bytestring.\n",
      " |      \n",
      " |      :param indent_level: Each line of the rendering will be\n",
      " |         indented this many spaces. Used internally in\n",
      " |         recursive calls while pretty-printing.\n",
      " |      \n",
      " |      :param eventual_encoding: The bytestring will be in this encoding.\n",
      " |      \n",
      " |      :param formatter: A Formatter object, or a string naming one of\n",
      " |          the standard Formatters.\n",
      " |      \n",
      " |      :return: A bytestring.\n",
      " |  \n",
      " |  find(self, name=None, attrs={}, recursive=True, text=None, **kwargs)\n",
      " |      Look in the children of this PageElement and find the first\n",
      " |      PageElement that matches the given criteria.\n",
      " |      \n",
      " |      All find_* methods take a common set of arguments. See the online\n",
      " |      documentation for detailed explanations.\n",
      " |      \n",
      " |      :param name: A filter on tag name.\n",
      " |      :param attrs: A dictionary of filters on attribute values.\n",
      " |      :param recursive: If this is True, find() will perform a\n",
      " |          recursive search of this PageElement's children. Otherwise,\n",
      " |          only the direct children will be considered.\n",
      " |      :param limit: Stop looking after finding this many results.\n",
      " |      :kwargs: A dictionary of filters on attribute values.\n",
      " |      :return: A PageElement.\n",
      " |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      " |  \n",
      " |  findAll = find_all(self, name=None, attrs={}, recursive=True, text=None, limit=None, **kwargs)\n",
      " |  \n",
      " |  findChild = find(self, name=None, attrs={}, recursive=True, text=None, **kwargs)\n",
      " |  \n",
      " |  findChildren = find_all(self, name=None, attrs={}, recursive=True, text=None, limit=None, **kwargs)\n",
      " |  \n",
      " |  find_all(self, name=None, attrs={}, recursive=True, text=None, limit=None, **kwargs)\n",
      " |      Look in the children of this PageElement and find all\n",
      " |      PageElements that match the given criteria.\n",
      " |      \n",
      " |      All find_* methods take a common set of arguments. See the online\n",
      " |      documentation for detailed explanations.\n",
      " |      \n",
      " |      :param name: A filter on tag name.\n",
      " |      :param attrs: A dictionary of filters on attribute values.\n",
      " |      :param recursive: If this is True, find_all() will perform a\n",
      " |          recursive search of this PageElement's children. Otherwise,\n",
      " |          only the direct children will be considered.\n",
      " |      :param limit: Stop looking after finding this many results.\n",
      " |      :kwargs: A dictionary of filters on attribute values.\n",
      " |      :return: A ResultSet of PageElements.\n",
      " |      :rtype: bs4.element.ResultSet\n",
      " |  \n",
      " |  get(self, key, default=None)\n",
      " |      Returns the value of the 'key' attribute for the tag, or\n",
      " |      the value given for 'default' if it doesn't have that\n",
      " |      attribute.\n",
      " |  \n",
      " |  getText = get_text(self, separator='', strip=False, types=(<class 'bs4.element.NavigableString'>, <class 'bs4.element.CData'>))\n",
      " |  \n",
      " |  get_attribute_list(self, key, default=None)\n",
      " |      The same as get(), but always returns a list.\n",
      " |      \n",
      " |      :param key: The attribute to look for.\n",
      " |      :param default: Use this value if the attribute is not present\n",
      " |          on this PageElement.\n",
      " |      :return: A list of values, probably containing only a single\n",
      " |          value.\n",
      " |  \n",
      " |  get_text(self, separator='', strip=False, types=(<class 'bs4.element.NavigableString'>, <class 'bs4.element.CData'>))\n",
      " |      Get all child strings, concatenated using the given separator.\n",
      " |      \n",
      " |      :param separator: Strings will be concatenated using this separator.\n",
      " |      \n",
      " |      :param strip: If True, strings will be stripped before being\n",
      " |          concatenated.\n",
      " |      \n",
      " |      :types: A tuple of NavigableString subclasses. Any strings of\n",
      " |          a subclass not found in this list will be ignored. By\n",
      " |          default, this means only NavigableString and CData objects\n",
      " |          will be considered. So no comments, processing instructions,\n",
      " |          stylesheets, etc.\n",
      " |      \n",
      " |      :return: A string.\n",
      " |  \n",
      " |  has_attr(self, key)\n",
      " |      Does this PageElement have an attribute with the given name?\n",
      " |  \n",
      " |  has_key(self, key)\n",
      " |      Deprecated method. This was kind of misleading because has_key()\n",
      " |      (attributes) was different from __in__ (contents).\n",
      " |      \n",
      " |      has_key() is gone in Python 3, anyway.\n",
      " |  \n",
      " |  index(self, element)\n",
      " |      Find the index of a child by identity, not value.\n",
      " |      \n",
      " |      Avoids issues with tag.contents.index(element) getting the\n",
      " |      index of equal elements.\n",
      " |      \n",
      " |      :param element: Look for this PageElement in `self.contents`.\n",
      " |  \n",
      " |  prettify(self, encoding=None, formatter='minimal')\n",
      " |      Pretty-print this PageElement as a string.\n",
      " |      \n",
      " |      :param encoding: The eventual encoding of the string. If this is None,\n",
      " |          a Unicode string will be returned.\n",
      " |      :param formatter: A Formatter object, or a string naming one of\n",
      " |          the standard formatters.\n",
      " |      :return: A Unicode string (if encoding==None) or a bytestring \n",
      " |          (otherwise).\n",
      " |  \n",
      " |  recursiveChildGenerator(self)\n",
      " |      Deprecated generator.\n",
      " |  \n",
      " |  renderContents(self, encoding='utf-8', prettyPrint=False, indentLevel=0)\n",
      " |      Deprecated method for BS3 compatibility.\n",
      " |  \n",
      " |  select(self, selector, namespaces=None, limit=None, **kwargs)\n",
      " |      Perform a CSS selection operation on the current element.\n",
      " |      \n",
      " |      This uses the SoupSieve library.\n",
      " |      \n",
      " |      :param selector: A string containing a CSS selector.\n",
      " |      \n",
      " |      :param namespaces: A dictionary mapping namespace prefixes\n",
      " |         used in the CSS selector to namespace URIs. By default,\n",
      " |         Beautiful Soup will use the prefixes it encountered while\n",
      " |         parsing the document.\n",
      " |      \n",
      " |      :param limit: After finding this number of results, stop looking.\n",
      " |      \n",
      " |      :param kwargs: Keyword arguments to be passed into SoupSieve's \n",
      " |         soupsieve.select() method.\n",
      " |      \n",
      " |      :return: A ResultSet of Tags.\n",
      " |      :rtype: bs4.element.ResultSet\n",
      " |  \n",
      " |  select_one(self, selector, namespaces=None, **kwargs)\n",
      " |      Perform a CSS selection operation on the current element.\n",
      " |      \n",
      " |      :param selector: A CSS selector.\n",
      " |      \n",
      " |      :param namespaces: A dictionary mapping namespace prefixes\n",
      " |         used in the CSS selector to namespace URIs. By default,\n",
      " |         Beautiful Soup will use the prefixes it encountered while\n",
      " |         parsing the document.\n",
      " |      \n",
      " |      :param kwargs: Keyword arguments to be passed into SoupSieve's \n",
      " |         soupsieve.select() method.\n",
      " |      \n",
      " |      :return: A Tag.\n",
      " |      :rtype: bs4.element.Tag\n",
      " |  \n",
      " |  smooth(self)\n",
      " |      Smooth out this element's children by consolidating consecutive\n",
      " |      strings.\n",
      " |      \n",
      " |      This makes pretty-printed output look more natural following a\n",
      " |      lot of operations that modified the tree.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from bs4.element.Tag:\n",
      " |  \n",
      " |  children\n",
      " |      Iterate over all direct children of this PageElement.\n",
      " |      \n",
      " |      :yield: A sequence of PageElements.\n",
      " |  \n",
      " |  descendants\n",
      " |      Iterate over all children of this PageElement in a\n",
      " |      breadth-first sequence.\n",
      " |      \n",
      " |      :yield: A sequence of PageElements.\n",
      " |  \n",
      " |  isSelfClosing\n",
      " |      Is this tag an empty-element tag? (aka a self-closing tag)\n",
      " |      \n",
      " |      A tag that has contents is never an empty-element tag.\n",
      " |      \n",
      " |      A tag that has no contents may or may not be an empty-element\n",
      " |      tag. It depends on the builder used to create the tag. If the\n",
      " |      builder has a designated list of empty-element tags, then only\n",
      " |      a tag whose name shows up in that list is considered an\n",
      " |      empty-element tag.\n",
      " |      \n",
      " |      If the builder has no designated list of empty-element tags,\n",
      " |      then any tag with no contents is an empty-element tag.\n",
      " |  \n",
      " |  is_empty_element\n",
      " |      Is this tag an empty-element tag? (aka a self-closing tag)\n",
      " |      \n",
      " |      A tag that has contents is never an empty-element tag.\n",
      " |      \n",
      " |      A tag that has no contents may or may not be an empty-element\n",
      " |      tag. It depends on the builder used to create the tag. If the\n",
      " |      builder has a designated list of empty-element tags, then only\n",
      " |      a tag whose name shows up in that list is considered an\n",
      " |      empty-element tag.\n",
      " |      \n",
      " |      If the builder has no designated list of empty-element tags,\n",
      " |      then any tag with no contents is an empty-element tag.\n",
      " |  \n",
      " |  strings\n",
      " |      Yield all strings of certain classes, possibly stripping them.\n",
      " |      \n",
      " |      :param strip: If True, all strings will be stripped before being\n",
      " |          yielded.\n",
      " |      \n",
      " |      :types: A tuple of NavigableString subclasses. Any strings of\n",
      " |          a subclass not found in this list will be ignored. By\n",
      " |          default, this means only NavigableString and CData objects\n",
      " |          will be considered. So no comments, processing instructions,\n",
      " |          etc.\n",
      " |      \n",
      " |      :yield: A sequence of strings.\n",
      " |  \n",
      " |  stripped_strings\n",
      " |      Yield all strings in the document, stripping them first.\n",
      " |      \n",
      " |      :yield: A sequence of stripped strings.\n",
      " |  \n",
      " |  text\n",
      " |      Get all child strings, concatenated using the given separator.\n",
      " |      \n",
      " |      :param separator: Strings will be concatenated using this separator.\n",
      " |      \n",
      " |      :param strip: If True, strings will be stripped before being\n",
      " |          concatenated.\n",
      " |      \n",
      " |      :types: A tuple of NavigableString subclasses. Any strings of\n",
      " |          a subclass not found in this list will be ignored. By\n",
      " |          default, this means only NavigableString and CData objects\n",
      " |          will be considered. So no comments, processing instructions,\n",
      " |          stylesheets, etc.\n",
      " |      \n",
      " |      :return: A string.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from bs4.element.Tag:\n",
      " |  \n",
      " |  parserClass\n",
      " |  \n",
      " |  string\n",
      " |      Convenience property to get the single string within this\n",
      " |      PageElement.\n",
      " |      \n",
      " |      TODO It might make sense to have NavigableString.string return\n",
      " |      itself.\n",
      " |      \n",
      " |      :return: If this element has a single string child, return\n",
      " |       value is that string. If this element has one child tag,\n",
      " |       return value is the 'string' attribute of the child tag,\n",
      " |       recursively. If this element is itself a string, has no\n",
      " |       children, or has more than one child, return value is None.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from bs4.element.PageElement:\n",
      " |  \n",
      " |  append(self, tag)\n",
      " |      Appends the given PageElement to the contents of this one.\n",
      " |      \n",
      " |      :param tag: A PageElement.\n",
      " |  \n",
      " |  extend(self, tags)\n",
      " |      Appends the given PageElements to this one's contents.\n",
      " |      \n",
      " |      :param tags: A list of PageElements.\n",
      " |  \n",
      " |  extract(self, _self_index=None)\n",
      " |      Destructively rips this element out of the tree.\n",
      " |      \n",
      " |      :param _self_index: The location of this element in its parent's\n",
      " |         .contents, if known. Passing this in allows for a performance\n",
      " |         optimization.\n",
      " |      \n",
      " |      :return: `self`, no longer part of the tree.\n",
      " |  \n",
      " |  fetchNextSiblings = find_next_siblings(self, name=None, attrs={}, text=None, limit=None, **kwargs)\n",
      " |  \n",
      " |  fetchParents = find_parents(self, name=None, attrs={}, limit=None, **kwargs)\n",
      " |  \n",
      " |  fetchPrevious = find_all_previous(self, name=None, attrs={}, text=None, limit=None, **kwargs)\n",
      " |  \n",
      " |  fetchPreviousSiblings = find_previous_siblings(self, name=None, attrs={}, text=None, limit=None, **kwargs)\n",
      " |  \n",
      " |  findAllNext = find_all_next(self, name=None, attrs={}, text=None, limit=None, **kwargs)\n",
      " |  \n",
      " |  findAllPrevious = find_all_previous(self, name=None, attrs={}, text=None, limit=None, **kwargs)\n",
      " |  \n",
      " |  findNext = find_next(self, name=None, attrs={}, text=None, **kwargs)\n",
      " |  \n",
      " |  findNextSibling = find_next_sibling(self, name=None, attrs={}, text=None, **kwargs)\n",
      " |  \n",
      " |  findNextSiblings = find_next_siblings(self, name=None, attrs={}, text=None, limit=None, **kwargs)\n",
      " |  \n",
      " |  findParent = find_parent(self, name=None, attrs={}, **kwargs)\n",
      " |  \n",
      " |  findParents = find_parents(self, name=None, attrs={}, limit=None, **kwargs)\n",
      " |  \n",
      " |  findPrevious = find_previous(self, name=None, attrs={}, text=None, **kwargs)\n",
      " |  \n",
      " |  findPreviousSibling = find_previous_sibling(self, name=None, attrs={}, text=None, **kwargs)\n",
      " |  \n",
      " |  findPreviousSiblings = find_previous_siblings(self, name=None, attrs={}, text=None, limit=None, **kwargs)\n",
      " |  \n",
      " |  find_all_next(self, name=None, attrs={}, text=None, limit=None, **kwargs)\n",
      " |      Find all PageElements that match the given criteria and appear\n",
      " |      later in the document than this PageElement.\n",
      " |      \n",
      " |      All find_* methods take a common set of arguments. See the online\n",
      " |      documentation for detailed explanations.\n",
      " |      \n",
      " |      :param name: A filter on tag name.\n",
      " |      :param attrs: A dictionary of filters on attribute values.\n",
      " |      :param text: A filter for a NavigableString with specific text.\n",
      " |      :param limit: Stop looking after finding this many results.\n",
      " |      :kwargs: A dictionary of filters on attribute values.\n",
      " |      :return: A ResultSet containing PageElements.\n",
      " |  \n",
      " |  find_all_previous(self, name=None, attrs={}, text=None, limit=None, **kwargs)\n",
      " |      Look backwards in the document from this PageElement and find all\n",
      " |      PageElements that match the given criteria.\n",
      " |      \n",
      " |      All find_* methods take a common set of arguments. See the online\n",
      " |      documentation for detailed explanations.\n",
      " |      \n",
      " |      :param name: A filter on tag name.\n",
      " |      :param attrs: A dictionary of filters on attribute values.\n",
      " |      :param text: A filter for a NavigableString with specific text.\n",
      " |      :param limit: Stop looking after finding this many results.\n",
      " |      :kwargs: A dictionary of filters on attribute values.\n",
      " |      :return: A ResultSet of PageElements.\n",
      " |      :rtype: bs4.element.ResultSet\n",
      " |  \n",
      " |  find_next(self, name=None, attrs={}, text=None, **kwargs)\n",
      " |      Find the first PageElement that matches the given criteria and\n",
      " |      appears later in the document than this PageElement.\n",
      " |      \n",
      " |      All find_* methods take a common set of arguments. See the online\n",
      " |      documentation for detailed explanations.\n",
      " |      \n",
      " |      :param name: A filter on tag name.\n",
      " |      :param attrs: A dictionary of filters on attribute values.\n",
      " |      :param text: A filter for a NavigableString with specific text.\n",
      " |      :kwargs: A dictionary of filters on attribute values.\n",
      " |      :return: A PageElement.\n",
      " |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      " |  \n",
      " |  find_next_sibling(self, name=None, attrs={}, text=None, **kwargs)\n",
      " |      Find the closest sibling to this PageElement that matches the\n",
      " |      given criteria and appears later in the document.\n",
      " |      \n",
      " |      All find_* methods take a common set of arguments. See the\n",
      " |      online documentation for detailed explanations.\n",
      " |      \n",
      " |      :param name: A filter on tag name.\n",
      " |      :param attrs: A dictionary of filters on attribute values.\n",
      " |      :param text: A filter for a NavigableString with specific text.\n",
      " |      :kwargs: A dictionary of filters on attribute values.\n",
      " |      :return: A PageElement.\n",
      " |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      " |  \n",
      " |  find_next_siblings(self, name=None, attrs={}, text=None, limit=None, **kwargs)\n",
      " |      Find all siblings of this PageElement that match the given criteria\n",
      " |      and appear later in the document.\n",
      " |      \n",
      " |      All find_* methods take a common set of arguments. See the online\n",
      " |      documentation for detailed explanations.\n",
      " |      \n",
      " |      :param name: A filter on tag name.\n",
      " |      :param attrs: A dictionary of filters on attribute values.\n",
      " |      :param text: A filter for a NavigableString with specific text.\n",
      " |      :param limit: Stop looking after finding this many results.\n",
      " |      :kwargs: A dictionary of filters on attribute values.\n",
      " |      :return: A ResultSet of PageElements.\n",
      " |      :rtype: bs4.element.ResultSet\n",
      " |  \n",
      " |  find_parent(self, name=None, attrs={}, **kwargs)\n",
      " |      Find the closest parent of this PageElement that matches the given\n",
      " |      criteria.\n",
      " |      \n",
      " |      All find_* methods take a common set of arguments. See the online\n",
      " |      documentation for detailed explanations.\n",
      " |      \n",
      " |      :param name: A filter on tag name.\n",
      " |      :param attrs: A dictionary of filters on attribute values.\n",
      " |      :kwargs: A dictionary of filters on attribute values.\n",
      " |      \n",
      " |      :return: A PageElement.\n",
      " |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      " |  \n",
      " |  find_parents(self, name=None, attrs={}, limit=None, **kwargs)\n",
      " |      Find all parents of this PageElement that match the given criteria.\n",
      " |      \n",
      " |      All find_* methods take a common set of arguments. See the online\n",
      " |      documentation for detailed explanations.\n",
      " |      \n",
      " |      :param name: A filter on tag name.\n",
      " |      :param attrs: A dictionary of filters on attribute values.\n",
      " |      :param limit: Stop looking after finding this many results.\n",
      " |      :kwargs: A dictionary of filters on attribute values.\n",
      " |      \n",
      " |      :return: A PageElement.\n",
      " |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      " |  \n",
      " |  find_previous(self, name=None, attrs={}, text=None, **kwargs)\n",
      " |      Look backwards in the document from this PageElement and find the\n",
      " |      first PageElement that matches the given criteria.\n",
      " |      \n",
      " |      All find_* methods take a common set of arguments. See the online\n",
      " |      documentation for detailed explanations.\n",
      " |      \n",
      " |      :param name: A filter on tag name.\n",
      " |      :param attrs: A dictionary of filters on attribute values.\n",
      " |      :param text: A filter for a NavigableString with specific text.\n",
      " |      :kwargs: A dictionary of filters on attribute values.\n",
      " |      :return: A PageElement.\n",
      " |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      " |  \n",
      " |  find_previous_sibling(self, name=None, attrs={}, text=None, **kwargs)\n",
      " |      Returns the closest sibling to this PageElement that matches the\n",
      " |      given criteria and appears earlier in the document.\n",
      " |      \n",
      " |      All find_* methods take a common set of arguments. See the online\n",
      " |      documentation for detailed explanations.\n",
      " |      \n",
      " |      :param name: A filter on tag name.\n",
      " |      :param attrs: A dictionary of filters on attribute values.\n",
      " |      :param text: A filter for a NavigableString with specific text.\n",
      " |      :kwargs: A dictionary of filters on attribute values.\n",
      " |      :return: A PageElement.\n",
      " |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      " |  \n",
      " |  find_previous_siblings(self, name=None, attrs={}, text=None, limit=None, **kwargs)\n",
      " |      Returns all siblings to this PageElement that match the\n",
      " |      given criteria and appear earlier in the document.\n",
      " |      \n",
      " |      All find_* methods take a common set of arguments. See the online\n",
      " |      documentation for detailed explanations.\n",
      " |      \n",
      " |      :param name: A filter on tag name.\n",
      " |      :param attrs: A dictionary of filters on attribute values.\n",
      " |      :param text: A filter for a NavigableString with specific text.\n",
      " |      :param limit: Stop looking after finding this many results.\n",
      " |      :kwargs: A dictionary of filters on attribute values.\n",
      " |      :return: A ResultSet of PageElements.\n",
      " |      :rtype: bs4.element.ResultSet\n",
      " |  \n",
      " |  format_string(self, s, formatter)\n",
      " |      Format the given string using the given formatter.\n",
      " |      \n",
      " |      :param s: A string.\n",
      " |      :param formatter: A Formatter object, or a string naming one of the standard formatters.\n",
      " |  \n",
      " |  formatter_for_name(self, formatter)\n",
      " |      Look up or create a Formatter for the given identifier,\n",
      " |      if necessary.\n",
      " |      \n",
      " |      :param formatter: Can be a Formatter object (used as-is), a\n",
      " |          function (used as the entity substitution hook for an\n",
      " |          XMLFormatter or HTMLFormatter), or a string (used to look\n",
      " |          up an XMLFormatter or HTMLFormatter in the appropriate\n",
      " |          registry.\n",
      " |  \n",
      " |  insert(self, position, new_child)\n",
      " |      Insert a new PageElement in the list of this PageElement's children.\n",
      " |      \n",
      " |      This works the same way as `list.insert`.\n",
      " |      \n",
      " |      :param position: The numeric position that should be occupied\n",
      " |         in `self.children` by the new PageElement. \n",
      " |      :param new_child: A PageElement.\n",
      " |  \n",
      " |  nextGenerator(self)\n",
      " |      # Old non-property versions of the generators, for backwards\n",
      " |      # compatibility with BS3.\n",
      " |  \n",
      " |  nextSiblingGenerator(self)\n",
      " |  \n",
      " |  parentGenerator(self)\n",
      " |  \n",
      " |  previousGenerator(self)\n",
      " |  \n",
      " |  previousSiblingGenerator(self)\n",
      " |  \n",
      " |  replaceWith = replace_with(self, replace_with)\n",
      " |  \n",
      " |  replaceWithChildren = unwrap(self)\n",
      " |  \n",
      " |  replace_with(self, replace_with)\n",
      " |      Replace this PageElement with another one, keeping the rest of the\n",
      " |      tree the same.\n",
      " |      \n",
      " |      :param replace_with: A PageElement.\n",
      " |      :return: `self`, no longer part of the tree.\n",
      " |  \n",
      " |  replace_with_children = unwrap(self)\n",
      " |  \n",
      " |  setup(self, parent=None, previous_element=None, next_element=None, previous_sibling=None, next_sibling=None)\n",
      " |      Sets up the initial relations between this element and\n",
      " |      other elements.\n",
      " |      \n",
      " |      :param parent: The parent of this element.\n",
      " |      \n",
      " |      :param previous_element: The element parsed immediately before\n",
      " |          this one.\n",
      " |      \n",
      " |      :param next_element: The element parsed immediately before\n",
      " |          this one.\n",
      " |      \n",
      " |      :param previous_sibling: The most recently encountered element\n",
      " |          on the same level of the parse tree as this one.\n",
      " |      \n",
      " |      :param previous_sibling: The next element to be encountered\n",
      " |          on the same level of the parse tree as this one.\n",
      " |  \n",
      " |  unwrap(self)\n",
      " |      Replace this PageElement with its contents.\n",
      " |      \n",
      " |      :return: `self`, no longer part of the tree.\n",
      " |  \n",
      " |  wrap(self, wrap_inside)\n",
      " |      Wrap this PageElement inside another one.\n",
      " |      \n",
      " |      :param wrap_inside: A PageElement.\n",
      " |      :return: `wrap_inside`, occupying the position in the tree that used\n",
      " |         to be occupied by `self`, and with `self` inside it.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from bs4.element.PageElement:\n",
      " |  \n",
      " |  decomposed\n",
      " |      Check whether a PageElement has been decomposed.\n",
      " |      \n",
      " |      :rtype: bool\n",
      " |  \n",
      " |  next\n",
      " |      The PageElement, if any, that was parsed just after this one.\n",
      " |      \n",
      " |      :return: A PageElement.\n",
      " |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      " |  \n",
      " |  next_elements\n",
      " |      All PageElements that were parsed after this one.\n",
      " |      \n",
      " |      :yield: A sequence of PageElements.\n",
      " |  \n",
      " |  next_siblings\n",
      " |      All PageElements that are siblings of this one but were parsed\n",
      " |      later.\n",
      " |      \n",
      " |      :yield: A sequence of PageElements.\n",
      " |  \n",
      " |  parents\n",
      " |      All PageElements that are parents of this PageElement.\n",
      " |      \n",
      " |      :yield: A sequence of PageElements.\n",
      " |  \n",
      " |  previous\n",
      " |      The PageElement, if any, that was parsed just before this one.\n",
      " |      \n",
      " |      :return: A PageElement.\n",
      " |      :rtype: bs4.element.Tag | bs4.element.NavigableString\n",
      " |  \n",
      " |  previous_elements\n",
      " |      All PageElements that were parsed before this one.\n",
      " |      \n",
      " |      :yield: A sequence of PageElements.\n",
      " |  \n",
      " |  previous_siblings\n",
      " |      All PageElements that are siblings of this one but were parsed\n",
      " |      earlier.\n",
      " |      \n",
      " |      :yield: A sequence of PageElements.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from bs4.element.PageElement:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  nextSibling\n",
      " |  \n",
      " |  previousSibling\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BeautifulSoup?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-credit",
   "metadata": {},
   "source": [
    "#### Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-fiction",
   "metadata": {},
   "source": [
    "Flask is a popular Python web framework, meaning it is a third party Python library used for developing web applications (APIs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "selected-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify, request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "final-phone",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Flask in module flask.app:\n",
      "\n",
      "class Flask(flask.helpers._PackageBoundObject)\n",
      " |  Flask(import_name, static_url_path=None, static_folder='static', static_host=None, host_matching=False, subdomain_matching=False, template_folder='templates', instance_path=None, instance_relative_config=False, root_path=None)\n",
      " |  \n",
      " |  The flask object implements a WSGI application and acts as the central\n",
      " |  object.  It is passed the name of the module or package of the\n",
      " |  application.  Once it is created it will act as a central registry for\n",
      " |  the view functions, the URL rules, template configuration and much more.\n",
      " |  \n",
      " |  The name of the package is used to resolve resources from inside the\n",
      " |  package or the folder the module is contained in depending on if the\n",
      " |  package parameter resolves to an actual python package (a folder with\n",
      " |  an :file:`__init__.py` file inside) or a standard module (just a ``.py`` file).\n",
      " |  \n",
      " |  For more information about resource loading, see :func:`open_resource`.\n",
      " |  \n",
      " |  Usually you create a :class:`Flask` instance in your main module or\n",
      " |  in the :file:`__init__.py` file of your package like this::\n",
      " |  \n",
      " |      from flask import Flask\n",
      " |      app = Flask(__name__)\n",
      " |  \n",
      " |  .. admonition:: About the First Parameter\n",
      " |  \n",
      " |      The idea of the first parameter is to give Flask an idea of what\n",
      " |      belongs to your application.  This name is used to find resources\n",
      " |      on the filesystem, can be used by extensions to improve debugging\n",
      " |      information and a lot more.\n",
      " |  \n",
      " |      So it's important what you provide there.  If you are using a single\n",
      " |      module, `__name__` is always the correct value.  If you however are\n",
      " |      using a package, it's usually recommended to hardcode the name of\n",
      " |      your package there.\n",
      " |  \n",
      " |      For example if your application is defined in :file:`yourapplication/app.py`\n",
      " |      you should create it with one of the two versions below::\n",
      " |  \n",
      " |          app = Flask('yourapplication')\n",
      " |          app = Flask(__name__.split('.')[0])\n",
      " |  \n",
      " |      Why is that?  The application will work even with `__name__`, thanks\n",
      " |      to how resources are looked up.  However it will make debugging more\n",
      " |      painful.  Certain extensions can make assumptions based on the\n",
      " |      import name of your application.  For example the Flask-SQLAlchemy\n",
      " |      extension will look for the code in your application that triggered\n",
      " |      an SQL query in debug mode.  If the import name is not properly set\n",
      " |      up, that debugging information is lost.  (For example it would only\n",
      " |      pick up SQL queries in `yourapplication.app` and not\n",
      " |      `yourapplication.views.frontend`)\n",
      " |  \n",
      " |  .. versionadded:: 0.7\n",
      " |     The `static_url_path`, `static_folder`, and `template_folder`\n",
      " |     parameters were added.\n",
      " |  \n",
      " |  .. versionadded:: 0.8\n",
      " |     The `instance_path` and `instance_relative_config` parameters were\n",
      " |     added.\n",
      " |  \n",
      " |  .. versionadded:: 0.11\n",
      " |     The `root_path` parameter was added.\n",
      " |  \n",
      " |  .. versionadded:: 1.0\n",
      " |     The ``host_matching`` and ``static_host`` parameters were added.\n",
      " |  \n",
      " |  .. versionadded:: 1.0\n",
      " |     The ``subdomain_matching`` parameter was added. Subdomain\n",
      " |     matching needs to be enabled manually now. Setting\n",
      " |     :data:`SERVER_NAME` does not implicitly enable it.\n",
      " |  \n",
      " |  :param import_name: the name of the application package\n",
      " |  :param static_url_path: can be used to specify a different path for the\n",
      " |                          static files on the web.  Defaults to the name\n",
      " |                          of the `static_folder` folder.\n",
      " |  :param static_folder: The folder with static files that is served at\n",
      " |      ``static_url_path``. Relative to the application ``root_path``\n",
      " |      or an absolute path. Defaults to ``'static'``.\n",
      " |  :param static_host: the host to use when adding the static route.\n",
      " |      Defaults to None. Required when using ``host_matching=True``\n",
      " |      with a ``static_folder`` configured.\n",
      " |  :param host_matching: set ``url_map.host_matching`` attribute.\n",
      " |      Defaults to False.\n",
      " |  :param subdomain_matching: consider the subdomain relative to\n",
      " |      :data:`SERVER_NAME` when matching routes. Defaults to False.\n",
      " |  :param template_folder: the folder that contains the templates that should\n",
      " |                          be used by the application.  Defaults to\n",
      " |                          ``'templates'`` folder in the root path of the\n",
      " |                          application.\n",
      " |  :param instance_path: An alternative instance path for the application.\n",
      " |                        By default the folder ``'instance'`` next to the\n",
      " |                        package or module is assumed to be the instance\n",
      " |                        path.\n",
      " |  :param instance_relative_config: if set to ``True`` relative filenames\n",
      " |                                   for loading the config are assumed to\n",
      " |                                   be relative to the instance path instead\n",
      " |                                   of the application root.\n",
      " |  :param root_path: Flask by default will automatically calculate the path\n",
      " |                    to the root of the application.  In certain situations\n",
      " |                    this cannot be achieved (for instance if the package\n",
      " |                    is a Python 3 namespace package) and needs to be\n",
      " |                    manually defined.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Flask\n",
      " |      flask.helpers._PackageBoundObject\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __call__(self, environ, start_response)\n",
      " |      The WSGI server calls the Flask application object as the\n",
      " |      WSGI application. This calls :meth:`wsgi_app` which can be\n",
      " |      wrapped to applying middleware.\n",
      " |  \n",
      " |  __init__(self, import_name, static_url_path=None, static_folder='static', static_host=None, host_matching=False, subdomain_matching=False, template_folder='templates', instance_path=None, instance_relative_config=False, root_path=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  add_template_filter(self, f, name=None)\n",
      " |      Register a custom template filter.  Works exactly like the\n",
      " |      :meth:`template_filter` decorator.\n",
      " |      \n",
      " |      :param name: the optional name of the filter, otherwise the\n",
      " |                   function name will be used.\n",
      " |  \n",
      " |  add_template_global(self, f, name=None)\n",
      " |      Register a custom template global function. Works exactly like the\n",
      " |      :meth:`template_global` decorator.\n",
      " |      \n",
      " |      .. versionadded:: 0.10\n",
      " |      \n",
      " |      :param name: the optional name of the global function, otherwise the\n",
      " |                   function name will be used.\n",
      " |  \n",
      " |  add_template_test(self, f, name=None)\n",
      " |      Register a custom template test.  Works exactly like the\n",
      " |      :meth:`template_test` decorator.\n",
      " |      \n",
      " |      .. versionadded:: 0.10\n",
      " |      \n",
      " |      :param name: the optional name of the test, otherwise the\n",
      " |                   function name will be used.\n",
      " |  \n",
      " |  add_url_rule(self, rule, endpoint=None, view_func=None, provide_automatic_options=None, **options)\n",
      " |      Connects a URL rule.  Works exactly like the :meth:`route`\n",
      " |      decorator.  If a view_func is provided it will be registered with the\n",
      " |      endpoint.\n",
      " |      \n",
      " |      Basically this example::\n",
      " |      \n",
      " |          @app.route('/')\n",
      " |          def index():\n",
      " |              pass\n",
      " |      \n",
      " |      Is equivalent to the following::\n",
      " |      \n",
      " |          def index():\n",
      " |              pass\n",
      " |          app.add_url_rule('/', 'index', index)\n",
      " |      \n",
      " |      If the view_func is not provided you will need to connect the endpoint\n",
      " |      to a view function like so::\n",
      " |      \n",
      " |          app.view_functions['index'] = index\n",
      " |      \n",
      " |      Internally :meth:`route` invokes :meth:`add_url_rule` so if you want\n",
      " |      to customize the behavior via subclassing you only need to change\n",
      " |      this method.\n",
      " |      \n",
      " |      For more information refer to :ref:`url-route-registrations`.\n",
      " |      \n",
      " |      .. versionchanged:: 0.2\n",
      " |         `view_func` parameter added.\n",
      " |      \n",
      " |      .. versionchanged:: 0.6\n",
      " |         ``OPTIONS`` is added automatically as method.\n",
      " |      \n",
      " |      :param rule: the URL rule as string\n",
      " |      :param endpoint: the endpoint for the registered URL rule.  Flask\n",
      " |                       itself assumes the name of the view function as\n",
      " |                       endpoint\n",
      " |      :param view_func: the function to call when serving a request to the\n",
      " |                        provided endpoint\n",
      " |      :param provide_automatic_options: controls whether the ``OPTIONS``\n",
      " |          method should be added automatically. This can also be controlled\n",
      " |          by setting the ``view_func.provide_automatic_options = False``\n",
      " |          before adding the rule.\n",
      " |      :param options: the options to be forwarded to the underlying\n",
      " |                      :class:`~werkzeug.routing.Rule` object.  A change\n",
      " |                      to Werkzeug is handling of method options.  methods\n",
      " |                      is a list of methods this rule should be limited\n",
      " |                      to (``GET``, ``POST`` etc.).  By default a rule\n",
      " |                      just listens for ``GET`` (and implicitly ``HEAD``).\n",
      " |                      Starting with Flask 0.6, ``OPTIONS`` is implicitly\n",
      " |                      added and handled by the standard request handling.\n",
      " |  \n",
      " |  after_request(self, f)\n",
      " |      Register a function to be run after each request.\n",
      " |      \n",
      " |      Your function must take one parameter, an instance of\n",
      " |      :attr:`response_class` and return a new response object or the\n",
      " |      same (see :meth:`process_response`).\n",
      " |      \n",
      " |      As of Flask 0.7 this function might not be executed at the end of the\n",
      " |      request in case an unhandled exception occurred.\n",
      " |  \n",
      " |  app_context(self)\n",
      " |      Create an :class:`~flask.ctx.AppContext`. Use as a ``with``\n",
      " |      block to push the context, which will make :data:`current_app`\n",
      " |      point at this application.\n",
      " |      \n",
      " |      An application context is automatically pushed by\n",
      " |      :meth:`RequestContext.push() <flask.ctx.RequestContext.push>`\n",
      " |      when handling a request, and when running a CLI command. Use\n",
      " |      this to manually create a context outside of these situations.\n",
      " |      \n",
      " |      ::\n",
      " |      \n",
      " |          with app.app_context():\n",
      " |              init_db()\n",
      " |      \n",
      " |      See :doc:`/appcontext`.\n",
      " |      \n",
      " |      .. versionadded:: 0.9\n",
      " |  \n",
      " |  auto_find_instance_path(self)\n",
      " |      Tries to locate the instance path if it was not provided to the\n",
      " |      constructor of the application class.  It will basically calculate\n",
      " |      the path to a folder named ``instance`` next to your main file or\n",
      " |      the package.\n",
      " |      \n",
      " |      .. versionadded:: 0.8\n",
      " |  \n",
      " |  before_first_request(self, f)\n",
      " |      Registers a function to be run before the first request to this\n",
      " |      instance of the application.\n",
      " |      \n",
      " |      The function will be called without any arguments and its return\n",
      " |      value is ignored.\n",
      " |      \n",
      " |      .. versionadded:: 0.8\n",
      " |  \n",
      " |  before_request(self, f)\n",
      " |      Registers a function to run before each request.\n",
      " |      \n",
      " |      For example, this can be used to open a database connection, or to load\n",
      " |      the logged in user from the session.\n",
      " |      \n",
      " |      The function will be called without any arguments. If it returns a\n",
      " |      non-None value, the value is handled as if it was the return value from\n",
      " |      the view, and further request handling is stopped.\n",
      " |  \n",
      " |  context_processor(self, f)\n",
      " |      Registers a template context processor function.\n",
      " |  \n",
      " |  create_global_jinja_loader(self)\n",
      " |      Creates the loader for the Jinja2 environment.  Can be used to\n",
      " |      override just the loader and keeping the rest unchanged.  It's\n",
      " |      discouraged to override this function.  Instead one should override\n",
      " |      the :meth:`jinja_loader` function instead.\n",
      " |      \n",
      " |      The global loader dispatches between the loaders of the application\n",
      " |      and the individual blueprints.\n",
      " |      \n",
      " |      .. versionadded:: 0.7\n",
      " |  \n",
      " |  create_jinja_environment(self)\n",
      " |      Create the Jinja environment based on :attr:`jinja_options`\n",
      " |      and the various Jinja-related methods of the app. Changing\n",
      " |      :attr:`jinja_options` after this will have no effect. Also adds\n",
      " |      Flask-related globals and filters to the environment.\n",
      " |      \n",
      " |      .. versionchanged:: 0.11\n",
      " |         ``Environment.auto_reload`` set in accordance with\n",
      " |         ``TEMPLATES_AUTO_RELOAD`` configuration option.\n",
      " |      \n",
      " |      .. versionadded:: 0.5\n",
      " |  \n",
      " |  create_url_adapter(self, request)\n",
      " |      Creates a URL adapter for the given request. The URL adapter\n",
      " |      is created at a point where the request context is not yet set\n",
      " |      up so the request is passed explicitly.\n",
      " |      \n",
      " |      .. versionadded:: 0.6\n",
      " |      \n",
      " |      .. versionchanged:: 0.9\n",
      " |         This can now also be called without a request object when the\n",
      " |         URL adapter is created for the application context.\n",
      " |      \n",
      " |      .. versionchanged:: 1.0\n",
      " |          :data:`SERVER_NAME` no longer implicitly enables subdomain\n",
      " |          matching. Use :attr:`subdomain_matching` instead.\n",
      " |  \n",
      " |  dispatch_request(self)\n",
      " |      Does the request dispatching.  Matches the URL and returns the\n",
      " |      return value of the view or error handler.  This does not have to\n",
      " |      be a response object.  In order to convert the return value to a\n",
      " |      proper response object, call :func:`make_response`.\n",
      " |      \n",
      " |      .. versionchanged:: 0.7\n",
      " |         This no longer does the exception handling, this code was\n",
      " |         moved to the new :meth:`full_dispatch_request`.\n",
      " |  \n",
      " |  do_teardown_appcontext(self, exc=<object object at 0x118332ad0>)\n",
      " |      Called right before the application context is popped.\n",
      " |      \n",
      " |      When handling a request, the application context is popped\n",
      " |      after the request context. See :meth:`do_teardown_request`.\n",
      " |      \n",
      " |      This calls all functions decorated with\n",
      " |      :meth:`teardown_appcontext`. Then the\n",
      " |      :data:`appcontext_tearing_down` signal is sent.\n",
      " |      \n",
      " |      This is called by\n",
      " |      :meth:`AppContext.pop() <flask.ctx.AppContext.pop>`.\n",
      " |      \n",
      " |      .. versionadded:: 0.9\n",
      " |  \n",
      " |  do_teardown_request(self, exc=<object object at 0x118332ad0>)\n",
      " |      Called after the request is dispatched and the response is\n",
      " |      returned, right before the request context is popped.\n",
      " |      \n",
      " |      This calls all functions decorated with\n",
      " |      :meth:`teardown_request`, and :meth:`Blueprint.teardown_request`\n",
      " |      if a blueprint handled the request. Finally, the\n",
      " |      :data:`request_tearing_down` signal is sent.\n",
      " |      \n",
      " |      This is called by\n",
      " |      :meth:`RequestContext.pop() <flask.ctx.RequestContext.pop>`,\n",
      " |      which may be delayed during testing to maintain access to\n",
      " |      resources.\n",
      " |      \n",
      " |      :param exc: An unhandled exception raised while dispatching the\n",
      " |          request. Detected from the current exception information if\n",
      " |          not passed. Passed to each teardown function.\n",
      " |      \n",
      " |      .. versionchanged:: 0.9\n",
      " |          Added the ``exc`` argument.\n",
      " |  \n",
      " |  endpoint(self, endpoint)\n",
      " |      A decorator to register a function as an endpoint.\n",
      " |      Example::\n",
      " |      \n",
      " |          @app.endpoint('example.endpoint')\n",
      " |          def example():\n",
      " |              return \"example\"\n",
      " |      \n",
      " |      :param endpoint: the name of the endpoint\n",
      " |  \n",
      " |  errorhandler(self, code_or_exception)\n",
      " |      Register a function to handle errors by code or exception class.\n",
      " |      \n",
      " |      A decorator that is used to register a function given an\n",
      " |      error code.  Example::\n",
      " |      \n",
      " |          @app.errorhandler(404)\n",
      " |          def page_not_found(error):\n",
      " |              return 'This page does not exist', 404\n",
      " |      \n",
      " |      You can also register handlers for arbitrary exceptions::\n",
      " |      \n",
      " |          @app.errorhandler(DatabaseError)\n",
      " |          def special_exception_handler(error):\n",
      " |              return 'Database connection failed', 500\n",
      " |      \n",
      " |      .. versionadded:: 0.7\n",
      " |          Use :meth:`register_error_handler` instead of modifying\n",
      " |          :attr:`error_handler_spec` directly, for application wide error\n",
      " |          handlers.\n",
      " |      \n",
      " |      .. versionadded:: 0.7\n",
      " |         One can now additionally also register custom exception types\n",
      " |         that do not necessarily have to be a subclass of the\n",
      " |         :class:`~werkzeug.exceptions.HTTPException` class.\n",
      " |      \n",
      " |      :param code_or_exception: the code as integer for the handler, or\n",
      " |                                an arbitrary exception\n",
      " |  \n",
      " |  finalize_request(self, rv, from_error_handler=False)\n",
      " |      Given the return value from a view function this finalizes\n",
      " |      the request by converting it into a response and invoking the\n",
      " |      postprocessing functions.  This is invoked for both normal\n",
      " |      request dispatching as well as error handlers.\n",
      " |      \n",
      " |      Because this means that it might be called as a result of a\n",
      " |      failure a special safe mode is available which can be enabled\n",
      " |      with the `from_error_handler` flag.  If enabled, failures in\n",
      " |      response processing will be logged and otherwise ignored.\n",
      " |      \n",
      " |      :internal:\n",
      " |  \n",
      " |  full_dispatch_request(self)\n",
      " |      Dispatches the request and on top of that performs request\n",
      " |      pre and postprocessing as well as HTTP exception catching and\n",
      " |      error handling.\n",
      " |      \n",
      " |      .. versionadded:: 0.7\n",
      " |  \n",
      " |  handle_exception(self, e)\n",
      " |      Handle an exception that did not have an error handler\n",
      " |      associated with it, or that was raised from an error handler.\n",
      " |      This always causes a 500 ``InternalServerError``.\n",
      " |      \n",
      " |      Always sends the :data:`got_request_exception` signal.\n",
      " |      \n",
      " |      If :attr:`propagate_exceptions` is ``True``, such as in debug\n",
      " |      mode, the error will be re-raised so that the debugger can\n",
      " |      display it. Otherwise, the original exception is logged, and\n",
      " |      an :exc:`~werkzeug.exceptions.InternalServerError` is returned.\n",
      " |      \n",
      " |      If an error handler is registered for ``InternalServerError`` or\n",
      " |      ``500``, it will be used. For consistency, the handler will\n",
      " |      always receive the ``InternalServerError``. The original\n",
      " |      unhandled exception is available as ``e.original_exception``.\n",
      " |      \n",
      " |      .. note::\n",
      " |          Prior to Werkzeug 1.0.0, ``InternalServerError`` will not\n",
      " |          always have an ``original_exception`` attribute. Use\n",
      " |          ``getattr(e, \"original_exception\", None)`` to simulate the\n",
      " |          behavior for compatibility.\n",
      " |      \n",
      " |      .. versionchanged:: 1.1.0\n",
      " |          Always passes the ``InternalServerError`` instance to the\n",
      " |          handler, setting ``original_exception`` to the unhandled\n",
      " |          error.\n",
      " |      \n",
      " |      .. versionchanged:: 1.1.0\n",
      " |          ``after_request`` functions and other finalization is done\n",
      " |          even for the default 500 response when there is no handler.\n",
      " |      \n",
      " |      .. versionadded:: 0.3\n",
      " |  \n",
      " |  handle_http_exception(self, e)\n",
      " |      Handles an HTTP exception.  By default this will invoke the\n",
      " |      registered error handlers and fall back to returning the\n",
      " |      exception as response.\n",
      " |      \n",
      " |      .. versionchanged:: 1.0.3\n",
      " |          ``RoutingException``, used internally for actions such as\n",
      " |           slash redirects during routing, is not passed to error\n",
      " |           handlers.\n",
      " |      \n",
      " |      .. versionchanged:: 1.0\n",
      " |          Exceptions are looked up by code *and* by MRO, so\n",
      " |          ``HTTPExcpetion`` subclasses can be handled with a catch-all\n",
      " |          handler for the base ``HTTPException``.\n",
      " |      \n",
      " |      .. versionadded:: 0.3\n",
      " |  \n",
      " |  handle_url_build_error(self, error, endpoint, values)\n",
      " |      Handle :class:`~werkzeug.routing.BuildError` on :meth:`url_for`.\n",
      " |  \n",
      " |  handle_user_exception(self, e)\n",
      " |      This method is called whenever an exception occurs that\n",
      " |      should be handled. A special case is :class:`~werkzeug\n",
      " |      .exceptions.HTTPException` which is forwarded to the\n",
      " |      :meth:`handle_http_exception` method. This function will either\n",
      " |      return a response value or reraise the exception with the same\n",
      " |      traceback.\n",
      " |      \n",
      " |      .. versionchanged:: 1.0\n",
      " |          Key errors raised from request data like ``form`` show the\n",
      " |          bad key in debug mode rather than a generic bad request\n",
      " |          message.\n",
      " |      \n",
      " |      .. versionadded:: 0.7\n",
      " |  \n",
      " |  inject_url_defaults(self, endpoint, values)\n",
      " |      Injects the URL defaults for the given endpoint directly into\n",
      " |      the values dictionary passed.  This is used internally and\n",
      " |      automatically called on URL building.\n",
      " |      \n",
      " |      .. versionadded:: 0.7\n",
      " |  \n",
      " |  iter_blueprints(self)\n",
      " |      Iterates over all blueprints by the order they were registered.\n",
      " |      \n",
      " |      .. versionadded:: 0.11\n",
      " |  \n",
      " |  jinja_env(...)\n",
      " |      The Jinja environment used to load templates.\n",
      " |      \n",
      " |      The environment is created the first time this property is\n",
      " |      accessed. Changing :attr:`jinja_options` after that will have no\n",
      " |      effect.\n",
      " |  \n",
      " |  log_exception(self, exc_info)\n",
      " |      Logs an exception.  This is called by :meth:`handle_exception`\n",
      " |      if debugging is disabled and right before the handler is called.\n",
      " |      The default implementation logs the exception as error on the\n",
      " |      :attr:`logger`.\n",
      " |      \n",
      " |      .. versionadded:: 0.8\n",
      " |  \n",
      " |  logger(...)\n",
      " |      A standard Python :class:`~logging.Logger` for the app, with\n",
      " |      the same name as :attr:`name`.\n",
      " |      \n",
      " |      In debug mode, the logger's :attr:`~logging.Logger.level` will\n",
      " |      be set to :data:`~logging.DEBUG`.\n",
      " |      \n",
      " |      If there are no handlers configured, a default handler will be\n",
      " |      added. See :doc:`/logging` for more information.\n",
      " |      \n",
      " |      .. versionchanged:: 1.1.0\n",
      " |          The logger takes the same name as :attr:`name` rather than\n",
      " |          hard-coding ``\"flask.app\"``.\n",
      " |      \n",
      " |      .. versionchanged:: 1.0.0\n",
      " |          Behavior was simplified. The logger is always named\n",
      " |          ``\"flask.app\"``. The level is only set during configuration,\n",
      " |          it doesn't check ``app.debug`` each time. Only one format is\n",
      " |          used, not different ones depending on ``app.debug``. No\n",
      " |          handlers are removed, and a handler is only added if no\n",
      " |          handlers are already configured.\n",
      " |      \n",
      " |      .. versionadded:: 0.3\n",
      " |  \n",
      " |  make_config(self, instance_relative=False)\n",
      " |      Used to create the config attribute by the Flask constructor.\n",
      " |      The `instance_relative` parameter is passed in from the constructor\n",
      " |      of Flask (there named `instance_relative_config`) and indicates if\n",
      " |      the config should be relative to the instance path or the root path\n",
      " |      of the application.\n",
      " |      \n",
      " |      .. versionadded:: 0.8\n",
      " |  \n",
      " |  make_default_options_response(self)\n",
      " |      This method is called to create the default ``OPTIONS`` response.\n",
      " |      This can be changed through subclassing to change the default\n",
      " |      behavior of ``OPTIONS`` responses.\n",
      " |      \n",
      " |      .. versionadded:: 0.7\n",
      " |  \n",
      " |  make_null_session(self)\n",
      " |      Creates a new instance of a missing session.  Instead of overriding\n",
      " |      this method we recommend replacing the :class:`session_interface`.\n",
      " |      \n",
      " |      .. deprecated: 1.0\n",
      " |          Will be removed in 2.0. Use\n",
      " |          ``session_interface.make_null_session`` instead.\n",
      " |      \n",
      " |      .. versionadded:: 0.7\n",
      " |  \n",
      " |  make_response(self, rv)\n",
      " |      Convert the return value from a view function to an instance of\n",
      " |      :attr:`response_class`.\n",
      " |      \n",
      " |      :param rv: the return value from the view function. The view function\n",
      " |          must return a response. Returning ``None``, or the view ending\n",
      " |          without returning, is not allowed. The following types are allowed\n",
      " |          for ``view_rv``:\n",
      " |      \n",
      " |          ``str`` (``unicode`` in Python 2)\n",
      " |              A response object is created with the string encoded to UTF-8\n",
      " |              as the body.\n",
      " |      \n",
      " |          ``bytes`` (``str`` in Python 2)\n",
      " |              A response object is created with the bytes as the body.\n",
      " |      \n",
      " |          ``dict``\n",
      " |              A dictionary that will be jsonify'd before being returned.\n",
      " |      \n",
      " |          ``tuple``\n",
      " |              Either ``(body, status, headers)``, ``(body, status)``, or\n",
      " |              ``(body, headers)``, where ``body`` is any of the other types\n",
      " |              allowed here, ``status`` is a string or an integer, and\n",
      " |              ``headers`` is a dictionary or a list of ``(key, value)``\n",
      " |              tuples. If ``body`` is a :attr:`response_class` instance,\n",
      " |              ``status`` overwrites the exiting value and ``headers`` are\n",
      " |              extended.\n",
      " |      \n",
      " |          :attr:`response_class`\n",
      " |              The object is returned unchanged.\n",
      " |      \n",
      " |          other :class:`~werkzeug.wrappers.Response` class\n",
      " |              The object is coerced to :attr:`response_class`.\n",
      " |      \n",
      " |          :func:`callable`\n",
      " |              The function is called as a WSGI application. The result is\n",
      " |              used to create a response object.\n",
      " |      \n",
      " |      .. versionchanged:: 0.9\n",
      " |         Previously a tuple was interpreted as the arguments for the\n",
      " |         response object.\n",
      " |  \n",
      " |  make_shell_context(self)\n",
      " |      Returns the shell context for an interactive shell for this\n",
      " |      application.  This runs all the registered shell context\n",
      " |      processors.\n",
      " |      \n",
      " |      .. versionadded:: 0.11\n",
      " |  \n",
      " |  name(...)\n",
      " |      The name of the application.  This is usually the import name\n",
      " |      with the difference that it's guessed from the run file if the\n",
      " |      import name is main.  This name is used as a display name when\n",
      " |      Flask needs the name of the application.  It can be set and overridden\n",
      " |      to change the value.\n",
      " |      \n",
      " |      .. versionadded:: 0.8\n",
      " |  \n",
      " |  open_instance_resource(self, resource, mode='rb')\n",
      " |      Opens a resource from the application's instance folder\n",
      " |      (:attr:`instance_path`).  Otherwise works like\n",
      " |      :meth:`open_resource`.  Instance resources can also be opened for\n",
      " |      writing.\n",
      " |      \n",
      " |      :param resource: the name of the resource.  To access resources within\n",
      " |                       subfolders use forward slashes as separator.\n",
      " |      :param mode: resource file opening mode, default is 'rb'.\n",
      " |  \n",
      " |  open_session(self, request)\n",
      " |      Creates or opens a new session.  Default implementation stores all\n",
      " |      session data in a signed cookie.  This requires that the\n",
      " |      :attr:`secret_key` is set.  Instead of overriding this method\n",
      " |      we recommend replacing the :class:`session_interface`.\n",
      " |      \n",
      " |      .. deprecated: 1.0\n",
      " |          Will be removed in 2.0. Use\n",
      " |          ``session_interface.open_session`` instead.\n",
      " |      \n",
      " |      :param request: an instance of :attr:`request_class`.\n",
      " |  \n",
      " |  preprocess_request(self)\n",
      " |      Called before the request is dispatched. Calls\n",
      " |      :attr:`url_value_preprocessors` registered with the app and the\n",
      " |      current blueprint (if any). Then calls :attr:`before_request_funcs`\n",
      " |      registered with the app and the blueprint.\n",
      " |      \n",
      " |      If any :meth:`before_request` handler returns a non-None value, the\n",
      " |      value is handled as if it was the return value from the view, and\n",
      " |      further request handling is stopped.\n",
      " |  \n",
      " |  process_response(self, response)\n",
      " |      Can be overridden in order to modify the response object\n",
      " |      before it's sent to the WSGI server.  By default this will\n",
      " |      call all the :meth:`after_request` decorated functions.\n",
      " |      \n",
      " |      .. versionchanged:: 0.5\n",
      " |         As of Flask 0.5 the functions registered for after request\n",
      " |         execution are called in reverse order of registration.\n",
      " |      \n",
      " |      :param response: a :attr:`response_class` object.\n",
      " |      :return: a new response object or the same, has to be an\n",
      " |               instance of :attr:`response_class`.\n",
      " |  \n",
      " |  raise_routing_exception(self, request)\n",
      " |      Exceptions that are recording during routing are reraised with\n",
      " |      this method.  During debug we are not reraising redirect requests\n",
      " |      for non ``GET``, ``HEAD``, or ``OPTIONS`` requests and we're raising\n",
      " |      a different error instead to help debug situations.\n",
      " |      \n",
      " |      :internal:\n",
      " |  \n",
      " |  register_blueprint(self, blueprint, **options)\n",
      " |      Register a :class:`~flask.Blueprint` on the application. Keyword\n",
      " |      arguments passed to this method will override the defaults set on the\n",
      " |      blueprint.\n",
      " |      \n",
      " |      Calls the blueprint's :meth:`~flask.Blueprint.register` method after\n",
      " |      recording the blueprint in the application's :attr:`blueprints`.\n",
      " |      \n",
      " |      :param blueprint: The blueprint to register.\n",
      " |      :param url_prefix: Blueprint routes will be prefixed with this.\n",
      " |      :param subdomain: Blueprint routes will match on this subdomain.\n",
      " |      :param url_defaults: Blueprint routes will use these default values for\n",
      " |          view arguments.\n",
      " |      :param options: Additional keyword arguments are passed to\n",
      " |          :class:`~flask.blueprints.BlueprintSetupState`. They can be\n",
      " |          accessed in :meth:`~flask.Blueprint.record` callbacks.\n",
      " |      \n",
      " |      .. versionadded:: 0.7\n",
      " |  \n",
      " |  register_error_handler(self, code_or_exception, f)\n",
      " |      Alternative error attach function to the :meth:`errorhandler`\n",
      " |      decorator that is more straightforward to use for non decorator\n",
      " |      usage.\n",
      " |      \n",
      " |      .. versionadded:: 0.7\n",
      " |  \n",
      " |  request_context(self, environ)\n",
      " |      Create a :class:`~flask.ctx.RequestContext` representing a\n",
      " |      WSGI environment. Use a ``with`` block to push the context,\n",
      " |      which will make :data:`request` point at this request.\n",
      " |      \n",
      " |      See :doc:`/reqcontext`.\n",
      " |      \n",
      " |      Typically you should not call this from your own code. A request\n",
      " |      context is automatically pushed by the :meth:`wsgi_app` when\n",
      " |      handling a request. Use :meth:`test_request_context` to create\n",
      " |      an environment and context instead of this method.\n",
      " |      \n",
      " |      :param environ: a WSGI environment\n",
      " |  \n",
      " |  route(self, rule, **options)\n",
      " |      A decorator that is used to register a view function for a\n",
      " |      given URL rule.  This does the same thing as :meth:`add_url_rule`\n",
      " |      but is intended for decorator usage::\n",
      " |      \n",
      " |          @app.route('/')\n",
      " |          def index():\n",
      " |              return 'Hello World'\n",
      " |      \n",
      " |      For more information refer to :ref:`url-route-registrations`.\n",
      " |      \n",
      " |      :param rule: the URL rule as string\n",
      " |      :param endpoint: the endpoint for the registered URL rule.  Flask\n",
      " |                       itself assumes the name of the view function as\n",
      " |                       endpoint\n",
      " |      :param options: the options to be forwarded to the underlying\n",
      " |                      :class:`~werkzeug.routing.Rule` object.  A change\n",
      " |                      to Werkzeug is handling of method options.  methods\n",
      " |                      is a list of methods this rule should be limited\n",
      " |                      to (``GET``, ``POST`` etc.).  By default a rule\n",
      " |                      just listens for ``GET`` (and implicitly ``HEAD``).\n",
      " |                      Starting with Flask 0.6, ``OPTIONS`` is implicitly\n",
      " |                      added and handled by the standard request handling.\n",
      " |  \n",
      " |  run(self, host=None, port=None, debug=None, load_dotenv=True, **options)\n",
      " |      Runs the application on a local development server.\n",
      " |      \n",
      " |      Do not use ``run()`` in a production setting. It is not intended to\n",
      " |      meet security and performance requirements for a production server.\n",
      " |      Instead, see :ref:`deployment` for WSGI server recommendations.\n",
      " |      \n",
      " |      If the :attr:`debug` flag is set the server will automatically reload\n",
      " |      for code changes and show a debugger in case an exception happened.\n",
      " |      \n",
      " |      If you want to run the application in debug mode, but disable the\n",
      " |      code execution on the interactive debugger, you can pass\n",
      " |      ``use_evalex=False`` as parameter.  This will keep the debugger's\n",
      " |      traceback screen active, but disable code execution.\n",
      " |      \n",
      " |      It is not recommended to use this function for development with\n",
      " |      automatic reloading as this is badly supported.  Instead you should\n",
      " |      be using the :command:`flask` command line script's ``run`` support.\n",
      " |      \n",
      " |      .. admonition:: Keep in Mind\n",
      " |      \n",
      " |         Flask will suppress any server error with a generic error page\n",
      " |         unless it is in debug mode.  As such to enable just the\n",
      " |         interactive debugger without the code reloading, you have to\n",
      " |         invoke :meth:`run` with ``debug=True`` and ``use_reloader=False``.\n",
      " |         Setting ``use_debugger`` to ``True`` without being in debug mode\n",
      " |         won't catch any exceptions because there won't be any to\n",
      " |         catch.\n",
      " |      \n",
      " |      :param host: the hostname to listen on. Set this to ``'0.0.0.0'`` to\n",
      " |          have the server available externally as well. Defaults to\n",
      " |          ``'127.0.0.1'`` or the host in the ``SERVER_NAME`` config variable\n",
      " |          if present.\n",
      " |      :param port: the port of the webserver. Defaults to ``5000`` or the\n",
      " |          port defined in the ``SERVER_NAME`` config variable if present.\n",
      " |      :param debug: if given, enable or disable debug mode. See\n",
      " |          :attr:`debug`.\n",
      " |      :param load_dotenv: Load the nearest :file:`.env` and :file:`.flaskenv`\n",
      " |          files to set environment variables. Will also change the working\n",
      " |          directory to the directory containing the first file found.\n",
      " |      :param options: the options to be forwarded to the underlying Werkzeug\n",
      " |          server. See :func:`werkzeug.serving.run_simple` for more\n",
      " |          information.\n",
      " |      \n",
      " |      .. versionchanged:: 1.0\n",
      " |          If installed, python-dotenv will be used to load environment\n",
      " |          variables from :file:`.env` and :file:`.flaskenv` files.\n",
      " |      \n",
      " |          If set, the :envvar:`FLASK_ENV` and :envvar:`FLASK_DEBUG`\n",
      " |          environment variables will override :attr:`env` and\n",
      " |          :attr:`debug`.\n",
      " |      \n",
      " |          Threaded mode is enabled by default.\n",
      " |      \n",
      " |      .. versionchanged:: 0.10\n",
      " |          The default port is now picked from the ``SERVER_NAME``\n",
      " |          variable.\n",
      " |  \n",
      " |  save_session(self, session, response)\n",
      " |      Saves the session if it needs updates.  For the default\n",
      " |      implementation, check :meth:`open_session`.  Instead of overriding this\n",
      " |      method we recommend replacing the :class:`session_interface`.\n",
      " |      \n",
      " |      .. deprecated: 1.0\n",
      " |          Will be removed in 2.0. Use\n",
      " |          ``session_interface.save_session`` instead.\n",
      " |      \n",
      " |      :param session: the session to be saved (a\n",
      " |                      :class:`~werkzeug.contrib.securecookie.SecureCookie`\n",
      " |                      object)\n",
      " |      :param response: an instance of :attr:`response_class`\n",
      " |  \n",
      " |  select_jinja_autoescape(self, filename)\n",
      " |      Returns ``True`` if autoescaping should be active for the given\n",
      " |      template name. If no template name is given, returns `True`.\n",
      " |      \n",
      " |      .. versionadded:: 0.5\n",
      " |  \n",
      " |  shell_context_processor(self, f)\n",
      " |      Registers a shell context processor function.\n",
      " |      \n",
      " |      .. versionadded:: 0.11\n",
      " |  \n",
      " |  should_ignore_error(self, error)\n",
      " |      This is called to figure out if an error should be ignored\n",
      " |      or not as far as the teardown system is concerned.  If this\n",
      " |      function returns ``True`` then the teardown handlers will not be\n",
      " |      passed the error.\n",
      " |      \n",
      " |      .. versionadded:: 0.10\n",
      " |  \n",
      " |  teardown_appcontext(self, f)\n",
      " |      Registers a function to be called when the application context\n",
      " |      ends.  These functions are typically also called when the request\n",
      " |      context is popped.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          ctx = app.app_context()\n",
      " |          ctx.push()\n",
      " |          ...\n",
      " |          ctx.pop()\n",
      " |      \n",
      " |      When ``ctx.pop()`` is executed in the above example, the teardown\n",
      " |      functions are called just before the app context moves from the\n",
      " |      stack of active contexts.  This becomes relevant if you are using\n",
      " |      such constructs in tests.\n",
      " |      \n",
      " |      Since a request context typically also manages an application\n",
      " |      context it would also be called when you pop a request context.\n",
      " |      \n",
      " |      When a teardown function was called because of an unhandled exception\n",
      " |      it will be passed an error object. If an :meth:`errorhandler` is\n",
      " |      registered, it will handle the exception and the teardown will not\n",
      " |      receive it.\n",
      " |      \n",
      " |      The return values of teardown functions are ignored.\n",
      " |      \n",
      " |      .. versionadded:: 0.9\n",
      " |  \n",
      " |  teardown_request(self, f)\n",
      " |      Register a function to be run at the end of each request,\n",
      " |      regardless of whether there was an exception or not.  These functions\n",
      " |      are executed when the request context is popped, even if not an\n",
      " |      actual request was performed.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          ctx = app.test_request_context()\n",
      " |          ctx.push()\n",
      " |          ...\n",
      " |          ctx.pop()\n",
      " |      \n",
      " |      When ``ctx.pop()`` is executed in the above example, the teardown\n",
      " |      functions are called just before the request context moves from the\n",
      " |      stack of active contexts.  This becomes relevant if you are using\n",
      " |      such constructs in tests.\n",
      " |      \n",
      " |      Generally teardown functions must take every necessary step to avoid\n",
      " |      that they will fail.  If they do execute code that might fail they\n",
      " |      will have to surround the execution of these code by try/except\n",
      " |      statements and log occurring errors.\n",
      " |      \n",
      " |      When a teardown function was called because of an exception it will\n",
      " |      be passed an error object.\n",
      " |      \n",
      " |      The return values of teardown functions are ignored.\n",
      " |      \n",
      " |      .. admonition:: Debug Note\n",
      " |      \n",
      " |         In debug mode Flask will not tear down a request on an exception\n",
      " |         immediately.  Instead it will keep it alive so that the interactive\n",
      " |         debugger can still access it.  This behavior can be controlled\n",
      " |         by the ``PRESERVE_CONTEXT_ON_EXCEPTION`` configuration variable.\n",
      " |  \n",
      " |  template_filter(self, name=None)\n",
      " |      A decorator that is used to register custom template filter.\n",
      " |      You can specify a name for the filter, otherwise the function\n",
      " |      name will be used. Example::\n",
      " |      \n",
      " |        @app.template_filter()\n",
      " |        def reverse(s):\n",
      " |            return s[::-1]\n",
      " |      \n",
      " |      :param name: the optional name of the filter, otherwise the\n",
      " |                   function name will be used.\n",
      " |  \n",
      " |  template_global(self, name=None)\n",
      " |      A decorator that is used to register a custom template global function.\n",
      " |      You can specify a name for the global function, otherwise the function\n",
      " |      name will be used. Example::\n",
      " |      \n",
      " |          @app.template_global()\n",
      " |          def double(n):\n",
      " |              return 2 * n\n",
      " |      \n",
      " |      .. versionadded:: 0.10\n",
      " |      \n",
      " |      :param name: the optional name of the global function, otherwise the\n",
      " |                   function name will be used.\n",
      " |  \n",
      " |  template_test(self, name=None)\n",
      " |      A decorator that is used to register custom template test.\n",
      " |      You can specify a name for the test, otherwise the function\n",
      " |      name will be used. Example::\n",
      " |      \n",
      " |        @app.template_test()\n",
      " |        def is_prime(n):\n",
      " |            if n == 2:\n",
      " |                return True\n",
      " |            for i in range(2, int(math.ceil(math.sqrt(n))) + 1):\n",
      " |                if n % i == 0:\n",
      " |                    return False\n",
      " |            return True\n",
      " |      \n",
      " |      .. versionadded:: 0.10\n",
      " |      \n",
      " |      :param name: the optional name of the test, otherwise the\n",
      " |                   function name will be used.\n",
      " |  \n",
      " |  test_cli_runner(self, **kwargs)\n",
      " |      Create a CLI runner for testing CLI commands.\n",
      " |      See :ref:`testing-cli`.\n",
      " |      \n",
      " |      Returns an instance of :attr:`test_cli_runner_class`, by default\n",
      " |      :class:`~flask.testing.FlaskCliRunner`. The Flask app object is\n",
      " |      passed as the first argument.\n",
      " |      \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  test_client(self, use_cookies=True, **kwargs)\n",
      " |      Creates a test client for this application.  For information\n",
      " |      about unit testing head over to :ref:`testing`.\n",
      " |      \n",
      " |      Note that if you are testing for assertions or exceptions in your\n",
      " |      application code, you must set ``app.testing = True`` in order for the\n",
      " |      exceptions to propagate to the test client.  Otherwise, the exception\n",
      " |      will be handled by the application (not visible to the test client) and\n",
      " |      the only indication of an AssertionError or other exception will be a\n",
      " |      500 status code response to the test client.  See the :attr:`testing`\n",
      " |      attribute.  For example::\n",
      " |      \n",
      " |          app.testing = True\n",
      " |          client = app.test_client()\n",
      " |      \n",
      " |      The test client can be used in a ``with`` block to defer the closing down\n",
      " |      of the context until the end of the ``with`` block.  This is useful if\n",
      " |      you want to access the context locals for testing::\n",
      " |      \n",
      " |          with app.test_client() as c:\n",
      " |              rv = c.get('/?vodka=42')\n",
      " |              assert request.args['vodka'] == '42'\n",
      " |      \n",
      " |      Additionally, you may pass optional keyword arguments that will then\n",
      " |      be passed to the application's :attr:`test_client_class` constructor.\n",
      " |      For example::\n",
      " |      \n",
      " |          from flask.testing import FlaskClient\n",
      " |      \n",
      " |          class CustomClient(FlaskClient):\n",
      " |              def __init__(self, *args, **kwargs):\n",
      " |                  self._authentication = kwargs.pop(\"authentication\")\n",
      " |                  super(CustomClient,self).__init__( *args, **kwargs)\n",
      " |      \n",
      " |          app.test_client_class = CustomClient\n",
      " |          client = app.test_client(authentication='Basic ....')\n",
      " |      \n",
      " |      See :class:`~flask.testing.FlaskClient` for more information.\n",
      " |      \n",
      " |      .. versionchanged:: 0.4\n",
      " |         added support for ``with`` block usage for the client.\n",
      " |      \n",
      " |      .. versionadded:: 0.7\n",
      " |         The `use_cookies` parameter was added as well as the ability\n",
      " |         to override the client to be used by setting the\n",
      " |         :attr:`test_client_class` attribute.\n",
      " |      \n",
      " |      .. versionchanged:: 0.11\n",
      " |         Added `**kwargs` to support passing additional keyword arguments to\n",
      " |         the constructor of :attr:`test_client_class`.\n",
      " |  \n",
      " |  test_request_context(self, *args, **kwargs)\n",
      " |      Create a :class:`~flask.ctx.RequestContext` for a WSGI\n",
      " |      environment created from the given values. This is mostly useful\n",
      " |      during testing, where you may want to run a function that uses\n",
      " |      request data without dispatching a full request.\n",
      " |      \n",
      " |      See :doc:`/reqcontext`.\n",
      " |      \n",
      " |      Use a ``with`` block to push the context, which will make\n",
      " |      :data:`request` point at the request for the created\n",
      " |      environment. ::\n",
      " |      \n",
      " |          with test_request_context(...):\n",
      " |              generate_report()\n",
      " |      \n",
      " |      When using the shell, it may be easier to push and pop the\n",
      " |      context manually to avoid indentation. ::\n",
      " |      \n",
      " |          ctx = app.test_request_context(...)\n",
      " |          ctx.push()\n",
      " |          ...\n",
      " |          ctx.pop()\n",
      " |      \n",
      " |      Takes the same arguments as Werkzeug's\n",
      " |      :class:`~werkzeug.test.EnvironBuilder`, with some defaults from\n",
      " |      the application. See the linked Werkzeug docs for most of the\n",
      " |      available arguments. Flask-specific behavior is listed here.\n",
      " |      \n",
      " |      :param path: URL path being requested.\n",
      " |      :param base_url: Base URL where the app is being served, which\n",
      " |          ``path`` is relative to. If not given, built from\n",
      " |          :data:`PREFERRED_URL_SCHEME`, ``subdomain``,\n",
      " |          :data:`SERVER_NAME`, and :data:`APPLICATION_ROOT`.\n",
      " |      :param subdomain: Subdomain name to append to\n",
      " |          :data:`SERVER_NAME`.\n",
      " |      :param url_scheme: Scheme to use instead of\n",
      " |          :data:`PREFERRED_URL_SCHEME`.\n",
      " |      :param data: The request body, either as a string or a dict of\n",
      " |          form keys and values.\n",
      " |      :param json: If given, this is serialized as JSON and passed as\n",
      " |          ``data``. Also defaults ``content_type`` to\n",
      " |          ``application/json``.\n",
      " |      :param args: other positional arguments passed to\n",
      " |          :class:`~werkzeug.test.EnvironBuilder`.\n",
      " |      :param kwargs: other keyword arguments passed to\n",
      " |          :class:`~werkzeug.test.EnvironBuilder`.\n",
      " |  \n",
      " |  trap_http_exception(self, e)\n",
      " |      Checks if an HTTP exception should be trapped or not.  By default\n",
      " |      this will return ``False`` for all exceptions except for a bad request\n",
      " |      key error if ``TRAP_BAD_REQUEST_ERRORS`` is set to ``True``.  It\n",
      " |      also returns ``True`` if ``TRAP_HTTP_EXCEPTIONS`` is set to ``True``.\n",
      " |      \n",
      " |      This is called for all HTTP exceptions raised by a view function.\n",
      " |      If it returns ``True`` for any exception the error handler for this\n",
      " |      exception is not called and it shows up as regular exception in the\n",
      " |      traceback.  This is helpful for debugging implicitly raised HTTP\n",
      " |      exceptions.\n",
      " |      \n",
      " |      .. versionchanged:: 1.0\n",
      " |          Bad request errors are not trapped by default in debug mode.\n",
      " |      \n",
      " |      .. versionadded:: 0.8\n",
      " |  \n",
      " |  try_trigger_before_first_request_functions(self)\n",
      " |      Called before each request and will ensure that it triggers\n",
      " |      the :attr:`before_first_request_funcs` and only exactly once per\n",
      " |      application instance (which means process usually).\n",
      " |      \n",
      " |      :internal:\n",
      " |  \n",
      " |  update_template_context(self, context)\n",
      " |      Update the template context with some commonly used variables.\n",
      " |      This injects request, session, config and g into the template\n",
      " |      context as well as everything template context processors want\n",
      " |      to inject.  Note that the as of Flask 0.6, the original values\n",
      " |      in the context will not be overridden if a context processor\n",
      " |      decides to return a value with the same key.\n",
      " |      \n",
      " |      :param context: the context as a dictionary that is updated in place\n",
      " |                      to add extra variables.\n",
      " |  \n",
      " |  url_defaults(self, f)\n",
      " |      Callback function for URL defaults for all view functions of the\n",
      " |      application.  It's called with the endpoint and values and should\n",
      " |      update the values passed in place.\n",
      " |  \n",
      " |  url_value_preprocessor(self, f)\n",
      " |      Register a URL value preprocessor function for all view\n",
      " |      functions in the application. These functions will be called before the\n",
      " |      :meth:`before_request` functions.\n",
      " |      \n",
      " |      The function can modify the values captured from the matched url before\n",
      " |      they are passed to the view. For example, this can be used to pop a\n",
      " |      common language code value and place it in ``g`` rather than pass it to\n",
      " |      every view.\n",
      " |      \n",
      " |      The function is passed the endpoint name and values dict. The return\n",
      " |      value is ignored.\n",
      " |  \n",
      " |  wsgi_app(self, environ, start_response)\n",
      " |      The actual WSGI application. This is not implemented in\n",
      " |      :meth:`__call__` so that middlewares can be applied without\n",
      " |      losing a reference to the app object. Instead of doing this::\n",
      " |      \n",
      " |          app = MyMiddleware(app)\n",
      " |      \n",
      " |      It's a better idea to do this instead::\n",
      " |      \n",
      " |          app.wsgi_app = MyMiddleware(app.wsgi_app)\n",
      " |      \n",
      " |      Then you still have the original application object around and\n",
      " |      can continue to call methods on it.\n",
      " |      \n",
      " |      .. versionchanged:: 0.7\n",
      " |          Teardown events for the request and app contexts are called\n",
      " |          even if an unhandled error occurs. Other events may not be\n",
      " |          called depending on when an error occurs during dispatch.\n",
      " |          See :ref:`callbacks-and-errors`.\n",
      " |      \n",
      " |      :param environ: A WSGI environment.\n",
      " |      :param start_response: A callable accepting a status code,\n",
      " |          a list of headers, and an optional exception context to\n",
      " |          start the response.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  got_first_request\n",
      " |      This attribute is set to ``True`` if the application started\n",
      " |      handling the first request.\n",
      " |      \n",
      " |      .. versionadded:: 0.8\n",
      " |  \n",
      " |  preserve_context_on_exception\n",
      " |      Returns the value of the ``PRESERVE_CONTEXT_ON_EXCEPTION``\n",
      " |      configuration value in case it's set, otherwise a sensible default\n",
      " |      is returned.\n",
      " |      \n",
      " |      .. versionadded:: 0.7\n",
      " |  \n",
      " |  propagate_exceptions\n",
      " |      Returns the value of the ``PROPAGATE_EXCEPTIONS`` configuration\n",
      " |      value in case it's set, otherwise a sensible default is returned.\n",
      " |      \n",
      " |      .. versionadded:: 0.7\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  debug\n",
      " |      Whether debug mode is enabled. When using ``flask run`` to start\n",
      " |      the development server, an interactive debugger will be shown for\n",
      " |      unhandled exceptions, and the server will be reloaded when code\n",
      " |      changes. This maps to the :data:`DEBUG` config key. This is\n",
      " |      enabled when :attr:`env` is ``'development'`` and is overridden\n",
      " |      by the ``FLASK_DEBUG`` environment variable. It may not behave as\n",
      " |      expected if set in code.\n",
      " |      \n",
      " |      **Do not enable debug mode when deploying in production.**\n",
      " |      \n",
      " |      Default: ``True`` if :attr:`env` is ``'development'``, or\n",
      " |      ``False`` otherwise.\n",
      " |  \n",
      " |  env\n",
      " |      Makes an attribute forward to the config\n",
      " |  \n",
      " |  permanent_session_lifetime\n",
      " |      Makes an attribute forward to the config\n",
      " |  \n",
      " |  secret_key\n",
      " |      Makes an attribute forward to the config\n",
      " |  \n",
      " |  send_file_max_age_default\n",
      " |      Makes an attribute forward to the config\n",
      " |  \n",
      " |  session_cookie_name\n",
      " |      Makes an attribute forward to the config\n",
      " |  \n",
      " |  templates_auto_reload\n",
      " |      Reload templates when they are changed. Used by\n",
      " |      :meth:`create_jinja_environment`.\n",
      " |      \n",
      " |      This attribute can be configured with :data:`TEMPLATES_AUTO_RELOAD`. If\n",
      " |      not set, it will be enabled in debug mode.\n",
      " |      \n",
      " |      .. versionadded:: 1.0\n",
      " |          This property was added but the underlying config and behavior\n",
      " |          already existed.\n",
      " |  \n",
      " |  testing\n",
      " |      Makes an attribute forward to the config\n",
      " |  \n",
      " |  use_x_sendfile\n",
      " |      Makes an attribute forward to the config\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  app_ctx_globals_class = <class 'flask.ctx._AppCtxGlobals'>\n",
      " |      A plain object. Used as a namespace for storing data during an\n",
      " |      application context.\n",
      " |      \n",
      " |      Creating an app context automatically creates this object, which is\n",
      " |      made available as the :data:`g` proxy.\n",
      " |      \n",
      " |      .. describe:: 'key' in g\n",
      " |      \n",
      " |          Check whether an attribute is present.\n",
      " |      \n",
      " |          .. versionadded:: 0.10\n",
      " |      \n",
      " |      .. describe:: iter(g)\n",
      " |      \n",
      " |          Return an iterator over the attribute names.\n",
      " |      \n",
      " |          .. versionadded:: 0.10\n",
      " |  \n",
      " |  config_class = <class 'flask.config.Config'>\n",
      " |      Works exactly like a dict but provides ways to fill it from files\n",
      " |      or special dictionaries.  There are two common patterns to populate the\n",
      " |      config.\n",
      " |      \n",
      " |      Either you can fill the config from a config file::\n",
      " |      \n",
      " |          app.config.from_pyfile('yourconfig.cfg')\n",
      " |      \n",
      " |      Or alternatively you can define the configuration options in the\n",
      " |      module that calls :meth:`from_object` or provide an import path to\n",
      " |      a module that should be loaded.  It is also possible to tell it to\n",
      " |      use the same module and with that provide the configuration values\n",
      " |      just before the call::\n",
      " |      \n",
      " |          DEBUG = True\n",
      " |          SECRET_KEY = 'development key'\n",
      " |          app.config.from_object(__name__)\n",
      " |      \n",
      " |      In both cases (loading from any Python file or loading from modules),\n",
      " |      only uppercase keys are added to the config.  This makes it possible to use\n",
      " |      lowercase values in the config file for temporary values that are not added\n",
      " |      to the config or to define the config keys in the same file that implements\n",
      " |      the application.\n",
      " |      \n",
      " |      Probably the most interesting way to load configurations is from an\n",
      " |      environment variable pointing to a file::\n",
      " |      \n",
      " |          app.config.from_envvar('YOURAPPLICATION_SETTINGS')\n",
      " |      \n",
      " |      In this case before launching the application you have to set this\n",
      " |      environment variable to the file you want to use.  On Linux and OS X\n",
      " |      use the export statement::\n",
      " |      \n",
      " |          export YOURAPPLICATION_SETTINGS='/path/to/config/file'\n",
      " |      \n",
      " |      On windows use `set` instead.\n",
      " |      \n",
      " |      :param root_path: path to which files are read relative from.  When the\n",
      " |                        config object is created by the application, this is\n",
      " |                        the application's :attr:`~flask.Flask.root_path`.\n",
      " |      :param defaults: an optional dictionary of default values\n",
      " |  \n",
      " |  default_config = ImmutableDict({'ENV': None, 'DEBUG': None, 'TEST...TE...\n",
      " |  \n",
      " |  import_name = None\n",
      " |  \n",
      " |  jinja_environment = <class 'flask.templating.Environment'>\n",
      " |      Works like a regular Jinja2 environment but has some additional\n",
      " |      knowledge of how Flask's blueprint works so that it can prepend the\n",
      " |      name of the blueprint to referenced templates if necessary.\n",
      " |  \n",
      " |  jinja_options = {'extensions': ['jinja2.ext.autoescape', 'jinja2.ext.w...\n",
      " |  \n",
      " |  json_decoder = <class 'flask.json.JSONDecoder'>\n",
      " |      The default JSON decoder.  This one does not change the behavior from\n",
      " |      the default simplejson decoder.  Consult the :mod:`json` documentation\n",
      " |      for more information.  This decoder is not only used for the load\n",
      " |      functions of this module but also :attr:`~flask.Request`.\n",
      " |  \n",
      " |  json_encoder = <class 'flask.json.JSONEncoder'>\n",
      " |      The default Flask JSON encoder. This one extends the default\n",
      " |      encoder by also supporting ``datetime``, ``UUID``, ``dataclasses``,\n",
      " |      and ``Markup`` objects.\n",
      " |      \n",
      " |      ``datetime`` objects are serialized as RFC 822 datetime strings.\n",
      " |      This is the same as the HTTP date format.\n",
      " |      \n",
      " |      In order to support more data types, override the :meth:`default`\n",
      " |      method.\n",
      " |  \n",
      " |  request_class = <class 'flask.wrappers.Request'>\n",
      " |      The request object used by default in Flask.  Remembers the\n",
      " |      matched endpoint and view arguments.\n",
      " |      \n",
      " |      It is what ends up as :class:`~flask.request`.  If you want to replace\n",
      " |      the request object used you can subclass this and set\n",
      " |      :attr:`~flask.Flask.request_class` to your subclass.\n",
      " |      \n",
      " |      The request object is a :class:`~werkzeug.wrappers.Request` subclass and\n",
      " |      provides all of the attributes Werkzeug defines plus a few Flask\n",
      " |      specific ones.\n",
      " |  \n",
      " |  response_class = <class 'flask.wrappers.Response'>\n",
      " |      The response object that is used by default in Flask.  Works like the\n",
      " |      response object from Werkzeug but is set to have an HTML mimetype by\n",
      " |      default.  Quite often you don't have to create this object yourself because\n",
      " |      :meth:`~flask.Flask.make_response` will take care of that for you.\n",
      " |      \n",
      " |      If you want to replace the response object used you can subclass this and\n",
      " |      set :attr:`~flask.Flask.response_class` to your subclass.\n",
      " |      \n",
      " |      .. versionchanged:: 1.0\n",
      " |          JSON support is added to the response, like the request. This is useful\n",
      " |          when testing to get the test client response data as JSON.\n",
      " |      \n",
      " |      .. versionchanged:: 1.0\n",
      " |      \n",
      " |          Added :attr:`max_cookie_size`.\n",
      " |  \n",
      " |  root_path = None\n",
      " |  \n",
      " |  session_interface = <flask.sessions.SecureCookieSessionInterface objec...\n",
      " |  \n",
      " |  template_folder = None\n",
      " |  \n",
      " |  test_cli_runner_class = None\n",
      " |  \n",
      " |  test_client_class = None\n",
      " |  \n",
      " |  url_map_class = <class 'werkzeug.routing.Map'>\n",
      " |      The map class stores all the URL rules and some configuration\n",
      " |      parameters.  Some of the configuration values are only stored on the\n",
      " |      `Map` instance since those affect all rules, others are just defaults\n",
      " |      and can be overridden for each rule.  Note that you have to specify all\n",
      " |      arguments besides the `rules` as keyword arguments!\n",
      " |      \n",
      " |      :param rules: sequence of url rules for this map.\n",
      " |      :param default_subdomain: The default subdomain for rules without a\n",
      " |                                subdomain defined.\n",
      " |      :param charset: charset of the url. defaults to ``\"utf-8\"``\n",
      " |      :param strict_slashes: If a rule ends with a slash but the matched\n",
      " |          URL does not, redirect to the URL with a trailing slash.\n",
      " |      :param merge_slashes: Merge consecutive slashes when matching or\n",
      " |          building URLs. Matches will redirect to the normalized URL.\n",
      " |          Slashes in variable parts are not merged.\n",
      " |      :param redirect_defaults: This will redirect to the default rule if it\n",
      " |                                wasn't visited that way. This helps creating\n",
      " |                                unique URLs.\n",
      " |      :param converters: A dict of converters that adds additional converters\n",
      " |                         to the list of converters. If you redefine one\n",
      " |                         converter this will override the original one.\n",
      " |      :param sort_parameters: If set to `True` the url parameters are sorted.\n",
      " |                              See `url_encode` for more details.\n",
      " |      :param sort_key: The sort key function for `url_encode`.\n",
      " |      :param encoding_errors: the error method to use for decoding\n",
      " |      :param host_matching: if set to `True` it enables the host matching\n",
      " |                            feature and disables the subdomain one.  If\n",
      " |                            enabled the `host` parameter to rules is used\n",
      " |                            instead of the `subdomain` one.\n",
      " |      \n",
      " |      .. versionchanged:: 1.0\n",
      " |          If ``url_scheme`` is ``ws`` or ``wss``, only WebSocket rules\n",
      " |          will match.\n",
      " |      \n",
      " |      .. versionchanged:: 1.0\n",
      " |          Added ``merge_slashes``.\n",
      " |      \n",
      " |      .. versionchanged:: 0.7\n",
      " |          Added ``encoding_errors`` and ``host_matching``.\n",
      " |      \n",
      " |      .. versionchanged:: 0.5\n",
      " |          Added ``sort_parameters`` and ``sort_key``.\n",
      " |  \n",
      " |  url_rule_class = <class 'werkzeug.routing.Rule'>\n",
      " |      A Rule represents one URL pattern.  There are some options for `Rule`\n",
      " |      that change the way it behaves and are passed to the `Rule` constructor.\n",
      " |      Note that besides the rule-string all arguments *must* be keyword arguments\n",
      " |      in order to not break the application on Werkzeug upgrades.\n",
      " |      \n",
      " |      `string`\n",
      " |          Rule strings basically are just normal URL paths with placeholders in\n",
      " |          the format ``<converter(arguments):name>`` where the converter and the\n",
      " |          arguments are optional.  If no converter is defined the `default`\n",
      " |          converter is used which means `string` in the normal configuration.\n",
      " |      \n",
      " |          URL rules that end with a slash are branch URLs, others are leaves.\n",
      " |          If you have `strict_slashes` enabled (which is the default), all\n",
      " |          branch URLs that are matched without a trailing slash will trigger a\n",
      " |          redirect to the same URL with the missing slash appended.\n",
      " |      \n",
      " |          The converters are defined on the `Map`.\n",
      " |      \n",
      " |      `endpoint`\n",
      " |          The endpoint for this rule. This can be anything. A reference to a\n",
      " |          function, a string, a number etc.  The preferred way is using a string\n",
      " |          because the endpoint is used for URL generation.\n",
      " |      \n",
      " |      `defaults`\n",
      " |          An optional dict with defaults for other rules with the same endpoint.\n",
      " |          This is a bit tricky but useful if you want to have unique URLs::\n",
      " |      \n",
      " |              url_map = Map([\n",
      " |                  Rule('/all/', defaults={'page': 1}, endpoint='all_entries'),\n",
      " |                  Rule('/all/page/<int:page>', endpoint='all_entries')\n",
      " |              ])\n",
      " |      \n",
      " |          If a user now visits ``http://example.com/all/page/1`` he will be\n",
      " |          redirected to ``http://example.com/all/``.  If `redirect_defaults` is\n",
      " |          disabled on the `Map` instance this will only affect the URL\n",
      " |          generation.\n",
      " |      \n",
      " |      `subdomain`\n",
      " |          The subdomain rule string for this rule. If not specified the rule\n",
      " |          only matches for the `default_subdomain` of the map.  If the map is\n",
      " |          not bound to a subdomain this feature is disabled.\n",
      " |      \n",
      " |          Can be useful if you want to have user profiles on different subdomains\n",
      " |          and all subdomains are forwarded to your application::\n",
      " |      \n",
      " |              url_map = Map([\n",
      " |                  Rule('/', subdomain='<username>', endpoint='user/homepage'),\n",
      " |                  Rule('/stats', subdomain='<username>', endpoint='user/stats')\n",
      " |              ])\n",
      " |      \n",
      " |      `methods`\n",
      " |          A sequence of http methods this rule applies to.  If not specified, all\n",
      " |          methods are allowed. For example this can be useful if you want different\n",
      " |          endpoints for `POST` and `GET`.  If methods are defined and the path\n",
      " |          matches but the method matched against is not in this list or in the\n",
      " |          list of another rule for that path the error raised is of the type\n",
      " |          `MethodNotAllowed` rather than `NotFound`.  If `GET` is present in the\n",
      " |          list of methods and `HEAD` is not, `HEAD` is added automatically.\n",
      " |      \n",
      " |      `strict_slashes`\n",
      " |          Override the `Map` setting for `strict_slashes` only for this rule. If\n",
      " |          not specified the `Map` setting is used.\n",
      " |      \n",
      " |      `merge_slashes`\n",
      " |          Override :attr:`Map.merge_slashes` for this rule.\n",
      " |      \n",
      " |      `build_only`\n",
      " |          Set this to True and the rule will never match but will create a URL\n",
      " |          that can be build. This is useful if you have resources on a subdomain\n",
      " |          or folder that are not handled by the WSGI application (like static data)\n",
      " |      \n",
      " |      `redirect_to`\n",
      " |          If given this must be either a string or callable.  In case of a\n",
      " |          callable it's called with the url adapter that triggered the match and\n",
      " |          the values of the URL as keyword arguments and has to return the target\n",
      " |          for the redirect, otherwise it has to be a string with placeholders in\n",
      " |          rule syntax::\n",
      " |      \n",
      " |              def foo_with_slug(adapter, id):\n",
      " |                  # ask the database for the slug for the old id.  this of\n",
      " |                  # course has nothing to do with werkzeug.\n",
      " |                  return 'foo/' + Foo.get_slug_for_id(id)\n",
      " |      \n",
      " |              url_map = Map([\n",
      " |                  Rule('/foo/<slug>', endpoint='foo'),\n",
      " |                  Rule('/some/old/url/<slug>', redirect_to='foo/<slug>'),\n",
      " |                  Rule('/other/old/url/<int:id>', redirect_to=foo_with_slug)\n",
      " |              ])\n",
      " |      \n",
      " |          When the rule is matched the routing system will raise a\n",
      " |          `RequestRedirect` exception with the target for the redirect.\n",
      " |      \n",
      " |          Keep in mind that the URL will be joined against the URL root of the\n",
      " |          script so don't use a leading slash on the target URL unless you\n",
      " |          really mean root of that domain.\n",
      " |      \n",
      " |      `alias`\n",
      " |          If enabled this rule serves as an alias for another rule with the same\n",
      " |          endpoint and arguments.\n",
      " |      \n",
      " |      `host`\n",
      " |          If provided and the URL map has host matching enabled this can be\n",
      " |          used to provide a match rule for the whole host.  This also means\n",
      " |          that the subdomain feature is disabled.\n",
      " |      \n",
      " |      `websocket`\n",
      " |          If ``True``, this rule is only matches for WebSocket (``ws://``,\n",
      " |          ``wss://``) requests. By default, rules will only match for HTTP\n",
      " |          requests.\n",
      " |      \n",
      " |      .. versionadded:: 1.0\n",
      " |          Added ``websocket``.\n",
      " |      \n",
      " |      .. versionadded:: 1.0\n",
      " |          Added ``merge_slashes``.\n",
      " |      \n",
      " |      .. versionadded:: 0.7\n",
      " |          Added ``alias`` and ``host``.\n",
      " |      \n",
      " |      .. versionchanged:: 0.6.1\n",
      " |         ``HEAD`` is added to ``methods`` if ``GET`` is present.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from flask.helpers._PackageBoundObject:\n",
      " |  \n",
      " |  get_send_file_max_age(self, filename)\n",
      " |      Provides default cache_timeout for the :func:`send_file` functions.\n",
      " |      \n",
      " |      By default, this function returns ``SEND_FILE_MAX_AGE_DEFAULT`` from\n",
      " |      the configuration of :data:`~flask.current_app`.\n",
      " |      \n",
      " |      Static file functions such as :func:`send_from_directory` use this\n",
      " |      function, and :func:`send_file` calls this function on\n",
      " |      :data:`~flask.current_app` when the given cache_timeout is ``None``. If a\n",
      " |      cache_timeout is given in :func:`send_file`, that timeout is used;\n",
      " |      otherwise, this method is called.\n",
      " |      \n",
      " |      This allows subclasses to change the behavior when sending files based\n",
      " |      on the filename.  For example, to set the cache timeout for .js files\n",
      " |      to 60 seconds::\n",
      " |      \n",
      " |          class MyFlask(flask.Flask):\n",
      " |              def get_send_file_max_age(self, name):\n",
      " |                  if name.lower().endswith('.js'):\n",
      " |                      return 60\n",
      " |                  return flask.Flask.get_send_file_max_age(self, name)\n",
      " |      \n",
      " |      .. versionadded:: 0.9\n",
      " |  \n",
      " |  jinja_loader(...)\n",
      " |      The Jinja loader for this package bound object.\n",
      " |      \n",
      " |      .. versionadded:: 0.5\n",
      " |  \n",
      " |  open_resource(self, resource, mode='rb')\n",
      " |      Opens a resource from the application's resource folder.  To see\n",
      " |      how this works, consider the following folder structure::\n",
      " |      \n",
      " |          /myapplication.py\n",
      " |          /schema.sql\n",
      " |          /static\n",
      " |              /style.css\n",
      " |          /templates\n",
      " |              /layout.html\n",
      " |              /index.html\n",
      " |      \n",
      " |      If you want to open the :file:`schema.sql` file you would do the\n",
      " |      following::\n",
      " |      \n",
      " |          with app.open_resource('schema.sql') as f:\n",
      " |              contents = f.read()\n",
      " |              do_something_with(contents)\n",
      " |      \n",
      " |      :param resource: the name of the resource.  To access resources within\n",
      " |                       subfolders use forward slashes as separator.\n",
      " |      :param mode: Open file in this mode. Only reading is supported,\n",
      " |          valid values are \"r\" (or \"rt\") and \"rb\".\n",
      " |  \n",
      " |  send_static_file(self, filename)\n",
      " |      Function used internally to send static files from the static\n",
      " |      folder to the browser.\n",
      " |      \n",
      " |      .. versionadded:: 0.5\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from flask.helpers._PackageBoundObject:\n",
      " |  \n",
      " |  has_static_folder\n",
      " |      This is ``True`` if the package bound object's container has a\n",
      " |      folder for static files.\n",
      " |      \n",
      " |      .. versionadded:: 0.5\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from flask.helpers._PackageBoundObject:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  static_folder\n",
      " |      The absolute path to the configured static folder.\n",
      " |  \n",
      " |  static_url_path\n",
      " |      The URL prefix that the static route will be accessible from.\n",
      " |      \n",
      " |      If it was not configured during init, it is derived from\n",
      " |      :attr:`static_folder`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Flask?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "provincial-stand",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function jsonify in module flask.json:\n",
      "\n",
      "jsonify(*args, **kwargs)\n",
      "    This function wraps :func:`dumps` to add a few enhancements that make\n",
      "    life easier.  It turns the JSON output into a :class:`~flask.Response`\n",
      "    object with the :mimetype:`application/json` mimetype.  For convenience, it\n",
      "    also converts multiple arguments into an array or multiple keyword arguments\n",
      "    into a dict.  This means that both ``jsonify(1,2,3)`` and\n",
      "    ``jsonify([1,2,3])`` serialize to ``[1,2,3]``.\n",
      "    \n",
      "    For clarity, the JSON serialization behavior has the following differences\n",
      "    from :func:`dumps`:\n",
      "    \n",
      "    1. Single argument: Passed straight through to :func:`dumps`.\n",
      "    2. Multiple arguments: Converted to an array before being passed to\n",
      "       :func:`dumps`.\n",
      "    3. Multiple keyword arguments: Converted to a dict before being passed to\n",
      "       :func:`dumps`.\n",
      "    4. Both args and kwargs: Behavior undefined and will throw an exception.\n",
      "    \n",
      "    Example usage::\n",
      "    \n",
      "        from flask import jsonify\n",
      "    \n",
      "        @app.route('/_get_current_user')\n",
      "        def get_current_user():\n",
      "            return jsonify(username=g.user.username,\n",
      "                           email=g.user.email,\n",
      "                           id=g.user.id)\n",
      "    \n",
      "    This will send a JSON response like this to the browser::\n",
      "    \n",
      "        {\n",
      "            \"username\": \"admin\",\n",
      "            \"email\": \"admin@localhost\",\n",
      "            \"id\": 42\n",
      "        }\n",
      "    \n",
      "    \n",
      "    .. versionchanged:: 0.11\n",
      "       Added support for serializing top-level arrays. This introduces a\n",
      "       security risk in ancient browsers. See :ref:`json-security` for details.\n",
      "    \n",
      "    This function's response will be pretty printed if the\n",
      "    ``JSONIFY_PRETTYPRINT_REGULAR`` config parameter is set to True or the\n",
      "    Flask app is running in debug mode. Compressed (not pretty) formatting\n",
      "    currently means no indents and no spaces after separators.\n",
      "    \n",
      "    .. versionadded:: 0.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "careful-turkey",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LocalProxy in module werkzeug.local object:\n",
      "\n",
      "class LocalProxy(builtins.object)\n",
      " |  Acts as a proxy for a werkzeug local.  Forwards all operations to\n",
      " |  a proxied object.  The only operations not supported for forwarding\n",
      " |  are right handed operands and any kind of assignment.\n",
      " |  \n",
      " |  Example usage::\n",
      " |  \n",
      " |      from werkzeug.local import Local\n",
      " |      l = Local()\n",
      " |  \n",
      " |      # these are proxies\n",
      " |      request = l('request')\n",
      " |      user = l('user')\n",
      " |  \n",
      " |  \n",
      " |      from werkzeug.local import LocalStack\n",
      " |      _response_local = LocalStack()\n",
      " |  \n",
      " |      # this is a proxy\n",
      " |      response = _response_local()\n",
      " |  \n",
      " |  Whenever something is bound to l.user / l.request the proxy objects\n",
      " |  will forward all operations.  If no object is bound a :exc:`RuntimeError`\n",
      " |  will be raised.\n",
      " |  \n",
      " |  To create proxies to :class:`Local` or :class:`LocalStack` objects,\n",
      " |  call the object as shown above.  If you want to have a proxy to an\n",
      " |  object looked up by a function, you can (as of Werkzeug 0.6.1) pass\n",
      " |  a function to the :class:`LocalProxy` constructor::\n",
      " |  \n",
      " |      session = LocalProxy(lambda: get_current_request().session)\n",
      " |  \n",
      " |  .. versionchanged:: 0.6.1\n",
      " |     The class can be instantiated with a callable as well now.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __abs__ lambda x\n",
      " |  \n",
      " |  __add__ lambda x, o\n",
      " |  \n",
      " |  __and__ lambda x, o\n",
      " |  \n",
      " |  __bool__(self)\n",
      " |  \n",
      " |  __call__ lambda x, *a, **kw\n",
      " |  \n",
      " |  __cmp__ lambda x, o\n",
      " |  \n",
      " |  __coerce__ lambda x, o\n",
      " |  \n",
      " |  __complex__ lambda x\n",
      " |  \n",
      " |  __contains__ lambda x, i\n",
      " |  \n",
      " |  __copy__ lambda x\n",
      " |  \n",
      " |  __deepcopy__ lambda x, memo\n",
      " |  \n",
      " |  __delattr__ lambda x, n\n",
      " |  \n",
      " |  __delitem__(self, key)\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __div__ lambda x, o\n",
      " |  \n",
      " |  __divmod__ lambda x, o\n",
      " |  \n",
      " |  __enter__ lambda x\n",
      " |  \n",
      " |  __eq__ lambda x, o\n",
      " |  \n",
      " |  __exit__ lambda x, *a, **kw\n",
      " |  \n",
      " |  __float__ lambda x\n",
      " |  \n",
      " |  __floordiv__ lambda x, o\n",
      " |  \n",
      " |  __ge__ lambda x, o\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __getitem__ lambda x, i\n",
      " |  \n",
      " |  __gt__ lambda x, o\n",
      " |  \n",
      " |  __hash__ lambda x\n",
      " |  \n",
      " |  __hex__ lambda x\n",
      " |  \n",
      " |  __index__ lambda x\n",
      " |  \n",
      " |  __init__(self, local, name=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  __int__ lambda x\n",
      " |  \n",
      " |  __invert__ lambda x\n",
      " |  \n",
      " |  __iter__ lambda x\n",
      " |  \n",
      " |  __le__ lambda x, o\n",
      " |  \n",
      " |  __len__ lambda x\n",
      " |  \n",
      " |  __long__ lambda x\n",
      " |  \n",
      " |  __lshift__ lambda x, o\n",
      " |  \n",
      " |  __lt__ lambda x, o\n",
      " |  \n",
      " |  __mod__ lambda x, o\n",
      " |  \n",
      " |  __mul__ lambda x, o\n",
      " |  \n",
      " |  __ne__ lambda x, o\n",
      " |  \n",
      " |  __neg__ lambda x\n",
      " |  \n",
      " |  __oct__ lambda x\n",
      " |  \n",
      " |  __or__ lambda x, o\n",
      " |  \n",
      " |  __pos__ lambda x\n",
      " |  \n",
      " |  __pow__ lambda x, o\n",
      " |  \n",
      " |  __radd__ lambda x, o\n",
      " |  \n",
      " |  __rdiv__ lambda x, o\n",
      " |  \n",
      " |  __rdivmod__ lambda x, o\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __rfloordiv__ lambda x, o\n",
      " |  \n",
      " |  __rmod__ lambda x, o\n",
      " |  \n",
      " |  __rmul__ lambda x, o\n",
      " |  \n",
      " |  __rshift__ lambda x, o\n",
      " |  \n",
      " |  __rsub__ lambda x, o\n",
      " |  \n",
      " |  __rtruediv__ lambda x, o\n",
      " |  \n",
      " |  __setattr__ lambda x, n, v\n",
      " |  \n",
      " |  __setitem__(self, key, value)\n",
      " |  \n",
      " |  __str__ lambda x\n",
      " |  \n",
      " |  __sub__ lambda x, o\n",
      " |  \n",
      " |  __truediv__ lambda x, o\n",
      " |  \n",
      " |  __unicode__(self)\n",
      " |  \n",
      " |  __xor__ lambda x, o\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __wrapped__\n",
      "\n"
     ]
    }
   ],
   "source": [
    "request?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f39da6",
   "metadata": {},
   "source": [
    "### NLP - Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53f81585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b6057c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a54dd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3af799fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "brown?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ee8a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f6d0f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0051ee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626afc59",
   "metadata": {},
   "source": [
    "Importing _\\__future____ can be used to use features which will appear in newer versions while having an older release of Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66661dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ConditionalFreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "967fb9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConditionalFreqDist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2160bbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b96272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FreqDist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ccf57ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30808896",
   "metadata": {},
   "outputs": [],
   "source": [
    "DecisionTreeClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ec330b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b26ff358",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aa71722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b33f971b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb6d63b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e58a64ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29ef1f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94bdf26",
   "metadata": {},
   "source": [
    "WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. Synsets are interlinked by means of conceptual-semantic and lexical relations. The resulting network of meaningfully related words and concepts can be navigated with the browser(link is external). WordNet is also freely and publicly available for download. WordNet's structure makes it a useful tool for computational linguistics and natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb706449",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... took 8.57317590713501 secs.\n"
     ]
    }
   ],
   "source": [
    "from pywsd.lesk import simple_lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25ff7e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_lesk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5bb0d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywsd import disambiguate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9649511",
   "metadata": {},
   "outputs": [],
   "source": [
    "disambiguate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0054dc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywsd.similarity import max_similarity as maxsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c72f80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_similarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab72799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywsd.baseline import random_sense, first_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50ffaca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd31ef25",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed9ce89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywsd.baseline import max_lemma_count as most_frequent_sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f19217ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_frequent_sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11d7614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywsd.lesk import adapted_lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9e8f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapted_lesk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aef287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywsd.lesk import cosine_lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22bd87cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_lesk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c610ea11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
